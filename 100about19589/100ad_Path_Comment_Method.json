[{
	"Path": "edu.stanford.nlp.util.TwoDimensionalMapTest.testMapFactory",
	"Comment": "tests that a different map factory is used when asked for.anidentity map will store two of the same key if the objectsthemselves are different.we can test for that.",
	"Method": "void testMapFactory(){\r\n    TwoDimensionalMap<String, String, String> map = new TwoDimensionalMap(MapFactory.<String, Map<String, String>>identityHashMapFactory(), MapFactory.<String, String>identityHashMapFactory());\r\n    map.put(new String(\"A\"), \"B\", \"C\");\r\n    map.put(new String(\"A\"), \"B\", \"C\");\r\n    assertEquals(2, map.size());\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.TwoDimensionalSet.addAllKeys",
	"Comment": "adds all the keys in the given twodimensionalmap.returns true iff at least one key is added.",
	"Method": "boolean addAllKeys(TwoDimensionalMap<? extends K1, ? extends K2, ?> map){\r\n    boolean result = false;\r\n    for (TwoDimensionalMap.Entry<? extends K1, ? extends K2, ?> entry : map) {\r\n        if (add(entry.getFirstKey(), entry.getSecondKey())) {\r\n            result = true;\r\n        }\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.deeplearning4j.integration.testcases.MLPTestCases.getMLPMoon",
	"Comment": "a test case that mirrors mlp moon example using csvrecordreader",
	"Method": "TestCase getMLPMoon(){\r\n    return new TestCase() {\r\n        {\r\n            testName = \"MLPMoon\";\r\n            testType = TestType.RANDOM_INIT;\r\n            testPredictions = true;\r\n            testTrainingCurves = true;\r\n            testGradients = true;\r\n            testParamsPostTraining = true;\r\n            testEvaluation = true;\r\n            testOverfitting = false;\r\n        }\r\n        @Override\r\n        public Object getConfiguration() {\r\n            int seed = 123;\r\n            double learningRate = 0.005;\r\n            int numInputs = 2;\r\n            int numOutputs = 2;\r\n            int numHiddenNodes = 20;\r\n            MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder().seed(seed).updater(new Nesterovs(learningRate, 0.9)).list().layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes).weightInit(WeightInit.XAVIER).activation(Activation.RELU).build()).layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).weightInit(WeightInit.XAVIER).activation(Activation.SOFTMAX).nIn(numHiddenNodes).nOut(numOutputs).build()).build();\r\n            return conf;\r\n        }\r\n        @Override\r\n        public List<Pair<INDArray[], INDArray[]>> getPredictionsTestData() throws Exception {\r\n            File f = new ClassPathResource(\"dl4j-integration-tests/data/moon_data_eval.csv\").getFile();\r\n            RecordReader rr = new CSVRecordReader();\r\n            rr.initialize(new FileSplit(f));\r\n            DataSetIterator testIter = new RecordReaderDataSetIterator(rr, 1, 0, 2);\r\n            INDArray next1 = testIter.next().getFeatures();\r\n            testIter = new RecordReaderDataSetIterator(rr, 10, 0, 2);\r\n            INDArray next10 = testIter.next().getFeatures();\r\n            return Arrays.asList(new Pair(new INDArray[] { next1 }, null), new Pair(new INDArray[] { next10 }, null));\r\n        }\r\n        @Override\r\n        public MultiDataSet getGradientsTestData() throws Exception {\r\n            File f = new ClassPathResource(\"dl4j-integration-tests/data/moon_data_eval.csv\").getFile();\r\n            RecordReader rr = new CSVRecordReader();\r\n            rr.initialize(new FileSplit(f));\r\n            DataSet ds = new RecordReaderDataSetIterator(rr, 5, 0, 2).next();\r\n            return ComputationGraphUtil.toMultiDataSet(ds);\r\n        }\r\n        @Override\r\n        public MultiDataSetIterator getTrainingData() throws Exception {\r\n            File f = new ClassPathResource(\"dl4j-integration-tests/data/moon_data_train.csv\").getFile();\r\n            RecordReader rr = new CSVRecordReader();\r\n            rr.initialize(new FileSplit(f));\r\n            DataSetIterator trainIter = new RecordReaderDataSetIterator(rr, 32, 0, 2);\r\n            return new MultiDataSetIteratorAdapter(trainIter);\r\n        }\r\n        @Override\r\n        public IEvaluation[] getNewEvaluations() {\r\n            return new IEvaluation[] { new Evaluation(), new ROCMultiClass(), new EvaluationCalibration() };\r\n        }\r\n        @Override\r\n        public MultiDataSetIterator getEvaluationTestData() throws Exception {\r\n            File f = new ClassPathResource(\"dl4j-integration-tests/data/moon_data_eval.csv\").getFile();\r\n            RecordReader rr = new CSVRecordReader();\r\n            rr.initialize(new FileSplit(f));\r\n            DataSetIterator testIter = new RecordReaderDataSetIterator(rr, 32, 0, 2);\r\n            return new MultiDataSetIteratorAdapter(testIter);\r\n        }\r\n        @Override\r\n        public int getOverfitNumIterations() {\r\n            return 200;\r\n        }\r\n    };\r\n}"
}, {
	"Path": "org.deeplearning4j.integration.testcases.MLPTestCases.getMLPMoon",
	"Comment": "a test case that mirrors mlp moon example using csvrecordreader",
	"Method": "TestCase getMLPMoon(){\r\n    int seed = 123;\r\n    double learningRate = 0.005;\r\n    int numInputs = 2;\r\n    int numOutputs = 2;\r\n    int numHiddenNodes = 20;\r\n    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder().seed(seed).updater(new Nesterovs(learningRate, 0.9)).list().layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes).weightInit(WeightInit.XAVIER).activation(Activation.RELU).build()).layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).weightInit(WeightInit.XAVIER).activation(Activation.SOFTMAX).nIn(numHiddenNodes).nOut(numOutputs).build()).build();\r\n    return conf;\r\n}"
}, {
	"Path": "org.deeplearning4j.integration.testcases.MLPTestCases.getMLPMoon",
	"Comment": "a test case that mirrors mlp moon example using csvrecordreader",
	"Method": "TestCase getMLPMoon(){\r\n    File f = new ClassPathResource(\"dl4j-integration-tests/data/moon_data_eval.csv\").getFile();\r\n    RecordReader rr = new CSVRecordReader();\r\n    rr.initialize(new FileSplit(f));\r\n    DataSetIterator testIter = new RecordReaderDataSetIterator(rr, 1, 0, 2);\r\n    INDArray next1 = testIter.next().getFeatures();\r\n    testIter = new RecordReaderDataSetIterator(rr, 10, 0, 2);\r\n    INDArray next10 = testIter.next().getFeatures();\r\n    return Arrays.asList(new Pair(new INDArray[] { next1 }, null), new Pair(new INDArray[] { next10 }, null));\r\n}"
}, {
	"Path": "org.deeplearning4j.integration.testcases.MLPTestCases.getMLPMoon",
	"Comment": "a test case that mirrors mlp moon example using csvrecordreader",
	"Method": "TestCase getMLPMoon(){\r\n    File f = new ClassPathResource(\"dl4j-integration-tests/data/moon_data_eval.csv\").getFile();\r\n    RecordReader rr = new CSVRecordReader();\r\n    rr.initialize(new FileSplit(f));\r\n    DataSet ds = new RecordReaderDataSetIterator(rr, 5, 0, 2).next();\r\n    return ComputationGraphUtil.toMultiDataSet(ds);\r\n}"
}, {
	"Path": "org.deeplearning4j.integration.testcases.MLPTestCases.getMLPMoon",
	"Comment": "a test case that mirrors mlp moon example using csvrecordreader",
	"Method": "TestCase getMLPMoon(){\r\n    File f = new ClassPathResource(\"dl4j-integration-tests/data/moon_data_train.csv\").getFile();\r\n    RecordReader rr = new CSVRecordReader();\r\n    rr.initialize(new FileSplit(f));\r\n    DataSetIterator trainIter = new RecordReaderDataSetIterator(rr, 32, 0, 2);\r\n    return new MultiDataSetIteratorAdapter(trainIter);\r\n}"
}, {
	"Path": "org.deeplearning4j.integration.testcases.MLPTestCases.getMLPMoon",
	"Comment": "a test case that mirrors mlp moon example using csvrecordreader",
	"Method": "TestCase getMLPMoon(){\r\n    return new IEvaluation[] { new Evaluation(), new ROCMultiClass(), new EvaluationCalibration() };\r\n}"
}, {
	"Path": "org.deeplearning4j.integration.testcases.MLPTestCases.getMLPMoon",
	"Comment": "a test case that mirrors mlp moon example using csvrecordreader",
	"Method": "TestCase getMLPMoon(){\r\n    File f = new ClassPathResource(\"dl4j-integration-tests/data/moon_data_eval.csv\").getFile();\r\n    RecordReader rr = new CSVRecordReader();\r\n    rr.initialize(new FileSplit(f));\r\n    DataSetIterator testIter = new RecordReaderDataSetIterator(rr, 32, 0, 2);\r\n    return new MultiDataSetIteratorAdapter(testIter);\r\n}"
}, {
	"Path": "org.deeplearning4j.integration.testcases.MLPTestCases.getMLPMoon",
	"Comment": "a test case that mirrors mlp moon example using csvrecordreader",
	"Method": "TestCase getMLPMoon(){\r\n    return 200;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractTreebankLanguagePack.getEncoding",
	"Comment": "return the input charset encoding for the treebank.see documentation for the charset class.",
	"Method": "String getEncoding(){\r\n    return DEFAULT_ENCODING;\r\n}"
}, {
	"Path": "org.deeplearning4j.common.resources.DL4JResources.getDirectory",
	"Comment": "get the storage location for the specified resource type and resource name",
	"Method": "File getDirectory(ResourceType resourceType,String resourceName){\r\n    File f = new File(baseDirectory, resourceType.resourceName());\r\n    f = new File(f, resourceName);\r\n    f.mkdirs();\r\n    return f;\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.string.StringListToCategoricalSetTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return newColumnNames.toArray(new String[newColumnNames.size()]);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.sequencevectors.transformers.impl.GraphTransformer.initialize",
	"Comment": "this method handles required initialization for graphtransformer",
	"Method": "void initialize(){\r\n    log.info(\"Building Huffman tree for source graph...\");\r\n    int nVertices = sourceGraph.numVertices();\r\n    log.info(\"Transferring Huffman tree info to nodes...\");\r\n    for (int i = 0; i < nVertices; i++) {\r\n        T element = sourceGraph.getVertex(i).getValue();\r\n        element.setElementFrequency(sourceGraph.getConnectedVertices(i).size());\r\n        if (vocabCache != null)\r\n            vocabCache.addToken(element);\r\n    }\r\n    if (vocabCache != null) {\r\n        Huffman huffman = new Huffman(vocabCache.vocabWords());\r\n        huffman.build();\r\n        huffman.applyIndexes(vocabCache);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.training.CenterLossOutputLayer.computeScoreForExamples",
	"Comment": "compute the score for each example individually, after labels and input have been set.",
	"Method": "INDArray computeScoreForExamples(double fullNetworkL1,double fullNetworkL2,LayerWorkspaceMgr workspaceMgr){\r\n    if (input == null || labels == null)\r\n        throw new IllegalStateException(\"Cannot calculate score without input and labels \" + layerId());\r\n    INDArray preOut = preOutput2d(false, workspaceMgr);\r\n    INDArray centers = params.get(CenterLossParamInitializer.CENTER_KEY);\r\n    INDArray centersForExamples = labels.mmul(centers);\r\n    INDArray intraClassScoreArray = input.sub(centersForExamples);\r\n    ILossFunction interClassLoss = layerConf().getLossFn();\r\n    INDArray scoreArray = interClassLoss.computeScoreArray(getLabels2d(workspaceMgr, ArrayType.FF_WORKING_MEM), preOut, layerConf().getActivationFn(), maskArray);\r\n    scoreArray.addi(intraClassScoreArray.muli(layerConf().getLambda() / 2));\r\n    double l1l2 = fullNetworkL1 + fullNetworkL2;\r\n    if (l1l2 != 0.0) {\r\n        scoreArray.addi(l1l2);\r\n    }\r\n    return workspaceMgr.leverageTo(ArrayType.ACTIVATIONS, scoreArray);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Trees.getTerminal",
	"Comment": "gets the nth terminal in tree.the first terminal is number zero.",
	"Method": "Tree getTerminal(Tree tree,int n,Tree getTerminal,Tree tree,MutableInteger i,int n){\r\n    if (i.intValue() == n) {\r\n        if (tree.isLeaf()) {\r\n            return tree;\r\n        } else {\r\n            return getTerminal(tree.children()[0], i, n);\r\n        }\r\n    } else {\r\n        if (tree.isLeaf()) {\r\n            i.set(i.intValue() + tree.yield().size());\r\n            return null;\r\n        } else {\r\n            for (Tree kid : tree.children()) {\r\n                Tree result = getTerminal(kid, i, n);\r\n                if (result != null) {\r\n                    return result;\r\n                }\r\n            }\r\n            return null;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.getVertex",
	"Comment": "return a given graphvertex by name, or null if no vertex with that name exists",
	"Method": "GraphVertex getVertex(String name){\r\n    return verticesMap.get(name);\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.BaseColumnsMathOpTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return new String[] { newColumnName };\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Trees.lexicalize",
	"Comment": "returns a lexicalized tree whose labels are categorywordtaginstances, all corresponds to the input tree.",
	"Method": "Tree lexicalize(Tree t,HeadFinder hf){\r\n    Function<Tree, Tree> a = TreeFunctions.getLabeledTreeToCategoryWordTagTreeFunction();\r\n    Tree t1 = a.apply(t);\r\n    t1.percolateHeads(hf);\r\n    return t1;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.StringUtils.getShortClassName",
	"Comment": "returns a short class name for an object.this is the class name stripped of any package name.",
	"Method": "String getShortClassName(Object o){\r\n    if (o == null) {\r\n        return \"null\";\r\n    }\r\n    String name = o.getClass().getName();\r\n    int index = name.lastIndexOf('.');\r\n    if (index >= 0) {\r\n        name = name.substring(index + 1);\r\n    }\r\n    return name;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.makeDependencyLabel",
	"Comment": "convert a constituency label to a dependency label. options are provided for selecting annotationsto copy.",
	"Method": "Label makeDependencyLabel(Label oldLabel,boolean copyLabel,boolean copyIndex,boolean copyPosTag){\r\n    if (!copyLabel)\r\n        return oldLabel;\r\n    String wordForm = (oldLabel instanceof HasWord) ? ((HasWord) oldLabel).word() : oldLabel.value();\r\n    Label newLabel = oldLabel.labelFactory().newLabel(wordForm);\r\n    if (newLabel instanceof HasWord)\r\n        ((HasWord) newLabel).setWord(wordForm);\r\n    if (copyPosTag && newLabel instanceof HasTag && oldLabel instanceof HasTag) {\r\n        String tag = ((HasTag) oldLabel).tag();\r\n        ((HasTag) newLabel).setTag(tag);\r\n    }\r\n    if (copyIndex && newLabel instanceof HasIndex && oldLabel instanceof HasIndex) {\r\n        int index = ((HasIndex) oldLabel).index();\r\n        ((HasIndex) newLabel).setIndex(index);\r\n    }\r\n    return newLabel;\r\n}"
}, {
	"Path": "org.deeplearning4j.text.tokenization.tokenizerfactory.UimaTokenizerFactory.getTokenPreProcessor",
	"Comment": "returns tokenpreprocessor set for this tokenizerfactory instance",
	"Method": "TokenPreProcess getTokenPreProcessor(){\r\n    return preProcess;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.lsh.RandomProjectionLSH.makeIndex",
	"Comment": "populates the index. beware, not incremental, any further call replaces the index instead of adding to it.",
	"Method": "void makeIndex(INDArray data){\r\n    index = hash(data);\r\n    indexData = data;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.totalIntCount",
	"Comment": "returns the current total count for all objects in this counter.all counts are summed each time, so cache it if you need it repeatedly.",
	"Method": "int totalIntCount(int totalIntCount,Predicate<E> filter){\r\n    int total = 0;\r\n    for (E key : map.keySet()) {\r\n        if (filter.test(key)) {\r\n            total += getIntCount(key);\r\n        }\r\n    }\r\n    return (total);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.percolateHeadAnnotations",
	"Comment": "finds the head words of each tree and assignsheadwordlabelannotation on each node pointing to the correctcorelabel.this relies on the nodes being corelabels, so itthrows an illegalargumentexception if this is ever not true.",
	"Method": "void percolateHeadAnnotations(HeadFinder hf){\r\n    if (!(label() instanceof CoreLabel)) {\r\n        throw new IllegalArgumentException(\"Expected CoreLabels in the trees\");\r\n    }\r\n    CoreLabel nodeLabel = (CoreLabel) label();\r\n    if (isLeaf()) {\r\n        return;\r\n    }\r\n    if (isPreTerminal()) {\r\n        nodeLabel.set(TreeCoreAnnotations.HeadWordLabelAnnotation.class, (CoreLabel) children()[0].label());\r\n        nodeLabel.set(TreeCoreAnnotations.HeadTagLabelAnnotation.class, nodeLabel);\r\n        return;\r\n    }\r\n    for (Tree kid : children()) {\r\n        kid.percolateHeadAnnotations(hf);\r\n    }\r\n    final Tree head = hf.determineHead(this);\r\n    if (head == null) {\r\n        throw new NullPointerException(\"HeadFinder \" + hf + \" returned null for \" + this);\r\n    } else if (head.isLeaf()) {\r\n        nodeLabel.set(TreeCoreAnnotations.HeadWordLabelAnnotation.class, (CoreLabel) head.label());\r\n        nodeLabel.set(TreeCoreAnnotations.HeadTagLabelAnnotation.class, (CoreLabel) head.parent(this).label());\r\n    } else if (head.isPreTerminal()) {\r\n        nodeLabel.set(TreeCoreAnnotations.HeadWordLabelAnnotation.class, (CoreLabel) head.children()[0].label());\r\n        nodeLabel.set(TreeCoreAnnotations.HeadTagLabelAnnotation.class, (CoreLabel) head.label());\r\n    } else {\r\n        if (!(head.label() instanceof CoreLabel)) {\r\n            throw new AssertionError(\"Horrible bug\");\r\n        }\r\n        CoreLabel headLabel = (CoreLabel) head.label();\r\n        nodeLabel.set(TreeCoreAnnotations.HeadWordLabelAnnotation.class, headLabel.get(TreeCoreAnnotations.HeadWordLabelAnnotation.class));\r\n        nodeLabel.set(TreeCoreAnnotations.HeadTagLabelAnnotation.class, headLabel.get(TreeCoreAnnotations.HeadTagLabelAnnotation.class));\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ConfusionMatrix.getContingency",
	"Comment": "returns the contingency table for the given class label, where all otherclass labels are treated as negative.",
	"Method": "Contingency getContingency(U positiveLabel){\r\n    int tp = 0;\r\n    int fp = 0;\r\n    int tn = 0;\r\n    int fn = 0;\r\n    for (Pair<U, U> pair : confTable.keySet()) {\r\n        int count = confTable.get(pair);\r\n        U guess = pair.first();\r\n        U gold = pair.second();\r\n        boolean guessP = guess.equals(positiveLabel);\r\n        boolean goldP = gold.equals(positiveLabel);\r\n        if (guessP && goldP) {\r\n            tp += count;\r\n        } else if (!guessP && goldP) {\r\n            fn += count;\r\n        } else if (guessP && !goldP) {\r\n            fp += count;\r\n        } else {\r\n            tn += count;\r\n        }\r\n    }\r\n    return new Contingency(tp, fp, tn, fn);\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.AbstractDataSetIterator.next",
	"Comment": "like the standard next method but allows acustomizable number of examples returned",
	"Method": "DataSet next(int num,DataSet next){\r\n    if (queue.isEmpty())\r\n        throw new NoSuchElementException();\r\n    DataSet dataSet = queue.poll();\r\n    if (preProcessor != null)\r\n        preProcessor.preProcess(dataSet);\r\n    return dataSet;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.learning.impl.sequence.DBOW.inferSequence",
	"Comment": "this method does training on previously unseen paragraph, and returns inferred vector",
	"Method": "INDArray inferSequence(Sequence<T> sequence,long nextRandom,double learningRate,double minLearningRate,int iterations){\r\n    AtomicLong nr = new AtomicLong(nextRandom);\r\n    if (sequence.isEmpty())\r\n        return null;\r\n    Random random = Nd4j.getRandomFactory().getNewRandomInstance(configuration.getSeed() * sequence.hashCode(), lookupTable.layerSize() + 1);\r\n    INDArray ret = Nd4j.rand(new int[] { 1, lookupTable.layerSize() }, random).subi(0.5).divi(lookupTable.layerSize());\r\n    for (int iter = 0; iter < iterations; iter++) {\r\n        nr.set(Math.abs(nr.get() * 25214903917L + 11));\r\n        dbow(0, sequence, (int) nr.get() % window, nr, learningRate, true, ret);\r\n        learningRate = ((learningRate - minLearningRate) / (iterations - iter)) + minLearningRate;\r\n    }\r\n    finish();\r\n    return ret;\r\n}"
}, {
	"Path": "org.deeplearning4j.gym.Client.reset",
	"Comment": "reset the state of the environment and return an initial observation.",
	"Method": "O reset(){\r\n    JsonNode resetRep = ClientUtils.post(url + ENVS_ROOT + instanceId + RESET, new JSONObject());\r\n    return observationSpace.getValue(resetRep.getObject(), \"observation\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.TregexMatcher.getVariableString",
	"Comment": "if there is a current match, and that match involves setting thisparticular variable string, this returns that string.otherwise,it returns null.",
	"Method": "String getVariableString(String var){\r\n    return variableStrings.getString(var);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.setParams",
	"Comment": "set the parameters for this model.this expects a linear ndarraywhich then be unpacked internallyrelative to the expected ordering of the model",
	"Method": "void setParams(INDArray params){\r\n    if (flattenedParams == params) {\r\n        return;\r\n    }\r\n    if (flattenedParams != null && params.length() == flattenedParams.length()) {\r\n        if (params != flattenedParams) {\r\n            flattenedParams.assign(params);\r\n        }\r\n    } else {\r\n        if (flattenedParams == null)\r\n            flattenedParams = params.dup();\r\n        int idx = 0;\r\n        for (int i = 0; i < getLayers().length; i++) {\r\n            Layer layer = getLayer(i);\r\n            long range = layer.numParams();\r\n            if (range <= 0)\r\n                continue;\r\n            INDArray get = params.get(NDArrayIndex.point(0), NDArrayIndex.interval(idx, range + idx));\r\n            layer.setParams(get);\r\n            idx += range;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.imports.tensorflow.TensorFlowImportValidator.checkAllModelsForImport",
	"Comment": "recursively scan the specified directory for .pb files, and evaluate",
	"Method": "TFImportStatus checkAllModelsForImport(File directory){\r\n    Preconditions.checkState(directory.isDirectory(), \"Specified directory %s is not actually a directory\", directory);\r\n    Collection<File> files = FileUtils.listFiles(directory, new String[] { \"pb\" }, true);\r\n    Preconditions.checkState(!files.isEmpty(), \"No .pb files found in directory %s\", directory);\r\n    TFImportStatus status = null;\r\n    for (File f : files) {\r\n        if (status == null) {\r\n            status = checkModelForImport(f);\r\n        } else {\r\n            status = status.merge(checkModelForImport(f));\r\n        }\r\n    }\r\n    return status;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.TregexGUI.matchesChanged",
	"Comment": "called by matchespanel to alert the frame when the matching trees have changed",
	"Method": "void matchesChanged(){\r\n    setSaveEnabled(!MatchesPanel.getInstance().isEmpty());\r\n}"
}, {
	"Path": "org.datavec.api.transform.metadata.FloatMetaData.isValid",
	"Comment": "is the given object valid for this column,given the column type and anyrestrictions given by thecolumnmetadata object?",
	"Method": "boolean isValid(Writable writable,boolean isValid,Object input){\r\n    Float d;\r\n    try {\r\n        d = (Float) input;\r\n    } catch (Exception e) {\r\n        return false;\r\n    }\r\n    if (allowNaN && Float.isNaN(d))\r\n        return true;\r\n    if (allowInfinite && Float.isInfinite(d))\r\n        return true;\r\n    if (minAllowedValue != null && d < minAllowedValue)\r\n        return false;\r\n    if (maxAllowedValue != null && d > maxAllowedValue)\r\n        return false;\r\n    return true;\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.scoring.ScoreFunctions.testSetAccuracy",
	"Comment": "calculate the accuracy on a test set, for a multilayernetwork",
	"Method": "ScoreFunction testSetAccuracy(){\r\n    return new TestSetAccuracyScoreFunction();\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.isAttached",
	"Comment": "this method returns true, if this indarray instance is attached to some workspace. false otherwise.",
	"Method": "boolean isAttached(){\r\n    if (isEmpty())\r\n        return false;\r\n    if (data == null && !isEmpty())\r\n        throw new IllegalStateException();\r\n    return data.isAttached() || (data.underlyingDataBuffer() != null && data.underlyingDataBuffer().isAttached()) || (data.originalDataBuffer() != null && data.originalDataBuffer().isAttached());\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.putColumn",
	"Comment": "insert a column in to this arraywill throw an exception if thisndarray is not a matrix",
	"Method": "INDArray putColumn(int column,INDArray toPut){\r\n    Nd4j.getCompressor().autoDecompress(this);\r\n    if (isColumnVector() && toPut.isVector()) {\r\n        return assign(toPut);\r\n    }\r\n    return put(new INDArrayIndex[] { NDArrayIndex.all(), NDArrayIndex.point(column) }, toPut);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.StringUtils.padLeftOrTrim",
	"Comment": "pad or trim so as to produce a string of exactly a certain length.",
	"Method": "String padLeftOrTrim(String str,int num){\r\n    if (str == null) {\r\n        str = \"null\";\r\n    }\r\n    int leng = str.length();\r\n    if (leng < num) {\r\n        StringBuilder sb = new StringBuilder();\r\n        for (int i = 0; i < num - leng; i++) {\r\n            sb.append(' ');\r\n        }\r\n        sb.append(str);\r\n        return sb.toString();\r\n    } else if (leng > num) {\r\n        return str.substring(str.length() - num);\r\n    } else {\r\n        return str;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.util.SparkUtils.checkKryoConfiguration",
	"Comment": "check the spark configuration for incorrect kryo configuration, logging a warning message if necessary",
	"Method": "boolean checkKryoConfiguration(JavaSparkContext javaSparkContext,Logger log){\r\n    String serializer = javaSparkContext.getConf().get(\"spark.serializer\", null);\r\n    if (serializer != null && serializer.equals(\"org.apache.spark.serializer.KryoSerializer\")) {\r\n        String kryoRegistrator = javaSparkContext.getConf().get(\"spark.kryo.registrator\", null);\r\n        if (kryoRegistrator == null || !kryoRegistrator.equals(\"org.nd4j.Nd4jRegistrator\")) {\r\n            SerializerInstance si;\r\n            ByteBuffer bb;\r\n            try {\r\n                si = javaSparkContext.env().serializer().newInstance();\r\n                bb = si.serialize(Nd4j.linspace(1, 5, 5), null);\r\n            } catch (Exception e) {\r\n                throw new RuntimeException(KRYO_EXCEPTION_MSG, e);\r\n            }\r\n            if (bb == null) {\r\n                throw new RuntimeException(KRYO_EXCEPTION_MSG + \"\\n(Got: null ByteBuffer from Spark SerializerInstance)\");\r\n            } else {\r\n                boolean equals;\r\n                INDArray deserialized;\r\n                try {\r\n                    deserialized = (INDArray) si.deserialize(bb, null);\r\n                    equals = Nd4j.linspace(1, 5, 5).equals(deserialized);\r\n                } catch (Exception e) {\r\n                    throw new RuntimeException(KRYO_EXCEPTION_MSG, e);\r\n                }\r\n                if (!equals) {\r\n                    throw new RuntimeException(KRYO_EXCEPTION_MSG + \"\\n(Error during deserialization: test array\" + \" was not deserialized successfully)\");\r\n                }\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.CoordinationTransformer.CCtransform",
	"Comment": "transforms t if it contains a coordination in a flat structure",
	"Method": "Tree CCtransform(Tree t){\r\n    boolean notDone = true;\r\n    while (notDone) {\r\n        Tree cc = findCCparent(t, t);\r\n        if (cc != null) {\r\n            t = cc;\r\n        } else {\r\n            notDone = false;\r\n        }\r\n    }\r\n    return t;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.calcL2",
	"Comment": "calculate the l2 regularization term for all layers in the entire network. this is the sum of the l2 termsfor each layer individually",
	"Method": "double calcL2(){\r\n    double l2 = 0.0;\r\n    for (Layer l : layers) {\r\n        l2 += l.calcL2(true);\r\n    }\r\n    return l2;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.calcL1",
	"Comment": "calculate the l1 regularization term for all layers in the entire network. this is the sum of the l1 termsfor each layer individually",
	"Method": "double calcL1(){\r\n    double l1 = 0.0;\r\n    for (Layer l : layers) {\r\n        l1 += l.calcL1(true);\r\n    }\r\n    return l1;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.size",
	"Comment": "gets the size of the cache, in terms of elements on disk.note that this is an expensive operation, as it reads the entire cache in from disk.",
	"Method": "int size(){\r\n    return readCache();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.pooling.KerasGlobalPooling.getInputPreprocessor",
	"Comment": "gets appropriate dl4j inputpreprocessor for given inputtypes.",
	"Method": "InputPreProcessor getInputPreprocessor(InputType inputType){\r\n    if (inputType.length > 1)\r\n        throw new InvalidKerasConfigurationException(\"Keras GlobalPooling layer accepts only one input (received \" + inputType.length + \")\");\r\n    InputPreProcessor preprocessor;\r\n    if (inputType[0].getType() == InputType.Type.FF && this.dimensions.length == 1) {\r\n        preprocessor = new FeedForwardToRnnPreProcessor();\r\n    } else {\r\n        preprocessor = this.getGlobalPoolingLayer().getPreProcessorForInputType(inputType[0]);\r\n    }\r\n    return preprocessor;\r\n}"
}, {
	"Path": "edu.stanford.nlp.time.XMLUtils.getAttribute",
	"Comment": "like element.getattribute except returns null if attribute not present",
	"Method": "String getAttribute(Element element,String name,Node getAttribute,Node node,String name){\r\n    return node.getAttributes().getNamedItem(name);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.calculateGradients",
	"Comment": "calculate parameter gradients and input activation gradients given the input and labels",
	"Method": "Pair<Gradient, INDArray> calculateGradients(INDArray features,INDArray label,INDArray fMask,INDArray labelMask){\r\n    try {\r\n        return calculateGradientsHelper(features, label, fMask, labelMask);\r\n    } catch (OutOfMemoryError e) {\r\n        CrashReportingUtil.writeMemoryCrashDump(this, e);\r\n        throw e;\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SDVariable.storeAndAllocateNewArray",
	"Comment": "allocate and return anew arraybased on the vertex id and weight initialization.",
	"Method": "INDArray storeAndAllocateNewArray(){\r\n    val shape = sameDiff.getShapeForVarName(getVarName());\r\n    INDArray currArr = getArr();\r\n    if (currArr != null && Arrays.equals(currArr.shape(), shape))\r\n        return getArr();\r\n    if (varName == null)\r\n        throw new ND4JIllegalStateException(\"Unable to store array for null variable name!\");\r\n    if (shape == null) {\r\n        throw new ND4JIllegalStateException(\"Unable to allocate new array. No shape found for variable \" + varName);\r\n    }\r\n    val arr = getWeightInitScheme().create(shape);\r\n    sameDiff.associateArrayWithVariable(arr, this);\r\n    if (log.isTraceEnabled()) {\r\n        log.trace(\"Generated and stored new array for variable \\\"{}\\\": old shape: {}, new shape {}\", getVarName(), (currArr == null ? \"null\" : Arrays.toString(currArr.shape())), Arrays.toString(arr.shape()));\r\n    }\r\n    return arr;\r\n}"
}, {
	"Path": "edu.stanford.nlp.patterns.surface.CreatePatterns.getAllPatterns",
	"Comment": "creates all patterns and saves them in the correct patternsforeachtoken class appropriately",
	"Method": "void getAllPatterns(Map<String, DataInstance> sents,Properties props,ConstantsAndVariables.PatternForEachTokenWay storePatsForEachTokenWay){\r\n    Date startDate = new Date();\r\n    List<String> keyset = new ArrayList(sents.keySet());\r\n    int num;\r\n    if (constVars.numThreads == 1)\r\n        num = keyset.size();\r\n    else\r\n        num = keyset.size() / (constVars.numThreads);\r\n    ExecutorService executor = Executors.newFixedThreadPool(constVars.numThreads);\r\n    Redwood.log(ConstantsAndVariables.extremedebug, \"Computing all patterns. keyset size is \" + keyset.size() + \". Assigning \" + num + \" values to each thread\");\r\n    List<Future<Boolean>> list = new ArrayList();\r\n    for (int i = 0; i < constVars.numThreads; i++) {\r\n        int from = i * num;\r\n        int to = -1;\r\n        if (i == constVars.numThreads - 1)\r\n            to = keyset.size();\r\n        else\r\n            to = Math.min(keyset.size(), (i + 1) * num);\r\n        List<String> ids = keyset.subList(from, to);\r\n        Callable<Boolean> task = new CreatePatternsThread(sents, ids, props, storePatsForEachTokenWay);\r\n        Future<Boolean> submit = executor.submit(task);\r\n        list.add(submit);\r\n    }\r\n    for (Future<Boolean> future : list) {\r\n        try {\r\n            future.get();\r\n        } catch (Exception e) {\r\n            executor.shutdownNow();\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n    executor.shutdown();\r\n    Date endDate = new Date();\r\n    String timeTaken = GetPatternsFromDataMultiClass.elapsedTime(startDate, endDate);\r\n    Redwood.log(Redwood.DBG, \"Done computing all patterns [\" + timeTaken + \"]\");\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.binomial",
	"Comment": "generates a binomial distributed number usingthe given rng",
	"Method": "int binomial(RandomGenerator rng,int n,double p){\r\n    if ((p < 0) || (p > 1)) {\r\n        return 0;\r\n    }\r\n    int c = 0;\r\n    for (int i = 0; i < n; i++) {\r\n        if (rng.nextDouble() < p) {\r\n            c++;\r\n        }\r\n    }\r\n    return c;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.SystemUtils.getStackTraceString",
	"Comment": "returns the string value of the stack trace for the given throwable.",
	"Method": "String getStackTraceString(Throwable t){\r\n    ByteArrayOutputStream bs = new ByteArrayOutputStream();\r\n    t.printStackTrace(new PrintStream(bs));\r\n    return bs.toString();\r\n}"
}, {
	"Path": "edu.stanford.nlp.sentiment.ConvertMatlabModel.copyWordVector",
	"Comment": "will not overwrite an existing word vector if it is already there",
	"Method": "void copyWordVector(Map<String, SimpleMatrix> wordVectors,String source,String target){\r\n    if (wordVectors.containsKey(target) || !wordVectors.containsKey(source)) {\r\n        return;\r\n    }\r\n    log.info(\"Using wordVector \" + source + \" for \" + target);\r\n    wordVectors.put(target, new SimpleMatrix(wordVectors.get(source)));\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.IntervalTree.leftRotate",
	"Comment": "moves this node to the left and the right child up and returns the new root",
	"Method": "TreeNode<E, T> leftRotate(TreeNode<E, T> oldRoot){\r\n    if (oldRoot == null || oldRoot.isEmpty() || oldRoot.right == null)\r\n        return oldRoot;\r\n    TreeNode<E, T> oldRightLeft = oldRoot.right.left;\r\n    TreeNode<E, T> newRoot = oldRoot.right;\r\n    newRoot.left = oldRoot;\r\n    oldRoot.right = oldRightLeft;\r\n    newRoot.parent = oldRoot.parent;\r\n    newRoot.maxEnd = oldRoot.maxEnd;\r\n    newRoot.size = oldRoot.size;\r\n    if (newRoot.parent != null) {\r\n        if (newRoot.parent.left == oldRoot) {\r\n            newRoot.parent.left = newRoot;\r\n        } else if (newRoot.parent.right == oldRoot) {\r\n            newRoot.parent.right = newRoot;\r\n        } else {\r\n            throw new IllegalStateException(\"Old root not a child of it's parent\");\r\n        }\r\n    }\r\n    oldRoot.parent = newRoot;\r\n    if (oldRightLeft != null)\r\n        oldRightLeft.parent = oldRoot;\r\n    adjust(oldRoot);\r\n    return newRoot;\r\n}"
}, {
	"Path": "edu.stanford.nlp.quoteattribution.Sieves.Sieve.rangeContainsCharIndex",
	"Comment": "convert token range to char range, check if charindex is in it.",
	"Method": "boolean rangeContainsCharIndex(Pair<Integer, Integer> tokenRange,int charIndex){\r\n    List<CoreLabel> tokens = doc.get(CoreAnnotations.TokensAnnotation.class);\r\n    CoreLabel startToken = tokens.get(tokenRange.first());\r\n    CoreLabel endToken = tokens.get(tokenRange.second());\r\n    int startTokenCharBegin = startToken.beginPosition();\r\n    int endTokenCharEnd = endToken.endPosition();\r\n    return (startTokenCharBegin <= charIndex && charIndex <= endTokenCharEnd);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.TregexGUI.doLoadFiles",
	"Comment": "method for bringing up the load file dialog box and conveyingthe chosen files to the filepanel",
	"Method": "void doLoadFiles(){\r\n    if (chooser == null) {\r\n        chooser = createFileChooser();\r\n    }\r\n    String approveText = chooser.getApproveButtonText();\r\n    chooser.setApproveButtonText(\"Load with file filters\");\r\n    int status = chooser.showOpenDialog(this);\r\n    chooser.setApproveButtonText(approveText);\r\n    if (status == JFileChooser.APPROVE_OPTION) {\r\n        File[] selectedFiles = chooser.getSelectedFiles();\r\n        boolean haveDirectory = false;\r\n        for (File f : selectedFiles) {\r\n            if (f.isDirectory()) {\r\n                haveDirectory = true;\r\n                break;\r\n            }\r\n        }\r\n        if (haveDirectory) {\r\n            doFileFilters(selectedFiles);\r\n        } else {\r\n            startFileLoadingThread(new EnumMap(FilterType.class), selectedFiles);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.setSpans",
	"Comment": "assign a spanannotation on each node of this tree. the index starts at zero.",
	"Method": "void setSpans(){\r\n    constituentsNodes(0);\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.blas.impl.BaseLevel1.iamax",
	"Comment": "finds the element of avector that has the largest absolute value.",
	"Method": "int iamax(long n,INDArray arr,int stride,int iamax,long n,DataBuffer x,int offsetX,int incrX,int iamax,INDArray arr){\r\n    if (arr.isSparse()) {\r\n        return Nd4j.getSparseBlasWrapper().level1().iamax(arr);\r\n    }\r\n    if (Nd4j.getExecutioner().getProfilingMode() == OpExecutioner.ProfilingMode.ALL)\r\n        OpProfiler.getInstance().processBlasCall(false, arr);\r\n    if (arr.data().dataType() == DataBuffer.Type.DOUBLE) {\r\n        DefaultOpExecutioner.validateDataType(DataBuffer.Type.DOUBLE, arr);\r\n        return idamax(arr.length(), arr, BlasBufferUtil.getBlasStride(arr));\r\n    } else {\r\n        DefaultOpExecutioner.validateDataType(DataBuffer.Type.FLOAT, arr);\r\n        return isamax(arr.length(), arr, BlasBufferUtil.getBlasStride(arr));\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.EnglishGrammaticalRelations.values",
	"Comment": "return a synchronized list of the known grammaticalrelation entries.",
	"Method": "List<GrammaticalRelation> values(){\r\n    return unmodifiableSynchronizedValues;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.retainAbove",
	"Comment": "removes all entries with counts below the given threshold, returning theset of removed entries.",
	"Method": "Set<E> retainAbove(Counter<E> counter,double countThreshold,Set<Pair<E1, E2>> retainAbove,TwoDimensionalCounter<E1, E2> counter,double countThreshold){\r\n    Set<Pair<E1, E2>> removed = new HashSet();\r\n    for (Entry<E1, ClassicCounter<E2>> en : counter.entrySet()) {\r\n        for (Entry<E2, Double> en2 : en.getValue().entrySet()) {\r\n            if (counter.getCount(en.getKey(), en2.getKey()) < countThreshold) {\r\n                removed.add(new Pair(en.getKey(), en2.getKey()));\r\n            }\r\n        }\r\n    }\r\n    for (Pair<E1, E2> key : removed) {\r\n        counter.remove(key.first(), key.second());\r\n    }\r\n    return removed;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.Clique.maxRight",
	"Comment": "convenience method for finding the most far rightrelative index.",
	"Method": "int maxRight(){\r\n    return relativeIndices[relativeIndices.length - 1];\r\n}"
}, {
	"Path": "org.deeplearning4j.util.CrashReportingUtil.crashDumpsEnabled",
	"Comment": "method that can be used to enable or disable memory crash reporting. memory crash reporting is enabled by default.",
	"Method": "void crashDumpsEnabled(boolean enabled){\r\n    crashDumpsEnabled = enabled;\r\n}"
}, {
	"Path": "dagger.internal.ArrayQueue.doubleCapacity",
	"Comment": "double the capacity of this queue.call only when full, i.e.,when head and tail have wrapped around to become equal.",
	"Method": "void doubleCapacity(){\r\n    int p = head;\r\n    int n = elements.length;\r\n    int r = n - p;\r\n    int newCapacity = n << 1;\r\n    if (newCapacity < 0)\r\n        throw new IllegalStateException(\"Sorry, queue too big\");\r\n    Object[] a = new Object[newCapacity];\r\n    System.arraycopy(elements, p, a, 0, r);\r\n    System.arraycopy(elements, 0, a, r, p);\r\n    elements = a;\r\n    head = 0;\r\n    tail = n;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.normalizeMoments",
	"Comment": "calculate the mean and variance from the sufficient statistics",
	"Method": "SDVariable[] normalizeMoments(SDVariable counts,SDVariable means,SDVariable variances,double shift,SDVariable[] normalizeMoments,String[] name,SDVariable counts,SDVariable means,SDVariable variances,double shift){\r\n    SDVariable[] res = f().normalizeMoments(counts, means, variances, shift);\r\n    return updateVariableNamesAndReferences(res, name);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.wordstore.VocabularyHolder.words",
	"Comment": "returns sorted list of words in vocabulary.sort is descending.",
	"Method": "List<VocabularyWord> words(){\r\n    List<VocabularyWord> vocab = new ArrayList(vocabulary.values());\r\n    Collections.sort(vocab, new Comparator<VocabularyWord>() {\r\n        @Override\r\n        public int compare(VocabularyWord o1, VocabularyWord o2) {\r\n            return Integer.compare(o2.getCount(), o1.getCount());\r\n        }\r\n    });\r\n    return vocab;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.wordstore.VocabularyHolder.words",
	"Comment": "returns sorted list of words in vocabulary.sort is descending.",
	"Method": "List<VocabularyWord> words(){\r\n    return Integer.compare(o2.getCount(), o1.getCount());\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.asNormalizedCounter",
	"Comment": "l1 normalize a counter. return a counter that is a probability distribution,so the sum of the resulting value equals 1.",
	"Method": "C asNormalizedCounter(C c){\r\n    return scale(c, 1.0 / c.totalCount());\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasModelBuilder.modelYamlFilename",
	"Comment": "set model architecture from file name pointing to model yaml string.",
	"Method": "KerasModelBuilder modelYamlFilename(String modelYamlFilename){\r\n    checkForExistence(modelYamlFilename);\r\n    this.modelJson = new String(Files.readAllBytes(Paths.get(modelYamlFilename)));\r\n    return this;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.feedForward",
	"Comment": "compute activations from input to output of the output layer",
	"Method": "List<INDArray> feedForward(INDArray input,boolean train,List<INDArray> feedForward,boolean train,List<INDArray> feedForward,boolean train,boolean clearInputs,List<INDArray> feedForward,List<INDArray> feedForward,INDArray input,List<INDArray> feedForward,INDArray input,INDArray featuresMask,INDArray labelsMask){\r\n    setLayerMaskArrays(featuresMask, labelsMask);\r\n    List<INDArray> list = feedForward(input);\r\n    clearLayerMaskArrays();\r\n    return list;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractTreebankLanguagePack.punctuationTagRejectFilter",
	"Comment": "return a filter that rejects a string that is a punctuationtag name, and rejects everything else.",
	"Method": "Predicate<String> punctuationTagRejectFilter(){\r\n    return Filters.notFilter(punctTagStringAcceptFilter);\r\n}"
}, {
	"Path": "org.nd4j.evaluation.regression.RegressionEvaluation.averagecorrelationR2",
	"Comment": "legacy method for the correlation average across all columns.",
	"Method": "double averagecorrelationR2(){\r\n    return averagePearsonCorrelation();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.getLeaves",
	"Comment": "gets the leaves of the tree.all leaves nodes are returned as a listordered by the natural left to right order of the tree.null values,if any, are inserted into the list like any other value.",
	"Method": "List<T> getLeaves(List<T> getLeaves,List<T> list){\r\n    if (isLeaf()) {\r\n        list.add((T) this);\r\n    } else {\r\n        for (Tree kid : children()) {\r\n            kid.getLeaves(list);\r\n        }\r\n    }\r\n    return list;\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.column.RemoveAllColumnsExceptForTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return columnsToKeep;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.LossLayer.f1Score",
	"Comment": "returns the f1 score for the given examples.think of this to be like a percentage right.the higher the number the more it got right.this is on a scale from 0 to 1.",
	"Method": "double f1Score(DataSet data,double f1Score,INDArray examples,INDArray labels){\r\n    Evaluation eval = new Evaluation();\r\n    eval.eval(labels, labelProbabilities(examples));\r\n    return eval.f1();\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.mentions",
	"Comment": "get all mentions of the given ner tag, as a list of surface forms.",
	"Method": "List<String> mentions(String nerTag,List<String> mentions){\r\n    List<String> mentionsOfTag = new ArrayList();\r\n    StringBuilder lastMention = new StringBuilder();\r\n    String lastTag = \"O\";\r\n    for (int i = 0; i < length(); ++i) {\r\n        String ner = nerTag(i);\r\n        if (!ner.equals(\"O\") && !lastTag.equals(ner)) {\r\n            if (lastMention.length() > 0) {\r\n                mentionsOfTag.add(lastMention.toString().trim());\r\n            }\r\n            lastMention.setLength(0);\r\n            lastMention.append(word(i)).append(' ');\r\n        } else if (!ner.equals(\"O\") && lastTag.equals(ner)) {\r\n            lastMention.append(word(i)).append(' ');\r\n        } else if (ner.equals(\"O\") && !lastTag.equals(\"O\")) {\r\n            if (lastMention.length() > 0) {\r\n                mentionsOfTag.add(lastMention.toString().trim());\r\n            }\r\n            lastMention.setLength(0);\r\n        }\r\n        lastTag = ner;\r\n    }\r\n    if (lastMention.length() > 0) {\r\n        mentionsOfTag.add(lastMention.toString().trim());\r\n    }\r\n    return mentionsOfTag;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.dependencies",
	"Comment": "an internal helper to get the dependency tree of the given type.",
	"Method": "CoreNLPProtos.DependencyGraph dependencies(SemanticGraphFactory.Mode mode){\r\n    switch(mode) {\r\n        case BASIC:\r\n            return impl.getBasicDependencies();\r\n        case ENHANCED:\r\n            return impl.getEnhancedDependencies();\r\n        case ENHANCED_PLUS_PLUS:\r\n            return impl.getEnhancedPlusPlusDependencies();\r\n        default:\r\n            throw new IllegalArgumentException(\"Unsupported dependency type: \" + mode);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.TregexTest.testTest",
	"Comment": "reruns one of the simpler tests using the test class to make surethe test class works",
	"Method": "void testTest(){\r\n    runTest(\"/^MW/\", \"(ROOT (MWE (N 1) (N 2) (N 3)) (MWV (A B)))\", \"(MWE (N 1) (N 2) (N 3))\", \"(MWV (A B))\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.XMLUtils.getTagElementTriplesFromFileNumBounded",
	"Comment": "returns the elements in the given file with the given tag associated withthe text content of the previous and next siblings up to max numincludedsiblings.",
	"Method": "List<Triple<String, Element, String>> getTagElementTriplesFromFileNumBounded(File f,String tag,int num){\r\n    List<Triple<String, Element, String>> sents = Generics.newArrayList();\r\n    try {\r\n        sents = getTagElementTriplesFromFileNumBoundedSAXException(f, tag, num);\r\n    } catch (SAXException e) {\r\n        log.warn(e);\r\n    }\r\n    return sents;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.caseless",
	"Comment": "make this sentence caseless. that is, from now on, run the caseless modelson the sentence by default rather than the standard corenlp models.",
	"Method": "Sentence caseless(){\r\n    return new Sentence(this.docFn, impl.build(), Document.CASELESS_PROPS);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.recurrent.KerasSimpleRnn.getInputPreprocessor",
	"Comment": "gets appropriate dl4j inputpreprocessor for given inputtypes.",
	"Method": "InputPreProcessor getInputPreprocessor(InputType inputType){\r\n    if (inputType.length > 1)\r\n        throw new InvalidKerasConfigurationException(\"Keras SimpleRnn layer accepts only one input (received \" + inputType.length + \")\");\r\n    return InputTypeUtil.getPreprocessorForInputTypeRnnLayers(inputType[0], layerName);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.functions",
	"Comment": "get an array of differential functions that have been defined for this samediff instance",
	"Method": "DifferentialFunction[] functions(){\r\n    val ret = functionInstancesById.values();\r\n    return ret.toArray(new DifferentialFunction[ret.size()]);\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.parallel.MultiBoolean.allTrue",
	"Comment": "this method returns true if all states are true. false otherwise.",
	"Method": "boolean allTrue(){\r\n    return holder == max;\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.util.LeafUtils.countUniqueParameters",
	"Comment": "count the number of unique parameters in the specified leaf nodes",
	"Method": "int countUniqueParameters(List<ParameterSpace> allLeaves){\r\n    List<ParameterSpace> unique = getUniqueObjects(allLeaves);\r\n    int count = 0;\r\n    for (ParameterSpace ps : unique) {\r\n        if (!ps.isLeaf()) {\r\n            throw new IllegalStateException(\"Method should only be used with leaf nodes\");\r\n        }\r\n        count += ps.numParameters();\r\n    }\r\n    return count;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasRegularizerUtils.getWeightRegularizerFromConfig",
	"Comment": "get weight regularization from keras weight regularization configuration.",
	"Method": "double getWeightRegularizerFromConfig(Map<String, Object> layerConfig,KerasLayerConfiguration conf,String configField,String regularizerType){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (innerConfig.containsKey(configField)) {\r\n        Map<String, Object> regularizerConfig = (Map<String, Object>) innerConfig.get(configField);\r\n        if (regularizerConfig != null) {\r\n            if (regularizerConfig.containsKey(regularizerType)) {\r\n                return (double) regularizerConfig.get(regularizerType);\r\n            }\r\n            if (regularizerConfig.containsKey(conf.getLAYER_FIELD_CLASS_NAME()) && regularizerConfig.get(conf.getLAYER_FIELD_CLASS_NAME()).equals(\"L1L2\")) {\r\n                Map<String, Object> innerRegularizerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(regularizerConfig, conf);\r\n                try {\r\n                    return (double) innerRegularizerConfig.get(regularizerType);\r\n                } catch (Exception e) {\r\n                    return (double) (int) innerRegularizerConfig.get(regularizerType);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return 0.0;\r\n}"
}, {
	"Path": "org.datavec.api.transform.metadata.IntegerMetaData.isValid",
	"Comment": "is the given object valid for this column,given the column type and anyrestrictions given by thecolumnmetadata object?",
	"Method": "boolean isValid(Writable writable,boolean isValid,Object input){\r\n    int value;\r\n    try {\r\n        value = Integer.parseInt(input.toString());\r\n    } catch (NumberFormatException e) {\r\n        return false;\r\n    }\r\n    if (minAllowedValue != null && value < minAllowedValue)\r\n        return false;\r\n    if (maxAllowedValue != null && value > maxAllowedValue)\r\n        return false;\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphFactory.duplicateKeepNodes",
	"Comment": "this creates a new graph based off the given, but uses the existing nodes objects.",
	"Method": "SemanticGraph duplicateKeepNodes(SemanticGraph sg){\r\n    SemanticGraph retSg = new SemanticGraph();\r\n    for (IndexedWord node : sg.vertexSet()) {\r\n        retSg.addVertex(node);\r\n    }\r\n    retSg.setRoots(sg.getRoots());\r\n    for (SemanticGraphEdge edge : sg.edgeIterable()) {\r\n        retSg.addEdge(edge.getGovernor(), edge.getDependent(), edge.getRelation(), edge.getWeight(), edge.isExtra());\r\n    }\r\n    return retSg;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.multiplyInPlace",
	"Comment": "multiplies each value in target by the count of the key in mult, in place. returns non zero entries",
	"Method": "Counter<E> multiplyInPlace(Counter<E> target,double multiplier,Counter<E> multiplyInPlace,Counter<E> target,Counter<E> mult){\r\n    for (Entry<E, Double> entry : target.entrySet()) {\r\n        target.setCount(entry.getKey(), entry.getValue() * mult.getCount(entry.getKey()));\r\n    }\r\n    Counters.retainNonZeros(target);\r\n    return target;\r\n}"
}, {
	"Path": "com.atilika.kuromoji.trie.DoubleArrayTrie.build",
	"Comment": "construct double array trie which is equivalent to input trie",
	"Method": "void build(Trie trie){\r\n    ProgressLog.begin(\"building \" + (compact ? \"compact\" : \"sparse\") + \" trie\");\r\n    baseBuffer = IntBuffer.allocate(BASE_CHECK_INITIAL_SIZE);\r\n    baseBuffer.put(0, 1);\r\n    checkBuffer = IntBuffer.allocate(BASE_CHECK_INITIAL_SIZE);\r\n    tailBuffer = CharBuffer.allocate(TAIL_INITIAL_SIZE);\r\n    add(-1, 0, trie.getRoot());\r\n    reportUtilizationRate();\r\n    ProgressLog.end();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.convolution.Cnn3DLossLayer.computeScoreForExamples",
	"Comment": "compute the score for each example individually, after labels and input have been set.",
	"Method": "INDArray computeScoreForExamples(double fullNetworkL1,double fullNetworkL2,LayerWorkspaceMgr workspaceMgr){\r\n    if (input == null || labels == null)\r\n        throw new IllegalStateException(\"Cannot calculate score without input and labels \" + layerId());\r\n    INDArray input2d = ConvolutionUtils.reshape5dTo2d(layerConf().getDataFormat(), input, workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n    INDArray labels2d = ConvolutionUtils.reshape5dTo2d(layerConf().getDataFormat(), labels, workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n    INDArray maskReshaped = ConvolutionUtils.reshapeCnn3dMask(layerConf().getDataFormat(), maskArray, input, workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n    ILossFunction lossFunction = layerConf().getLossFn();\r\n    INDArray scoreArray = lossFunction.computeScoreArray(labels2d, input2d, layerConf().getActivationFn(), maskReshaped);\r\n    val newShape = input.shape().clone();\r\n    newShape[1] = 1;\r\n    int n = (int) input.size(0);\r\n    int d, h, w, c;\r\n    if (layerConf().getDataFormat() == Convolution3D.DataFormat.NDHWC) {\r\n        d = (int) input.size(1);\r\n        h = (int) input.size(2);\r\n        w = (int) input.size(3);\r\n        c = (int) input.size(4);\r\n    } else {\r\n        d = (int) input.size(2);\r\n        h = (int) input.size(3);\r\n        w = (int) input.size(4);\r\n        c = (int) input.size(1);\r\n    }\r\n    INDArray scoreArrayTs = ConvolutionUtils.reshape2dTo5d(layerConf().getDataFormat(), scoreArray, n, d, h, w, c, workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n    INDArray summedScores = scoreArrayTs.sum(1, 2, 3, 4);\r\n    double l1l2 = fullNetworkL1 + fullNetworkL2;\r\n    if (l1l2 != 0.0) {\r\n        summedScores.addi(l1l2);\r\n    }\r\n    return workspaceMgr.leverageTo(ArrayType.ACTIVATIONS, summedScores);\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.AsyncMultiDataSetIterator.next",
	"Comment": "like the standard next method but allows acustomizable number of examples returned",
	"Method": "MultiDataSet next(int num,MultiDataSet next){\r\n    if (throwable != null)\r\n        throw throwable;\r\n    if (hasDepleted.get())\r\n        return null;\r\n    MultiDataSet temp = nextElement;\r\n    nextElement = null;\r\n    return temp;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SDVariable.assign",
	"Comment": "return a variable with equal shape to the input, but all elements set to the specified value",
	"Method": "SDVariable assign(Number value){\r\n    return sameDiff.scalarSet(this, value);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.pennchinese.ChineseTreebankLanguagePack.getEncoding",
	"Comment": "return the input charset encoding for the treebank.see documentation for the charset class.",
	"Method": "String getEncoding(){\r\n    return ENCODING;\r\n}"
}, {
	"Path": "edu.stanford.nlp.wordseg.Gale2007ChineseSegmenterFeatureFactory.getCliqueFeatures",
	"Comment": "extracts all the features from the input data at a certain index.",
	"Method": "Collection<String> getCliqueFeatures(PaddedList<IN> cInfo,int loc,Clique clique){\r\n    Collection<String> features = Generics.newHashSet();\r\n    if (clique == cliqueC) {\r\n        addAllInterningAndSuffixing(features, featuresC(cInfo, loc), \"C\");\r\n    } else if (clique == cliqueCpC) {\r\n        addAllInterningAndSuffixing(features, featuresCpC(cInfo, loc), \"CpC\");\r\n        addAllInterningAndSuffixing(features, featuresCnC(cInfo, loc - 1), \"CnC\");\r\n    } else if (clique == cliqueCpCp2C) {\r\n        addAllInterningAndSuffixing(features, featuresCpCp2C(cInfo, loc), \"CpCp2C\");\r\n    } else if (clique == cliqueCpCp2Cp3C) {\r\n        addAllInterningAndSuffixing(features, featuresCpCp2Cp3C(cInfo, loc), \"CpCp2Cp3C\");\r\n    }\r\n    if (DEBUG > 0) {\r\n        EncodingPrintWriter.err.println(\"For \" + cInfo.get(loc) + \", features: \" + features, \"UTF-8\");\r\n    }\r\n    return features;\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.string.ConcatenateStringColumns.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return new String[] { newColumnName };\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.allEdgesInSet",
	"Comment": "given a set of vertices from the same graph, returns the set of all edges between thesevertices.",
	"Method": "Set<SemanticGraphEdge> allEdgesInSet(Iterable<IndexedWord> vertices,SemanticGraph sg){\r\n    Set<SemanticGraphEdge> edges = Generics.newHashSet();\r\n    for (IndexedWord v1 : vertices) {\r\n        for (SemanticGraphEdge edge : sg.outgoingEdgeIterable(v1)) {\r\n            edges.add(edge);\r\n        }\r\n        for (SemanticGraphEdge edge : sg.incomingEdgeIterable(v1)) {\r\n            edges.add(edge);\r\n        }\r\n    }\r\n    return edges;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphTest.verifyTopologicalSort",
	"Comment": "tests that a particular topological sort is correct by verifyingfor each node that it appears in the sort and all of its childrenoccur later in the sort",
	"Method": "void verifyTopologicalSort(SemanticGraph graph){\r\n    List<IndexedWord> sorted = graph.topologicalSort();\r\n    Map<IndexedWord, Integer> indices = Generics.newHashMap();\r\n    for (int index = 0; index < sorted.size(); ++index) {\r\n        indices.put(sorted.get(index), index);\r\n    }\r\n    for (IndexedWord parent : graph.vertexSet()) {\r\n        assertTrue(indices.containsKey(parent));\r\n        int parentIndex = indices.get(parent);\r\n        for (IndexedWord child : graph.getChildren(parent)) {\r\n            assertTrue(indices.containsKey(child));\r\n            int childIndex = indices.get(child);\r\n            assertTrue(parentIndex < childIndex);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.registerCustomAnnotators",
	"Comment": "register any custom annotators defined in the input properties, and add them to the pool.",
	"Method": "void registerCustomAnnotators(AnnotatorPool pool,AnnotatorImplementations annotatorImplementation,Properties inputProps){\r\n    for (String property : inputProps.stringPropertyNames()) {\r\n        if (property.startsWith(CUSTOM_ANNOTATOR_PREFIX)) {\r\n            final String customName = property.substring(CUSTOM_ANNOTATOR_PREFIX.length());\r\n            final String customClassName = inputProps.getProperty(property);\r\n            logger.info(\"Registering annotator \" + customName + \" with class \" + customClassName);\r\n            AnnotatorSignature key = new AnnotatorSignature(customName, PropertiesUtils.getSignature(customName, inputProps));\r\n            pool.register(customName, inputProps, GLOBAL_ANNOTATOR_CACHE.computeIfAbsent(key, (sig) -> Lazy.cache(() -> annotatorImplementation.custom(inputProps, property))));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.score",
	"Comment": "sets the input and labels and returns a score for the prediction with respect to the true labels",
	"Method": "double score(DataSet dataSet,double score,DataSet dataSet,boolean training,double score,MultiDataSet dataSet,double score,MultiDataSet dataSet,boolean training,double score){\r\n    return score;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.getChildrenWithRelnPrefix",
	"Comment": "returns a list of all children bearing a grammatical relation starting with the given set of relation prefixes",
	"Method": "List<IndexedWord> getChildrenWithRelnPrefix(SemanticGraph graph,IndexedWord vertex,String relnPrefix,List<IndexedWord> getChildrenWithRelnPrefix,SemanticGraph graph,IndexedWord vertex,Collection<String> relnPrefixes){\r\n    if (vertex.equals(IndexedWord.NO_WORD))\r\n        return new ArrayList();\r\n    if (!graph.containsVertex(vertex)) {\r\n        throw new IllegalArgumentException();\r\n    }\r\n    List<IndexedWord> childList = new ArrayList();\r\n    for (SemanticGraphEdge edge : graph.outgoingEdgeIterable(vertex)) {\r\n        String edgeString = edge.getRelation().toString();\r\n        for (String relnPrefix : relnPrefixes) {\r\n            if (edgeString.startsWith(relnPrefix)) {\r\n                childList.add(edge.getTarget());\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    return childList;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Maps.sortedEntries",
	"Comment": "sorts a list of entries.this method is here since the entries might come from a counter.",
	"Method": "List<Map.Entry<K, V>> sortedEntries(Collection<Map.Entry<K, V>> entries,List<Map.Entry<K, V>> sortedEntries,Map<K, V> map){\r\n    return sortedEntries(map.entrySet());\r\n}"
}, {
	"Path": "org.datavec.api.transform.condition.column.BooleanColumnCondition.columnCondition",
	"Comment": "returns whether the given elementmeets the condition set by this operation",
	"Method": "boolean columnCondition(Writable writable){\r\n    BooleanWritable booleanWritable = (BooleanWritable) writable;\r\n    return booleanWritable.get();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Characters.isControl",
	"Comment": "returns true if a character is a control character, andfalse otherwise.",
	"Method": "boolean isControl(char c){\r\n    return Character.getType(c) == Character.CONTROL;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.WordToSentenceProcessorTest.testParagraphSeparator",
	"Comment": "ensure that the unicode paragraph separator alwaysstarts a new sentence.",
	"Method": "void testParagraphSeparator(){\r\n    checkResult(wts, \"Hello?World.\", \"Hello\", \"World.\");\r\n    checkResult(wts, \"Hello.?World.\", \"Hello.\", \"World.\");\r\n    checkResult(wts, \"Hello  ?World.\", \"Hello\", \"World.\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.KBPAnnotator.main",
	"Comment": "a debugging method to try relation extraction from the console.",
	"Method": "void main(String[] args){\r\n    Properties props = StringUtils.argsToProperties(args);\r\n    props.setProperty(\"annotators\", \"tokenize,ssplit,pos,lemma,ner,regexner,parse,mention,coref,kbp\");\r\n    props.setProperty(\"regexner.mapping\", \"ignorecase=true,validpospattern=^(NN|JJ).*,edu/stanford/nlp/models/kbp/regexner_caseless.tab;edu/stanford/nlp/models/kbp/regexner_cased.tab\");\r\n    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\r\n    IOUtils.console(\"sentence> \", line -> {\r\n        Annotation ann = new Annotation(line);\r\n        pipeline.annotate(ann);\r\n        for (CoreMap sentence : ann.get(CoreAnnotations.SentencesAnnotation.class)) {\r\n            sentence.get(CoreAnnotations.KBPTriplesAnnotation.class).forEach(System.err::println);\r\n        }\r\n    });\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.retainTopMass",
	"Comment": "retains the minimal set of top keys such that their count sum is more than thresholdcount.",
	"Method": "void retainTopMass(Counter<E> counter,double thresholdCount){\r\n    PriorityQueue<E> queue = Counters.toPriorityQueue(counter);\r\n    counter.clear();\r\n    double mass = 0;\r\n    while (mass < thresholdCount && !queue.isEmpty()) {\r\n        double value = queue.getPriority();\r\n        E key = queue.removeFirst();\r\n        counter.setCount(key, value);\r\n        mass += value;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.pennchinese.ChineseEnglishWordMap.addMap",
	"Comment": "add all of the mappings from the specified map to the current map.",
	"Method": "int addMap(Map<String, Set<String>> addM){\r\n    int newTrans = 0;\r\n    for (Map.Entry<String, Set<String>> me : addM.entrySet()) {\r\n        String k = me.getKey();\r\n        Set<String> addList = me.getValue();\r\n        Set<String> origList = map.get(k);\r\n        if (origList == null) {\r\n            map.put(k, new LinkedHashSet(addList));\r\n            Set<String> newList = map.get(k);\r\n            if (newList != null && newList.size() != 0) {\r\n                newTrans += addList.size();\r\n            }\r\n        } else {\r\n            for (String toAdd : addList) {\r\n                if (!(origList.contains(toAdd))) {\r\n                    origList.add(toAdd);\r\n                    newTrans++;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return newTrans;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.Morpha.yypushback",
	"Comment": "pushes the specified amount of characters back into the input stream.they will be read again by then next call of the scanning method",
	"Method": "void yypushback(int number){\r\n    if (number > yylength())\r\n        zzScanError(ZZ_PUSHBACK_2BIG);\r\n    zzMarkedPos -= number;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.evaluate",
	"Comment": "evaluate the network on the provided data set. used for evaluating the performance of classifiers",
	"Method": "T evaluate(DataSetIterator iterator,Evaluation evaluate,DataSetIterator iterator,List<String> labelsList,Evaluation evaluate,DataSetIterator iterator,List<String> labelsList,int topN){\r\n    if (layers == null || !(getOutputLayer() instanceof IOutputLayer)) {\r\n        throw new IllegalStateException(\"Cannot evaluate network with no output layer\");\r\n    }\r\n    if (labelsList == null) {\r\n        try {\r\n            labelsList = iterator.getLabels();\r\n        } catch (Throwable t) {\r\n        }\r\n    }\r\n    Layer outputLayer = getOutputLayer();\r\n    if (getLayerWiseConfigurations().isValidateOutputLayerConfig()) {\r\n        OutputLayerUtil.validateOutputLayerForClassifierEvaluation(outputLayer.conf().getLayer(), Evaluation.class);\r\n    }\r\n    Evaluation e = new org.deeplearning4j.eval.Evaluation(labelsList, topN);\r\n    doEvaluation(iterator, e);\r\n    return e;\r\n}"
}, {
	"Path": "org.datavec.image.loader.NativeImageLoader.asMat",
	"Comment": "converts an indarray to an opencv mat. only intended for images with rank 3.",
	"Method": "Mat asMat(INDArray array,Mat asMat,INDArray array,int dataType){\r\n    if (array.rank() > 4 || (array.rank() > 3 && array.size(0) != 1)) {\r\n        throw new UnsupportedOperationException(\"Only rank 3 (or rank 4 with size(0) == 1) arrays supported\");\r\n    }\r\n    int rank = array.rank();\r\n    long[] stride = array.stride();\r\n    long offset = array.data().offset();\r\n    Pointer pointer = array.data().pointer().position(offset);\r\n    long rows = array.size(rank == 3 ? 1 : 2);\r\n    long cols = array.size(rank == 3 ? 2 : 3);\r\n    long channels = array.size(rank == 3 ? 0 : 1);\r\n    boolean done = false;\r\n    if (dataType < 0) {\r\n        dataType = pointer instanceof DoublePointer ? CV_64F : CV_32F;\r\n    }\r\n    Mat mat = new Mat((int) Math.min(rows, Integer.MAX_VALUE), (int) Math.min(cols, Integer.MAX_VALUE), CV_MAKETYPE(dataType, (int) Math.min(channels, Integer.MAX_VALUE)));\r\n    Indexer matidx = mat.createIndexer(direct);\r\n    Nd4j.getAffinityManager().ensureLocation(array, AffinityManager.Location.HOST);\r\n    if (pointer instanceof FloatPointer && dataType == CV_32F) {\r\n        FloatIndexer ptridx = FloatIndexer.create((FloatPointer) pointer, new long[] { channels, rows, cols }, new long[] { stride[rank == 3 ? 0 : 1], stride[rank == 3 ? 1 : 2], stride[rank == 3 ? 2 : 3] }, direct);\r\n        FloatIndexer idx = (FloatIndexer) matidx;\r\n        for (long k = 0; k < channels; k++) {\r\n            for (long i = 0; i < rows; i++) {\r\n                for (long j = 0; j < cols; j++) {\r\n                    idx.put(i, j, k, ptridx.get(k, i, j));\r\n                }\r\n            }\r\n        }\r\n        done = true;\r\n        ptridx.release();\r\n    } else if (pointer instanceof DoublePointer && dataType == CV_64F) {\r\n        DoubleIndexer ptridx = DoubleIndexer.create((DoublePointer) pointer, new long[] { channels, rows, cols }, new long[] { stride[rank == 3 ? 0 : 1], stride[rank == 3 ? 1 : 2], stride[rank == 3 ? 2 : 3] }, direct);\r\n        DoubleIndexer idx = (DoubleIndexer) matidx;\r\n        for (long k = 0; k < channels; k++) {\r\n            for (long i = 0; i < rows; i++) {\r\n                for (long j = 0; j < cols; j++) {\r\n                    idx.put(i, j, k, ptridx.get(k, i, j));\r\n                }\r\n            }\r\n        }\r\n        done = true;\r\n        ptridx.release();\r\n    }\r\n    if (!done) {\r\n        for (long k = 0; k < channels; k++) {\r\n            for (long i = 0; i < rows; i++) {\r\n                for (long j = 0; j < cols; j++) {\r\n                    if (rank == 3) {\r\n                        matidx.putDouble(new long[] { i, j, k }, array.getDouble(k, i, j));\r\n                    } else {\r\n                        matidx.putDouble(new long[] { i, j, k }, array.getDouble(0, k, i, j));\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    matidx.release();\r\n    return mat;\r\n}"
}, {
	"Path": "org.datavec.nlp.movingwindow.ContextLabelRetriever.stringWithLabels",
	"Comment": "returns a stripped sentence with the indices of wordswith certain kinds of labels.",
	"Method": "Pair<String, MultiDimensionalMap<Integer, Integer, String>> stringWithLabels(String sentence,TokenizerFactory tokenizerFactory){\r\n    MultiDimensionalMap<Integer, Integer, String> map = MultiDimensionalMap.newHashBackedMap();\r\n    Tokenizer t = tokenizerFactory.create(sentence);\r\n    List<String> currTokens = new ArrayList();\r\n    String currLabel = null;\r\n    String endLabel = null;\r\n    List<Pair<String, List<String>>> tokensWithSameLabel = new ArrayList();\r\n    while (t.hasMoreTokens()) {\r\n        String token = t.nextToken();\r\n        if (token.matches(BEGIN_LABEL)) {\r\n            currLabel = token;\r\n            if (!currTokens.isEmpty()) {\r\n                tokensWithSameLabel.add(new Pair(\"NONE\", (List<String>) new ArrayList(currTokens)));\r\n                currTokens.clear();\r\n            }\r\n        } else if (token.matches(END_LABEL)) {\r\n            if (currLabel == null)\r\n                throw new IllegalStateException(\"Found an ending label with no matching begin label\");\r\n            endLabel = token;\r\n        } else\r\n            currTokens.add(token);\r\n        if (currLabel != null && endLabel != null) {\r\n            currLabel = currLabel.replaceAll(\"[<>/]\", \"\");\r\n            endLabel = endLabel.replaceAll(\"[<>/]\", \"\");\r\n            Preconditions.checkState(!currLabel.isEmpty(), \"Current label is empty!\");\r\n            Preconditions.checkState(!endLabel.isEmpty(), \"End label is empty!\");\r\n            Preconditions.checkState(currLabel.equals(endLabel), \"Current label begin and end did not match for the parse. Was: %s ending with %s\", currLabel, endLabel);\r\n            tokensWithSameLabel.add(new Pair(currLabel, (List<String>) new ArrayList(currTokens)));\r\n            currTokens.clear();\r\n            currLabel = null;\r\n            endLabel = null;\r\n        }\r\n    }\r\n    if (!currTokens.isEmpty()) {\r\n        tokensWithSameLabel.add(new Pair(\"none\", (List<String>) new ArrayList(currTokens)));\r\n        currTokens.clear();\r\n    }\r\n    StringBuilder strippedSentence = new StringBuilder();\r\n    for (Pair<String, List<String>> tokensWithLabel : tokensWithSameLabel) {\r\n        String joinedSentence = StringUtils.join(tokensWithLabel.getSecond(), \" \");\r\n        if (!(strippedSentence.length() < 1))\r\n            strippedSentence.append(\" \");\r\n        strippedSentence.append(joinedSentence);\r\n        int begin = strippedSentence.toString().indexOf(joinedSentence);\r\n        int end = begin + joinedSentence.length();\r\n        map.put(begin, end, tokensWithLabel.getFirst());\r\n    }\r\n    return new Pair(strippedSentence.toString(), map);\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.MaxentTagger.lemmatize",
	"Comment": "adds lemmas to the given list of corelabels, using the givenmorphology object.the input list must already have tags set.",
	"Method": "void lemmatize(List<CoreLabel> sentence,Morphology morpha){\r\n    for (CoreLabel label : sentence) {\r\n        morpha.stem(label);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.keysBelow",
	"Comment": "returns the set of keys whose counts are at or below the given threshold.this set may have 0 elements but will not be null.",
	"Method": "Set<E> keysBelow(Counter<E> c,double countThreshold){\r\n    Set<E> keys = Generics.newHashSet();\r\n    for (E key : c.keySet()) {\r\n        if (c.getCount(key) <= countThreshold) {\r\n            keys.add(key);\r\n        }\r\n    }\r\n    return (keys);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.HashIndex.loadFromFileWithList",
	"Comment": "this assumes each line is one value and creates index by adding values in the order of the lines in the file",
	"Method": "Index<String> loadFromFileWithList(String file){\r\n    Index<String> index = new HashIndex();\r\n    try (BufferedReader br = new BufferedReader(new FileReader(file))) {\r\n        for (String line; (line = br.readLine()) != null; ) {\r\n            index.add(line.trim());\r\n        }\r\n    } catch (Exception e) {\r\n        throw new RuntimeIOException(e);\r\n    }\r\n    return index;\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.AsyncDataSetIterator.next",
	"Comment": "like the standard next method but allows acustomizable number of examples returned",
	"Method": "DataSet next(int num,DataSet next){\r\n    if (throwable != null)\r\n        throw throwable;\r\n    if (hasDepleted.get())\r\n        return null;\r\n    DataSet temp = nextElement;\r\n    nextElement = null;\r\n    return temp;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.tuebadz.TueBaDZHeadFinder.determineNonTrivialHead",
	"Comment": "called by determinehead and may be overridden in subclassesif special treatment is necessary for particular categories.",
	"Method": "Tree determineNonTrivialHead(Tree t,Tree parent){\r\n    Tree theHead = null;\r\n    String motherCat = basicCategory(t.label().value());\r\n    if (DEBUG) {\r\n        log.info(\"Looking for head of \" + t.label() + \"; value is |\" + t.label().value() + \"|, \" + \" baseCat is |\" + motherCat + \"|\");\r\n    }\r\n    String[][] how = nonTerminalInfo.get(motherCat);\r\n    if (how == null) {\r\n        if (DEBUG) {\r\n            log.info(\"Warning: No rule found for \" + motherCat + \" (first char: \" + motherCat.charAt(0) + \")\");\r\n            log.info(\"Known nonterms are: \" + nonTerminalInfo.keySet());\r\n        }\r\n        if (defaultRule != null) {\r\n            if (DEBUG) {\r\n                log.info(\"  Using defaultRule\");\r\n            }\r\n            return traverseLocate(t.children(), defaultRule, true);\r\n        } else {\r\n            return null;\r\n        }\r\n    }\r\n    for (int i = 0; i < how.length; i++) {\r\n        boolean deflt = (i == how.length - 1);\r\n        theHead = traverseLocate(t.children(), how[i], deflt);\r\n        if (theHead != null) {\r\n            break;\r\n        }\r\n    }\r\n    if (DEBUG) {\r\n        log.info(\"  Chose \" + theHead.label());\r\n    }\r\n    return theHead;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.MutableLong.incValue",
	"Comment": "add the argument to the value of this long.a convenience method.",
	"Method": "void incValue(long val){\r\n    i += val;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.replaceWhere",
	"Comment": "replaces all elements in this ndarray that are matching give condition, with corresponding elements from given array",
	"Method": "INDArray replaceWhere(INDArray arr,Condition condition){\r\n    Nd4j.getCompressor().autoDecompress(this);\r\n    BooleanIndexing.replaceWhere(this, arr, condition);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.MorphaAnnotator.phrasalVerb",
	"Comment": "if a token is a phrasal verb with an underscore between a verb and a particle, return the phrasal verb lemmatized. if not, return null",
	"Method": "String phrasalVerb(Morphology morpha,String word,String tag){\r\n    assert (word != null);\r\n    assert (tag != null);\r\n    if (!tag.startsWith(\"VB\") || !word.contains(\"_\"))\r\n        return null;\r\n    String[] verb = word.split(\"_\");\r\n    if (verb.length != 2)\r\n        return null;\r\n    String particle = verb[1];\r\n    if (particles.contains(particle)) {\r\n        String base = verb[0];\r\n        String lemma = morpha.lemma(base, tag);\r\n        return lemma + '_' + particle;\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.serde.FlatBuffersMapper.getOrderAsByte",
	"Comment": "this method returns current byte order for this jvm as libnd4j enum",
	"Method": "byte getOrderAsByte(){\r\n    if (ByteOrder.nativeOrder().equals(ByteOrder.BIG_ENDIAN))\r\n        return org.nd4j.graph.ByteOrder.BE;\r\n    else\r\n        return org.nd4j.graph.ByteOrder.LE;\r\n}"
}, {
	"Path": "org.datavec.api.transform.filter.InvalidNumColumns.transform",
	"Comment": "get the output schema for this transformation, given an input schema",
	"Method": "Schema transform(Schema inputSchema){\r\n    return inputSchema;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.inference.TableFactorTest.subsetAssignment",
	"Comment": "takes a full assignment from a superset factor, and figures out how to map it into a subset factor. this is veryuseful for testing that functional properties are not violated across both product and marginalization steps.",
	"Method": "int[] subsetAssignment(int[] supersetAssignment,TableFactor superset,TableFactor subset){\r\n    int[] subsetAssignment = new int[subset.neighborIndices.length];\r\n    for (int i = 0; i < subset.neighborIndices.length; i++) {\r\n        int var = subset.neighborIndices[i];\r\n        subsetAssignment[i] = -1;\r\n        for (int j = 0; j < superset.neighborIndices.length; j++) {\r\n            if (superset.neighborIndices[j] == var) {\r\n                subsetAssignment[i] = supersetAssignment[j];\r\n                break;\r\n            }\r\n        }\r\n        assert (subsetAssignment[i] != -1);\r\n    }\r\n    return subsetAssignment;\r\n}"
}, {
	"Path": "edu.stanford.nlp.time.SUTimeMain.processTimebankCsv",
	"Comment": "process csv file with just timebank sentences with time expressions",
	"Method": "void processTimebankCsv(AnnotationPipeline pipeline,String in,String out,String eval){\r\n    BufferedReader br = IOUtils.getBufferedFileReader(in);\r\n    PrintWriter pw = (out != null) ? IOUtils.getPrintWriter(out) : new PrintWriter(System.out);\r\n    String line;\r\n    boolean dataStarted = true;\r\n    TimebankSent sent = new TimebankSent();\r\n    String item = null;\r\n    EvalStats evalStats = new EvalStats();\r\n    line = br.readLine();\r\n    while ((line = br.readLine()) != null) {\r\n        if (line.trim().length() == 0)\r\n            continue;\r\n        if (dataStarted) {\r\n            if (line.contains(\"|\")) {\r\n                if (item != null) {\r\n                    boolean addOld = sent.add(item);\r\n                    if (!addOld) {\r\n                        processTimebankCsvSent(pipeline, sent, pw, evalStats);\r\n                        sent = new TimebankSent();\r\n                        sent.add(item);\r\n                    }\r\n                }\r\n                item = line;\r\n            } else {\r\n                item += \" \" + line;\r\n            }\r\n        } else {\r\n            if (line.matches(\"#+ BEGIN DATA #+\")) {\r\n                dataStarted = true;\r\n            }\r\n        }\r\n    }\r\n    if (item != null) {\r\n        boolean addOld = sent.add(item);\r\n        if (!addOld) {\r\n            processTimebankCsvSent(pipeline, sent, pw, evalStats);\r\n            sent = new TimebankSent();\r\n            sent.add(item);\r\n        }\r\n        processTimebankCsvSent(pipeline, sent, pw, evalStats);\r\n    }\r\n    br.close();\r\n    if (out != null) {\r\n        pw.close();\r\n    }\r\n    System.out.println(\"Estimate: \" + evalStats.estPrStats.toString(2));\r\n    System.out.println(\"Overall: \" + evalStats.prStats.toString(2));\r\n    System.out.println(\"Value: \" + evalStats.valPrStats.toString(2));\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.TestSentence.updateConfusionMatrix",
	"Comment": "update a confusion matrix with the errors from this sentence.",
	"Method": "void updateConfusionMatrix(String[] finalTags,ConfusionMatrix<String> confusionMatrix){\r\n    for (int i = 0; i < correctTags.length; i++) confusionMatrix.add(finalTags[i], correctTags[i]);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.StringLabeledScoredTreeReaderFactory.newTreeReader",
	"Comment": "an implementation of the treereaderfactory interface.it creates a simple treereader which literallyreproduces trees in the treebank as labeledscoredtreeobjects, with stringlabel labels.",
	"Method": "TreeReader newTreeReader(Reader in){\r\n    return new PennTreeReader(in, new LabeledScoredTreeFactory(new StringLabelFactory()));\r\n}"
}, {
	"Path": "org.datavec.arrow.ArrowConverter.toArrowColumnsTimeSeries",
	"Comment": "convert a set of input strings to arrow columnsfor a time series.",
	"Method": "List<FieldVector> toArrowColumnsTimeSeries(BufferAllocator bufferAllocator,Schema schema,List<List<List<Writable>>> dataVecRecord){\r\n    return toArrowColumnsTimeSeriesHelper(bufferAllocator, schema, dataVecRecord);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.getEdgesSpannedByVertices",
	"Comment": "given a set of nodes from a semanticgraph, returns the set ofedges that are spanned between these nodes.",
	"Method": "Collection<SemanticGraphEdge> getEdgesSpannedByVertices(Collection<IndexedWord> nodes,SemanticGraph sg){\r\n    Collection<SemanticGraphEdge> ret = Generics.newHashSet();\r\n    for (IndexedWord n1 : nodes) for (IndexedWord n2 : nodes) {\r\n        if (n1 != n2) {\r\n            Collection<SemanticGraphEdge> edges = sg.getAllEdges(n1, n2);\r\n            if (edges != null)\r\n                ret.addAll(edges);\r\n        }\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.MetaClass.createFactory",
	"Comment": "creates a factory for producing instances of this class from aconstructor taking objects of the types given",
	"Method": "ClassFactory<E> createFactory(Class<?> classes,ClassFactory<E> createFactory,String classes,ClassFactory<E> createFactory,Object objects){\r\n    try {\r\n        return new ClassFactory(classname, objects);\r\n    } catch (ClassCreationException e) {\r\n        throw e;\r\n    } catch (Exception e) {\r\n        throw new ClassCreationException(e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.AbstractListProcessor.processLists",
	"Comment": "process a list of lists of tokens.for example this might be a list of lists of words.",
	"Method": "List<List<OUT>> processLists(List<List<IN>> lists){\r\n    List<List<OUT>> result = new ArrayList(lists.size());\r\n    for (List<IN> list : lists) {\r\n        List<OUT> outList = process(list);\r\n        result.add(outList);\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.TokenizerAnnotator.annotate",
	"Comment": "does the actual work of splitting textannotation into corelabels,which are then attached to the tokensannotation.",
	"Method": "void annotate(Annotation annotation){\r\n    if (VERBOSE) {\r\n        log.info(\"Tokenizing ... \");\r\n    }\r\n    if (useSegmenter) {\r\n        segmenterAnnotator.annotate(annotation);\r\n        setTokenBeginTokenEnd(annotation.get(CoreAnnotations.TokensAnnotation.class));\r\n        setNewlineStatus(annotation.get(CoreAnnotations.TokensAnnotation.class));\r\n        return;\r\n    }\r\n    if (annotation.containsKey(CoreAnnotations.TextAnnotation.class)) {\r\n        String text = annotation.get(CoreAnnotations.TextAnnotation.class);\r\n        Reader r = new StringReader(text);\r\n        List<CoreLabel> tokens = getTokenizer(r).tokenize();\r\n        setNewlineStatus(tokens);\r\n        setTokenBeginTokenEnd(tokens);\r\n        annotation.set(CoreAnnotations.TokensAnnotation.class, tokens);\r\n        if (VERBOSE) {\r\n            log.info(\"done.\");\r\n            log.info(\"Tokens: \" + annotation.get(CoreAnnotations.TokensAnnotation.class));\r\n        }\r\n    } else {\r\n        throw new RuntimeException(\"Tokenizer unable to find text in annotation: \" + annotation);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.zero",
	"Comment": "create a new variable with the specified shape, with all values initialized to 0",
	"Method": "SDVariable zero(String name,long shape,SDVariable zero,String name,int[] shape){\r\n    return var(name, ArrayUtil.toLongArray(shape), new ZeroInitScheme());\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.recurrent.KerasLSTM.getInputPreprocessor",
	"Comment": "gets appropriate dl4j inputpreprocessor for given inputtypes.",
	"Method": "InputPreProcessor getInputPreprocessor(InputType inputType){\r\n    if (inputType.length > 1 && inputType.length != 3)\r\n        throw new InvalidKerasConfigurationException(\"Keras LSTM layer accepts only one single input\" + \"or three (input to LSTM and two states tensors, but \" + \"received \" + inputType.length + \".\");\r\n    return InputTypeUtil.getPreprocessorForInputTypeRnnLayers(inputType[0], layerName);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.samediff.SameDiffOutputLayer.params",
	"Comment": "returns the parameters of the neural network as a flattened row vector",
	"Method": "INDArray params(){\r\n    return params;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.randomprojection.RPUtils.getCandidates",
	"Comment": "get the search candidates as indices given the inputand similarity function",
	"Method": "List<Integer> getCandidates(INDArray x,List<RPTree> roots,String similarityFunction){\r\n    Set<Integer> ret = new LinkedHashSet();\r\n    for (RPTree tree : roots) {\r\n        RPNode root = tree.getRoot();\r\n        RPNode query = query(root, tree.getRpHyperPlanes(), x, similarityFunction);\r\n        ret.addAll(query.getIndices());\r\n    }\r\n    return new ArrayList(ret);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.BobChrisTreeNormalizer.normalizeTerminal",
	"Comment": "normalizes a leaf contents.this implementation interns the leaf.",
	"Method": "String normalizeTerminal(String leaf){\r\n    return leaf.intern();\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.CoreDocument.buildDocumentEntityMentionsList",
	"Comment": "build a list of all entity mentions in the document from the sentences",
	"Method": "void buildDocumentEntityMentionsList(){\r\n    entityMentions = sentences.stream().flatMap(sentence -> sentence.entityMentions().stream()).collect(Collectors.toList());\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.tsurgeon.TsurgeonTest.testBackReference",
	"Comment": "this was buggy in 2009 since the label started pointing to the node with ~n on it.",
	"Method": "void testBackReference(){\r\n    TregexPattern tregex = TregexPattern.compile(\"__ <1 B=n <2 ~n\");\r\n    TsurgeonPattern tsurgeon = Tsurgeon.parseOperation(\"relabel n X\");\r\n    runTest(tregex, tsurgeon, \"(A (B w) (B w))\", \"(A (X w) (B w))\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.TregexParser.Node",
	"Comment": "pertains to this node gets passed all the way down to the description node",
	"Method": "DescriptionPattern Node(Relation r){\r\n    DescriptionPattern node;\r\n    switch((jj_ntk == -1) ? jj_ntk_f() : jj_ntk) {\r\n        case 14:\r\n            {\r\n                jj_consume_token(14);\r\n                node = SubNode(r);\r\n                jj_consume_token(15);\r\n                break;\r\n            }\r\n        case IDENTIFIER:\r\n        case BLANK:\r\n        case REGEX:\r\n        case 16:\r\n        case 17:\r\n        case 20:\r\n        case 21:\r\n            {\r\n                node = ModDescription(r);\r\n                break;\r\n            }\r\n        default:\r\n            jj_la1[0] = jj_gen;\r\n            jj_consume_token(-1);\r\n            throw new ParseException();\r\n    }\r\n    {\r\n        if (\"\" != null)\r\n            return node;\r\n    }\r\n    throw new Error(\"Missing return statement in function\");\r\n}"
}, {
	"Path": "org.datavec.api.records.reader.impl.TestDb.buildCoffeeTable",
	"Comment": "the buildcoffeetable method creates the coffee table and adds some rows to it.",
	"Method": "void buildCoffeeTable(Connection conn){\r\n    try {\r\n        Statement stmt = conn.createStatement();\r\n        stmt.execute(\"CREATE TABLE Coffee (\" + \"Description CHAR(25), \" + \"ProdNum CHAR(10) NOT NULL PRIMARY KEY, \" + \"Price DOUBLE \" + \")\");\r\n        stmt.execute(\"INSERT INTO Coffee VALUES ( \" + \"'Bolivian Dark', \" + \"'14-001', \" + \"8.95 )\");\r\n        stmt.execute(\"INSERT INTO Coffee VALUES ( \" + \"'Bolivian Medium', \" + \"'14-002', \" + \"8.95 )\");\r\n    } catch (SQLException ex) {\r\n        System.out.println(\"ERROR: \" + ex.getMessage());\r\n        ex.printStackTrace();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.SimpleTree.children",
	"Comment": "returns an array of children for the current node, or nullif it is a leaf.",
	"Method": "Tree[] children(){\r\n    return daughterTrees;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.memory.MemoryReport.cacheModeMapFor",
	"Comment": "get a map of cachemode with all keys associated with the specified value",
	"Method": "Map<CacheMode, Long> cacheModeMapFor(long value){\r\n    if (value == 0) {\r\n        return CACHE_MODE_ALL_ZEROS;\r\n    }\r\n    Map<CacheMode, Long> m = new HashMap();\r\n    for (CacheMode cm : CacheMode.values()) {\r\n        m.put(cm, value);\r\n    }\r\n    return m;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.convolution.CnnLossLayer.computeScoreForExamples",
	"Comment": "compute the score for each example individually, after labels and input have been set.",
	"Method": "INDArray computeScoreForExamples(double fullNetworkL1,double fullNetworkL2,LayerWorkspaceMgr workspaceMgr){\r\n    if (input == null || labels == null)\r\n        throw new IllegalStateException(\"Cannot calculate score without input and labels \" + layerId());\r\n    INDArray input2d = ConvolutionUtils.reshape4dTo2d(input, workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n    INDArray labels2d = ConvolutionUtils.reshape4dTo2d(labels, workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n    INDArray maskReshaped = ConvolutionUtils.reshapeMaskIfRequired(maskArray, input, workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n    ILossFunction lossFunction = layerConf().getLossFn();\r\n    INDArray scoreArray = lossFunction.computeScoreArray(labels2d, input2d, layerConf().getActivationFn(), maskReshaped);\r\n    val newShape = input.shape().clone();\r\n    newShape[1] = 1;\r\n    INDArray scoreArrayTs = ConvolutionUtils.reshape2dTo4d(scoreArray, ArrayUtil.toInts(newShape), workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n    INDArray summedScores = scoreArrayTs.sum(1, 2, 3);\r\n    double l1l2 = fullNetworkL1 + fullNetworkL2;\r\n    if (l1l2 != 0.0) {\r\n        summedScores.addi(l1l2);\r\n    }\r\n    return workspaceMgr.leverageTo(ArrayType.ACTIVATIONS, summedScores);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.tuebadz.TueBaDZLanguagePack.main",
	"Comment": "prints a few aspects of the treebanklanguagepack, just for debugging.",
	"Method": "void main(String[] args){\r\n    TreebankLanguagePack tlp = new TueBaDZLanguagePack();\r\n    System.out.println(\"Start symbol: \" + tlp.startSymbol());\r\n    String start = tlp.startSymbol();\r\n    System.out.println(\"Should be true: \" + (tlp.isStartSymbol(start)));\r\n    String[] strs = new String[] { \"-\", \"-LLB-\", \"NP-2\", \"NP=3\", \"NP-LGS\", \"NP-TMP=3\", \"CARD-HD\" };\r\n    for (String str : strs) {\r\n        System.out.println(\"String: \" + str + \" basic: \" + tlp.basicCategory(str) + \" basicAndFunc: \" + tlp.categoryAndFunction(str));\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.preprocessing.text.KerasTokenizer.textsToSequences",
	"Comment": "transforms a bunch of texts into their index representations.",
	"Method": "Integer[][] textsToSequences(String[] texts){\r\n    Integer oovTokenIndex = wordIndex.get(outOfVocabularyToken);\r\n    String[] wordSequence;\r\n    ArrayList<Integer[]> sequences = new ArrayList();\r\n    for (String text : texts) {\r\n        if (charLevel) {\r\n            if (lower) {\r\n                text = text.toLowerCase();\r\n            }\r\n            wordSequence = text.split(\"\");\r\n        } else {\r\n            wordSequence = textToWordSequence(text, filters, lower, split);\r\n        }\r\n        ArrayList<Integer> indexVector = new ArrayList();\r\n        for (String word : wordSequence) {\r\n            if (wordIndex.containsKey(word)) {\r\n                int index = wordIndex.get(word);\r\n                if (numWords != null && index >= numWords) {\r\n                    if (oovTokenIndex != null)\r\n                        indexVector.add(oovTokenIndex);\r\n                } else {\r\n                    indexVector.add(index);\r\n                }\r\n            } else if (oovTokenIndex != null) {\r\n                indexVector.add(oovTokenIndex);\r\n            }\r\n        }\r\n        Integer[] indices = indexVector.toArray(new Integer[indexVector.size()]);\r\n        sequences.add(indices);\r\n    }\r\n    return sequences.toArray(new Integer[sequences.size()][]);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Treebank.encoding",
	"Comment": "returns the encoding in use for treebank file bytestream access.",
	"Method": "String encoding(){\r\n    return encoding;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.SentenceAlgorithms.loopyDependencyPathBetween",
	"Comment": "run a proper bfs over a dependency graph, finding the shortest path between two vertices.",
	"Method": "List<String> loopyDependencyPathBetween(int start,int end,Optional<Function<Sentence, List<String>>> selector){\r\n    SemanticGraph graph = this.sentence.dependencyGraph();\r\n    IndexedWord[] indexedWords = new IndexedWord[this.sentence.length()];\r\n    for (IndexedWord vertex : graph.vertexSet()) {\r\n        indexedWords[vertex.index() - 1] = vertex;\r\n    }\r\n    BitSet seen = new BitSet();\r\n    int[] backpointers = new int[sentence.length()];\r\n    Arrays.fill(backpointers, -1);\r\n    Queue<IndexedWord> fringe = new LinkedList();\r\n    fringe.add(indexedWords[start]);\r\n    while (!fringe.isEmpty()) {\r\n        IndexedWord vertex = fringe.poll();\r\n        int vertexIndex = vertex.index() - 1;\r\n        if (seen.get(vertexIndex)) {\r\n            continue;\r\n        }\r\n        seen.set(vertexIndex);\r\n        for (SemanticGraphEdge inEdge : graph.incomingEdgeIterable(vertex)) {\r\n            IndexedWord governor = inEdge.getGovernor();\r\n            int govIndex = governor.index() - 1;\r\n            if (!seen.get(govIndex)) {\r\n                backpointers[govIndex] = vertexIndex;\r\n                if (govIndex == end) {\r\n                    break;\r\n                } else {\r\n                    fringe.add(governor);\r\n                }\r\n            }\r\n        }\r\n        for (SemanticGraphEdge outEdge : graph.outgoingEdgeIterable(vertex)) {\r\n            IndexedWord dependent = outEdge.getDependent();\r\n            int depIndex = dependent.index() - 1;\r\n            if (!seen.get(depIndex)) {\r\n                backpointers[depIndex] = vertexIndex;\r\n                if (depIndex == end) {\r\n                    break;\r\n                } else {\r\n                    fringe.add(dependent);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    ArrayList<String> path = new ArrayList();\r\n    Optional<List<String>> words = selector.map(x -> x.apply(sentence));\r\n    int vertex = end;\r\n    while (vertex != start) {\r\n        if (words.isPresent()) {\r\n            path.add(words.get().get(vertex));\r\n        }\r\n        for (SemanticGraphEdge inEdge : graph.incomingEdgeIterable(indexedWords[vertex])) {\r\n            int governor = inEdge.getGovernor().index() - 1;\r\n            if (backpointers[vertex] == governor) {\r\n                path.add(\"-\" + inEdge.getRelation().toString() + \"->\");\r\n                break;\r\n            }\r\n        }\r\n        for (SemanticGraphEdge outEdge : graph.outgoingEdgeIterable(indexedWords[vertex])) {\r\n            int dependent = outEdge.getDependent().index() - 1;\r\n            if (backpointers[vertex] == dependent) {\r\n                path.add(\"<-\" + outEdge.getRelation().toString() + \"-\");\r\n                break;\r\n            }\r\n        }\r\n        vertex = backpointers[vertex];\r\n    }\r\n    words.ifPresent(strings -> path.add(strings.get(start)));\r\n    Collections.reverse(path);\r\n    return path;\r\n}"
}, {
	"Path": "edu.stanford.nlp.time.JodaTimeUtils.maximumValue",
	"Comment": "return the maximum value of a field, closest to the reference time",
	"Method": "int maximumValue(DateTimeFieldType type,ReadableDateTime reference){\r\n    return reference.toDateTime().property(type).getMaximumValue();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.FilePanel.getActiveTreebanks",
	"Comment": "returns all treebanks corresponding to the files stored in the panel thatare selected",
	"Method": "List<FileTreeNode> getActiveTreebanks(){\r\n    List<FileTreeNode> active = new ArrayList();\r\n    setActiveTreebanksFromParent(active, treeModel.getRoot());\r\n    return active;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.Alignment.getMap",
	"Comment": "returns the map from hypothesis words to text words for thisalignment.",
	"Method": "Map<IndexedWord, IndexedWord> getMap(){\r\n    return map;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.SimpleCharStream.adjustBeginLineColumn",
	"Comment": "method to adjust line and column numbers for the start of a token.",
	"Method": "void adjustBeginLineColumn(int newLine,int newCol){\r\n    int start = tokenBegin;\r\n    int len;\r\n    if (bufpos >= tokenBegin) {\r\n        len = bufpos - tokenBegin + inBuf + 1;\r\n    } else {\r\n        len = bufsize - tokenBegin + bufpos + 1 + inBuf;\r\n    }\r\n    int i = 0, j = 0, k = 0;\r\n    int nextColDiff = 0, columnDiff = 0;\r\n    while (i < len && bufline[j = start % bufsize] == bufline[k = ++start % bufsize]) {\r\n        bufline[j] = newLine;\r\n        nextColDiff = columnDiff + bufcolumn[k] - bufcolumn[j];\r\n        bufcolumn[j] = newCol + columnDiff;\r\n        columnDiff = nextColDiff;\r\n        i++;\r\n    }\r\n    if (i < len) {\r\n        bufline[j] = newLine++;\r\n        bufcolumn[j] = newCol + columnDiff;\r\n        while (i++ < len) {\r\n            if (bufline[j = start % bufsize] != bufline[++start % bufsize])\r\n                bufline[j] = newLine++;\r\n            else\r\n                bufline[j] = newLine;\r\n        }\r\n    }\r\n    line = bufline[j];\r\n    column = bufcolumn[j];\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.MapFactory.treeMapFactory",
	"Comment": "return a mapfactory that returns a treemap with the given comparator.",
	"Method": "MapFactory<K, V> treeMapFactory(MapFactory<K, V> treeMapFactory,Comparator<? super K> comparator){\r\n    return new TreeMapFactory(comparator);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.constant",
	"Comment": "return a variable of given shape in which all values have a given constant value.",
	"Method": "SDVariable constant(SDVariable value,long shape,SDVariable constant,String name,SDVariable value,long shape){\r\n    SDVariable ret = f().constant(value, shape);\r\n    return updateVariableNameAndReference(ret, name);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.core.KerasPermute.getInputPreprocessor",
	"Comment": "gets appropriate dl4j inputpreprocessor for given inputtypes.",
	"Method": "InputPreProcessor getInputPreprocessor(InputType inputType){\r\n    if (inputType.length > 1)\r\n        throw new InvalidKerasConfigurationException(\"Keras Permute layer accepts only one input (received \" + inputType.length + \")\");\r\n    InputPreProcessor preprocessor = null;\r\n    if (inputType[0] instanceof InputType.InputTypeConvolutional) {\r\n        switch(this.getDimOrder()) {\r\n            case THEANO:\r\n                preprocessor = new PermutePreprocessor(permutationIndices);\r\n                break;\r\n            case NONE:\r\n            case TENSORFLOW:\r\n                permutationIndices = new int[] { permutationIndices[2], permutationIndices[0], permutationIndices[1] };\r\n                preprocessor = new PermutePreprocessor(new int[] { 1, 3, 2 });\r\n        }\r\n    } else if (inputType[0] instanceof InputType.InputTypeRecurrent) {\r\n        if (Arrays.equals(permutationIndices, new int[] { 2, 1 }))\r\n            preprocessor = new PermutePreprocessor(permutationIndices);\r\n        else\r\n            throw new InvalidKerasConfigurationException(\"For RNN type input data, permutation dims have to be\" + \"(2, 1) in Permute layer, got \" + Arrays.toString(permutationIndices));\r\n    } else if (inputType[0] instanceof InputType.InputTypeFeedForward) {\r\n        preprocessor = null;\r\n    } else {\r\n        throw new InvalidKerasConfigurationException(\"Input type not supported: \" + inputType[0]);\r\n    }\r\n    return preprocessor;\r\n}"
}, {
	"Path": "com.atilika.kuromoji.viterbi.ViterbiBuilder.repairBrokenLatticeBefore",
	"Comment": "tries to repair the lattice by creating and adding an additional viterbi node to the left of the newlyinserted user dictionary entry by using the substring of the node in the lattice that overlaps the least",
	"Method": "void repairBrokenLatticeBefore(ViterbiLattice lattice,int index){\r\n    ViterbiNode[][] nodeStartIndices = lattice.getStartIndexArr();\r\n    for (int startIndex = index; startIndex > 0; startIndex--) {\r\n        if (nodeStartIndices[startIndex] != null) {\r\n            ViterbiNode glueBase = findGlueNodeCandidate(index, nodeStartIndices[startIndex], startIndex);\r\n            if (glueBase != null) {\r\n                int length = index + 1 - startIndex;\r\n                String surface = glueBase.getSurface().substring(0, length);\r\n                ViterbiNode glueNode = createGlueNode(startIndex, glueBase, surface);\r\n                lattice.addNode(glueNode, startIndex, startIndex + glueNode.getSurface().length());\r\n                return;\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.text.tokenization.tokenizerfactory.NGramTokenizerFactory.getTokenPreProcessor",
	"Comment": "returns tokenpreprocessor set for this tokenizerfactory instance",
	"Method": "TokenPreProcess getTokenPreProcessor(){\r\n    return preProcess;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseSparseNDArrayCOO.getUnderlyingIndicesOf",
	"Comment": "returns the underlying indices of the element of the given indexsuch as there really are in the original ndarray",
	"Method": "DataBuffer getUnderlyingIndicesOf(int i){\r\n    int from = underlyingRank() * i;\r\n    int[] res = new int[underlyingRank()];\r\n    for (int j = 0; j < underlyingRank(); j++) {\r\n        res[j] = indices.getInt(from + j);\r\n    }\r\n    return Nd4j.getDataBufferFactory().createInt(res);\r\n}"
}, {
	"Path": "org.datavec.api.transform.metadata.BooleanMetaData.isValid",
	"Comment": "is the given object valid for this column,given the column type and anyrestrictions given by thecolumnmetadata object?",
	"Method": "boolean isValid(Writable writable,boolean isValid,Object input){\r\n    boolean value;\r\n    try {\r\n        value = Boolean.parseBoolean(input.toString());\r\n    } catch (NumberFormatException e) {\r\n        return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getTrimmedStringCollection",
	"Comment": "get the comma delimited values of the name property asa collection of strings, trimmed of the leading and trailing whitespace.if no such property is specified then empty collection is returned.",
	"Method": "Collection<String> getTrimmedStringCollection(String name){\r\n    String valueString = get(name);\r\n    if (null == valueString) {\r\n        return Collections.emptyList();\r\n    }\r\n    return StringUtils.getTrimmedStringCollection(valueString);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.WordVectorSerializerTest.testStaticLoaderGoogleModel",
	"Comment": "this method here is only to test real google model few gigabytes worthkeep it ignored, since it requirs full google model being present in system, which is 1.6gb compressed",
	"Method": "void testStaticLoaderGoogleModel(){\r\n    logger.info(\"Executor name: {}\", Nd4j.getExecutioner().getClass().getSimpleName());\r\n    long time1 = System.currentTimeMillis();\r\n    WordVectors vectors = WordVectorSerializer.loadStaticModel(new File(\"C:\\\\Users\\\\raver\\\\develop\\\\GoogleNews-vectors-negative300.bin.gz\"));\r\n    long time2 = System.currentTimeMillis();\r\n    logger.info(\"Loading time: {} ms\", (time2 - time1));\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.TaggerConfig.getTagInside",
	"Comment": "return a regex of xml elements to tag inside of.this may return an empty string, but never null.",
	"Method": "String getTagInside(){\r\n    String str = getProperty(\"tagInside\");\r\n    if (str == null) {\r\n        return \"\";\r\n    }\r\n    return str;\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.JointMultiDataSetIterator.setPreProcessor",
	"Comment": "set the preprocessor to be applied to each multidataset, before each multidataset is returned.",
	"Method": "void setPreProcessor(MultiDataSetPreProcessor preProcessor){\r\n    this.preProcessor = preProcessor;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.choleskyFromMatrix",
	"Comment": "this will return the cholesky decomposition ofthe given matrix",
	"Method": "CholeskyDecomposition choleskyFromMatrix(RealMatrix m){\r\n    return new CholeskyDecomposition(m);\r\n}"
}, {
	"Path": "org.datavec.local.transforms.LocalTransformExecutor.convertStringInput",
	"Comment": "convert a string time series tothe proper writable set based on the schema.note that this does not use arrow.this just uses normal writable objects.",
	"Method": "List<List<Writable>> convertStringInput(List<List<String>> stringInput,Schema schema){\r\n    List<List<Writable>> ret = new ArrayList();\r\n    List<List<Writable>> timeStepAdd = new ArrayList();\r\n    for (int j = 0; j < stringInput.size(); j++) {\r\n        List<String> record = stringInput.get(j);\r\n        List<Writable> recordAdd = new ArrayList();\r\n        for (int k = 0; k < record.size(); k++) {\r\n            switch(schema.getType(k)) {\r\n                case Double:\r\n                    recordAdd.add(new DoubleWritable(Double.parseDouble(record.get(k))));\r\n                    break;\r\n                case Float:\r\n                    recordAdd.add(new FloatWritable(Float.parseFloat(record.get(k))));\r\n                    break;\r\n                case Integer:\r\n                    recordAdd.add(new IntWritable(Integer.parseInt(record.get(k))));\r\n                    break;\r\n                case Long:\r\n                    recordAdd.add(new LongWritable(Long.parseLong(record.get(k))));\r\n                    break;\r\n                case String:\r\n                    recordAdd.add(new Text(record.get(k)));\r\n                    break;\r\n                case Time:\r\n                    recordAdd.add(new LongWritable(Long.parseLong(record.get(k))));\r\n                    break;\r\n            }\r\n        }\r\n        timeStepAdd.add(recordAdd);\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.deeplearning4j.text.documentiterator.SimpleLabelAwareIterator.getLabelsSource",
	"Comment": "this method returns labelssource instance, containing all labels derived from this iterator",
	"Method": "LabelsSource getLabelsSource(){\r\n    return labels;\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getClasses",
	"Comment": "get the value of the name propertyas an array of class.the value of the property specifies a list of comma separated class names.if no such property is specified, then defaultvalue isreturned.",
	"Method": "Class<?>[] getClasses(String name,Class<?> defaultValue){\r\n    String[] classnames = getStrings(name);\r\n    if (classnames == null)\r\n        return defaultValue;\r\n    try {\r\n        Class<?>[] classes = new Class<?>[classnames.length];\r\n        for (int i = 0; i < classnames.length; i++) {\r\n            classes[i] = getClassByName(classnames[i]);\r\n        }\r\n        return classes;\r\n    } catch (ClassNotFoundException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.formats.input.impl.ListStringInputFormat.toFloat",
	"Comment": "convert writable to float. whether this is supported depends on the specific writable.",
	"Method": "float toFloat(){\r\n    return 0;\r\n}"
}, {
	"Path": "org.datavec.api.transform.filter.FilterInvalidValues.transform",
	"Comment": "get the output schema for this transformation, given an input schema",
	"Method": "Schema transform(Schema inputSchema){\r\n    return inputSchema;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.sequencevectors.SequenceVectors.buildVocab",
	"Comment": "builds vocabulary from provided sequenceiterator instance",
	"Method": "void buildVocab(){\r\n    val constructor = new VocabConstructor.Builder<T>().addSource(iterator, minWordFrequency).setTargetVocabCache(vocab).fetchLabels(trainSequenceVectors).setStopWords(stopWords).enableScavenger(enableScavenger).setEntriesLimit(vocabLimit).allowParallelTokenization(configuration.isAllowParallelTokenization()).setUnk(useUnknown && unknownElement != null ? unknownElement : null).build();\r\n    if (existingModel != null && lookupTable instanceof InMemoryLookupTable && existingModel.lookupTable() instanceof InMemoryLookupTable) {\r\n        log.info(\"Merging existing vocabulary into the current one...\");\r\n        constructor.buildMergedVocabulary(existingModel, true);\r\n        ((InMemoryLookupTable<VocabWord>) lookupTable).consume((InMemoryLookupTable<VocabWord>) existingModel.lookupTable());\r\n    } else {\r\n        log.info(\"Starting vocabulary building...\");\r\n        constructor.buildJointVocabulary(false, true);\r\n        if (vocab.numWords() / constructor.getNumberOfSequences() > 1000) {\r\n            log.warn(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\");\r\n            log.warn(\"!                                                                                       !\");\r\n            log.warn(\"! Your input looks malformed: number of sentences is too low, model accuracy may suffer !\");\r\n            log.warn(\"!                                                                                       !\");\r\n            log.warn(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\");\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.datavec.audio.fingerprint.MapRankInteger.locate",
	"Comment": "sort the partitions by quick sort, and locate the target index",
	"Method": "void locate(int[] array,int left,int right,int index){\r\n    int mid = (left + right) / 2;\r\n    if (right == left) {\r\n        return;\r\n    }\r\n    if (left < right) {\r\n        int s = array[mid];\r\n        int i = left - 1;\r\n        int j = right + 1;\r\n        while (true) {\r\n            while (array[++i] < s) ;\r\n            while (array[--j] > s) ;\r\n            if (i >= j)\r\n                break;\r\n            swap(array, i, j);\r\n        }\r\n        if (i > index) {\r\n            locate(array, left, i - 1, index);\r\n        } else {\r\n            locate(array, j + 1, right, index);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.optimize.api.data.DataSetIteratorFactoryProvider.testData",
	"Comment": "get training data given some parameters for the data. data parameters mapis used to specify things like batchsize data preprocessing",
	"Method": "DataSetIteratorFactory testData(Map<String, Object> dataParameters){\r\n    return create(dataParameters);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.UniversalSemanticHeadFinder.determineNonTrivialHead",
	"Comment": "determine which daughter of the current parse tree is thehead.it assumes that the daughters already have had theirheads determined.uses special rule for vp heads",
	"Method": "Tree determineNonTrivialHead(Tree t,Tree parent){\r\n    String motherCat = tlp.basicCategory(t.label().value());\r\n    if (DEBUG) {\r\n        log.info(\"At \" + motherCat + \", my parent is \" + parent);\r\n    }\r\n    if (motherCat.equals(\"CONJP\")) {\r\n        for (TregexPattern pattern : headOfConjpTregex) {\r\n            TregexMatcher matcher = pattern.matcher(t);\r\n            if (matcher.matchesAt(t)) {\r\n                return matcher.getNode(\"head\");\r\n            }\r\n        }\r\n    }\r\n    if (motherCat.equals(\"SBARQ\") || motherCat.equals(\"SINV\")) {\r\n        if (!makeCopulaHead) {\r\n            for (TregexPattern pattern : headOfCopulaTregex) {\r\n                TregexMatcher matcher = pattern.matcher(t);\r\n                if (matcher.matchesAt(t)) {\r\n                    return matcher.getNode(\"head\");\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if ((motherCat.equals(\"VP\") || motherCat.equals(\"SQ\") || motherCat.equals(\"SINV\"))) {\r\n        Tree[] kids = t.children();\r\n        if (DEBUG) {\r\n            log.info(\"Semantic head finder: at VP\");\r\n            log.info(\"Class is \" + t.getClass().getName());\r\n            t.pennPrint(System.err);\r\n        }\r\n        Tree[] tmpFilteredChildren = null;\r\n        if (hasVerbalAuxiliary(kids, verbalAuxiliaries, true) || hasPassiveProgressiveAuxiliary(kids)) {\r\n            String[] how;\r\n            if (hasVerbalAuxiliary(kids, copulars, true)) {\r\n                how = new String[] { \"left\", \"VP\", \"ADJP\" };\r\n            } else {\r\n                how = new String[] { \"left\", \"VP\" };\r\n            }\r\n            if (tmpFilteredChildren == null) {\r\n                tmpFilteredChildren = ArrayUtils.filter(kids, REMOVE_TMP_AND_ADV);\r\n            }\r\n            Tree pti = traverseLocate(tmpFilteredChildren, how, false);\r\n            if (DEBUG) {\r\n                log.info(\"Determined head (case 1) for \" + t.value() + \" is: \" + pti);\r\n            }\r\n            if (pti != null) {\r\n                return pti;\r\n            }\r\n        }\r\n        if (hasVerbalAuxiliary(kids, copulars, false) && !isExistential(t, parent) && !isWHQ(t, parent)) {\r\n            String[][] how;\r\n            if (motherCat.equals(\"SQ\")) {\r\n                how = new String[][] { { \"right\", \"VP\", \"ADJP\", \"NP\", \"UCP\", \"PP\", \"WHADJP\", \"WHNP\" } };\r\n            } else {\r\n                how = new String[][] { { \"left\", \"VP\", \"ADJP\", \"NP\", \"UCP\", \"PP\", \"WHADJP\", \"WHNP\" } };\r\n            }\r\n            if (tmpFilteredChildren == null) {\r\n                tmpFilteredChildren = ArrayUtils.filter(kids, REMOVE_TMP_AND_ADV);\r\n            }\r\n            Tree pti = null;\r\n            for (int i = 0; i < how.length && pti == null; i++) {\r\n                pti = traverseLocate(tmpFilteredChildren, how[i], false);\r\n            }\r\n            if (motherCat.equals(\"SQ\") && pti != null && pti.label() != null && pti.label().value().startsWith(\"NP\")) {\r\n                boolean foundAnotherNp = false;\r\n                for (Tree kid : kids) {\r\n                    if (kid == pti) {\r\n                        break;\r\n                    } else if (kid.label() != null && kid.label().value().startsWith(\"NP\")) {\r\n                        foundAnotherNp = true;\r\n                        break;\r\n                    }\r\n                }\r\n                if (!foundAnotherNp) {\r\n                    pti = null;\r\n                }\r\n            }\r\n            if (DEBUG) {\r\n                log.info(\"Determined head (case 2) for \" + t.value() + \" is: \" + pti);\r\n            }\r\n            if (pti != null) {\r\n                return pti;\r\n            } else {\r\n                if (DEBUG) {\r\n                    log.info(\"------\");\r\n                    log.info(\"SemanticHeadFinder failed to reassign head for\");\r\n                    t.pennPrint(System.err);\r\n                    log.info(\"------\");\r\n                }\r\n            }\r\n        }\r\n    }\r\n    Tree hd = super.determineNonTrivialHead(t, parent);\r\n    if (DEBUG) {\r\n        log.info(\"Determined head (case 3) for \" + t.value() + \" is: \" + hd);\r\n    }\r\n    return hd;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.stringSimilarity",
	"Comment": "calculate string similarity with tfidf weights relative to each characterfrequency and how many times a character appears in a given string",
	"Method": "double stringSimilarity(String strings){\r\n    if (strings == null)\r\n        return 0;\r\n    Counter<String> counter = new Counter();\r\n    Counter<String> counter2 = new Counter();\r\n    for (int i = 0; i < strings[0].length(); i++) counter.incrementCount(String.valueOf(strings[0].charAt(i)), 1.0f);\r\n    for (int i = 0; i < strings[1].length(); i++) counter2.incrementCount(String.valueOf(strings[1].charAt(i)), 1.0f);\r\n    Set<String> v1 = counter.keySet();\r\n    Set<String> v2 = counter2.keySet();\r\n    Set<String> both = SetUtils.intersection(v1, v2);\r\n    double sclar = 0, norm1 = 0, norm2 = 0;\r\n    for (String k : both) sclar += counter.getCount(k) * counter2.getCount(k);\r\n    for (String k : v1) norm1 += counter.getCount(k) * counter.getCount(k);\r\n    for (String k : v2) norm2 += counter2.getCount(k) * counter2.getCount(k);\r\n    return sclar / Math.sqrt(norm1 * norm2);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.TregexGUI.setSaveHistoryEnabled",
	"Comment": "used to change the status of the savehistory file menu item to reflectwhether any search statistics are available to save",
	"Method": "void setSaveHistoryEnabled(boolean enabled){\r\n    if (saveHistory.isEnabled() != enabled)\r\n        saveHistory.setEnabled(enabled);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.isPhrasal",
	"Comment": "return whether this node is a phrasal node or not.a phrasal nodeis defined to be a node which is not a leaf or a preterminal.worded positively, this means that it must have two or more children,or one child that is not a leaf.",
	"Method": "boolean isPhrasal(){\r\n    Tree[] kids = children();\r\n    return !(kids == null || kids.length == 0 || (kids.length == 1 && kids[0].isLeaf()));\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProtoBuilder",
	"Comment": "create a protobuf builder, rather than a compiled protobuf.useful for, e.g., the simple corenlp interface.",
	"Method": "CoreNLPProtos.Token.Builder toProtoBuilder(CoreLabel coreLabel,Set<Class<?>> keysToSerialize,CoreNLPProtos.Sentence.Builder toProtoBuilder,CoreMap sentence,CoreNLPProtos.Sentence.Builder toProtoBuilder,CoreMap sentence,Set<Class<?>> keysToSerialize,CoreNLPProtos.Document.Builder toProtoBuilder,Annotation doc,CoreNLPProtos.Document.Builder toProtoBuilder,Annotation doc,Set<Class<?>> keysToSerialize){\r\n    CoreNLPProtos.Document.Builder builder = CoreNLPProtos.Document.newBuilder();\r\n    builder.setText(doc.get(TextAnnotation.class));\r\n    keysToSerialize.remove(TextAnnotation.class);\r\n    if (doc.containsKey(SectionsAnnotation.class)) {\r\n        builder.setXmlDoc(true);\r\n    } else {\r\n        builder.setXmlDoc(false);\r\n    }\r\n    if (doc.containsKey(SentencesAnnotation.class)) {\r\n        for (CoreMap sentence : doc.get(SentencesAnnotation.class)) {\r\n            builder.addSentence(toProto(sentence));\r\n        }\r\n        keysToSerialize.remove(SentencesAnnotation.class);\r\n    } else if (doc.containsKey(TokensAnnotation.class)) {\r\n        for (CoreLabel token : doc.get(TokensAnnotation.class)) {\r\n            builder.addSentencelessToken(toProto(token));\r\n        }\r\n    }\r\n    if (doc.containsKey(DocIDAnnotation.class)) {\r\n        builder.setDocID(doc.get(DocIDAnnotation.class));\r\n        keysToSerialize.remove(DocIDAnnotation.class);\r\n    }\r\n    if (doc.containsKey(DocDateAnnotation.class)) {\r\n        builder.setDocDate(doc.get(DocDateAnnotation.class));\r\n        keysToSerialize.remove(DocDateAnnotation.class);\r\n    }\r\n    if (doc.containsKey(CalendarAnnotation.class)) {\r\n        builder.setCalendar(doc.get(CalendarAnnotation.class).toInstant().toEpochMilli());\r\n        keysToSerialize.remove(CalendarAnnotation.class);\r\n    }\r\n    if (doc.containsKey(CorefChainAnnotation.class)) {\r\n        builder.setHasCorefAnnotation(true);\r\n        for (Map.Entry<Integer, CorefChain> chain : doc.get(CorefChainAnnotation.class).entrySet()) {\r\n            builder.addCorefChain(toProto(chain.getValue()));\r\n        }\r\n        keysToSerialize.remove(CorefChainAnnotation.class);\r\n    } else {\r\n        builder.setHasCorefAnnotation(false);\r\n    }\r\n    if (doc.containsKey(CorefMentionsAnnotation.class)) {\r\n        builder.setHasCorefMentionAnnotation(true);\r\n        for (Mention corefMention : doc.get(CorefMentionsAnnotation.class)) {\r\n            builder.addMentionsForCoref(toProto(corefMention));\r\n        }\r\n        keysToSerialize.remove(CorefMentionsAnnotation.class);\r\n    } else {\r\n        builder.setHasCorefMentionAnnotation(false);\r\n    }\r\n    if (doc.containsKey(QuotationsAnnotation.class)) {\r\n        for (CoreMap quote : doc.get(QuotationsAnnotation.class)) {\r\n            builder.addQuote(toProtoQuote(quote));\r\n        }\r\n        keysToSerialize.remove(QuotationsAnnotation.class);\r\n    }\r\n    if (doc.containsKey(MentionsAnnotation.class)) {\r\n        for (CoreMap mention : doc.get(MentionsAnnotation.class)) {\r\n            builder.addMentions(toProtoMention(mention));\r\n        }\r\n        keysToSerialize.remove(MentionsAnnotation.class);\r\n        builder.setHasEntityMentionsAnnotation(true);\r\n    } else {\r\n        builder.setHasEntityMentionsAnnotation(false);\r\n    }\r\n    if (doc.containsKey(EntityMentionToCorefMentionMappingAnnotation.class)) {\r\n        Map<Integer, Integer> entityMentionToCorefMention = doc.get(EntityMentionToCorefMentionMappingAnnotation.class);\r\n        int numEntityMentions = doc.get(MentionsAnnotation.class).size();\r\n        for (int entityMentionIndex = 0; entityMentionIndex < numEntityMentions; entityMentionIndex++) {\r\n            if (entityMentionToCorefMention.keySet().contains(entityMentionIndex)) {\r\n                builder.addEntityMentionToCorefMentionMappings(entityMentionToCorefMention.get(entityMentionIndex));\r\n            } else {\r\n                builder.addEntityMentionToCorefMentionMappings(-1);\r\n            }\r\n        }\r\n        keysToSerialize.remove(EntityMentionToCorefMentionMappingAnnotation.class);\r\n    }\r\n    if (doc.containsKey(CorefMentionToEntityMentionMappingAnnotation.class)) {\r\n        Map<Integer, Integer> corefMentionToEntityMention = doc.get(CorefMentionToEntityMentionMappingAnnotation.class);\r\n        int numCorefMentions = doc.get(CorefMentionsAnnotation.class).size();\r\n        for (int corefMentionIndex = 0; corefMentionIndex < numCorefMentions; corefMentionIndex++) {\r\n            if (corefMentionToEntityMention.keySet().contains(corefMentionIndex)) {\r\n                builder.addCorefMentionToEntityMentionMappings(corefMentionToEntityMention.get(corefMentionIndex));\r\n            } else {\r\n                builder.addCorefMentionToEntityMentionMappings(-1);\r\n            }\r\n        }\r\n        keysToSerialize.remove(CorefMentionToEntityMentionMappingAnnotation.class);\r\n    }\r\n    if (doc.containsKey(SegmenterCoreAnnotations.CharactersAnnotation.class)) {\r\n        for (CoreLabel c : doc.get(SegmenterCoreAnnotations.CharactersAnnotation.class)) {\r\n            builder.addCharacter(toProto(c));\r\n        }\r\n        keysToSerialize.remove(SegmenterCoreAnnotations.CharactersAnnotation.class);\r\n    }\r\n    if (doc.containsKey(SectionsAnnotation.class)) {\r\n        for (CoreMap section : doc.get(SectionsAnnotation.class)) {\r\n            builder.addSections(toProtoSection(section));\r\n        }\r\n        keysToSerialize.remove(SectionsAnnotation.class);\r\n    }\r\n    return builder;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArrayUtils.asSet",
	"Comment": "return a set containing the same elements as the specified array.",
	"Method": "Set<T> asSet(T[] a){\r\n    return Generics.newHashSet(Arrays.asList(a));\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.one",
	"Comment": "create a new variable with the specified shape, with all values initialized to 1.0",
	"Method": "SDVariable one(String name,int[] shape,SDVariable one,String name,long shape){\r\n    return var(name, shape, new ConstantInitScheme('f', 1.0));\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.quadtree.QuadTree.subDivide",
	"Comment": "create four children which fully divide this cell into four quads of equal area",
	"Method": "void subDivide(){\r\n    northWest = new QuadTree(this, data, new Cell(boundary.getX() - .5 * boundary.getHw(), boundary.getY() - .5 * boundary.getHh(), .5 * boundary.getHw(), .5 * boundary.getHh()));\r\n    northEast = new QuadTree(this, data, new Cell(boundary.getX() + .5 * boundary.getHw(), boundary.getY() - .5 * boundary.getHh(), .5 * boundary.getHw(), .5 * boundary.getHh()));\r\n    southWest = new QuadTree(this, data, new Cell(boundary.getX() - .5 * boundary.getHw(), boundary.getY() + .5 * boundary.getHh(), .5 * boundary.getHw(), .5 * boundary.getHh()));\r\n    southEast = new QuadTree(this, data, new Cell(boundary.getX() + .5 * boundary.getHw(), boundary.getY() + .5 * boundary.getHh(), .5 * boundary.getHw(), .5 * boundary.getHh()));\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.asArray",
	"Comment": "convert a counter to an array, the order of the array is random",
	"Method": "double[] asArray(Counter<E> counter,Index<E> index,double[] asArray,Counter<E> counter,Index<E> index,int dimension,double[] asArray,Counter<E> counter){\r\n    Set<E> keys = counter.keySet();\r\n    double[] array = new double[counter.size()];\r\n    int i = 0;\r\n    for (E key : keys) {\r\n        array[i] = counter.getCount(key);\r\n        i++;\r\n    }\r\n    return array;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractTreebankLanguagePack.morphFeatureSpec",
	"Comment": "returns a morphological feature specification for words in this language.",
	"Method": "MorphoFeatureSpecification morphFeatureSpec(){\r\n    return null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.WikidictAnnotator.normalizeTimex",
	"Comment": "try to normalize timex values to the form they would appear in the knowledge base.",
	"Method": "String normalizeTimex(String timex){\r\n    if (timex.contains(\"T\") && !\"PRESENT\".equals(timex)) {\r\n        return timex.substring(0, timex.indexOf(\"T\"));\r\n    } else {\r\n        return timex;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasModelBuilder.trainingJsonInputStream",
	"Comment": "provide training configuration as file input stream from json",
	"Method": "KerasModelBuilder trainingJsonInputStream(InputStream trainingJsonInputStream){\r\n    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\r\n    IOUtils.copy(trainingJsonInputStream, byteArrayOutputStream);\r\n    this.trainingJson = new String(byteArrayOutputStream.toByteArray());\r\n    return this;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.doEvaluation",
	"Comment": "perform evaluation using an arbitrary ievaluation instance.",
	"Method": "T[] doEvaluation(DataSetIterator iterator,T evaluations,T[] doEvaluation,MultiDataSetIterator iterator,T[] evaluations){\r\n    return doEvaluation(new MultiDataSetWrapperIterator(iterator), evaluations);\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseSparseNDArrayCOO.createSparseInformationBuffer",
	"Comment": "create a sparseinfo databuffer given rank if of the sparse matrix.",
	"Method": "DataBuffer createSparseInformationBuffer(int rank){\r\n    int[] flags = new int[rank];\r\n    long[] sparseOffsets = new long[rank];\r\n    int[] hiddenDimension = new int[] { -1 };\r\n    return Nd4j.getSparseInfoProvider().createSparseInformation(flags, sparseOffsets, hiddenDimension, rank);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CollectionUtils.transformAsSet",
	"Comment": "transforms the keyset of collection according to the given function and returns a set of the keys.",
	"Method": "Set<T2> transformAsSet(Collection<? extends T1> original,Function<T1, ? extends T2> f){\r\n    Set<T2> transformed = Generics.newHashSet();\r\n    for (T1 t : original) {\r\n        transformed.add(f.apply(t));\r\n    }\r\n    return transformed;\r\n}"
}, {
	"Path": "org.deeplearning4j.aws.s3.reader.S3Downloader.iterateBucket",
	"Comment": "iterate over individual buckets.returns input streams to each object.it is your responsibility to close the input streams",
	"Method": "Iterator<InputStream> iterateBucket(String bucket){\r\n    return new BucketIterator(bucket, this);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.BobChrisTreeNormalizer.normalizeNonterminal",
	"Comment": "normalizes a nonterminal contents.this implementation strips functional tags, etc. and interns thenonterminal.",
	"Method": "String normalizeNonterminal(String category){\r\n    return cleanUpLabel(category).intern();\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.updateVariableNamesAndReferences",
	"Comment": "updates the variable name property on the passed in variables, its reference in samediff, and returns the variable.",
	"Method": "SDVariable[] updateVariableNamesAndReferences(SDVariable[] variablesToUpdate,String[] newVariableNames){\r\n    int numVariables = variablesToUpdate.length;\r\n    SDVariable[] updatedVariables = new SDVariable[numVariables];\r\n    for (int i = 0; i < numVariables; i++) {\r\n        SDVariable varToUpdate = variablesToUpdate[i];\r\n        String name = newVariableNames == null ? null : newVariableNames[i];\r\n        updatedVariables[i] = updateVariableNameAndReference(varToUpdate, name);\r\n    }\r\n    return updatedVariables;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.sequencevectors.listeners.SerializingListener.validateEvent",
	"Comment": "this method is called prior each processevent call, to check if this specific vectorslistener implementation is viable for specific event",
	"Method": "boolean validateEvent(ListenerEvent event,long argument){\r\n    try {\r\n        locker.acquire();\r\n        if (event == targetEvent && argument % targetFrequency == 0) {\r\n            return true;\r\n        } else\r\n            return false;\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    } finally {\r\n        locker.release();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.tsurgeon.JJTTsurgeonParserState.popNode",
	"Comment": "returns the node on the top of the stack, and remove it from the stack.",
	"Method": "Node popNode(){\r\n    if (--sp < mk) {\r\n        mk = marks.remove(marks.size() - 1);\r\n    }\r\n    return nodes.remove(nodes.size() - 1);\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.FactoredSequenceListener.updateSequenceElement",
	"Comment": "informs this sequence model that the value of the element at position pos has changed.this allows this sequence model to update its internal model if desired.",
	"Method": "void updateSequenceElement(int[] sequence,int pos,int oldVal){\r\n    if (models != null) {\r\n        for (SequenceListener model : models) model.updateSequenceElement(sequence, pos, oldVal);\r\n        return;\r\n    }\r\n    model1.updateSequenceElement(sequence, pos, oldVal);\r\n    model2.updateSequenceElement(sequence, pos, oldVal);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.FilteringTreebank.apply",
	"Comment": "applies the treevisitor, but only to the trees that pass thefilter.applies the visitor to a copy of the tree.",
	"Method": "void apply(TreeVisitor tv){\r\n    if (VERBOSE) {\r\n        System.out.println(\"Applying \" + tv + \" to treebank\");\r\n    }\r\n    for (Tree t : treebank) {\r\n        if (!filter.test(t)) {\r\n            if (VERBOSE)\r\n                System.out.println(\"  Skipping \" + t);\r\n            continue;\r\n        }\r\n        Tree tmpT = t.deepCopy();\r\n        if (VERBOSE)\r\n            System.out.println(\"  Applying to \" + tmpT);\r\n        tv.visitTree(tmpT);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Characters.isSymbol",
	"Comment": "returns true if a character is a symbol, and falseotherwise.",
	"Method": "boolean isSymbol(char c){\r\n    int cType = Character.getType(c);\r\n    return cType == Character.MATH_SYMBOL || cType == Character.CURRENCY_SYMBOL || cType == Character.MODIFIER_SYMBOL || cType == Character.OTHER_SYMBOL;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.concurrent.AtomicDouble.getAndSet",
	"Comment": "atomically sets to the given value and returns the old value.",
	"Method": "double getAndSet(double newValue){\r\n    long next = doubleToRawLongBits(newValue);\r\n    return longBitsToDouble(updater.getAndSet(this, next));\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Document.getOrCreate",
	"Comment": "either get a custom annotator which was recently defined, or create it if it has never been defined.this method is synchronized to avoid race conditions when loading the annotators.",
	"Method": "Supplier<Annotator> getOrCreate(String name,Properties props,Supplier<Annotator> annotator){\r\n    StanfordCoreNLP.AnnotatorSignature key = new StanfordCoreNLP.AnnotatorSignature(name, PropertiesUtils.getSignature(name, props));\r\n    customAnnotators.register(name, props, StanfordCoreNLP.GLOBAL_ANNOTATOR_CACHE.computeIfAbsent(key, (sig) -> Lazy.cache(annotator)));\r\n    return () -> customAnnotators.get(name);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.UniversalSemanticHeadFinder.hasPassiveProgressiveAuxiliary",
	"Comment": "now overly complex so it deals with coordinations.maybe change this class to use tregrex?",
	"Method": "boolean hasPassiveProgressiveAuxiliary(Tree[] kids){\r\n    if (DEBUG) {\r\n        log.info(\"Checking for passive/progressive auxiliary\");\r\n    }\r\n    boolean foundPassiveVP = false;\r\n    boolean foundPassiveAux = false;\r\n    for (Tree kid : kids) {\r\n        if (DEBUG) {\r\n            log.info(\"  checking in \" + kid);\r\n        }\r\n        if (isVerbalAuxiliary(kid, passiveAuxiliaries, false)) {\r\n            foundPassiveAux = true;\r\n        } else if (kid.isPhrasal()) {\r\n            Label kidLabel = kid.label();\r\n            String cat = null;\r\n            if (kidLabel instanceof HasCategory) {\r\n                cat = ((HasCategory) kidLabel).category();\r\n            }\r\n            if (cat == null) {\r\n                cat = kid.value();\r\n            }\r\n            if (!cat.startsWith(\"VP\")) {\r\n                continue;\r\n            }\r\n            if (DEBUG) {\r\n                log.info(\"hasPassiveProgressiveAuxiliary found VP\");\r\n            }\r\n            Tree[] kidkids = kid.children();\r\n            boolean foundParticipleInVp = false;\r\n            for (Tree kidkid : kidkids) {\r\n                if (DEBUG) {\r\n                    log.info(\"  hasPassiveProgressiveAuxiliary examining \" + kidkid);\r\n                }\r\n                if (kidkid.isPreTerminal()) {\r\n                    Label kidkidLabel = kidkid.label();\r\n                    String tag = null;\r\n                    if (kidkidLabel instanceof HasTag) {\r\n                        tag = ((HasTag) kidkidLabel).tag();\r\n                    }\r\n                    if (tag == null) {\r\n                        tag = kidkid.value();\r\n                    }\r\n                    if (\"VBN\".equals(tag) || \"VBG\".equals(tag) || \"VBD\".equals(tag)) {\r\n                        foundPassiveVP = true;\r\n                        if (DEBUG) {\r\n                            log.info(\"hasPassiveAuxiliary found VBN/VBG/VBD VP\");\r\n                        }\r\n                        break;\r\n                    } else if (\"CC\".equals(tag) && foundParticipleInVp) {\r\n                        foundPassiveVP = true;\r\n                        if (DEBUG) {\r\n                            log.info(\"hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC\");\r\n                        }\r\n                        break;\r\n                    }\r\n                } else if (kidkid.isPhrasal()) {\r\n                    String catcat = null;\r\n                    if (kidLabel instanceof HasCategory) {\r\n                        catcat = ((HasCategory) kidLabel).category();\r\n                    }\r\n                    if (catcat == null) {\r\n                        catcat = kid.value();\r\n                    }\r\n                    if (\"VP\".equals(catcat)) {\r\n                        if (DEBUG) {\r\n                            log.info(\"hasPassiveAuxiliary found (VP (VP)), recursing\");\r\n                        }\r\n                        foundParticipleInVp = vpContainsParticiple(kidkid);\r\n                    } else if ((\"CONJP\".equals(catcat) || \"PRN\".equals(catcat)) && foundParticipleInVp) {\r\n                        foundPassiveVP = true;\r\n                        if (DEBUG) {\r\n                            log.info(\"hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP\");\r\n                        }\r\n                        break;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        if (foundPassiveAux && foundPassiveVP) {\r\n            break;\r\n        }\r\n    }\r\n    if (DEBUG) {\r\n        log.info(\"hasPassiveProgressiveAuxiliary returns \" + (foundPassiveAux && foundPassiveVP));\r\n    }\r\n    return foundPassiveAux && foundPassiveVP;\r\n}"
}, {
	"Path": "org.datavec.api.transform.sequence.window.ReduceSequenceByWindowTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return columnNames();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.tsurgeon.JJTTsurgeonParserState.rootNode",
	"Comment": "returns the root node of the ast.it only makes sense to call this after a successful parse.",
	"Method": "Node rootNode(){\r\n    return nodes.get(0);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.Word2Vec.setSentenceIterator",
	"Comment": "this method defines sentenceiterator instance, that will be used as training corpus source",
	"Method": "void setSentenceIterator(SentenceIterator iterator){\r\n    if (tokenizerFactory != null) {\r\n        SentenceTransformer transformer = new SentenceTransformer.Builder().iterator(iterator).tokenizerFactory(tokenizerFactory).allowMultithreading(configuration == null || configuration.isAllowParallelTokenization()).build();\r\n        this.iterator = new AbstractSequenceIterator.Builder(transformer).build();\r\n    } else\r\n        log.error(\"Please call setTokenizerFactory() prior to setSentenceIter() call.\");\r\n}"
}, {
	"Path": "org.deeplearning4j.integration.BaseDL4JTest.getProfilingMode",
	"Comment": "override this to set the profiling mode for the tests defined in the child class",
	"Method": "OpExecutioner.ProfilingMode getProfilingMode(){\r\n    return OpExecutioner.ProfilingMode.SCOPE_PANIC;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Distribution.dynamicCounterWithDirichletPrior",
	"Comment": "like normalizedcounterwithdirichletprior except probabilities arecomputed dynamically from the counter and prior instead of all at once up front.the main advantage of this is if you are making many distributions from relativelysparse counters using the same relatively dense prior, the prior is only representedonce, for major memory savings.",
	"Method": "Distribution<E> dynamicCounterWithDirichletPrior(Counter<E> c,Distribution<E> prior,double weight){\r\n    double totalWeight = c.totalCount() + weight;\r\n    Distribution<E> norm = new DynamicDistribution(prior, weight / totalWeight);\r\n    norm.counter = new ClassicCounter();\r\n    for (E key : c.keySet()) {\r\n        double count = c.getCount(key) / totalWeight;\r\n        prior.addToKeySet(key);\r\n        norm.counter.setCount(key, count);\r\n    }\r\n    norm.numberOfKeys = prior.numberOfKeys;\r\n    return norm;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiffOpExecutioner.thresholdEncode",
	"Comment": "this method encodes array as thresholds, updating input array at the same time",
	"Method": "INDArray thresholdEncode(INDArray input,double threshold,INDArray thresholdEncode,INDArray input,double threshold,Integer boundary){\r\n    return backendExecutioner.thresholdEncode(input, threshold, boundary);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractTreebankLanguagePack.punctuationTagAcceptFilter",
	"Comment": "return a filter that accepts a string that is a punctuationtag name, and rejects everything else.",
	"Method": "Predicate<String> punctuationTagAcceptFilter(){\r\n    return punctTagStringAcceptFilter;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.paragraphvectors.ParagraphVectorsTest.testsParallelFit1",
	"Comment": "this is very long test, to track memory consumption over time",
	"Method": "void testsParallelFit1(){\r\n    final File file = new ClassPathResource(\"/big/raw_sentences.txt\").getFile();\r\n    for (int i = 0; i < 1000; i++) {\r\n        List<Thread> threads = new ArrayList();\r\n        for (int t = 0; t < 3; t++) {\r\n            threads.add(new Thread(new Runnable() {\r\n                @Override\r\n                public void run() {\r\n                    try {\r\n                        TokenizerFactory t = new DefaultTokenizerFactory();\r\n                        LabelsSource source = new LabelsSource(\"DOC_\");\r\n                        SentenceIteratorConverter sic = new SentenceIteratorConverter(new BasicLineIterator(file), source);\r\n                        ParagraphVectors vec = // .batchSize(10)\r\n                        new ParagraphVectors.Builder().seed(42).minWordFrequency(1).iterations(1).epochs(5).layerSize(100).learningRate(// .labelsSource(source)\r\n                        0.05).windowSize(5).trainWordVectors(true).allowParallelTokenization(// .vocabCache(cache)\r\n                        false).tokenizerFactory(t).workers(1).iterate(sic).build();\r\n                        vec.fit();\r\n                    } catch (Exception e) {\r\n                        throw new RuntimeException(e);\r\n                    }\r\n                }\r\n            }));\r\n        }\r\n        for (Thread t : threads) {\r\n            t.start();\r\n        }\r\n        for (Thread t : threads) {\r\n            t.join();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.paragraphvectors.ParagraphVectorsTest.testsParallelFit1",
	"Comment": "this is very long test, to track memory consumption over time",
	"Method": "void testsParallelFit1(){\r\n    try {\r\n        TokenizerFactory t = new DefaultTokenizerFactory();\r\n        LabelsSource source = new LabelsSource(\"DOC_\");\r\n        SentenceIteratorConverter sic = new SentenceIteratorConverter(new BasicLineIterator(file), source);\r\n        ParagraphVectors vec = // .batchSize(10)\r\n        new ParagraphVectors.Builder().seed(42).minWordFrequency(1).iterations(1).epochs(5).layerSize(100).learningRate(// .labelsSource(source)\r\n        0.05).windowSize(5).trainWordVectors(true).allowParallelTokenization(// .vocabCache(cache)\r\n        false).tokenizerFactory(t).workers(1).iterate(sic).build();\r\n        vec.fit();\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.updateVariableNameAndReference",
	"Comment": "updates the variable name property on the passed in variable, the reference in samediff, and returns the variable.note that if null for the new variable is passed in, it will just return the original input variable.",
	"Method": "SDVariable updateVariableNameAndReference(SDVariable varToUpdate,String newVarName){\r\n    if (varToUpdate == null) {\r\n        throw new NullPointerException(\"Null input: No variable found for updating!\");\r\n    }\r\n    if (newVarName != null && variableMap.containsKey(newVarName) && varToUpdate != variableMap.get(newVarName)) {\r\n        throw new IllegalStateException(\"Variable name \\\"\" + newVarName + \"\\\" already exists for a different SDVariable\");\r\n    }\r\n    if (newVarName == null && variableMap.containsKey(varToUpdate.getVarName())) {\r\n        newVarName = generateNewVarName(varToUpdate.getVarName(), 0);\r\n    }\r\n    if (newVarName == null || varToUpdate.getVarName().equals(newVarName)) {\r\n        return varToUpdate;\r\n    }\r\n    val oldVarName = varToUpdate.getVarName();\r\n    varToUpdate.setVarName(newVarName);\r\n    updateVariableName(oldVarName, newVarName);\r\n    return varToUpdate;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.sum",
	"Comment": "sum array reduction operation, optionally along specified dimensions",
	"Method": "SDVariable sum(SDVariable x,int dimensions,SDVariable sum,String name,SDVariable x,int dimensions,SDVariable sum,String name,SDVariable x,boolean keepDims,int dimensions,SDVariable sum,SDVariable x,boolean keepDims,int dimensions){\r\n    return sum(null, x, keepDims, dimensions);\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.keysAt",
	"Comment": "returns the set of keys that have exactly the given count. this set mayhave 0 elements but will not be null.",
	"Method": "Set<E> keysAt(Counter<E> c,double count){\r\n    Set<E> keys = Generics.newHashSet();\r\n    for (E key : c.keySet()) {\r\n        if (c.getCount(key) == count) {\r\n            keys.add(key);\r\n        }\r\n    }\r\n    return (keys);\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getClass",
	"Comment": "get the value of the name property as a classimplementing the interface specified by xface.if no such property is specified, then defaultvalue isreturned.an exception is thrown if the returned class does not implement the namedinterface.",
	"Method": "Class<?> getClass(String name,Class<?> defaultValue,Class<? extends U> getClass,String name,Class<? extends U> defaultValue,Class<U> xface){\r\n    try {\r\n        Class<?> theClass = getClass(name, defaultValue);\r\n        if (theClass != null && !xface.isAssignableFrom(theClass))\r\n            throw new RuntimeException(theClass + \" not \" + xface.getName());\r\n        else if (theClass != null)\r\n            return theClass.asSubclass(xface);\r\n        else\r\n            return null;\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractCollinsHeadFinder.postOperationFix",
	"Comment": "a way for subclasses to fix any heads under special conditions.the default does nothing.",
	"Method": "int postOperationFix(int headIdx,Tree[] daughterTrees){\r\n    return headIdx;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.take",
	"Comment": "returns a shortened view of an iterator.returns at most max elements.",
	"Method": "Iterable<T> take(T[] array,int max,Iterable<T> take,Iterable<T> iterable,int max){\r\n    return new Iterable<T>() {\r\n        final Iterator<T> iterator = iterable.iterator();\r\n        public Iterator<T> iterator() {\r\n            return new Iterator<T>() {\r\n                int i = 0;\r\n                public boolean hasNext() {\r\n                    return i < max && iterator.hasNext();\r\n                }\r\n                public T next() {\r\n                    i++;\r\n                    return iterator.next();\r\n                }\r\n                public void remove() {\r\n                    iterator.remove();\r\n                }\r\n            };\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.take",
	"Comment": "returns a shortened view of an iterator.returns at most max elements.",
	"Method": "Iterable<T> take(T[] array,int max,Iterable<T> take,Iterable<T> iterable,int max){\r\n    return new Iterator<T>() {\r\n        int i = 0;\r\n        public boolean hasNext() {\r\n            return i < max && iterator.hasNext();\r\n        }\r\n        public T next() {\r\n            i++;\r\n            return iterator.next();\r\n        }\r\n        public void remove() {\r\n            iterator.remove();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.take",
	"Comment": "returns a shortened view of an iterator.returns at most max elements.",
	"Method": "Iterable<T> take(T[] array,int max,Iterable<T> take,Iterable<T> iterable,int max){\r\n    return i < max && iterator.hasNext();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.take",
	"Comment": "returns a shortened view of an iterator.returns at most max elements.",
	"Method": "Iterable<T> take(T[] array,int max,Iterable<T> take,Iterable<T> iterable,int max){\r\n    i++;\r\n    return iterator.next();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.take",
	"Comment": "returns a shortened view of an iterator.returns at most max elements.",
	"Method": "Iterable<T> take(T[] array,int max,Iterable<T> take,Iterable<T> iterable,int max){\r\n    iterator.remove();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Filters.collectionAcceptFilter",
	"Comment": "the collectionacceptfilter accepts a certain collection.",
	"Method": "Predicate<E> collectionAcceptFilter(E[] objs,Predicate<E> collectionAcceptFilter,Collection<E> objs){\r\n    return new CollectionAcceptFilter(objs, true);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.iterator.Word2VecDataSetIterator.next",
	"Comment": "like the standard next method but allows acustomizable number of examples returned",
	"Method": "DataSet next(int num,DataSet next){\r\n    return next(batch);\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.LabeledChunkIdentifier.isEndOfChunk",
	"Comment": "returns whether a chunk ended between the previous and current token.",
	"Method": "boolean isEndOfChunk(String prevTag,String prevType,String curTag,String curType,boolean isEndOfChunk,LabelTagType prev,LabelTagType cur){\r\n    if (prev == null)\r\n        return false;\r\n    return isEndOfChunk(prev.tag, prev.type, cur.tag, cur.type);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraPennLexer.main",
	"Comment": "runs the scanner on input files.this is a standalone scanner, it will print any unmatchedtext to system.out unchanged.",
	"Method": "void main(String argv){\r\n    if (argv.length == 0) {\r\n        System.out.println(\"Usage : java NegraPennLexer [ --encoding <name> ] <inputfile(s)>\");\r\n    } else {\r\n        int firstFilePos = 0;\r\n        String encodingName = \"UTF-8\";\r\n        if (argv[0].equals(\"--encoding\")) {\r\n            firstFilePos = 2;\r\n            encodingName = argv[1];\r\n            try {\r\n                java.nio.charset.Charset.forName(encodingName);\r\n            } catch (Exception e) {\r\n                System.out.println(\"Invalid encoding '\" + encodingName + \"'\");\r\n                return;\r\n            }\r\n        }\r\n        for (int i = firstFilePos; i < argv.length; i++) {\r\n            NegraPennLexer scanner = null;\r\n            try {\r\n                java.io.FileInputStream stream = new java.io.FileInputStream(argv[i]);\r\n                java.io.Reader reader = new java.io.InputStreamReader(stream, encodingName);\r\n                scanner = new NegraPennLexer(reader);\r\n                while (!scanner.zzAtEOF) scanner.yylex();\r\n            } catch (java.io.FileNotFoundException e) {\r\n                System.out.println(\"File not found : \\\"\" + argv[i] + \"\\\"\");\r\n            } catch (java.io.IOException e) {\r\n                System.out.println(\"IO error scanning file \\\"\" + argv[i] + \"\\\"\");\r\n                System.out.println(e);\r\n            } catch (Exception e) {\r\n                System.out.println(\"Unexpected exception:\");\r\n                e.printStackTrace();\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.learning.impl.elements.RandomUtils.nextFloat",
	"Comment": "returns the next pseudorandom, uniformly distributed float valuebetween 0.0 and 1.0 from the given randomsequence.",
	"Method": "float nextFloat(float nextFloat,Random random){\r\n    return random.nextFloat();\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.getSiblings",
	"Comment": "method for getting the siblings of a particular node. siblings are theother children of your parent, where parent is determined as the parentreturned by getparent",
	"Method": "Collection<IndexedWord> getSiblings(IndexedWord vertex){\r\n    IndexedWord parent = this.getParent(vertex);\r\n    if (parent != null) {\r\n        Set<IndexedWord> result = wordMapFactory.newSet();\r\n        result.addAll(this.getChildren(parent));\r\n        result.remove(vertex);\r\n        return result;\r\n    } else {\r\n        return Collections.emptySet();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.StringUtils.decodeArray",
	"Comment": "decode an array encoded as a string. this entails a comma separated value enclosed in bracketsor parentheses.",
	"Method": "String[] decodeArray(String encoded){\r\n    if (encoded.isEmpty())\r\n        return EMPTY_STRING_ARRAY;\r\n    char[] chars = encoded.trim().toCharArray();\r\n    char quoteCloseChar = (char) 0;\r\n    List<String> terms = new ArrayList();\r\n    StringBuilder current = new StringBuilder();\r\n    int start = 0;\r\n    int end = chars.length;\r\n    if (chars[0] == '(') {\r\n        start += 1;\r\n        end -= 1;\r\n        if (chars[end] != ')')\r\n            throw new IllegalArgumentException(\"Unclosed paren in encoded array: \" + encoded);\r\n    }\r\n    if (chars[0] == '[') {\r\n        start += 1;\r\n        end -= 1;\r\n        if (chars[end] != ']')\r\n            throw new IllegalArgumentException(\"Unclosed bracket in encoded array: \" + encoded);\r\n    }\r\n    if (chars[0] == '{') {\r\n        start += 1;\r\n        end -= 1;\r\n        if (chars[end] != '}')\r\n            throw new IllegalArgumentException(\"Unclosed bracket in encoded array: \" + encoded);\r\n    }\r\n    for (int i = start; i < end; i++) {\r\n        if (chars[i] == '\\r') {\r\n            continue;\r\n        } else if (quoteCloseChar != 0) {\r\n            if (chars[i] == quoteCloseChar) {\r\n                quoteCloseChar = (char) 0;\r\n            } else {\r\n                current.append(chars[i]);\r\n            }\r\n        } else if (chars[i] == '\\\\') {\r\n            if (i == chars.length - 1)\r\n                throw new IllegalArgumentException(\"Last character of encoded array is escape character: \" + encoded);\r\n            current.append(chars[i + 1]);\r\n            i += 1;\r\n        } else {\r\n            if (chars[i] == '\"') {\r\n                quoteCloseChar = '\"';\r\n            } else if (chars[i] == '\\'') {\r\n                quoteCloseChar = '\\'';\r\n            } else if (chars[i] == ',' || chars[i] == ';' || chars[i] == ' ' || chars[i] == '\\t' || chars[i] == '\\n') {\r\n                if (current.length() > 0) {\r\n                    terms.add(current.toString().trim());\r\n                }\r\n                current = new StringBuilder();\r\n            } else {\r\n                current.append(chars[i]);\r\n            }\r\n        }\r\n    }\r\n    if (current.length() > 0) {\r\n        terms.add(current.toString().trim());\r\n    }\r\n    return terms.toArray(EMPTY_STRING_ARRAY);\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getInstances",
	"Comment": "get the value of the name property as a listof objects implementing the interface specified by xface.an exception is thrown if any of the classes does not exist, or if it doesnot implement the named interface.",
	"Method": "List<U> getInstances(String name,Class<U> xface){\r\n    List<U> ret = new ArrayList();\r\n    Class<?>[] classes = getClasses(name);\r\n    for (Class<?> cl : classes) {\r\n        if (!xface.isAssignableFrom(cl)) {\r\n            throw new RuntimeException(cl + \" does not implement \" + xface);\r\n        }\r\n        ret.add((U) ReflectionUtils.newInstance(cl, this));\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.SemanticHeadFinder.isVerbalAuxiliary",
	"Comment": "returns true if this tree is a preterminal that is a verbal auxiliary.",
	"Method": "boolean isVerbalAuxiliary(Tree preterminal,Set<String> verbalSet,boolean allowJustTagMatch,boolean isVerbalAuxiliary,Tree t){\r\n    return isVerbalAuxiliary(t, verbalAuxiliaries, true);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SDVariable.dup",
	"Comment": "create a new sdvariable, the contents of which is copied from this current variable",
	"Method": "SDVariable dup(){\r\n    return sameDiff.var(this);\r\n}"
}, {
	"Path": "org.deeplearning4j.text.documentiterator.SimpleLabelAwareIterator.nextDocument",
	"Comment": "this method returns next labelleddocument from underlying iterator",
	"Method": "LabelledDocument nextDocument(){\r\n    LabelledDocument document = currentIterator.next();\r\n    for (String label : document.getLabels()) {\r\n        labels.storeLabel(label);\r\n    }\r\n    return document;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.originalOffset",
	"Comment": "returns the start of where the ndarray is for the original data buffer",
	"Method": "long originalOffset(){\r\n    if (data().originalOffset() >= Integer.MAX_VALUE)\r\n        throw new IllegalArgumentException(\"Original offset of buffer can not be >= Integer.MAX_VALUE\");\r\n    return data().originalOffset();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.XMLUtils.getTagElementsFromFile",
	"Comment": "returns the text content of all nodes in the given file with the given tag.",
	"Method": "List<Element> getTagElementsFromFile(File f,String tag){\r\n    List<Element> sents = Generics.newArrayList();\r\n    try {\r\n        sents = getTagElementsFromFileSAXException(f, tag);\r\n    } catch (SAXException e) {\r\n        log.warn(e);\r\n    }\r\n    return sents;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.concurrency.BasicAffinityManager.replicateToDevice",
	"Comment": "this method replicates given databuffer, and places it to target device.",
	"Method": "INDArray replicateToDevice(Integer deviceId,INDArray array,DataBuffer replicateToDevice,Integer deviceId,DataBuffer buffer){\r\n    return null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArrayHeap.extractMin",
	"Comment": "finds the object with the minimum key, removes it from the heap,and returns it.",
	"Method": "E extractMin(){\r\n    if (isEmpty()) {\r\n        throw new NoSuchElementException();\r\n    }\r\n    HeapEntry<E> minEntry = indexToEntry.get(0);\r\n    int lastIndex = size() - 1;\r\n    if (lastIndex > 0) {\r\n        HeapEntry<E> lastEntry = indexToEntry.get(lastIndex);\r\n        swap(lastEntry, minEntry);\r\n        removeLast(minEntry);\r\n        heapifyDown(lastEntry);\r\n    } else {\r\n        removeLast(minEntry);\r\n    }\r\n    return minEntry.object;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.WhitespaceLexer.yypushback",
	"Comment": "pushes the specified amount of characters back into the input stream.they will be read again by then next call of the scanning method",
	"Method": "void yypushback(int number){\r\n    if (number > yylength())\r\n        zzScanError(ZZ_PUSHBACK_2BIG);\r\n    zzMarkedPos -= number;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasModelBuilder.weightsHdf5Filename",
	"Comment": "set weights of the model by providing the file name of the corresponding weights hdf5 file.the root of the hdf5 group containing weights will be read and set from the configuration of thismodel builder instance.",
	"Method": "KerasModelBuilder weightsHdf5Filename(String weightsHdf5Filename){\r\n    checkForExistence(weightsHdf5Filename);\r\n    this.weightsArchive = new Hdf5Archive(weightsHdf5Filename);\r\n    this.weightsRoot = config.getTrainingWeightsRoot();\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Interval.isIntervalComparable",
	"Comment": "checks whether this interval is comparable with another interval comes before or after",
	"Method": "boolean isIntervalComparable(Interval<E> other){\r\n    int flags = getRelationFlags(other);\r\n    if (checkMultipleBitSet(flags & REL_FLAGS_INTERVAL_UNKNOWN)) {\r\n        return false;\r\n    }\r\n    return checkFlagSet(flags, REL_FLAGS_INTERVAL_BEFORE) || checkFlagSet(flags, REL_FLAGS_INTERVAL_AFTER);\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getBoolean",
	"Comment": "get the value of the name property as a boolean.if no such property is specified, or if the specified value is not a validboolean, then defaultvalue is returned.",
	"Method": "boolean getBoolean(String name,boolean defaultValue){\r\n    String valueString = get(name);\r\n    return \"true\".equals(valueString) || !\"false\".equals(valueString) && defaultValue;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.iterator",
	"Comment": "iterates over the entries of the cache.in the end, this loads the entire cache, but it can do it incrementally.",
	"Method": "Iterator<Entry<KEY, T>> iterator(){\r\n    final File[] files = cacheDir.listFiles();\r\n    if (files == null || files.length == 0)\r\n        return Generics.<Entry<KEY, T>>newLinkedList().iterator();\r\n    for (int i = 0; i < files.length; ++i) {\r\n        try {\r\n            files[i] = canonicalFile.intern(files[i].getCanonicalFile());\r\n        } catch (IOException e) {\r\n            throw throwSafe(e);\r\n        }\r\n    }\r\n    return new Iterator<Entry<KEY, T>>() {\r\n        Iterator<Pair<KEY, T>> elements = readBlock(files[0]).iterator();\r\n        int index = 1;\r\n        @Override\r\n        public boolean hasNext() {\r\n            if (elements.hasNext())\r\n                return true;\r\n            elements = null;\r\n            while (index < files.length && elements == null) {\r\n                try {\r\n                    elements = readBlock(files[index]).iterator();\r\n                } catch (OutOfMemoryError e) {\r\n                    warn(\"FileBackedCache\", \"Caught out of memory error (clearing cache): \" + e.getMessage());\r\n                    FileBackedCache.this.clear();\r\n                    try {\r\n                        Thread.sleep(1000);\r\n                    } catch (InterruptedException e2) {\r\n                        throw new RuntimeInterruptedException(e2);\r\n                    }\r\n                    elements = readBlock(files[index]).iterator();\r\n                } catch (RuntimeException e) {\r\n                    err(e);\r\n                }\r\n                index += 1;\r\n            }\r\n            return elements != null && hasNext();\r\n        }\r\n        @Override\r\n        public Entry<KEY, T> next() {\r\n            if (!hasNext())\r\n                throw new NoSuchElementException();\r\n            final Pair<KEY, T> pair = elements.next();\r\n            return new Entry<KEY, T>() {\r\n                @Override\r\n                public KEY getKey() {\r\n                    return pair.first;\r\n                }\r\n                @Override\r\n                public T getValue() {\r\n                    return pair.second;\r\n                }\r\n                @Override\r\n                public T setValue(T value) {\r\n                    throw new RuntimeException(\"Cannot set entry\");\r\n                }\r\n            };\r\n        }\r\n        @Override\r\n        public void remove() {\r\n            throw new RuntimeException(\"Remove not implemented\");\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.iterator",
	"Comment": "iterates over the entries of the cache.in the end, this loads the entire cache, but it can do it incrementally.",
	"Method": "Iterator<Entry<KEY, T>> iterator(){\r\n    if (elements.hasNext())\r\n        return true;\r\n    elements = null;\r\n    while (index < files.length && elements == null) {\r\n        try {\r\n            elements = readBlock(files[index]).iterator();\r\n        } catch (OutOfMemoryError e) {\r\n            warn(\"FileBackedCache\", \"Caught out of memory error (clearing cache): \" + e.getMessage());\r\n            FileBackedCache.this.clear();\r\n            try {\r\n                Thread.sleep(1000);\r\n            } catch (InterruptedException e2) {\r\n                throw new RuntimeInterruptedException(e2);\r\n            }\r\n            elements = readBlock(files[index]).iterator();\r\n        } catch (RuntimeException e) {\r\n            err(e);\r\n        }\r\n        index += 1;\r\n    }\r\n    return elements != null && hasNext();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.iterator",
	"Comment": "iterates over the entries of the cache.in the end, this loads the entire cache, but it can do it incrementally.",
	"Method": "Iterator<Entry<KEY, T>> iterator(){\r\n    if (!hasNext())\r\n        throw new NoSuchElementException();\r\n    final Pair<KEY, T> pair = elements.next();\r\n    return new Entry<KEY, T>() {\r\n        @Override\r\n        public KEY getKey() {\r\n            return pair.first;\r\n        }\r\n        @Override\r\n        public T getValue() {\r\n            return pair.second;\r\n        }\r\n        @Override\r\n        public T setValue(T value) {\r\n            throw new RuntimeException(\"Cannot set entry\");\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.iterator",
	"Comment": "iterates over the entries of the cache.in the end, this loads the entire cache, but it can do it incrementally.",
	"Method": "Iterator<Entry<KEY, T>> iterator(){\r\n    return pair.first;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.iterator",
	"Comment": "iterates over the entries of the cache.in the end, this loads the entire cache, but it can do it incrementally.",
	"Method": "Iterator<Entry<KEY, T>> iterator(){\r\n    return pair.second;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.iterator",
	"Comment": "iterates over the entries of the cache.in the end, this loads the entire cache, but it can do it incrementally.",
	"Method": "Iterator<Entry<KEY, T>> iterator(){\r\n    throw new RuntimeException(\"Cannot set entry\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.iterator",
	"Comment": "iterates over the entries of the cache.in the end, this loads the entire cache, but it can do it incrementally.",
	"Method": "Iterator<Entry<KEY, T>> iterator(){\r\n    throw new RuntimeException(\"Remove not implemented\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.TwoDimensionalCounter.sumInnerCounter",
	"Comment": "returns the counters with keys as the first key and count as thetotal count of the inner counter for that key",
	"Method": "Counter<K1> sumInnerCounter(){\r\n    Counter<K1> summed = new ClassicCounter();\r\n    for (K1 key : this.firstKeySet()) {\r\n        summed.incrementCount(key, this.getCounter(key).totalCount());\r\n    }\r\n    return summed;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.updateArrayForVarName",
	"Comment": "update the indarray for the given variable. note that the array must exist to use this method.",
	"Method": "void updateArrayForVarName(String varName,INDArray arr){\r\n    if (!variableNameToArr.containsKey(varName)) {\r\n        throw new ND4JIllegalStateException(\"Array for \" + varName + \" does not exist. Please use putArrayForVertexId instead.\");\r\n    }\r\n    variableNameToArr.put(varName, arr);\r\n}"
}, {
	"Path": "org.deeplearning4j.graph.data.GraphLoader.loadGraph",
	"Comment": "load graph, assuming vertices are in one file and edges are in another file.",
	"Method": "Graph<V, E> loadGraph(String path,EdgeLineProcessor<E> lineProcessor,VertexFactory<V> vertexFactory,int numVertices,boolean allowMultipleEdges,Graph<V, E> loadGraph,String vertexFilePath,String edgeFilePath,VertexLoader<V> vertexLoader,EdgeLineProcessor<E> edgeLineProcessor,boolean allowMultipleEdges){\r\n    List<Vertex<V>> vertices = vertexLoader.loadVertices(vertexFilePath);\r\n    Graph<V, E> graph = new Graph(vertices, allowMultipleEdges);\r\n    try (BufferedReader br = new BufferedReader(new FileReader(new File(edgeFilePath)))) {\r\n        String line;\r\n        while ((line = br.readLine()) != null) {\r\n            Edge<E> edge = edgeLineProcessor.processLine(line);\r\n            if (edge != null) {\r\n                graph.addEdge(edge);\r\n            }\r\n        }\r\n    }\r\n    return graph;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.glove.AbstractCoOccurrences.getMemoryFootprint",
	"Comment": "this method returns estimated memory footrpint, based on current countmap content",
	"Method": "long getMemoryFootprint(){\r\n    try {\r\n        lock.readLock().lock();\r\n        return ((long) coOccurrenceCounts.size()) * 24L * 5L;\r\n    } finally {\r\n        lock.readLock().unlock();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.spanish.SpanishHeadFinder.insertVerbs",
	"Comment": "build a list of head rules containing all of the possible verbtags. the verbs are inserted in between toleft andtoright.",
	"Method": "String[] insertVerbs(String[] toLeft,String[] toRight){\r\n    return ArrayUtils.concatenate(toLeft, ArrayUtils.concatenate(allVerbs, toRight));\r\n}"
}, {
	"Path": "org.datavec.api.transform.schema.Schema.inferMultiple",
	"Comment": "infers a schema based on the record.the column names are based on indexing.",
	"Method": "Schema inferMultiple(List<List<Writable>> record){\r\n    return infer(record.get(0));\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.concurrent.MulticoreWrapper.getProcessor",
	"Comment": "returns the next available thread id.subclasses may wish tooverride this, for example if they implement a timeout",
	"Method": "Integer getProcessor(){\r\n    try {\r\n        return idleProcessors.take();\r\n    } catch (InterruptedException e) {\r\n        throw new RuntimeInterruptedException(e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.SemgrexMatcher.getRelationNames",
	"Comment": "returns the set of names for named relations in this pattern.",
	"Method": "Set<String> getRelationNames(){\r\n    return namesToRelations.keySet();\r\n}"
}, {
	"Path": "org.nd4j.evaluation.regression.RegressionEvaluation.averagePearsonCorrelation",
	"Comment": "average pearson correlation coefficient across all columns",
	"Method": "double averagePearsonCorrelation(){\r\n    double ret = 0.0;\r\n    for (int i = 0; i < numColumns(); i++) {\r\n        ret += pearsonCorrelation(i);\r\n    }\r\n    return ret / (double) numColumns();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.scale",
	"Comment": "returns a new counter which is scaled by the given scale factor.",
	"Method": "C scale(C c,double s,TwoDimensionalCounter<T1, T2> scale,TwoDimensionalCounter<T1, T2> c,double d){\r\n    TwoDimensionalCounter<T1, T2> result = new TwoDimensionalCounter(c.getOuterMapFactory(), c.getInnerMapFactory());\r\n    for (T1 key : c.firstKeySet()) {\r\n        ClassicCounter<T2> ctr = c.getCounter(key);\r\n        result.setCounter(key, scale(ctr, d));\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.column.RemoveColumnsTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return leftOverColumns;\r\n}"
}, {
	"Path": "org.deeplearning4j.util.ConvolutionUtils.getDeconvolutionOutputSize",
	"Comment": "get the output size of a deconvolution operation for given input data. in deconvolution, we compute the inverseof the shape computation of a convolution.",
	"Method": "int[] getDeconvolutionOutputSize(INDArray inputData,int[] kernel,int[] strides,int[] padding,ConvolutionMode convolutionMode,int[] dilation){\r\n    int hIn = (int) inputData.size(2);\r\n    int wIn = (int) inputData.size(3);\r\n    int[] eKernel = effectiveKernelSize(kernel, dilation);\r\n    if (convolutionMode == ConvolutionMode.Same) {\r\n        int hOut = strides[0] * hIn;\r\n        int wOut = strides[1] * wIn;\r\n        return new int[] { hOut, wOut };\r\n    }\r\n    int hOut = strides[0] * (hIn - 1) + eKernel[0] - 2 * padding[0];\r\n    int wOut = strides[1] * (wIn - 1) + eKernel[1] - 2 * padding[1];\r\n    return new int[] { hOut, wOut };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.PropertiesUtils.getDouble",
	"Comment": "load a double property.if the key is not present, returns defaultvalue.",
	"Method": "double getDouble(Properties props,String key,double getDouble,Properties props,String key,double defaultValue){\r\n    String value = props.getProperty(key);\r\n    if (value != null) {\r\n        return Double.parseDouble(value);\r\n    } else {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasModelBuilder.enforceTrainingConfig",
	"Comment": "determine whether to enforce loading a training configuration or not.",
	"Method": "KerasModelBuilder enforceTrainingConfig(boolean enforceTrainingConfig){\r\n    this.enforceTrainingConfig = enforceTrainingConfig;\r\n    return this;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.paragraphvectors.ParagraphVectors.nearestLabels",
	"Comment": "this method returns top n labels nearest to specified features vector",
	"Method": "Collection<String> nearestLabels(LabelledDocument document,int topN,Collection<String> nearestLabels,String rawText,int topN,Collection<String> nearestLabels,Collection<VocabWord> document,int topN,Collection<String> nearestLabels,INDArray labelVector,int topN){\r\n    if (labelsMatrix == null || labelsList == null || labelsList.isEmpty())\r\n        extractLabels();\r\n    List<BasicModelUtils.WordSimilarity> result = new ArrayList();\r\n    if (labelsMatrix == null || labelsList == null || labelsList.isEmpty()) {\r\n        log.warn(\"Labels list is empty!\");\r\n        return new ArrayList();\r\n    }\r\n    if (!normalizedLabels) {\r\n        synchronized (this) {\r\n            if (!normalizedLabels) {\r\n                labelsMatrix.diviColumnVector(labelsMatrix.norm1(1));\r\n                normalizedLabels = true;\r\n            }\r\n        }\r\n    }\r\n    INDArray similarity = Transforms.unitVec(labelVector).mmul(labelsMatrix.transpose());\r\n    List<Double> highToLowSimList = getTopN(similarity, topN + 20);\r\n    for (int i = 0; i < highToLowSimList.size(); i++) {\r\n        String word = labelsList.get(highToLowSimList.get(i).intValue()).getLabel();\r\n        if (word != null && !word.equals(\"UNK\") && !word.equals(\"STOP\")) {\r\n            INDArray otherVec = lookupTable.vector(word);\r\n            double sim = Transforms.cosineSim(labelVector, otherVec);\r\n            result.add(new BasicModelUtils.WordSimilarity(word, sim));\r\n        }\r\n    }\r\n    Collections.sort(result, new BasicModelUtils.SimilarityComparator());\r\n    return BasicModelUtils.getLabels(result, topN);\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.CoreQuote.canonicalSpeakerEntityMention",
	"Comment": "retrieve the entity mention corresponding to the canonical speaker if there is one",
	"Method": "Optional<CoreEntityMention> canonicalSpeakerEntityMention(){\r\n    return this.canonicalSpeakerEntityMention;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.wordstore.VocabConstructor.getNumberOfSequences",
	"Comment": "this method returns total number of sequences passed through vocabconstructor",
	"Method": "long getNumberOfSequences(){\r\n    return seqCount.get();\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.isMatrix",
	"Comment": "returns true if this ndarray is 2dor 3d with a singleton element",
	"Method": "boolean isMatrix(){\r\n    int rank = rank();\r\n    return (rank == 2 && (size(0) != 1 && size(1) != 1));\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.tsurgeon.ParseException.add_escapes",
	"Comment": "used to convert raw characters to their escaped versionwhen these raw version cannot be used as part of an asciistring literal.",
	"Method": "String add_escapes(String str){\r\n    StringBuffer retval = new StringBuffer();\r\n    char ch;\r\n    for (int i = 0; i < str.length(); i++) {\r\n        switch(str.charAt(i)) {\r\n            case 0:\r\n                continue;\r\n            case '\\b':\r\n                retval.append(\"\\\\b\");\r\n                continue;\r\n            case '\\t':\r\n                retval.append(\"\\\\t\");\r\n                continue;\r\n            case '\\n':\r\n                retval.append(\"\\\\n\");\r\n                continue;\r\n            case '\\f':\r\n                retval.append(\"\\\\f\");\r\n                continue;\r\n            case '\\r':\r\n                retval.append(\"\\\\r\");\r\n                continue;\r\n            case '\\\"':\r\n                retval.append(\"\\\\\\\"\");\r\n                continue;\r\n            case '\\'':\r\n                retval.append(\"\\\\\\'\");\r\n                continue;\r\n            case '\\\\':\r\n                retval.append(\"\\\\\\\\\");\r\n                continue;\r\n            default:\r\n                if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {\r\n                    String s = \"0000\" + Integer.toString(ch, 16);\r\n                    retval.append(\"\\\%u\" + s.substring(s.length() - 4, s.length()));\r\n                } else {\r\n                    retval.append(ch);\r\n                }\r\n                continue;\r\n        }\r\n    }\r\n    return retval.toString();\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Document.coref",
	"Comment": "returns the coref chains in the document. this is a map from coref cluster ids, to the coref chainwith that id.",
	"Method": "Map<Integer, CorefChain> coref(Properties props,Map<Integer, CorefChain> coref){\r\n    return coref(defaultProps);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerTest.testSummary",
	"Comment": "summary should pick up preprocessors set manually on inputs as well",
	"Method": "void testSummary(){\r\n    int V_WIDTH = 130;\r\n    int V_HEIGHT = 130;\r\n    int V_NFRAMES = 150;\r\n    // l2 regularization on all layers\r\n    MultiLayerConfiguration confForArchitecture = // 3 channels: RGB\r\n    new NeuralNetConfiguration.Builder().seed(12345).l2(0.001).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).list().layer(0, // Output: (130-10+0)/4+1 = 31 -> 31*31*30\r\n    new ConvolutionLayer.Builder(10, 10).nIn(3).nOut(30).stride(4, 4).activation(Activation.RELU).weightInit(WeightInit.RELU).updater(Updater.ADAGRAD).build()).layer(1, // (31-3+0)/2+1 = 15\r\n    new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(3, 3).stride(2, 2).build()).layer(2, // Output: (15-3+0)/2+1 = 7 -> 7*7*10 = 490\r\n    new ConvolutionLayer.Builder(3, 3).nIn(30).nOut(10).stride(2, 2).activation(Activation.RELU).weightInit(WeightInit.RELU).updater(Updater.ADAGRAD).build()).layer(3, new DenseLayer.Builder().activation(Activation.RELU).nIn(490).nOut(50).weightInit(WeightInit.RELU).updater(Updater.ADAGRAD).gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue).gradientNormalizationThreshold(10).build()).layer(4, new GravesLSTM.Builder().activation(Activation.SOFTSIGN).nIn(50).nOut(50).weightInit(WeightInit.XAVIER).updater(Updater.ADAGRAD).gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue).gradientNormalizationThreshold(10).build()).layer(5, // 4 possible shapes: circle, square, arc, line\r\n    new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(50).nOut(4).updater(Updater.ADAGRAD).weightInit(WeightInit.XAVIER).gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue).gradientNormalizationThreshold(10).build()).inputPreProcessor(0, new RnnToCnnPreProcessor(V_HEIGHT, V_WIDTH, 3)).inputPreProcessor(3, new CnnToFeedForwardPreProcessor(7, 7, 10)).inputPreProcessor(4, new FeedForwardToRnnPreProcessor()).backpropType(BackpropType.TruncatedBPTT).tBPTTForwardLength(V_NFRAMES / 5).tBPTTBackwardLength(V_NFRAMES / 5).build();\r\n    MultiLayerNetwork modelExpectedArch = new MultiLayerNetwork(confForArchitecture);\r\n    modelExpectedArch.init();\r\n    MultiLayerNetwork modelMow = new TransferLearning.Builder(modelExpectedArch).setFeatureExtractor(2).build();\r\n    System.out.println(modelExpectedArch.summary());\r\n    System.out.println(modelMow.summary());\r\n    System.out.println(modelMow.summary(InputType.recurrent(V_HEIGHT * V_WIDTH * 3)));\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.SequenceGibbsSampler.samplePositionHelper",
	"Comment": "samples a single position in the sequence.does not modify the sequence passed in.returns the score of the new label for the position to sample",
	"Method": "Pair<Integer, Double> samplePositionHelper(SequenceModel model,int[] sequence,int pos,double temperature){\r\n    double[] distribution = model.scoresOf(sequence, pos);\r\n    if (temperature != 1.0) {\r\n        if (temperature == 0.0) {\r\n            int argmax = ArrayMath.argmax(distribution);\r\n            Arrays.fill(distribution, Double.NEGATIVE_INFINITY);\r\n            distribution[argmax] = 0.0;\r\n        } else {\r\n            ArrayMath.multiplyInPlace(distribution, 1.0 / temperature);\r\n        }\r\n    }\r\n    ArrayMath.logNormalize(distribution);\r\n    ArrayMath.expInPlace(distribution);\r\n    int newTag = ArrayMath.sampleFromDistribution(distribution, random);\r\n    double newProb = distribution[newTag];\r\n    return new Pair(newTag, newProb);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.semgrexFromGraphOrderedNodes",
	"Comment": "same as semgrexfromgraph except the node traversal is ordered by sorting",
	"Method": "String semgrexFromGraphOrderedNodes(SemanticGraph sg,Collection<IndexedWord> wildcardNodes,Map<IndexedWord, String> nodeNameMap,Function<IndexedWord, String> wordTransformation){\r\n    IndexedWord patternRoot = sg.getFirstRoot();\r\n    StringWriter buf = new StringWriter();\r\n    Set<IndexedWord> tabu = Generics.newHashSet();\r\n    Set<SemanticGraphEdge> seenEdges = Generics.newHashSet();\r\n    buf.append(semgrexFromGraphHelper(patternRoot, sg, tabu, seenEdges, true, true, wildcardNodes, nodeNameMap, true, wordTransformation));\r\n    String patternString = buf.toString();\r\n    return patternString;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.concurrent.MulticoreWrapper.put",
	"Comment": "allocate instance to a process and return. this call blocks until itemcan be assigned to a thread.",
	"Method": "void put(I item){\r\n    Integer procId = getProcessor();\r\n    if (procId == null) {\r\n        throw new RejectedExecutionException(\"Couldn't submit item to threadpool: \" + item);\r\n    }\r\n    final int itemId = submittedItemCounter++;\r\n    CallableJob<I, O> job = new CallableJob(item, itemId, processorList.get(procId), procId, callback);\r\n    threadPool.submit(job);\r\n}"
}, {
	"Path": "org.deeplearning4j.zoo.ModelMetaData.useMDS",
	"Comment": "if number of inputs are greater than 1, this states that theimplementation should use multidataset.",
	"Method": "boolean useMDS(){\r\n    return inputShape.length > 1 ? true : false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.FeatureFactory.getWord",
	"Comment": "convenience methods for subclasses which use corelabel.gets theword after applying any wordfunction present in theseqclassifierflags.",
	"Method": "String getWord(CoreLabel label){\r\n    String word = label.getString(CoreAnnotations.TextAnnotation.class);\r\n    if (flags.wordFunction != null) {\r\n        word = flags.wordFunction.apply(word);\r\n    }\r\n    return word;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.EquivalenceClassEval.removeItem",
	"Comment": "there is some discomfort here, we should really be using an equalitychecker for checker, buti screwed up the api.",
	"Method": "void removeItem(T o,Collection<T> c,Eval.CollectionContainsChecker<T> checker){\r\n    for (T o1 : c) {\r\n        if (checker.contained(o, Collections.singleton(o1))) {\r\n            c.remove(o1);\r\n            return;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.retainMatchingKeys",
	"Comment": "removes all entries with keys that does not match one of the given patterns.",
	"Method": "Set<String> retainMatchingKeys(Counter<String> counter,List<Pattern> matchPatterns){\r\n    Set<String> removed = Generics.newHashSet();\r\n    for (String key : counter.keySet()) {\r\n        boolean matched = false;\r\n        for (Pattern pattern : matchPatterns) {\r\n            if (pattern.matcher(key).matches()) {\r\n                matched = true;\r\n                break;\r\n            }\r\n        }\r\n        if (!matched) {\r\n            removed.add(key);\r\n        }\r\n    }\r\n    for (String key : removed) {\r\n        counter.remove(key);\r\n    }\r\n    return removed;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.BinaryHeapPriorityQueue.removeFirst",
	"Comment": "finds the e with the highest priority, removes it,and returns it.",
	"Method": "E removeFirst(){\r\n    E first = getFirst();\r\n    remove(first);\r\n    return first;\r\n}"
}, {
	"Path": "org.deeplearning4j.util.Convolution3DUtils.get3DSameModeTopLeftPadding",
	"Comment": "get top and left padding for same mode only for 3d convolutions",
	"Method": "int[] get3DSameModeTopLeftPadding(int[] outSize,int[] inSize,int[] kernel,int[] strides,int[] dilation){\r\n    int[] eKernel = effectiveKernelSize(kernel, dilation);\r\n    int[] outPad = new int[3];\r\n    outPad[0] = ((outSize[0] - 1) * strides[0] + eKernel[0] - inSize[0]) / 2;\r\n    outPad[1] = ((outSize[1] - 1) * strides[1] + eKernel[1] - inSize[1]) / 2;\r\n    outPad[2] = ((outSize[2] - 1) * strides[2] + eKernel[2] - inSize[2]) / 2;\r\n    return outPad;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sentiment.SentimentUtils.readTreesWithGoldLabels",
	"Comment": "given a file name, reads in those trees and returns them as a list",
	"Method": "List<Tree> readTreesWithGoldLabels(String path){\r\n    return readTreesWithLabels(path, RNNCoreAnnotations.GoldClass.class);\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.process",
	"Comment": "runs the entire pipeline on the content of the given text passed in.",
	"Method": "Annotation process(String text){\r\n    Annotation annotation = new Annotation(text);\r\n    annotate(annotation);\r\n    return annotation;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.keysBelow",
	"Comment": "returns the set of keys whose counts are at or below the given threshold.this set may have 0 elements but will not be null.",
	"Method": "Set<E> keysBelow(int countThreshold){\r\n    Set<E> keys = Generics.newHashSet();\r\n    for (E key : map.keySet()) {\r\n        if (getIntCount(key) <= countThreshold) {\r\n            keys.add(key);\r\n        }\r\n    }\r\n    return keys;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.putRow",
	"Comment": "insert a row in to this arraywill throw an exception if thisndarray is not a matrix",
	"Method": "INDArray putRow(long row,INDArray toPut){\r\n    if (isRowVector() && toPut.isVector()) {\r\n        return assign(toPut);\r\n    }\r\n    return put(new INDArrayIndex[] { NDArrayIndex.point(row), NDArrayIndex.all() }, toPut);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.Redwood.stop",
	"Comment": "stop redwood, closing all tracks and prohibiting future log messages.",
	"Method": "void stop(){\r\n    isClosed = true;\r\n    Thread.yield();\r\n    Thread.yield();\r\n    while (depth > 0) {\r\n        depth -= 1;\r\n        handlers.process(null, MessageType.END_TRACK, depth, System.currentTimeMillis());\r\n    }\r\n    handlers.process(null, MessageType.SHUTDOWN, 0, System.currentTimeMillis());\r\n}"
}, {
	"Path": "org.datavec.api.transform.filter.ConditionFilter.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return condition.outputColumnNames();\r\n}"
}, {
	"Path": "org.deeplearning4j.util.MovingWindowMatrix.windows",
	"Comment": "moving window, capture a row x column moving window ofa given matrix",
	"Method": "List<INDArray> windows(List<INDArray> windows,boolean flattened){\r\n    List<INDArray> ret = new ArrayList();\r\n    int window = 0;\r\n    for (int i = 0; i < toSlice.length(); i++) {\r\n        if (window >= toSlice.length())\r\n            break;\r\n        double[] w = new double[this.windowRowSize * this.windowColumnSize];\r\n        for (int count = 0; count < this.windowRowSize * this.windowColumnSize; count++) {\r\n            w[count] = toSlice.getDouble(count + window);\r\n        }\r\n        INDArray add = Nd4j.create(w);\r\n        if (flattened)\r\n            add = add.ravel();\r\n        else\r\n            add = add.reshape(windowRowSize, windowColumnSize);\r\n        if (addRotate) {\r\n            INDArray currRotation = add.dup();\r\n            for (int rotation = 0; rotation < 3; rotation++) {\r\n                Nd4j.rot90(currRotation);\r\n                ret.add(currRotation.dup());\r\n            }\r\n        }\r\n        window += this.windowRowSize * this.windowColumnSize;\r\n        ret.add(add);\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraPennLanguagePack.containsKeptGF",
	"Comment": "helper method for determining if the gf in categoryis one of those in the array gftokeeparray.index is theindex where the gfcharacter appears.",
	"Method": "boolean containsKeptGF(String category,int index){\r\n    for (String gf : gfToKeepArray) {\r\n        int gfLength = gf.length();\r\n        if (gfLength < (category.length() - index)) {\r\n            if (category.substring(index + 1, index + 1 + gfLength).equals(gf))\r\n                return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.SequenceGibbsSampler.sampleSequenceForward",
	"Comment": "samples the complete sequence once in the forward directiondestructively modifies the sequence in place.",
	"Method": "double sampleSequenceForward(SequenceModel model,int[] sequence,double sampleSequenceForward,SequenceModel model,int[] sequence,double temperature,Set<Integer> onlySampleThesePositions){\r\n    double returnScore = Double.NEGATIVE_INFINITY;\r\n    if (onlySampleThesePositions != null) {\r\n        for (int pos : onlySampleThesePositions) {\r\n            returnScore = samplePosition(model, sequence, pos, temperature);\r\n        }\r\n    } else {\r\n        if (samplingStyle == SEQUENTIAL_SAMPLING) {\r\n            for (int pos = 0; pos < sequence.length; pos++) {\r\n                returnScore = samplePosition(model, sequence, pos, temperature);\r\n            }\r\n        } else if (samplingStyle == RANDOM_SAMPLING) {\r\n            for (int aSequence : sequence) {\r\n                int pos = random.nextInt(sequence.length);\r\n                returnScore = samplePosition(model, sequence, pos, temperature);\r\n            }\r\n        } else if (samplingStyle == CHROMATIC_SAMPLING) {\r\n            List<Pair<Integer, Integer>> results = new ArrayList();\r\n            for (List<Integer> indieList : partition) {\r\n                if (indieList.size() <= chromaticSize) {\r\n                    for (int pos : indieList) {\r\n                        Pair<Integer, Double> newPosProb = samplePositionHelper(model, sequence, pos, temperature);\r\n                        sequence[pos] = newPosProb.first();\r\n                    }\r\n                } else {\r\n                    MulticoreWrapper<List<Integer>, List<Pair<Integer, Integer>>> wrapper = new MulticoreWrapper(chromaticSize, new ThreadsafeProcessor<List<Integer>, List<Pair<Integer, Integer>>>() {\r\n                        @Override\r\n                        public List<Pair<Integer, Integer>> process(List<Integer> posList) {\r\n                            List<Pair<Integer, Integer>> allPos = new ArrayList(posList.size());\r\n                            Pair<Integer, Double> newPosProb = null;\r\n                            for (int pos : posList) {\r\n                                newPosProb = samplePositionHelper(model, sequence, pos, temperature);\r\n                                allPos.add(new Pair(pos, newPosProb.first()));\r\n                            }\r\n                            return allPos;\r\n                        }\r\n                        @Override\r\n                        public ThreadsafeProcessor<List<Integer>, List<Pair<Integer, Integer>>> newInstance() {\r\n                            return this;\r\n                        }\r\n                    });\r\n                    results.clear();\r\n                    int interval = Math.max(1, indieList.size() / chromaticSize);\r\n                    for (int begin = 0, end = 0, indieListSize = indieList.size(); end < indieListSize; begin += interval) {\r\n                        end = Math.min(begin + interval, indieListSize);\r\n                        wrapper.put(indieList.subList(begin, end));\r\n                        while (wrapper.peek()) {\r\n                            results.addAll(wrapper.poll());\r\n                        }\r\n                    }\r\n                    wrapper.join();\r\n                    while (wrapper.peek()) {\r\n                        results.addAll(wrapper.poll());\r\n                    }\r\n                    for (Pair<Integer, Integer> posVal : results) {\r\n                        sequence[posVal.first()] = posVal.second();\r\n                    }\r\n                }\r\n            }\r\n            returnScore = model.scoreOf(sequence);\r\n        }\r\n    }\r\n    return returnScore;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.SequenceGibbsSampler.sampleSequenceForward",
	"Comment": "samples the complete sequence once in the forward directiondestructively modifies the sequence in place.",
	"Method": "double sampleSequenceForward(SequenceModel model,int[] sequence,double sampleSequenceForward,SequenceModel model,int[] sequence,double temperature,Set<Integer> onlySampleThesePositions){\r\n    List<Pair<Integer, Integer>> allPos = new ArrayList(posList.size());\r\n    Pair<Integer, Double> newPosProb = null;\r\n    for (int pos : posList) {\r\n        newPosProb = samplePositionHelper(model, sequence, pos, temperature);\r\n        allPos.add(new Pair(pos, newPosProb.first()));\r\n    }\r\n    return allPos;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.SequenceGibbsSampler.sampleSequenceForward",
	"Comment": "samples the complete sequence once in the forward directiondestructively modifies the sequence in place.",
	"Method": "double sampleSequenceForward(SequenceModel model,int[] sequence,double sampleSequenceForward,SequenceModel model,int[] sequence,double temperature,Set<Integer> onlySampleThesePositions){\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.spanish.SpanishXMLTreeReader.buildWordNode",
	"Comment": "build a parse tree node corresponding to the word in the given xml node.",
	"Method": "Tree buildWordNode(Node root){\r\n    Element eRoot = (Element) root;\r\n    String posStr = getPOS(eRoot);\r\n    posStr = treeNormalizer.normalizeNonterminal(posStr);\r\n    String lemma = eRoot.getAttribute(ATTR_LEMMA);\r\n    String word = getWord(eRoot);\r\n    String leafStr = treeNormalizer.normalizeTerminal(word);\r\n    Tree leafNode = treeFactory.newLeaf(leafStr);\r\n    if (leafNode.label() instanceof HasWord)\r\n        ((HasWord) leafNode.label()).setWord(leafStr);\r\n    if (leafNode.label() instanceof HasLemma && lemma != null)\r\n        ((HasLemma) leafNode.label()).setLemma(lemma);\r\n    List<Tree> kids = new ArrayList();\r\n    kids.add(leafNode);\r\n    Tree t = treeFactory.newTreeNode(posStr, kids);\r\n    if (t.label() instanceof HasTag)\r\n        ((HasTag) t.label()).setTag(posStr);\r\n    return t;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.InputPanel.setTsurgeonState",
	"Comment": "assumes that it will be called only if tsurgeon is already enabled",
	"Method": "void setTsurgeonState(boolean running){\r\n    cancelTsurgeon.setEnabled(running);\r\n    runScript.setEnabled(!running);\r\n    findMatches.setEnabled(!running);\r\n    browseButton.setEnabled(!running);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.Ssurgeon.expandFromPatterns",
	"Comment": "given a list of ssurgeonpattern edit scripts, and a semanticgraphto operate over, returns a list of expansions of that graph, withthe result of each edit applied against a copy of the graph.",
	"Method": "List<SemanticGraph> expandFromPatterns(List<SsurgeonPattern> patternList,SemanticGraph sg){\r\n    List<SemanticGraph> retList = new ArrayList();\r\n    for (SsurgeonPattern pattern : patternList) {\r\n        Collection<SemanticGraph> generated = pattern.execute(sg);\r\n        for (SemanticGraph orderedGraph : generated) {\r\n            retList.add(orderedGraph);\r\n            System.out.println(\"\\ncompact = \" + orderedGraph.toCompactString());\r\n            System.out.println(\"regular=\" + orderedGraph);\r\n        }\r\n        if (generated.size() > 0) {\r\n            if (log != null) {\r\n                log.info(\"* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\");\r\n                log.info(\"Pre remove duplicates, num=\" + generated.size());\r\n            }\r\n            SemanticGraphUtils.removeDuplicates(generated, sg);\r\n            if (log != null) {\r\n                log.info(\"Expand from patterns\");\r\n                if (logPrefix != null)\r\n                    log.info(logPrefix);\r\n                log.info(\"Pattern = '\" + pattern.getUID() + \"' generated \" + generated.size() + \" matches\");\r\n                log.info(\"= = = = = = = = = =\\nSrc graph:\\n\" + sg + \"\\n= = = = = = = = = =\\n\");\r\n                int index = 1;\r\n                for (SemanticGraph genSg : generated) {\r\n                    log.info(\"REWRITE \" + (index++));\r\n                    log.info(genSg.toString());\r\n                    log.info(\". . . . .\\n\");\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return retList;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CollectionValuedMap.removeMapping",
	"Comment": "removes the value from the collection mapped to by this key, leaving therest of the collection intact.",
	"Method": "void removeMapping(K key,V value){\r\n    if (treatCollectionsAsImmutable) {\r\n        Collection<V> c = map.get(key);\r\n        if (c != null) {\r\n            Collection<V> newC = cf.newCollection();\r\n            newC.addAll(c);\r\n            newC.remove(value);\r\n            map.put(key, newC);\r\n        }\r\n    } else {\r\n        Collection<V> c = get(key);\r\n        c.remove(value);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.linearCombination",
	"Comment": "returns a counter which is a weighted average of c1 and c2. counts from c1are weighted with weight w1 and counts from c2 are weighted with w2.",
	"Method": "Counter<E> linearCombination(Counter<E> c1,double w1,Counter<E> c2,double w2){\r\n    Counter<E> result = c1.getFactory().create();\r\n    for (E o : c1.keySet()) {\r\n        result.incrementCount(o, c1.getCount(o) * w1);\r\n    }\r\n    for (E o : c2.keySet()) {\r\n        result.incrementCount(o, c2.getCount(o) * w2);\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.datavec.spark.transform.Normalization.stdDevMeanColumns",
	"Comment": "returns the standard deviationand mean of the given columnsthe list returned is a list of size 2 where each rowrepresents the standard deviation of each column and the mean of each column",
	"Method": "List<Row> stdDevMeanColumns(DataRowsFacade data,List<String> columns,List<Row> stdDevMeanColumns,DataRowsFacade data,String columns){\r\n    return aggregate(data, columns, new String[] { \"stddev\", \"mean\" });\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.SequenceGibbsSampler.findBestUsingSampling",
	"Comment": "finds the best sequence by collecting numsamples samples, scoring them, and then choosingthe highest scoring sample.",
	"Method": "int[] findBestUsingSampling(SequenceModel model,int numSamples,int sampleInterval,int[] initialSequence){\r\n    List samples = collectSamples(model, numSamples, sampleInterval, initialSequence);\r\n    int[] best = null;\r\n    double bestScore = Double.NEGATIVE_INFINITY;\r\n    for (Object sample : samples) {\r\n        int[] sequence = (int[]) sample;\r\n        double score = model.scoreOf(sequence);\r\n        if (score > bestScore) {\r\n            best = sequence;\r\n            bestScore = score;\r\n            log.info(\"found new best (\" + bestScore + \")\");\r\n            log.info(ArrayMath.toString(best));\r\n        }\r\n    }\r\n    return best;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.Redwood.hideChannelsEverywhere",
	"Comment": "hide multiple channels.all other channels will be unaffected.",
	"Method": "void hideChannelsEverywhere(Object channels){\r\n    for (LogRecordHandler handler : handlers) {\r\n        if (handler instanceof VisibilityHandler) {\r\n            VisibilityHandler visHandler = (VisibilityHandler) handler;\r\n            for (Object channel : channels) {\r\n                visHandler.alsoHide(channel);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.setNetwork",
	"Comment": "set the network that underlies this sparkdl4jmultilayer instacne",
	"Method": "void setNetwork(MultiLayerNetwork network){\r\n    this.network = network;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.XMLUtils.getTagElementTriplesFromFileNumBoundedSAXException",
	"Comment": "returns the elements in the given file with the given tag associated withthe text content of the previous and next siblings up to max numincludedsiblings.",
	"Method": "List<Triple<String, Element, String>> getTagElementTriplesFromFileNumBoundedSAXException(File f,String tag,int numIncludedSiblings){\r\n    List<Triple<String, Element, String>> sents = Generics.newArrayList();\r\n    try {\r\n        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();\r\n        DocumentBuilder db = dbf.newDocumentBuilder();\r\n        Document doc = db.parse(f);\r\n        doc.getDocumentElement().normalize();\r\n        NodeList nodeList = doc.getElementsByTagName(tag);\r\n        for (int i = 0; i < nodeList.getLength(); i++) {\r\n            Node prevNode = nodeList.item(i).getPreviousSibling();\r\n            String prev = \"\";\r\n            int count = 0;\r\n            while (prevNode != null && count <= numIncludedSiblings) {\r\n                prev = prevNode.getTextContent() + prev;\r\n                prevNode = prevNode.getPreviousSibling();\r\n                count++;\r\n            }\r\n            Node nextNode = nodeList.item(i).getNextSibling();\r\n            String next = \"\";\r\n            count = 0;\r\n            while (nextNode != null && count <= numIncludedSiblings) {\r\n                next = next + nextNode.getTextContent();\r\n                nextNode = nextNode.getNextSibling();\r\n                count++;\r\n            }\r\n            Element element = (Element) nodeList.item(i);\r\n            Triple<String, Element, String> t = new Triple(prev, element, next);\r\n            sents.add(t);\r\n        }\r\n    } catch (IOException | ParserConfigurationException e) {\r\n        log.warn(e);\r\n    }\r\n    return sents;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.removeFromMemory",
	"Comment": "remove a given key from memory, not removing it from the disk.",
	"Method": "boolean removeFromMemory(KEY key){\r\n    return mapping.remove(key) != null;\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getTrimmedStrings",
	"Comment": "get the comma delimited values of the name property asan array of strings, trimmed of the leading and trailing whitespace.if no such property is specified then default value is returned.",
	"Method": "String[] getTrimmedStrings(String name,String[] getTrimmedStrings,String name,String defaultValue){\r\n    String valueString = get(name);\r\n    if (null == valueString) {\r\n        return defaultValue;\r\n    } else {\r\n        return StringUtils.getTrimmedStrings(valueString);\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.transform.filter.FilterInvalidValues.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return columnNames();\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.Extractor.extract",
	"Comment": "by default the bound is ignored, but a few subclasses make use of it.",
	"Method": "String extract(History h,String extract,History h,PairsHolder pH,String extract,History h,PairsHolder pH,int bound){\r\n    return extract(h, pH);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.tuebadz.TueBaDZLanguagePack.containsKeptGF",
	"Comment": "helper method for determining if the gf in categoryis one of those in the array gftokeeparray.index is theindex where the gfcharacter appears.",
	"Method": "boolean containsKeptGF(String category,int index){\r\n    for (String gf : gfToKeepArray) {\r\n        int gfLength = gf.length();\r\n        if (gfLength < (category.length() - index)) {\r\n            if (category.substring(index + 1).equals(gf))\r\n                return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.PrecisionRecallStats.getPrecisionDescription",
	"Comment": "returns a string summarizing precision that will print nicely.",
	"Method": "String getPrecisionDescription(int numDigits){\r\n    NumberFormat nf = NumberFormat.getNumberInstance();\r\n    nf.setMaximumFractionDigits(numDigits);\r\n    return nf.format(getPrecision()) + \"  (\" + tpCount + \"/\" + (tpCount + fpCount) + \")\";\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.params.SeparableConvolutionParamInitializer.numDepthWiseParams",
	"Comment": "for each input feature we separately compute depthmultiplier manyoutput maps for the given kernel size",
	"Method": "long numDepthWiseParams(SeparableConvolution2D layerConf){\r\n    int[] kernel = layerConf.getKernelSize();\r\n    val nIn = layerConf.getNIn();\r\n    val depthMultiplier = layerConf.getDepthMultiplier();\r\n    return nIn * depthMultiplier * kernel[0] * kernel[1];\r\n}"
}, {
	"Path": "org.datavec.arrow.ArrowConverter.toArrowColumnsTimeSeriesHelper",
	"Comment": "convert a set of input strings to arrow columnsfor a time series.",
	"Method": "List<FieldVector> toArrowColumnsTimeSeriesHelper(BufferAllocator bufferAllocator,Schema schema,List<List<List<T>>> dataVecRecord){\r\n    int numRows = 0;\r\n    for (List<List<T>> timeStep : dataVecRecord) {\r\n        numRows += timeStep.get(0).size() * timeStep.size();\r\n    }\r\n    numRows /= schema.numColumns();\r\n    List<FieldVector> ret = createFieldVectors(bufferAllocator, schema, numRows);\r\n    Map<Integer, Integer> currIndex = new HashMap(ret.size());\r\n    for (int i = 0; i < ret.size(); i++) {\r\n        currIndex.put(i, 0);\r\n    }\r\n    for (int i = 0; i < dataVecRecord.size(); i++) {\r\n        List<List<T>> record = dataVecRecord.get(i);\r\n        for (int j = 0; j < record.size(); j++) {\r\n            List<T> curr = record.get(j);\r\n            for (int k = 0; k < curr.size(); k++) {\r\n                Integer idx = currIndex.get(k);\r\n                FieldVector fieldVector = ret.get(k);\r\n                T writable = curr.get(k);\r\n                setValue(schema.getType(k), fieldVector, writable, idx);\r\n                currIndex.put(k, idx + 1);\r\n            }\r\n        }\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.TreePrint.printTreeInternal",
	"Comment": "print the internal part of a tree having already identified it. the id and outer xml element is printed wrapping this method, but none of the internal content.",
	"Method": "void printTreeInternal(Tree t,PrintWriter pw,boolean inXml){\r\n    Tree outputTree = t;\r\n    if (formats.containsKey(\"conll2007\") || removeEmpty) {\r\n        outputTree = outputTree.prune(new BobChrisTreeNormalizer.EmptyFilter());\r\n    }\r\n    if (formats.containsKey(\"words\")) {\r\n        if (inXml) {\r\n            ArrayList<Label> sentUnstemmed = outputTree.yield();\r\n            pw.println(\"  <words>\");\r\n            int i = 1;\r\n            for (Label w : sentUnstemmed) {\r\n                pw.println(\"    <word ind=\\\"\" + i + \"\\\">\" + XMLUtils.escapeXML(w.value()) + \"<\/word>\");\r\n                i++;\r\n            }\r\n            pw.println(\"  <\/words>\");\r\n        } else {\r\n            String sent = SentenceUtils.listToString(outputTree.yield(), false);\r\n            if (ptb2text) {\r\n                pw.println(PTBTokenizer.ptb2Text(sent));\r\n            } else {\r\n                pw.println(sent);\r\n                pw.println();\r\n            }\r\n        }\r\n    }\r\n    if (propertyToBoolean(options, \"removeTopBracket\")) {\r\n        String s = outputTree.label().value();\r\n        if (tlp.isStartSymbol(s)) {\r\n            if (outputTree.isUnaryRewrite()) {\r\n                outputTree = outputTree.firstChild();\r\n            } else {\r\n                log.info(\"TreePrint: can't remove top bracket: not unary\");\r\n            }\r\n        }\r\n    }\r\n    if (stemmer != null) {\r\n        stemmer.visitTree(outputTree);\r\n    }\r\n    if (lexicalize) {\r\n        outputTree = Trees.lexicalize(outputTree, hf);\r\n        Function<Tree, Tree> a = TreeFunctions.getLabeledToDescriptiveCoreLabelTreeFunction();\r\n        outputTree = a.apply(outputTree);\r\n    }\r\n    if (formats.containsKey(\"collocations\")) {\r\n        outputTree = getCollocationProcessedTree(outputTree, hf);\r\n    }\r\n    if (!lexicalize) {\r\n        Function<Tree, Tree> a = TreeFunctions.getLabeledTreeToStringLabeledTreeFunction();\r\n        outputTree = a.apply(outputTree);\r\n    }\r\n    Tree outputPSTree = outputTree;\r\n    if (markHeadNodes) {\r\n        outputPSTree = markHeadNodes(outputPSTree);\r\n    }\r\n    if (transChinese) {\r\n        TreeTransformer tt = t1 -> {\r\n            t1 = t1.treeSkeletonCopy();\r\n            for (Tree subtree : t1) {\r\n                if (subtree.isLeaf()) {\r\n                    Label oldLabel = subtree.label();\r\n                    String translation = ChineseEnglishWordMap.getInstance().getFirstTranslation(oldLabel.value());\r\n                    if (translation == null)\r\n                        translation = \"[UNK]\";\r\n                    Label newLabel = new StringLabel(oldLabel.value() + ':' + translation);\r\n                    subtree.setLabel(newLabel);\r\n                }\r\n            }\r\n            return t1;\r\n        };\r\n        outputPSTree = tt.transformTree(outputPSTree);\r\n    }\r\n    if (propertyToBoolean(options, \"xml\")) {\r\n        if (formats.containsKey(\"wordsAndTags\")) {\r\n            ArrayList<TaggedWord> sent = outputTree.taggedYield();\r\n            pw.println(\"  <words pos=\\\"true\\\">\");\r\n            int i = 1;\r\n            for (TaggedWord tw : sent) {\r\n                pw.println(\"    <word ind=\\\"\" + i + \"\\\" pos=\\\"\" + XMLUtils.escapeXML(tw.tag()) + \"\\\">\" + XMLUtils.escapeXML(tw.word()) + \"<\/word>\");\r\n                i++;\r\n            }\r\n            pw.println(\"  <\/words>\");\r\n        }\r\n        if (formats.containsKey(\"penn\")) {\r\n            pw.println(\"  <tree style=\\\"penn\\\">\");\r\n            StringWriter sw = new StringWriter();\r\n            PrintWriter psw = new PrintWriter(sw);\r\n            outputPSTree.pennPrint(psw);\r\n            pw.print(XMLUtils.escapeXML(sw.toString()));\r\n            pw.println(\"  <\/tree>\");\r\n        }\r\n        if (formats.containsKey(\"latexTree\")) {\r\n            pw.println(\"    <tree style=\\\"latexTrees\\\">\");\r\n            pw.println(\".[\");\r\n            StringWriter sw = new StringWriter();\r\n            PrintWriter psw = new PrintWriter(sw);\r\n            outputTree.indentedListPrint(psw, false);\r\n            pw.print(XMLUtils.escapeXML(sw.toString()));\r\n            pw.println(\".]\");\r\n            pw.println(\"  <\/tree>\");\r\n        }\r\n        if (formats.containsKey(\"xmlTree\")) {\r\n            pw.println(\"<tree style=\\\"xml\\\">\");\r\n            outputTree.indentedXMLPrint(pw, false);\r\n            pw.println(\"<\/tree>\");\r\n        }\r\n        if (formats.containsKey(\"dependencies\")) {\r\n            Tree indexedTree = outputTree.deepCopy(outputTree.treeFactory(), CoreLabel.factory());\r\n            indexedTree.indexLeaves();\r\n            Set<Dependency<Label, Label, Object>> depsSet = indexedTree.mapDependencies(dependencyWordFilter, hf);\r\n            List<Dependency<Label, Label, Object>> sortedDeps = new ArrayList(depsSet);\r\n            sortedDeps.sort(Dependencies.dependencyIndexComparator());\r\n            pw.println(\"<dependencies style=\\\"untyped\\\">\");\r\n            for (Dependency<Label, Label, Object> d : sortedDeps) {\r\n                pw.println(d.toString(\"xml\"));\r\n            }\r\n            pw.println(\"<\/dependencies>\");\r\n        }\r\n        if (formats.containsKey(\"conll2007\") || formats.containsKey(\"conllStyleDependencies\")) {\r\n            log.info(\"The \\\"conll2007\\\" and \\\"conllStyleDependencies\\\" formats are ignored in xml.\");\r\n        }\r\n        if (formats.containsKey(\"typedDependencies\")) {\r\n            GrammaticalStructure gs = gsf.newGrammaticalStructure(outputTree);\r\n            if (basicDependencies) {\r\n                print(gs.typedDependencies(), \"xml\", includeTags, pw);\r\n            }\r\n            if (nonCollapsedDependencies || nonCollapsedDependenciesSeparated) {\r\n                print(gs.allTypedDependencies(), \"xml\", includeTags, pw);\r\n            }\r\n            if (collapsedDependencies) {\r\n                print(gs.typedDependenciesCollapsed(GrammaticalStructure.Extras.MAXIMAL), \"xml\", includeTags, pw);\r\n            }\r\n            if (CCPropagatedDependencies) {\r\n                print(gs.typedDependenciesCCprocessed(), \"xml\", includeTags, pw);\r\n            }\r\n            if (treeDependencies) {\r\n                print(gs.typedDependenciesCollapsedTree(), \"xml\", includeTags, pw);\r\n            }\r\n        }\r\n        if (formats.containsKey(\"typedDependenciesCollapsed\")) {\r\n            GrammaticalStructure gs = gsf.newGrammaticalStructure(outputTree);\r\n            print(gs.typedDependenciesCCprocessed(), \"xml\", includeTags, pw);\r\n        }\r\n    } else {\r\n        if (formats.containsKey(\"wordsAndTags\")) {\r\n            pw.println(SentenceUtils.listToString(outputTree.taggedYield(), false));\r\n            pw.println();\r\n        }\r\n        if (formats.containsKey(\"oneline\")) {\r\n            pw.println(outputPSTree);\r\n        }\r\n        if (formats.containsKey(\"penn\")) {\r\n            outputPSTree.pennPrint(pw);\r\n            pw.println();\r\n        }\r\n        if (formats.containsKey(rootLabelOnlyFormat)) {\r\n            pw.println(outputTree.label().value());\r\n        }\r\n        if (formats.containsKey(\"latexTree\")) {\r\n            pw.println(\".[\");\r\n            outputTree.indentedListPrint(pw, false);\r\n            pw.println(\".]\");\r\n        }\r\n        if (formats.containsKey(\"xmlTree\")) {\r\n            outputTree.indentedXMLPrint(pw, false);\r\n        }\r\n        if (formats.containsKey(\"dependencies\")) {\r\n            Tree indexedTree = outputTree.deepCopy(outputTree.treeFactory());\r\n            indexedTree.indexLeaves();\r\n            List<Dependency<Label, Label, Object>> sortedDeps = getSortedDeps(indexedTree, dependencyWordFilter);\r\n            for (Dependency<Label, Label, Object> d : sortedDeps) {\r\n                pw.println(d.toString(\"predicate\"));\r\n            }\r\n            pw.println();\r\n        }\r\n        if (formats.containsKey(\"conll2007\")) {\r\n            Tree it = outputTree.deepCopy(outputTree.treeFactory(), CoreLabel.factory());\r\n            it.indexLeaves();\r\n            List<CoreLabel> tagged = it.taggedLabeledYield();\r\n            List<Dependency<Label, Label, Object>> sortedDeps = getSortedDeps(it, Filters.acceptFilter());\r\n            for (Dependency<Label, Label, Object> d : sortedDeps) {\r\n                if (!dependencyFilter.test(d)) {\r\n                    continue;\r\n                }\r\n                if (!(d.dependent() instanceof HasIndex) || !(d.governor() instanceof HasIndex)) {\r\n                    throw new IllegalArgumentException(\"Expected labels to have indices\");\r\n                }\r\n                HasIndex dep = (HasIndex) d.dependent();\r\n                HasIndex gov = (HasIndex) d.governor();\r\n                int depi = dep.index();\r\n                int govi = gov.index();\r\n                CoreLabel w = tagged.get(depi - 1);\r\n                String tag = PTBTokenizer.ptbToken2Text(w.tag());\r\n                String word = PTBTokenizer.ptbToken2Text(w.word());\r\n                String lemma = \"_\";\r\n                String feats = \"_\";\r\n                String pHead = \"_\";\r\n                String pDepRel = \"_\";\r\n                String depRel;\r\n                if (d.name() != null) {\r\n                    depRel = d.name().toString();\r\n                } else {\r\n                    depRel = (govi == 0) ? \"ROOT\" : \"NULL\";\r\n                }\r\n                pw.printf(\"%d\\t%s\\t%s\\t%s\\t%s\\t%s\\t%d\\t%s\\t%s\\t%s%n\", depi, word, lemma, tag, tag, feats, govi, depRel, pHead, pDepRel);\r\n            }\r\n            pw.println();\r\n        }\r\n        if (formats.containsKey(\"conllStyleDependencies\")) {\r\n            BobChrisTreeNormalizer tn = new BobChrisTreeNormalizer();\r\n            Tree indexedTree = outputTree.deepCopy(outputTree.treeFactory(), CoreLabel.factory());\r\n            for (Tree node : indexedTree) {\r\n                if (node.label().value().startsWith(\"NML\")) {\r\n                    node.label().setValue(\"NP\");\r\n                }\r\n            }\r\n            indexedTree = tn.normalizeWholeTree(indexedTree, outputTree.treeFactory());\r\n            indexedTree.indexLeaves();\r\n            Set<Dependency<Label, Label, Object>> depsSet = null;\r\n            boolean failed = false;\r\n            try {\r\n                depsSet = indexedTree.mapDependencies(dependencyFilter, hf);\r\n            } catch (Exception e) {\r\n                failed = true;\r\n            }\r\n            if (failed) {\r\n                log.info(\"failed: \");\r\n                log.info(t);\r\n                log.info();\r\n            } else {\r\n                Map<Integer, Integer> deps = Generics.newHashMap();\r\n                for (Dependency<Label, Label, Object> dep : depsSet) {\r\n                    CoreLabel child = (CoreLabel) dep.dependent();\r\n                    CoreLabel parent = (CoreLabel) dep.governor();\r\n                    Integer childIndex = child.get(CoreAnnotations.IndexAnnotation.class);\r\n                    Integer parentIndex = parent.get(CoreAnnotations.IndexAnnotation.class);\r\n                    deps.put(childIndex, parentIndex);\r\n                }\r\n                boolean foundRoot = false;\r\n                int index = 1;\r\n                for (Tree node : indexedTree.getLeaves()) {\r\n                    String word = node.label().value();\r\n                    String tag = node.parent(indexedTree).label().value();\r\n                    int parent = 0;\r\n                    if (deps.containsKey(index)) {\r\n                        parent = deps.get(index);\r\n                    } else {\r\n                        if (foundRoot) {\r\n                            throw new RuntimeException();\r\n                        }\r\n                        foundRoot = true;\r\n                    }\r\n                    pw.println(index + '\\t' + word + '\\t' + tag + '\\t' + parent);\r\n                    index++;\r\n                }\r\n                pw.println();\r\n            }\r\n        }\r\n        if (formats.containsKey(\"typedDependencies\")) {\r\n            GrammaticalStructure gs = gsf.newGrammaticalStructure(outputTree);\r\n            if (basicDependencies) {\r\n                print(gs.typedDependencies(), includeTags, pw);\r\n            }\r\n            if (nonCollapsedDependencies) {\r\n                print(gs.allTypedDependencies(), includeTags, pw);\r\n            }\r\n            if (nonCollapsedDependenciesSeparated) {\r\n                print(gs.allTypedDependencies(), \"separator\", includeTags, pw);\r\n            }\r\n            if (collapsedDependencies) {\r\n                print(gs.typedDependenciesCollapsed(GrammaticalStructure.Extras.MAXIMAL), includeTags, pw);\r\n            }\r\n            if (CCPropagatedDependencies) {\r\n                print(gs.typedDependenciesCCprocessed(), includeTags, pw);\r\n            }\r\n            if (treeDependencies) {\r\n                print(gs.typedDependenciesCollapsedTree(), includeTags, pw);\r\n            }\r\n        }\r\n        if (formats.containsKey(\"typedDependenciesCollapsed\")) {\r\n            GrammaticalStructure gs = gsf.newGrammaticalStructure(outputTree);\r\n            print(gs.typedDependenciesCCprocessed(), includeTags, pw);\r\n        }\r\n    }\r\n    pw.flush();\r\n}"
}, {
	"Path": "org.datavec.api.transform.filter.BaseColumnFilter.removeExample",
	"Comment": "should the example or sequence be removed, based on the values from the specified column?",
	"Method": "boolean removeExample(List<Writable> writables,boolean removeExample,Writable writable){\r\n    return removeExample(writables.get(columnIdx));\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.optimize.config.OptimizationConfiguration.toYaml",
	"Comment": "return a yaml configuration of this optimization configuration",
	"Method": "String toYaml(){\r\n    try {\r\n        return JsonMapper.getYamlMapper().writeValueAsString(this);\r\n    } catch (JsonProcessingException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.toCounter",
	"Comment": "turns the given map and index into a counter instance. for each entry incounts, its key is converted to a counter key via lookup in the givenindex.",
	"Method": "Counter<T> toCounter(double[] counts,Index<T> index,Counter<E> toCounter,Map<Integer, ? extends Number> counts,Index<E> index){\r\n    Counter<E> counter = new ClassicCounter();\r\n    for (Map.Entry<Integer, ? extends Number> entry : counts.entrySet()) {\r\n        counter.setCount(index.get(entry.getKey()), entry.getValue().doubleValue());\r\n    }\r\n    return counter;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.subtractAll",
	"Comment": "subtracts the counts in the given counter from the counts in this counter.to copy the values from another counter rather than subtracting them, use",
	"Method": "void subtractAll(IntCounter<E> counter){\r\n    for (E key : map.keySet()) {\r\n        decrementCount(key, counter.getIntCount(key));\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.SeqClassifierFlags.getNotNullTrueStringRep",
	"Comment": "note that this doesreturn string representation of arrays, lists andenums",
	"Method": "String getNotNullTrueStringRep(){\r\n    try {\r\n        StringBuilder rep = new StringBuilder();\r\n        String joiner = \"\\n\";\r\n        Field[] f = this.getClass().getFields();\r\n        for (Field ff : f) {\r\n            String name = ff.getName();\r\n            Class<?> type = ff.getType();\r\n            if (type.equals(Boolean.class) || type.equals(boolean.class)) {\r\n                boolean val = ff.getBoolean(this);\r\n                if (val) {\r\n                    rep.append(joiner).append(name).append('=').append(val);\r\n                }\r\n            } else if (type.equals(String.class)) {\r\n                String val = (String) ff.get(this);\r\n                if (val != null)\r\n                    rep.append(joiner).append(name).append('=').append(val);\r\n            } else if (type.equals(Double.class)) {\r\n                Double val = (Double) ff.get(this);\r\n                rep.append(joiner).append(name).append('=').append(val);\r\n            } else if (type.equals(double.class)) {\r\n                double val = ff.getDouble(this);\r\n                rep.append(joiner).append(name).append('=').append(val);\r\n            } else if (type.equals(Integer.class)) {\r\n                Integer val = (Integer) ff.get(this);\r\n                rep.append(joiner).append(name).append('=').append(val);\r\n            } else if (type.equals(int.class)) {\r\n                int val = ff.getInt(this);\r\n                rep.append(joiner).append(name).append('=').append(val);\r\n            } else if (type.equals(Float.class)) {\r\n                Float val = (Float) ff.get(this);\r\n                rep.append(joiner).append(name).append('=').append(val);\r\n            } else if (type.equals(float.class)) {\r\n                float val = ff.getFloat(this);\r\n                rep.append(joiner).append(name).append('=').append(val);\r\n            } else if (type.equals(Byte.class)) {\r\n                Byte val = (Byte) ff.get(this);\r\n                rep.append(joiner).append(name).append('=').append(val);\r\n            } else if (type.equals(byte.class)) {\r\n                byte val = ff.getByte(this);\r\n                rep.append(joiner).append(name).append('=').append(val);\r\n            } else if (type.equals(char.class)) {\r\n                char val = ff.getChar(this);\r\n                rep.append(joiner).append(name).append('=').append(val);\r\n            } else if (type.equals(Long.class)) {\r\n                Long val = (Long) ff.get(this);\r\n                rep.append(joiner).append(name).append('=').append(val);\r\n            } else if (type.equals(long.class)) {\r\n                long val = ff.getLong(this);\r\n                rep.append(joiner).append(name).append('=').append(val);\r\n            }\r\n        }\r\n        return rep.toString();\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n        return \"\";\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructureTest.testNonCollapsedSeparator",
	"Comment": "tests printing of the extra dependencies after the basic ones.",
	"Method": "void testNonCollapsedSeparator(){\r\n    TreeReaderFactory trf = new PennTreeReaderFactory();\r\n    Tree tree = Tree.valueOf(testTree, trf);\r\n    GrammaticalStructure gs = new UniversalEnglishGrammaticalStructure(tree);\r\n    assertEquals(\"Unexpected basic dependencies for tree \" + testTree, testAnswer, GrammaticalStructureConversionUtils.dependenciesToString(gs, gs.allTypedDependencies(), tree, false, true, false));\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.parallel.MultiBoolean.allFalse",
	"Comment": "this method returns true if all states are false. false otherwise",
	"Method": "boolean allFalse(){\r\n    return holder == 0;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.layers.BaseLayer.getUpdaterByParam",
	"Comment": "get the updater for the given parameter. typically the same updater will be used for all updaters, but thisis not necessarily the case",
	"Method": "IUpdater getUpdaterByParam(String paramName){\r\n    if (biasUpdater != null && initializer().isBiasParam(this, paramName)) {\r\n        return biasUpdater;\r\n    }\r\n    return iUpdater;\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.Evaluation.gMeasure",
	"Comment": "calculates the average g measure for all outputs using micro or macro averaging",
	"Method": "double gMeasure(int output,double gMeasure,EvaluationAveraging averaging){\r\n    int nClasses = confusion().getClasses().size();\r\n    if (averaging == EvaluationAveraging.Macro) {\r\n        double macroGMeasure = 0.0;\r\n        for (int i = 0; i < nClasses; i++) {\r\n            macroGMeasure += gMeasure(i);\r\n        }\r\n        macroGMeasure /= nClasses;\r\n        return macroGMeasure;\r\n    } else if (averaging == EvaluationAveraging.Micro) {\r\n        long tpCount = 0;\r\n        long fpCount = 0;\r\n        long fnCount = 0;\r\n        for (int i = 0; i < nClasses; i++) {\r\n            tpCount += truePositives.getCount(i);\r\n            fpCount += falsePositives.getCount(i);\r\n            fnCount += falseNegatives.getCount(i);\r\n        }\r\n        double precision = EvaluationUtils.precision(tpCount, fpCount, DEFAULT_EDGE_VALUE);\r\n        double recall = EvaluationUtils.recall(tpCount, fnCount, DEFAULT_EDGE_VALUE);\r\n        return EvaluationUtils.gMeasure(precision, recall);\r\n    } else {\r\n        throw new UnsupportedOperationException(\"Unknown averaging approach: \" + averaging);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.childRelns",
	"Comment": "returns a list of relations which this node has with its children.",
	"Method": "Set<GrammaticalRelation> childRelns(IndexedWord vertex){\r\n    if (!containsVertex(vertex)) {\r\n        throw new IllegalArgumentException();\r\n    }\r\n    Set<GrammaticalRelation> relns = Generics.newHashSet();\r\n    List<Pair<GrammaticalRelation, IndexedWord>> pairs = childPairs(vertex);\r\n    for (Pair<GrammaticalRelation, IndexedWord> p : pairs) {\r\n        relns.add(p.first());\r\n    }\r\n    return relns;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructure.getCaseMarkedRelation",
	"Comment": "returns a grammaticalrelation which combines the original relation andthe preposition.",
	"Method": "GrammaticalRelation getCaseMarkedRelation(GrammaticalRelation reln,String relationName){\r\n    GrammaticalRelation newReln = reln;\r\n    if (reln.getSpecific() != null) {\r\n        reln = reln.getParent();\r\n    }\r\n    if (reln == NOMINAL_MODIFIER) {\r\n        newReln = UniversalEnglishGrammaticalRelations.getNmod(relationName);\r\n    } else if (reln == ADV_CLAUSE_MODIFIER) {\r\n        newReln = UniversalEnglishGrammaticalRelations.getAdvcl(relationName);\r\n    } else if (reln == CLAUSAL_MODIFIER) {\r\n        newReln = UniversalEnglishGrammaticalRelations.getAcl(relationName);\r\n    }\r\n    return newReln;\r\n}"
}, {
	"Path": "org.datavec.spark.transform.AnalyzeSpark.getUniqueSequence",
	"Comment": "get a list of unique values from the specified columns of a sequence",
	"Method": "List<Writable> getUniqueSequence(String columnName,Schema schema,JavaRDD<List<List<Writable>>> sequenceData,Map<String, List<Writable>> getUniqueSequence,List<String> columnNames,Schema schema,JavaRDD<List<List<Writable>>> sequenceData){\r\n    JavaRDD<List<Writable>> flattenedSequence = sequenceData.flatMap(new SequenceFlatMapFunction());\r\n    return getUnique(columnNames, schema, flattenedSequence);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.arabic.ATBTreeUtils.escape",
	"Comment": "escapes tokens from flat strings that are reserved for usage in the atb.",
	"Method": "String escape(String s){\r\n    if (s == null)\r\n        return null;\r\n    s = s.replaceAll(\"\\\\(\", \"-LRB-\");\r\n    s = s.replaceAll(\"\\\\)\", \"-RRB-\");\r\n    s = s.replaceAll(\"\\\\+\", \"-PLUS-\");\r\n    return s;\r\n}"
}, {
	"Path": "edu.stanford.nlp.wordseg.ChineseStringUtils.postProcessingAnswer",
	"Comment": "post process the answer to be outputthese post processing are not dependent on original input",
	"Method": "String postProcessingAnswer(String ans,SeqClassifierFlags flags,String postProcessingAnswer,String ans,String postProcessingAnswer,String ans,Boolean keepAllWhitespaces,String postProcessingAnswer,String ans,String postProcessingAnswer,String ans,Boolean suppressMidDotPostprocessing,String postProcessingAnswer,String ans,String postProcessingAnswer,String ans,String postProcessingAnswer,String ans){\r\n    if (flags.useHk) {\r\n        return hkPostProcessor.postProcessingAnswer(ans);\r\n    } else if (flags.useAs) {\r\n        return asPostProcessor.postProcessingAnswer(ans);\r\n    } else if (flags.usePk) {\r\n        return pkPostProcessor.postProcessingAnswer(ans, flags.keepAllWhitespaces);\r\n    } else if (flags.useMsr) {\r\n        return basicPostsProcessor.postProcessingAnswer(ans);\r\n    } else {\r\n        return ctpPostProcessor.postProcessingAnswer(ans, flags.suppressMidDotPostprocessing);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.quoteattribution.Sieves.Sieve.scanForNames",
	"Comment": "scan for all potential names based on names list, based on coremaps and returns their indices in doc.tokens as well.",
	"Method": "Pair<ArrayList<String>, ArrayList<Pair<Integer, Integer>>> scanForNames(Pair<Integer, Integer> textRun){\r\n    ArrayList<String> potentialNames = new ArrayList();\r\n    ArrayList<Pair<Integer, Integer>> nameIndices = new ArrayList();\r\n    List<CoreLabel> tokens = doc.get(CoreAnnotations.TokensAnnotation.class);\r\n    Set<String> aliases = characterMap.keySet();\r\n    String potentialName = \"\";\r\n    Pair<Integer, Integer> potentialIndex = null;\r\n    for (int index = textRun.first; index <= textRun.second; index++) {\r\n        CoreLabel token = tokens.get(index);\r\n        String tokenText = token.word();\r\n        if (Character.isUpperCase(tokenText.charAt(0)) || tokenText.equals(\"de\")) {\r\n            potentialName += \" \" + tokenText;\r\n            if (potentialIndex == null)\r\n                potentialIndex = new Pair(index, index);\r\n            else\r\n                potentialIndex.second = index;\r\n        } else {\r\n            if (potentialName.length() != 0) {\r\n                String actual = potentialName.substring(1);\r\n                if (aliases.contains(actual)) {\r\n                    potentialNames.add(actual);\r\n                    nameIndices.add(potentialIndex);\r\n                } else {\r\n                    String removeFirstWord = actual.substring(actual.indexOf(\" \") + 1);\r\n                    if (aliases.contains(removeFirstWord)) {\r\n                        potentialNames.add(removeFirstWord);\r\n                        nameIndices.add(new Pair(potentialIndex.first + 1, potentialIndex.second));\r\n                    }\r\n                }\r\n                potentialName = \"\";\r\n                potentialIndex = null;\r\n            }\r\n        }\r\n    }\r\n    if (potentialName.length() != 0) {\r\n        if (aliases.contains(potentialName.substring(1))) {\r\n            potentialNames.add(potentialName.substring(1));\r\n            nameIndices.add(potentialIndex);\r\n        }\r\n    }\r\n    return new Pair(potentialNames, nameIndices);\r\n}"
}, {
	"Path": "org.deeplearning4j.BaseDL4JTest.getProfilingMode",
	"Comment": "override this to set the profiling mode for the tests defined in the child class",
	"Method": "OpExecutioner.ProfilingMode getProfilingMode(){\r\n    return OpExecutioner.ProfilingMode.SCOPE_PANIC;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.arabic.ArabicHeadFinder.findMarkedHead",
	"Comment": "predicatively marked elements in a sentence should be noted as heads",
	"Method": "Tree findMarkedHead(Tree t){\r\n    String cat = t.value();\r\n    if (cat.equals(\"S\")) {\r\n        Tree[] kids = t.children();\r\n        for (Tree kid : kids) {\r\n            if (predPattern.matcher(kid.value()).matches()) {\r\n                return kid;\r\n            }\r\n        }\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.learning.impl.elements.RandomUtils.nextLong",
	"Comment": "returns the next pseudorandom, uniformly distributed long valuefrom the given random sequence.",
	"Method": "long nextLong(long nextLong,Random random){\r\n    return random.nextLong();\r\n}"
}, {
	"Path": "org.deeplearning4j.util.ConvolutionUtils.validateConvolutionModePadding",
	"Comment": "check that the convolution mode is consistent with the padding specification",
	"Method": "void validateConvolutionModePadding(ConvolutionMode mode,int[] padding){\r\n    if (mode == ConvolutionMode.Same) {\r\n        boolean nullPadding = true;\r\n        for (int i : padding) {\r\n            if (i != 0)\r\n                nullPadding = false;\r\n        }\r\n        if (!nullPadding)\r\n            throw new IllegalArgumentException(\"Padding cannot be used when using the `same' convolution mode\");\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence",
	"Comment": "runs the viterbi algorithm on the sequence model given by the tagscorerin order to find the best sequence.",
	"Method": "int[] bestSequence(SequenceModel ts,Pair<int[], Double> bestSequence,SequenceModel ts,double[][] linearConstraints){\r\n    int length = ts.length();\r\n    int leftWindow = ts.leftWindow();\r\n    int rightWindow = ts.rightWindow();\r\n    int padLength = length + leftWindow + rightWindow;\r\n    if (linearConstraints != null && linearConstraints.length != padLength)\r\n        throw new RuntimeException(\"linearConstraints.length (\" + linearConstraints.length + \") does not match padLength (\" + padLength + \") of SequenceModel\" + \", length==\" + length + \", leftW=\" + leftWindow + \", rightW=\" + rightWindow);\r\n    int[][] tags = new int[padLength][];\r\n    int[] tagNum = new int[padLength];\r\n    if (DEBUG) {\r\n        log.info(\"Doing bestSequence length \" + length + \"; leftWin \" + leftWindow + \"; rightWin \" + rightWindow + \"; padLength \" + padLength);\r\n    }\r\n    for (int pos = 0; pos < padLength; pos++) {\r\n        if (Thread.interrupted()) {\r\n            throw new RuntimeInterruptedException();\r\n        }\r\n        tags[pos] = ts.getPossibleValues(pos);\r\n        tagNum[pos] = tags[pos].length;\r\n        if (DEBUG) {\r\n            log.info(\"There are \" + tagNum[pos] + \" values at position \" + pos + \": \" + Arrays.toString(tags[pos]));\r\n        }\r\n    }\r\n    int[] tempTags = new int[padLength];\r\n    int[] productSizes = new int[padLength];\r\n    int curProduct = 1;\r\n    for (int i = 0; i < leftWindow + rightWindow; i++) {\r\n        curProduct *= tagNum[i];\r\n    }\r\n    for (int pos = leftWindow + rightWindow; pos < padLength; pos++) {\r\n        if (Thread.interrupted()) {\r\n            throw new RuntimeInterruptedException();\r\n        }\r\n        if (pos > leftWindow + rightWindow) {\r\n            curProduct /= tagNum[pos - leftWindow - rightWindow - 1];\r\n        }\r\n        curProduct *= tagNum[pos];\r\n        productSizes[pos - rightWindow] = curProduct;\r\n    }\r\n    double[][] windowScore = new double[padLength][];\r\n    for (int pos = leftWindow; pos < leftWindow + length; pos++) {\r\n        if (Thread.interrupted()) {\r\n            throw new RuntimeInterruptedException();\r\n        }\r\n        if (DEBUG) {\r\n            log.info(\"scoring word \" + pos + \" / \" + (leftWindow + length) + \", productSizes =  \" + productSizes[pos] + \", tagNum = \" + tagNum[pos] + \"...\");\r\n        }\r\n        windowScore[pos] = new double[productSizes[pos]];\r\n        Arrays.fill(tempTags, tags[0][0]);\r\n        if (DEBUG) {\r\n            log.info(\"windowScore[\" + pos + \"] has size (productSizes[pos]) \" + windowScore[pos].length);\r\n        }\r\n        for (int product = 0; product < productSizes[pos]; product++) {\r\n            int p = product;\r\n            int shift = 1;\r\n            for (int curPos = pos + rightWindow; curPos >= pos - leftWindow; curPos--) {\r\n                tempTags[curPos] = tags[curPos][p % tagNum[curPos]];\r\n                p /= tagNum[curPos];\r\n                if (curPos > pos) {\r\n                    shift *= tagNum[curPos];\r\n                }\r\n            }\r\n            if (tempTags[pos] == tags[pos][0]) {\r\n                double[] scores = ts.scoresOf(tempTags, pos);\r\n                if (DEBUG) {\r\n                    log.info(\"Matched at array index [product] \" + product + \"; tempTags[pos] == tags[pos][0] == \" + tempTags[pos]);\r\n                }\r\n                if (DEBUG) {\r\n                    log.info(\"For pos \" + pos + \" scores.length is \" + scores.length + \"; tagNum[pos] = \" + tagNum[pos] + \"; windowScore[pos].length = \" + windowScore[pos].length);\r\n                }\r\n                if (DEBUG) {\r\n                    log.info(\"scores: \" + Arrays.toString(scores));\r\n                }\r\n                for (int t = 0; t < tagNum[pos]; t++) {\r\n                    if (DEBUG) {\r\n                        log.info(\"Setting value of windowScore[\" + pos + \"][\" + product + \"+\" + t + \"*\" + shift + \"] = \" + scores[t]);\r\n                    }\r\n                    windowScore[pos][product + t * shift] = scores[t];\r\n                }\r\n            }\r\n        }\r\n    }\r\n    double[][] score = new double[padLength][];\r\n    int[][] trace = new int[padLength][];\r\n    for (int pos = 0; pos < padLength; pos++) {\r\n        score[pos] = new double[productSizes[pos]];\r\n        trace[pos] = new int[productSizes[pos]];\r\n    }\r\n    for (int pos = leftWindow; pos < length + leftWindow; pos++) {\r\n        for (int product = 0; product < productSizes[pos]; product++) {\r\n            if (Thread.interrupted()) {\r\n                throw new RuntimeInterruptedException();\r\n            }\r\n            if (pos == leftWindow) {\r\n                score[pos][product] = windowScore[pos][product];\r\n                if (linearConstraints != null) {\r\n                    if (DEBUG) {\r\n                        if (linearConstraints[pos][product % tagNum[pos]] != 0) {\r\n                            log.info(\"Applying linear constraints=\" + linearConstraints[pos][product % tagNum[pos]] + \" to preScore=\" + windowScore[pos][product] + \" at pos=\" + pos + \" for tag=\" + (product % tagNum[pos]));\r\n                        }\r\n                    }\r\n                    score[pos][product] += linearConstraints[pos][product % tagNum[pos]];\r\n                }\r\n                trace[pos][product] = -1;\r\n            } else {\r\n                score[pos][product] = Double.NEGATIVE_INFINITY;\r\n                trace[pos][product] = -1;\r\n                int sharedProduct = product / tagNum[pos + rightWindow];\r\n                int factor = productSizes[pos] / tagNum[pos + rightWindow];\r\n                for (int newTagNum = 0; newTagNum < tagNum[pos - leftWindow - 1]; newTagNum++) {\r\n                    int predProduct = newTagNum * factor + sharedProduct;\r\n                    double predScore = score[pos - 1][predProduct] + windowScore[pos][product];\r\n                    if (linearConstraints != null) {\r\n                        if (DEBUG) {\r\n                            if (pos == 2 && linearConstraints[pos][product % tagNum[pos]] != 0) {\r\n                                log.info(\"Applying linear constraints=\" + linearConstraints[pos][product % tagNum[pos]] + \" to preScore=\" + predScore + \" at pos=\" + pos + \" for tag=\" + (product % tagNum[pos]));\r\n                                log.info(\"predScore:\" + predScore + \" = score[\" + (pos - 1) + \"][\" + predProduct + \"]:\" + score[pos - 1][predProduct] + \" + windowScore[\" + pos + \"][\" + product + \"]:\" + windowScore[pos][product]);\r\n                            }\r\n                        }\r\n                        predScore += linearConstraints[pos][product % tagNum[pos]];\r\n                    }\r\n                    if (predScore > score[pos][product]) {\r\n                        score[pos][product] = predScore;\r\n                        trace[pos][product] = predProduct;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    double bestFinalScore = Double.NEGATIVE_INFINITY;\r\n    int bestCurrentProduct = -1;\r\n    for (int product = 0; product < productSizes[leftWindow + length - 1]; product++) {\r\n        if (score[leftWindow + length - 1][product] > bestFinalScore) {\r\n            bestCurrentProduct = product;\r\n            bestFinalScore = score[leftWindow + length - 1][product];\r\n        }\r\n    }\r\n    int lastProduct = bestCurrentProduct;\r\n    for (int last = padLength - 1; last >= length - 1 && last >= 0; last--) {\r\n        tempTags[last] = tags[last][lastProduct % tagNum[last]];\r\n        lastProduct /= tagNum[last];\r\n    }\r\n    for (int pos = leftWindow + length - 2; pos >= leftWindow; pos--) {\r\n        int bestNextProduct = bestCurrentProduct;\r\n        bestCurrentProduct = trace[pos + 1][bestNextProduct];\r\n        tempTags[pos - leftWindow] = tags[pos - leftWindow][bestCurrentProduct / (productSizes[pos] / tagNum[pos - leftWindow])];\r\n    }\r\n    return new Pair(tempTags, bestFinalScore);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiffOpExecutioner.push",
	"Comment": "this method ensures all operations that supposed to be executed at this moment, are executed.",
	"Method": "void push(){\r\n    backendExecutioner.push();\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.blas.impl.BaseLevel2.tbsv",
	"Comment": "?tbsv solves a system of linear equations whose coefficients are in a triangular band matrix.",
	"Method": "void tbsv(char order,char Uplo,char TransA,char Diag,INDArray A,INDArray X){\r\n    if (Nd4j.getExecutioner().getProfilingMode() == OpExecutioner.ProfilingMode.ALL)\r\n        OpProfiler.getInstance().processBlasCall(false, A, X);\r\n    if (X.data().dataType() == DataBuffer.Type.DOUBLE) {\r\n        DefaultOpExecutioner.validateDataType(DataBuffer.Type.DOUBLE, A, X);\r\n        dtbsv(order, Uplo, TransA, Diag, (int) X.length(), (int) A.columns(), A, (int) A.size(0), X, X.majorStride());\r\n    } else {\r\n        DefaultOpExecutioner.validateDataType(DataBuffer.Type.FLOAT, A, X);\r\n        stbsv(order, Uplo, TransA, Diag, (int) X.length(), (int) A.columns(), A, (int) A.size(0), X, X.majorStride());\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.XMLOutputter.annotationToDoc",
	"Comment": "converts the given annotation to an xml document using the specified options",
	"Method": "Document annotationToDoc(Annotation annotation,StanfordCoreNLP pipeline,Document annotationToDoc,Annotation annotation,Options options){\r\n    Element root = new Element(\"root\", NAMESPACE_URI);\r\n    Document xmlDoc = new Document(root);\r\n    ProcessingInstruction pi = new ProcessingInstruction(\"xml-stylesheet\", \"href=\\\"\" + STYLESHEET_NAME + \"\\\" type=\\\"text/xsl\\\"\");\r\n    xmlDoc.insertChild(pi, 0);\r\n    Element docElem = new Element(\"document\", NAMESPACE_URI);\r\n    root.appendChild(docElem);\r\n    setSingleElement(docElem, \"docId\", NAMESPACE_URI, annotation.get(CoreAnnotations.DocIDAnnotation.class));\r\n    setSingleElement(docElem, \"docDate\", NAMESPACE_URI, annotation.get(CoreAnnotations.DocDateAnnotation.class));\r\n    setSingleElement(docElem, \"docSourceType\", NAMESPACE_URI, annotation.get(CoreAnnotations.DocSourceTypeAnnotation.class));\r\n    setSingleElement(docElem, \"docType\", NAMESPACE_URI, annotation.get(CoreAnnotations.DocTypeAnnotation.class));\r\n    setSingleElement(docElem, \"author\", NAMESPACE_URI, annotation.get(CoreAnnotations.AuthorAnnotation.class));\r\n    setSingleElement(docElem, \"location\", NAMESPACE_URI, annotation.get(CoreAnnotations.LocationAnnotation.class));\r\n    if (options.includeText) {\r\n        setSingleElement(docElem, \"text\", NAMESPACE_URI, annotation.get(CoreAnnotations.TextAnnotation.class));\r\n    }\r\n    Element sentencesElem = new Element(\"sentences\", NAMESPACE_URI);\r\n    docElem.appendChild(sentencesElem);\r\n    if (annotation.get(CoreAnnotations.SentencesAnnotation.class) != null) {\r\n        int sentCount = 1;\r\n        for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {\r\n            Element sentElem = new Element(\"sentence\", NAMESPACE_URI);\r\n            sentElem.addAttribute(new Attribute(\"id\", Integer.toString(sentCount)));\r\n            Integer lineNumber = sentence.get(CoreAnnotations.LineNumberAnnotation.class);\r\n            if (lineNumber != null) {\r\n                sentElem.addAttribute(new Attribute(\"line\", Integer.toString(lineNumber)));\r\n            }\r\n            sentCount++;\r\n            Element wordTable = new Element(\"tokens\", NAMESPACE_URI);\r\n            List<CoreLabel> tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);\r\n            for (int j = 0; j < tokens.size(); j++) {\r\n                Element wordInfo = new Element(\"token\", NAMESPACE_URI);\r\n                addWordInfo(wordInfo, tokens.get(j), j + 1, NAMESPACE_URI);\r\n                wordTable.appendChild(wordInfo);\r\n            }\r\n            sentElem.appendChild(wordTable);\r\n            Tree tree = sentence.get(TreeCoreAnnotations.TreeAnnotation.class);\r\n            if (tree != null) {\r\n                Element parseInfo = new Element(\"parse\", NAMESPACE_URI);\r\n                addConstituentTreeInfo(parseInfo, tree, options.constituencyTreePrinter);\r\n                sentElem.appendChild(parseInfo);\r\n            }\r\n            SemanticGraph basicDependencies = sentence.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class);\r\n            if (basicDependencies != null) {\r\n                Element depInfo = buildDependencyTreeInfo(\"basic-dependencies\", sentence.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class), tokens, NAMESPACE_URI);\r\n                if (depInfo != null) {\r\n                    sentElem.appendChild(depInfo);\r\n                }\r\n                depInfo = buildDependencyTreeInfo(\"collapsed-dependencies\", sentence.get(SemanticGraphCoreAnnotations.CollapsedDependenciesAnnotation.class), tokens, NAMESPACE_URI);\r\n                if (depInfo != null) {\r\n                    sentElem.appendChild(depInfo);\r\n                }\r\n                depInfo = buildDependencyTreeInfo(\"collapsed-ccprocessed-dependencies\", sentence.get(SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation.class), tokens, NAMESPACE_URI);\r\n                if (depInfo != null) {\r\n                    sentElem.appendChild(depInfo);\r\n                }\r\n                depInfo = buildDependencyTreeInfo(\"enhanced-dependencies\", sentence.get(SemanticGraphCoreAnnotations.EnhancedDependenciesAnnotation.class), tokens, NAMESPACE_URI);\r\n                if (depInfo != null) {\r\n                    sentElem.appendChild(depInfo);\r\n                }\r\n                depInfo = buildDependencyTreeInfo(\"enhanced-plus-plus-dependencies\", sentence.get(SemanticGraphCoreAnnotations.EnhancedPlusPlusDependenciesAnnotation.class), tokens, NAMESPACE_URI);\r\n                if (depInfo != null) {\r\n                    sentElem.appendChild(depInfo);\r\n                }\r\n            }\r\n            Collection<RelationTriple> openieTriples = sentence.get(NaturalLogicAnnotations.RelationTriplesAnnotation.class);\r\n            if (openieTriples != null) {\r\n                Element openieElem = new Element(\"openie\", NAMESPACE_URI);\r\n                addTriples(openieTriples, openieElem, NAMESPACE_URI);\r\n                sentElem.appendChild(openieElem);\r\n            }\r\n            Collection<RelationTriple> kbpTriples = sentence.get(CoreAnnotations.KBPTriplesAnnotation.class);\r\n            if (kbpTriples != null) {\r\n                Element kbpElem = new Element(\"kbp\", NAMESPACE_URI);\r\n                addTriples(kbpTriples, kbpElem, NAMESPACE_URI);\r\n                sentElem.appendChild(kbpElem);\r\n            }\r\n            List<EntityMention> entities = sentence.get(MachineReadingAnnotations.EntityMentionsAnnotation.class);\r\n            List<RelationMention> relations = sentence.get(MachineReadingAnnotations.RelationMentionsAnnotation.class);\r\n            if (entities != null && !entities.isEmpty()) {\r\n                Element mrElem = new Element(\"MachineReading\", NAMESPACE_URI);\r\n                Element entElem = new Element(\"entities\", NAMESPACE_URI);\r\n                addEntities(entities, entElem, NAMESPACE_URI);\r\n                mrElem.appendChild(entElem);\r\n                if (relations != null) {\r\n                    Element relElem = new Element(\"relations\", NAMESPACE_URI);\r\n                    addRelations(relations, relElem, NAMESPACE_URI, options.relationsBeam);\r\n                    mrElem.appendChild(relElem);\r\n                }\r\n                sentElem.appendChild(mrElem);\r\n            }\r\n            Tree sentimentTree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);\r\n            if (sentimentTree != null) {\r\n                int sentiment = RNNCoreAnnotations.getPredictedClass(sentimentTree);\r\n                sentElem.addAttribute(new Attribute(\"sentimentValue\", Integer.toString(sentiment)));\r\n                String sentimentClass = sentence.get(SentimentCoreAnnotations.SentimentClass.class);\r\n                sentElem.addAttribute(new Attribute(\"sentiment\", sentimentClass.replaceAll(\" \", \"\")));\r\n            }\r\n            sentencesElem.appendChild(sentElem);\r\n        }\r\n    }\r\n    Map<Integer, CorefChain> corefChains = annotation.get(CorefCoreAnnotations.CorefChainAnnotation.class);\r\n    if (corefChains != null) {\r\n        List<CoreMap> sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);\r\n        Element corefInfo = new Element(\"coreference\", NAMESPACE_URI);\r\n        addCorefGraphInfo(options, corefInfo, sentences, corefChains, NAMESPACE_URI);\r\n        docElem.appendChild(corefInfo);\r\n    }\r\n    return xmlDoc;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.setCacheMode",
	"Comment": "this method sets specified cachemode for all layers within network",
	"Method": "void setCacheMode(CacheMode mode){\r\n    if (mode == null)\r\n        mode = CacheMode.NONE;\r\n    for (Layer layer : layers) {\r\n        layer.setCacheMode(mode);\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.records.Buffer.getCapacity",
	"Comment": "get the capacity, which is the maximum count that could handled withoutresizing the backing storage.",
	"Method": "int getCapacity(){\r\n    return this.get().length;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Trilean.toBoolean",
	"Comment": "convert this trilean to a boolean, with a specified default value if the truth value is unknown.",
	"Method": "boolean toBoolean(boolean valueForUnknown){\r\n    switch(value) {\r\n        case 1:\r\n            return true;\r\n        case 0:\r\n            return false;\r\n        case 2:\r\n            return valueForUnknown;\r\n        default:\r\n            throw new IllegalStateException(\"Something went very very wrong.\");\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.Clique.valueOfHelper",
	"Comment": "this version assumes relativeindices array no longer needs to be copied. further it is assumed that it has already been checked or assured by construction that relativeindices is sorted.",
	"Method": "Clique valueOfHelper(int[] relativeIndices){\r\n    Clique c = new Clique(relativeIndices);\r\n    return intern(c);\r\n}"
}, {
	"Path": "dagger.internal.SingletonBindingTest.testSingletonBindingDelegatesSetLinked",
	"Comment": "this next batch of tests validates that singletonbinding consistently delegates to the wrapped binding for state.",
	"Method": "void testSingletonBindingDelegatesSetLinked(){\r\n    singletonBinding.setLinked();\r\n    assertThat(wrappedBinding.isLinked()).isTrue();\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.setClass",
	"Comment": "set the value of the name property to the name of atheclass implementing the given interface xface.an exception is thrown if theclass does not implement theinterface xface.",
	"Method": "void setClass(String name,Class<?> theClass,Class<?> xface){\r\n    if (!xface.isAssignableFrom(theClass))\r\n        throw new RuntimeException(theClass + \" not \" + xface.getName());\r\n    set(name, theClass.getName());\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.addDefaultResource",
	"Comment": "add a default resource. resources are loaded in the order of the resourcesadded.",
	"Method": "void addDefaultResource(String name){\r\n    ArrayList<Configuration> toReload;\r\n    synchronized (Configuration.class) {\r\n        if (defaultResources.contains(name)) {\r\n            return;\r\n        }\r\n        defaultResources.add(name);\r\n        toReload = new ArrayList(REGISTRY.size());\r\n        toReload.addAll(REGISTRY.keySet());\r\n    }\r\n    for (Configuration conf : toReload) {\r\n        if (conf.loadDefaults) {\r\n            conf.reloadConfiguration();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.records.Buffer.copy",
	"Comment": "copy the specified byte array to the buffer. replaces the current buffer.",
	"Method": "void copy(byte[] bytes,int offset,int length){\r\n    if (this.bytes == null || this.bytes.length < length) {\r\n        this.bytes = new byte[length];\r\n    }\r\n    System.arraycopy(bytes, offset, this.bytes, 0, length);\r\n    this.count = length;\r\n}"
}, {
	"Path": "org.datavec.spark.transform.AnalyzeSpark.sampleMostFrequentFromColumn",
	"Comment": "sample the n most frequently occurring values in the specified column",
	"Method": "Map<Writable, Long> sampleMostFrequentFromColumn(int nMostFrequent,String columnName,Schema schema,JavaRDD<List<Writable>> data){\r\n    int columnIdx = schema.getIndexOfColumn(columnName);\r\n    JavaPairRDD<Writable, Long> keyedByWritable = data.mapToPair(new ColumnToKeyPairTransform(columnIdx));\r\n    JavaPairRDD<Writable, Long> reducedByWritable = keyedByWritable.reduceByKey(new SumLongsFunction2());\r\n    List<Tuple2<Writable, Long>> list = reducedByWritable.takeOrdered(nMostFrequent, new Tuple2Comparator<Writable>(false));\r\n    List<Tuple2<Writable, Long>> sorted = new ArrayList(list);\r\n    Collections.sort(sorted, new Tuple2Comparator<Writable>(false));\r\n    Map<Writable, Long> map = new LinkedHashMap();\r\n    for (Tuple2<Writable, Long> t2 : sorted) {\r\n        map.put(t2._1(), t2._2());\r\n    }\r\n    return map;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.vertex.VertexIndices.getVertexEdgeNumber",
	"Comment": "the edge number. represents the index of the output of the vertex index, or the index of theinput to the vertex, depending on the context",
	"Method": "int getVertexEdgeNumber(){\r\n    return this.vertexEdgeNumber;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.variance",
	"Comment": "variance array reduction operation, optionally along specified dimensions",
	"Method": "SDVariable variance(SDVariable x,boolean biasCorrected,int dimensions,SDVariable variance,String name,SDVariable x,boolean biasCorrected,int dimensions,SDVariable variance,String name,SDVariable x,boolean biasCorrected,boolean keepDims,int dimensions){\r\n    SDVariable result = functionFactory.variance(x, biasCorrected, keepDims, dimensions);\r\n    return updateVariableNameAndReference(result, name);\r\n}"
}, {
	"Path": "org.datavec.arrow.ArrowConverter.field",
	"Comment": "shortcut method for returning a fieldgiven an arrow type and namewith no sub fields",
	"Method": "Field field(String name,ArrowType arrowType){\r\n    return new Field(name, FieldType.nullable(arrowType), new ArrayList<Field>());\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.LexerUtils.normalizeFractions",
	"Comment": "change precomposed fraction characters to spelled out letter forms.",
	"Method": "String normalizeFractions(boolean normalizeFractions,boolean escapeForwardSlashAsterisk,String in){\r\n    String out = in;\r\n    if (normalizeFractions) {\r\n        if (escapeForwardSlashAsterisk) {\r\n            out = ONE_FOURTH_PATTERN.matcher(out).replaceAll(\"1\\\\\\\\/4\");\r\n            out = ONE_HALF_PATTERN.matcher(out).replaceAll(\"1\\\\\\\\/2\");\r\n            out = THREE_FOURTHS_PATTERN.matcher(out).replaceAll(\"3\\\\\\\\/4\");\r\n            out = ONE_THIRD_PATTERN.matcher(out).replaceAll(\"1\\\\\\\\/3\");\r\n            out = TWO_THIRDS_PATTERN.matcher(out).replaceAll(\"2\\\\\\\\/3\");\r\n        } else {\r\n            out = ONE_FOURTH_PATTERN.matcher(out).replaceAll(\"1/4\");\r\n            out = ONE_HALF_PATTERN.matcher(out).replaceAll(\"1/2\");\r\n            out = THREE_FOURTHS_PATTERN.matcher(out).replaceAll(\"3/4\");\r\n            out = ONE_THIRD_PATTERN.matcher(out).replaceAll(\"1/3\");\r\n            out = TWO_THIRDS_PATTERN.matcher(out).replaceAll(\"2/3\");\r\n        }\r\n    }\r\n    return out;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.sequencevectors.graph.walkers.impl.WeightedWalker.hasNext",
	"Comment": "this method checks, if walker has any more sequences left in queue",
	"Method": "boolean hasNext(){\r\n    return super.hasNext();\r\n}"
}, {
	"Path": "org.datavec.local.transforms.AnalyzeLocal.getUniqueSequence",
	"Comment": "get a list of unique values from the specified columns of a sequence",
	"Method": "Set<Writable> getUniqueSequence(String columnName,Schema schema,SequenceRecordReader sequenceData,Map<String, Set<Writable>> getUniqueSequence,List<String> columnNames,Schema schema,SequenceRecordReader sequenceData){\r\n    Map<String, Set<Writable>> m = new HashMap();\r\n    for (String s : columnNames) {\r\n        m.put(s, new HashSet());\r\n    }\r\n    while (sequenceData.hasNext()) {\r\n        List<List<Writable>> next = sequenceData.sequenceRecord();\r\n        for (List<Writable> step : next) {\r\n            for (String s : columnNames) {\r\n                int idx = schema.getIndexOfColumn(s);\r\n                m.get(s).add(step.get(idx));\r\n            }\r\n        }\r\n    }\r\n    return m;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.JFlexDummyLexer.yypushback",
	"Comment": "pushes the specified amount of characters back into the input stream.they will be read again by then next call of the scanning method",
	"Method": "void yypushback(int number){\r\n    if (number > yylength())\r\n        zzScanError(ZZ_PUSHBACK_2BIG);\r\n    zzMarkedPos -= number;\r\n}"
}, {
	"Path": "org.datavec.api.formats.input.impl.ListStringInputFormat.toDouble",
	"Comment": "convert writable to double. whether this is supported depends on the specific writable.",
	"Method": "double toDouble(){\r\n    return 0;\r\n}"
}, {
	"Path": "org.datavec.api.records.reader.impl.jdbc.JDBCRecordReader.close",
	"Comment": "expected to be called by the user. jdbc connections will not be closed automatically.",
	"Method": "void close(){\r\n    closeJdbc();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.memoryInfo",
	"Comment": "generate information regarding memory use for the network, for the given input type and minibatch size.note that when using workspaces or cudnn, the network should be trained for some iterations so that the memoryworkspaces have time to initialize. without this, the memory requirements during training may be underestimated.note also that this is the same information that is generated during an oom crash when training or performinginference.",
	"Method": "String memoryInfo(int minibatch,InputType inputType){\r\n    return CrashReportingUtil.generateMemoryStatus(this, minibatch, inputType);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.Redwood.restoreSystemStreams",
	"Comment": "restores system.out and system.err to their original values",
	"Method": "void restoreSystemStreams(){\r\n    System.setOut(realSysOut);\r\n    System.setErr(realSysErr);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.treeSkeletonConstituentCopy",
	"Comment": "returns a deep copy of everything but the leaf labels.the leaflabels are reused from the original tree.this is useful forcases such as the dependency converter, which wants to finishwith the same labels in the dependencies as the parse tree.",
	"Method": "Tree treeSkeletonConstituentCopy(Tree treeSkeletonConstituentCopy,TreeFactory tf,LabelFactory lf){\r\n    if (isLeaf()) {\r\n        Tree newLeaf = tf.newLeaf(label());\r\n        newLeaf.setLabel(label());\r\n        return newLeaf;\r\n    }\r\n    Label label = lf.newLabel(label());\r\n    Tree[] kids = children();\r\n    List<Tree> newKids = new ArrayList(kids.length);\r\n    for (Tree kid : kids) {\r\n        newKids.add(kid.treeSkeletonConstituentCopy(tf, lf));\r\n    }\r\n    return tf.newTreeNode(label, newKids);\r\n}"
}, {
	"Path": "com.atilika.kuromoji.viterbi.ViterbiBuilder.createGlueNode",
	"Comment": "create a glue node to be inserted based on viterbinode already in the lattice.the new node takes the same parameters as the node it is based on, but the word is truncated to match thehole in the lattice caused by the new user entry",
	"Method": "ViterbiNode createGlueNode(int startIndex,ViterbiNode glueBase,String surface){\r\n    return new ViterbiNode(glueBase.getWordId(), surface, glueBase.getLeftId(), glueBase.getRightId(), glueBase.getWordCost(), startIndex, ViterbiNode.Type.INSERTED);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.getUpdater",
	"Comment": "get the computationgraphupdater for the network. creates one on demand, if required",
	"Method": "ComputationGraphUpdater getUpdater(ComputationGraphUpdater getUpdater,boolean initializeIfAbsent){\r\n    if (solver == null && initializeIfAbsent) {\r\n        solver = new Solver.Builder().configure(conf()).listeners(getListeners()).model(this).build();\r\n        solver.getOptimizer().setUpdaterComputationGraph(new ComputationGraphUpdater(this));\r\n    }\r\n    if (solver != null) {\r\n        return solver.getOptimizer().getComputationGraphUpdater();\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.KBPAnnotator.convertRelationNameToLatest",
	"Comment": "convert between older naming convention and current for relation names",
	"Method": "String convertRelationNameToLatest(String relationName){\r\n    if (relationNameConversionMap.containsKey(relationName)) {\r\n        return relationNameConversionMap.get(relationName);\r\n    } else {\r\n        return relationName;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Distribution.drawSample",
	"Comment": "a method to draw a sample, providing an own random number generator.needed for the probabilitydistribution interface.",
	"Method": "E drawSample(E drawSample,Random random){\r\n    return sampleFrom(random);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.tsurgeon.JJTTsurgeonParserState.nodeCreated",
	"Comment": "determines whether the current node was actually closed and pushed.this should only be called in the final user action of a node scope.",
	"Method": "boolean nodeCreated(){\r\n    return node_created;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.inputs.InputType.getShape",
	"Comment": "returns the shape of this inputtype without minibatch dimension in the returned array",
	"Method": "long[] getShape(boolean includeBatchDim,long[] getShape,long[] getShape,boolean includeBatchDim,long[] getShape,boolean includeBatchDim,long[] getShape,boolean includeBatchDim,long[] getShape,boolean includeBatchDim,long[] getShape,boolean includeBatchDim){\r\n    return getShape(false);\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.data.DataSetIteratorFactoryProvider.trainData",
	"Comment": "get training data given some parameters for the data.data parameters map is used to specify things like batchsize data preprocessing",
	"Method": "DataSetIteratorFactory trainData(Map<String, Object> dataParameters){\r\n    return create(dataParameters);\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.randomprojection.RPUtils.query",
	"Comment": "query the tree starting from the given nodeusing the given hyper plane and similarity function",
	"Method": "RPNode query(RPNode from,RPHyperPlanes planes,INDArray x,String similarityFunction){\r\n    if (from.getLeft() == null && from.getRight() == null) {\r\n        return from;\r\n    }\r\n    INDArray hyperPlane = planes.getHyperPlaneAt(from.getDepth());\r\n    double dist = computeDistance(similarityFunction, x, hyperPlane);\r\n    if (dist <= from.getMedian()) {\r\n        return query(from.getLeft(), planes, x, similarityFunction);\r\n    } else {\r\n        return query(from.getRight(), planes, x, similarityFunction);\r\n    }\r\n}"
}, {
	"Path": "org.datavec.image.transform.ColorConversionTransform.doTransform",
	"Comment": "takes an image and returns a transformed image.uses the random object in the case of random transformations.",
	"Method": "ImageWritable doTransform(ImageWritable image,Random random){\r\n    if (image == null) {\r\n        return null;\r\n    }\r\n    Mat mat = (Mat) converter.convert(image.getFrame());\r\n    Mat result = new Mat();\r\n    try {\r\n        cvtColor(mat, result, conversionCode);\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    return new ImageWritable(converter.convert(result));\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.SentenceAlgorithms.keyphrases",
	"Comment": "the keyphrases of the sentence, using the words of the sentence to convert a span into a keyphrase.",
	"Method": "List<String> keyphrases(Function<Sentence, List<String>> toString,List<String> keyphrases){\r\n    return keyphrases(Sentence::words);\r\n}"
}, {
	"Path": "org.datavec.local.transforms.LocalTransformExecutor.convertStringInputTimeSeries",
	"Comment": "convert a string time series tothe proper writable set based on the schema.note that this does not use arrow.this just uses normal writable objects.",
	"Method": "List<List<List<Writable>>> convertStringInputTimeSeries(List<List<List<String>>> stringInput,Schema schema){\r\n    List<List<List<Writable>>> ret = new ArrayList();\r\n    for (int i = 0; i < stringInput.size(); i++) {\r\n        List<List<String>> currInput = stringInput.get(i);\r\n        List<List<Writable>> timeStepAdd = new ArrayList();\r\n        for (int j = 0; j < currInput.size(); j++) {\r\n            List<String> record = currInput.get(j);\r\n            List<Writable> recordAdd = new ArrayList();\r\n            for (int k = 0; k < record.size(); k++) {\r\n                switch(schema.getType(k)) {\r\n                    case Double:\r\n                        recordAdd.add(new DoubleWritable(Double.parseDouble(record.get(k))));\r\n                        break;\r\n                    case Float:\r\n                        recordAdd.add(new FloatWritable(Float.parseFloat(record.get(k))));\r\n                        break;\r\n                    case Integer:\r\n                        recordAdd.add(new IntWritable(Integer.parseInt(record.get(k))));\r\n                        break;\r\n                    case Long:\r\n                        recordAdd.add(new LongWritable(Long.parseLong(record.get(k))));\r\n                        break;\r\n                    case String:\r\n                        recordAdd.add(new Text(record.get(k)));\r\n                        break;\r\n                    case Time:\r\n                        recordAdd.add(new LongWritable(Long.parseLong(record.get(k))));\r\n                        break;\r\n                }\r\n            }\r\n            timeStepAdd.add(recordAdd);\r\n        }\r\n        ret.add(timeStepAdd);\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.deeplearning4j.text.tokenization.tokenizer.DefaultStreamTokenizer.nextToken",
	"Comment": "this method returns next token from prebuffered list of tokens or underlying inputstream",
	"Method": "String nextToken(){\r\n    if (!tokens.isEmpty() && position.get() < tokens.size())\r\n        return tokens.get(position.getAndIncrement());\r\n    return nextTokenFromStream();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Trees.leftEdge",
	"Comment": "returns the positional index of the left edge of a tree twithin a given root, as defined by the size of the yield of allmaterial preceding t.",
	"Method": "int leftEdge(Tree t,Tree root,boolean leftEdge,Tree t,Tree t1,MutableInteger i){\r\n    if (t == t1) {\r\n        return true;\r\n    } else if (t1.isLeaf()) {\r\n        int j = t1.yield().size();\r\n        i.set(i.intValue() + j);\r\n        return false;\r\n    } else {\r\n        for (Tree kid : t1.children()) {\r\n            if (leftEdge(t, kid, i)) {\r\n                return true;\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.matchConditionCount",
	"Comment": "returns a count of the number of elements that satisfy the condition",
	"Method": "SDVariable matchConditionCount(SDVariable in,Condition condition,SDVariable matchConditionCount,String name,SDVariable in,Condition condition,SDVariable matchConditionCount,String name,SDVariable in,Condition condition,boolean keepDim,int dimensions){\r\n    SDVariable ret = f().matchConditionCount(in, condition, keepDim, dimensions);\r\n    return updateVariableNameAndReference(ret, name);\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.JointParallelDataSetIteratorTest.testJointIterator1",
	"Comment": "simple test, checking datasets alignment. they all should have the same data for the same cycle",
	"Method": "void testJointIterator1(){\r\n    DataSetIterator iteratorA = new SimpleVariableGenerator(119, 100, 32, 100, 10);\r\n    DataSetIterator iteratorB = new SimpleVariableGenerator(119, 100, 32, 100, 10);\r\n    JointParallelDataSetIterator jpdsi = new JointParallelDataSetIterator.Builder(InequalityHandling.STOP_EVERYONE).addSourceIterator(iteratorA).addSourceIterator(iteratorB).build();\r\n    int cnt = 0;\r\n    int example = 0;\r\n    while (jpdsi.hasNext()) {\r\n        DataSet ds = jpdsi.next();\r\n        assertNotNull(\"Failed on iteration \" + cnt, ds);\r\n        assertEquals(\"Failed on iteration \" + cnt, (double) example, ds.getFeatures().meanNumber().doubleValue(), 0.001);\r\n        assertEquals(\"Failed on iteration \" + cnt, (double) example + 0.5, ds.getLabels().meanNumber().doubleValue(), 0.001);\r\n        cnt++;\r\n        if (cnt % 2 == 0)\r\n            example++;\r\n    }\r\n    assertEquals(100, example);\r\n    assertEquals(200, cnt);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.StringUtils.objectToColumnString",
	"Comment": "converts an object into a tab delimited string with given fieldsrequires the object has public access for the specified fields",
	"Method": "String objectToColumnString(Object object,String delimiter,String[] fieldNames){\r\n    StringBuilder sb = new StringBuilder();\r\n    for (String fieldName : fieldNames) {\r\n        if (sb.length() > 0) {\r\n            sb.append(delimiter);\r\n        }\r\n        try {\r\n            Field field = object.getClass().getDeclaredField(fieldName);\r\n            sb.append(field.get(object));\r\n        } catch (IllegalAccessException ex) {\r\n            Method method = object.getClass().getDeclaredMethod(\"get\" + StringUtils.capitalize(fieldName));\r\n            sb.append(method.invoke(object));\r\n        }\r\n    }\r\n    return sb.toString();\r\n}"
}, {
	"Path": "edu.stanford.nlp.time.SUTime.parseDateTime",
	"Comment": "converts a string that represents some kind of date into iso 8601 format and returns it as a sutime.timeyyyymmddthhmmss",
	"Method": "SUTime.Time parseDateTime(String dateStr,boolean allowPartial,SUTime.Time parseDateTime,String dateStr){\r\n    return parseDateTime(dateStr, false);\r\n}"
}, {
	"Path": "org.deeplearning4j.text.corpora.treeparser.TreeParser.getTreebankTrees",
	"Comment": "gets trees from text.first a sentence segmenter is used to segment the training examples in to sentences.sentences are then turned in to trees and returned.",
	"Method": "List<TreebankNode> getTreebankTrees(String text){\r\n    if (text.isEmpty())\r\n        return new ArrayList();\r\n    CAS c = pool.getCas();\r\n    c.setDocumentText(text);\r\n    tokenizer.process(c);\r\n    List<TreebankNode> ret = new ArrayList();\r\n    for (Sentence sentence : JCasUtil.select(c.getJCas(), Sentence.class)) {\r\n        List<String> tokens = new ArrayList();\r\n        CAS c2 = tokenizer.newCAS();\r\n        for (Token t : JCasUtil.selectCovered(Token.class, sentence)) tokens.add(t.getCoveredText());\r\n        c2.setDocumentText(sentence.getCoveredText());\r\n        tokenizer.process(c2);\r\n        parser.process(c2);\r\n        TopTreebankNode node = JCasUtil.selectSingle(c2.getJCas(), TopTreebankNode.class);\r\n        ret.add(node);\r\n    }\r\n    pool.releaseCas(c);\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.LogRecordHandler.signalStartTrack",
	"Comment": "signal the start of a track, i.e. that we have descended a level deeper.",
	"Method": "List<Record> signalStartTrack(Record signal){\r\n    return EMPTY;\r\n}"
}, {
	"Path": "org.datavec.api.io.DataOutputBuffer.getLength",
	"Comment": "returns the length of the valid data currently in the buffer.",
	"Method": "int getLength(int getLength){\r\n    return buffer.getLength();\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLPClient.doAnnotation",
	"Comment": "actually try to perform the annotation on the server side.this is factored out so that we can retry up to 3 times.",
	"Method": "void doAnnotation(Annotation annotation,Backend backend,URL serverURL,byte[] message,int tries){\r\n    try {\r\n        URLConnection connection = serverURL.openConnection();\r\n        if (apiKey != null && apiSecret != null) {\r\n            String userpass = apiKey + ':' + apiSecret;\r\n            String basicAuth = \"Basic \" + new String(Base64.getEncoder().encode(userpass.getBytes()));\r\n            connection.setRequestProperty(\"Authorization\", basicAuth);\r\n        }\r\n        connection.setDoOutput(true);\r\n        connection.setRequestProperty(\"Content-Type\", \"application/x-protobuf\");\r\n        connection.setRequestProperty(\"Content-Length\", Integer.toString(message.length));\r\n        connection.setRequestProperty(\"Accept-Charset\", \"utf-8\");\r\n        connection.setRequestProperty(\"User-Agent\", StanfordCoreNLPClient.class.getName());\r\n        switch(backend.protocol) {\r\n            case \"https\":\r\n            case \"http\":\r\n                ((HttpURLConnection) connection).setRequestMethod(\"POST\");\r\n                break;\r\n            default:\r\n                throw new IllegalStateException(\"Haven't implemented protocol: \" + backend.protocol);\r\n        }\r\n        connection.connect();\r\n        connection.getOutputStream().write(message);\r\n        connection.getOutputStream().flush();\r\n        Annotation response = serializer.read(connection.getInputStream()).first;\r\n        for (Class key : response.keySet()) {\r\n            annotation.set(key, response.get(key));\r\n        }\r\n    } catch (Throwable t) {\r\n        if (tries < 3) {\r\n            log.warn(t);\r\n            doAnnotation(annotation, backend, serverURL, message, tries + 1);\r\n        } else {\r\n            throw new RuntimeException(t);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.transform.metadata.TimeMetaData.isValid",
	"Comment": "is the given object valid for this column,given the column type and anyrestrictions given by thecolumnmetadata object?",
	"Method": "boolean isValid(Writable writable,boolean isValid,Object input){\r\n    long epochMillisec;\r\n    try {\r\n        epochMillisec = Long.parseLong(input.toString());\r\n    } catch (NumberFormatException e) {\r\n        return false;\r\n    }\r\n    if (minValidTime != null && epochMillisec < minValidTime)\r\n        return false;\r\n    return !(maxValidTime != null && epochMillisec > maxValidTime);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.UniversalSemanticHeadFinder.isVerbalAuxiliary",
	"Comment": "returns true if this tree is a preterminal that is a verbal auxiliary.",
	"Method": "boolean isVerbalAuxiliary(Tree preterminal,Set<String> verbalSet,boolean allowJustTagMatch,boolean isVerbalAuxiliary,Tree t){\r\n    return isVerbalAuxiliary(t, verbalAuxiliaries, true);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.functions.DifferentialFunction.outputVariables",
	"Comment": "return the output functions for this differential function.",
	"Method": "SDVariable[] outputVariables(SDVariable[] outputVariables,String baseName){\r\n    return outputVariables(getOwnName() != null ? getOwnName() : opName());\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.Redwood.clearHandlers",
	"Comment": "remove all log handlers from redwood, presumably in order toconstruct a custom pipeline afterwards",
	"Method": "void clearHandlers(){\r\n    handlers = new RecordHandlerTree();\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.shiftreduce.OracleTest.testEndToEndCompoundUnaries",
	"Comment": "tests that if you give the oracle a tree and ask it for asequence of transitions, applying the given transition each time,it produces the original tree again.",
	"Method": "void testEndToEndCompoundUnaries(){\r\n    List<Tree> binarizedTrees = buildTestTreebank();\r\n    Oracle oracle = new Oracle(binarizedTrees, true, Collections.singleton(\"ROOT\"));\r\n    runEndToEndTest(binarizedTrees, oracle);\r\n}"
}, {
	"Path": "org.datavec.api.transform.metadata.StringMetaData.isValid",
	"Comment": "is the given object valid for this column,given the column type and anyrestrictions given by thecolumnmetadata object?",
	"Method": "boolean isValid(Writable writable,boolean isValid,Object input){\r\n    String str = input.toString();\r\n    int len = str.length();\r\n    if (minLength != null && len < minLength)\r\n        return false;\r\n    if (maxLength != null && len > maxLength)\r\n        return false;\r\n    return regex == null || str.matches(regex);\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseSparseNDArrayCOO.translateToPhysical",
	"Comment": "translate the view index to the corresponding index of the original ndarray",
	"Method": "long[] translateToPhysical(long[] virtualIndexes,int[] translateToPhysical,int[] virtualIndexes){\r\n    int[] physicalIndexes = new int[underlyingRank()];\r\n    int idxPhy = 0;\r\n    int hidden = 0;\r\n    for (int idxVir = 0; idxVir < virtualIndexes.length; idxVir++) {\r\n        if (hidden < getNumHiddenDimension() && hiddenDimensions()[hidden] == idxVir) {\r\n            hidden++;\r\n        } else {\r\n            while (idxPhy < underlyingRank() && isDimensionFixed(idxPhy)) {\r\n                physicalIndexes[idxPhy] = sparseOffsets()[idxPhy];\r\n                idxPhy++;\r\n            }\r\n            if (idxPhy < underlyingRank() && !isDimensionFixed(idxPhy)) {\r\n                physicalIndexes[idxPhy] = sparseOffsets()[idxPhy] + virtualIndexes[idxVir];\r\n                idxPhy++;\r\n            }\r\n        }\r\n    }\r\n    return physicalIndexes;\r\n}"
}, {
	"Path": "org.deeplearning4j.optimize.solvers.accumulation.IndexedTail.updatesSize",
	"Comment": "this method returns actual number of updates stored within tail",
	"Method": "int updatesSize(){\r\n    return updates.size();\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.coref",
	"Comment": "get the coreference chain for just this sentence.note that this method is actually fairly computationally expensive to call, as it constructs and prunesthe coreference data structure for the entire document.",
	"Method": "Map<Integer, CorefChain> coref(){\r\n    Map<Integer, CorefChain> allCorefs = document.coref();\r\n    Set<Integer> toDeleteEntirely = new HashSet();\r\n    for (Map.Entry<Integer, CorefChain> integerCorefChainEntry : allCorefs.entrySet()) {\r\n        CorefChain chain = integerCorefChainEntry.getValue();\r\n        List<CorefChain.CorefMention> mentions = new ArrayList(chain.getMentionsInTextualOrder());\r\n        mentions.stream().filter(m -> m.sentNum != this.sentenceIndex() + 1).forEach(chain::deleteMention);\r\n        if (chain.getMentionsInTextualOrder().isEmpty()) {\r\n            toDeleteEntirely.add(integerCorefChainEntry.getKey());\r\n        }\r\n    }\r\n    toDeleteEntirely.forEach(allCorefs::remove);\r\n    return allCorefs;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sentiment.SentimentPipeline.getAnnotations",
	"Comment": "reads an annotation from the given filename using the requested input.",
	"Method": "List<Annotation> getAnnotations(StanfordCoreNLP tokenizer,Input inputFormat,String filename,boolean filterUnknown){\r\n    switch(inputFormat) {\r\n        case TEXT:\r\n            {\r\n                String text = IOUtils.slurpFileNoExceptions(filename);\r\n                Annotation annotation = new Annotation(text);\r\n                tokenizer.annotate(annotation);\r\n                List<Annotation> annotations = Generics.newArrayList();\r\n                for (CoreMap sentence : annotation.get(CoreAnnotations.SentencesAnnotation.class)) {\r\n                    Annotation nextAnnotation = new Annotation(sentence.get(CoreAnnotations.TextAnnotation.class));\r\n                    nextAnnotation.set(CoreAnnotations.SentencesAnnotation.class, Collections.singletonList(sentence));\r\n                    annotations.add(nextAnnotation);\r\n                }\r\n                return annotations;\r\n            }\r\n        case TREES:\r\n            {\r\n                List<Tree> trees;\r\n                if (filterUnknown) {\r\n                    trees = SentimentUtils.readTreesWithGoldLabels(filename);\r\n                    trees = SentimentUtils.filterUnknownRoots(trees);\r\n                } else {\r\n                    MemoryTreebank treebank = new MemoryTreebank(\"utf-8\");\r\n                    treebank.loadPath(filename, null);\r\n                    trees = new ArrayList(treebank);\r\n                }\r\n                List<Annotation> annotations = Generics.newArrayList();\r\n                for (Tree tree : trees) {\r\n                    CoreMap sentence = new Annotation(SentenceUtils.listToString(tree.yield()));\r\n                    sentence.set(TreeCoreAnnotations.TreeAnnotation.class, tree);\r\n                    List<CoreMap> sentences = Collections.singletonList(sentence);\r\n                    Annotation annotation = new Annotation(\"\");\r\n                    annotation.set(CoreAnnotations.SentencesAnnotation.class, sentences);\r\n                    annotations.add(annotation);\r\n                }\r\n                return annotations;\r\n            }\r\n        default:\r\n            throw new IllegalArgumentException(\"Unknown format \" + inputFormat);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.lsh.RandomProjectionLSH.rawBucketOf",
	"Comment": "data elements in the same bucket as the query, without entropy",
	"Method": "INDArray rawBucketOf(INDArray query){\r\n    INDArray pattern = hash(query);\r\n    INDArray res = Nd4j.zeros(index.shape());\r\n    Nd4j.getExecutioner().exec(new BroadcastEqualTo(index, pattern, res, -1));\r\n    return res.min(-1);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.arrayToList",
	"Comment": "this method is used only for vocabcache compatibility purposes",
	"Method": "List<Byte> arrayToList(byte[] array,int codeLen,List<Integer> arrayToList,int[] array,int codeLen){\r\n    List<Integer> result = new ArrayList();\r\n    for (int x = 0; x < codeLen; x++) {\r\n        result.add(array[x]);\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.DependencyScoring.readDeps",
	"Comment": "read in typed dependencies. warning created typed dependencies are notbacked by any sort of a tree structure.",
	"Method": "List<Collection<TypedDependency>> readDeps(String filename){\r\n    LineNumberReader breader = new LineNumberReader(new FileReader(filename));\r\n    List<Collection<TypedDependency>> readDeps = new ArrayList();\r\n    Collection<TypedDependency> deps = new ArrayList();\r\n    for (String line = breader.readLine(); line != null; line = breader.readLine()) {\r\n        if (line.equals(\"null(-0,-0)\") || line.equals(\"null(-1,-1)\")) {\r\n            readDeps.add(deps);\r\n            deps = new ArrayList();\r\n            continue;\r\n        }\r\n        try {\r\n            if (line.equals(\"\")) {\r\n                if (deps.size() != 0) {\r\n                    readDeps.add(deps);\r\n                    deps = new ArrayList();\r\n                }\r\n                continue;\r\n            }\r\n            int firstParen = line.indexOf(\"(\");\r\n            int commaSpace = line.indexOf(\", \");\r\n            String depName = line.substring(0, firstParen);\r\n            String govName = line.substring(firstParen + 1, commaSpace);\r\n            String childName = line.substring(commaSpace + 2, line.length() - 1);\r\n            GrammaticalRelation grel = GrammaticalRelation.valueOf(depName);\r\n            if (depName.startsWith(\"prep_\")) {\r\n                String prep = depName.substring(5);\r\n                grel = EnglishGrammaticalRelations.getPrep(prep);\r\n            }\r\n            if (depName.startsWith(\"prepc_\")) {\r\n                String prepc = depName.substring(6);\r\n                grel = EnglishGrammaticalRelations.getPrepC(prepc);\r\n            }\r\n            if (depName.startsWith(\"conj_\")) {\r\n                String conj = depName.substring(5);\r\n                grel = EnglishGrammaticalRelations.getConj(conj);\r\n            }\r\n            if (grel == null) {\r\n                throw new RuntimeException(\"Unknown grammatical relation '\" + depName + \"'\");\r\n            }\r\n            IndexedWord govWord = new IndexedWord();\r\n            govWord.setValue(normalizeNumbers(govName));\r\n            govWord.setWord(govWord.value());\r\n            IndexedWord childWord = new IndexedWord();\r\n            childWord.setValue(normalizeNumbers(childName));\r\n            childWord.setWord(childWord.value());\r\n            TypedDependency dep = new TypedDependencyStringEquality(grel, govWord, childWord);\r\n            deps.add(dep);\r\n        } catch (Exception e) {\r\n            breader.close();\r\n            throw new RuntimeException(\"Error on line \" + breader.getLineNumber() + \":\\n\\n\" + e);\r\n        }\r\n    }\r\n    if (deps.size() != 0) {\r\n        readDeps.add(deps);\r\n    }\r\n    breader.close();\r\n    return readDeps;\r\n}"
}, {
	"Path": "org.nd4j.evaluation.EvaluationUtils.matthewsCorrelation",
	"Comment": "calculate the binary matthews correlation coefficient from counts",
	"Method": "double matthewsCorrelation(long tp,long fp,long fn,long tn){\r\n    double numerator = ((double) tp) * tn - ((double) fp) * fn;\r\n    double denominator = Math.sqrt(((double) tp + fp) * (tp + fn) * (tn + fp) * (tn + fn));\r\n    return numerator / denominator;\r\n}"
}, {
	"Path": "org.datavec.api.transform.condition.BooleanCondition.transform",
	"Comment": "get the output schema for this transformation, given an input schema",
	"Method": "Schema transform(Schema inputSchema){\r\n    return inputSchema;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.Clique.valueOf",
	"Comment": "make a clique over the provided relativeindices. relativeindices should be sorted.",
	"Method": "Clique valueOf(int maxLeft,int maxRight,Clique valueOf,int[] relativeIndices,Clique valueOf,Clique c,int offset){\r\n    int[] ri = new int[c.relativeIndices.length];\r\n    for (int i = 0; i < ri.length; i++) {\r\n        ri[i] = c.relativeIndices[i] + offset;\r\n    }\r\n    return valueOfHelper(ri);\r\n}"
}, {
	"Path": "dagger.testing.it.BuildLogValidator.assertDoesNotHaveText",
	"Comment": "processes a log file, ensuring it does not contain any of the provided strings within it.",
	"Method": "void assertDoesNotHaveText(File buildLogfile,String unexpectedStrings){\r\n    String buildOutput = getBuildOutput(buildLogfile);\r\n    StringBuilder sb = new StringBuilder(\"Build output contained unexpected text:\");\r\n    boolean found = false;\r\n    for (String unexpected : unexpectedStrings) {\r\n        if (buildOutput.contains(unexpected)) {\r\n            found = true;\r\n            sb.append(\"\\n    \\\"\").append(unexpected).append(\"\\\"\");\r\n        }\r\n    }\r\n    if (found) {\r\n        appendBuildStatus(sb, buildOutput);\r\n        throw new Exception(sb.toString());\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.optimize.api.data.DataSetIteratorFactoryProvider.trainData",
	"Comment": "get training data given some parameters for the data.data parameters map is used to specify things like batchsize data preprocessing",
	"Method": "DataSetIteratorFactory trainData(Map<String, Object> dataParameters){\r\n    return create(dataParameters);\r\n}"
}, {
	"Path": "org.deeplearning4j.nearestneighbor.model.CSVRecord.fromRow",
	"Comment": "instantiate a csv record from a vectorgiven either an input dataset and aone hot matrix, the index will be appended tothe end of the record, or for regressionit will append all values in the labels",
	"Method": "CSVRecord fromRow(DataSet row){\r\n    if (!row.getFeatures().isVector() && !row.getFeatures().isScalar())\r\n        throw new IllegalArgumentException(\"Passed in dataset must represent a scalar or vector\");\r\n    if (!row.getLabels().isVector() && !row.getLabels().isScalar())\r\n        throw new IllegalArgumentException(\"Passed in dataset labels must be a scalar or vector\");\r\n    CSVRecord record;\r\n    int idx = 0;\r\n    if (row.getLabels().sumNumber().doubleValue() == 1.0) {\r\n        String[] values = new String[row.getFeatures().columns() + 1];\r\n        for (int i = 0; i < row.getFeatures().length(); i++) {\r\n            values[idx++] = String.valueOf(row.getFeatures().getDouble(i));\r\n        }\r\n        int maxIdx = 0;\r\n        for (int i = 0; i < row.getLabels().length(); i++) {\r\n            if (row.getLabels().getDouble(maxIdx) < row.getLabels().getDouble(i)) {\r\n                maxIdx = i;\r\n            }\r\n        }\r\n        values[idx++] = String.valueOf(maxIdx);\r\n        record = new CSVRecord(values);\r\n    } else {\r\n        String[] values = new String[row.getFeatures().columns() + row.getLabels().columns()];\r\n        for (int i = 0; i < row.getFeatures().length(); i++) {\r\n            values[idx++] = String.valueOf(row.getFeatures().getDouble(i));\r\n        }\r\n        for (int i = 0; i < row.getLabels().length(); i++) {\r\n            values[idx++] = String.valueOf(row.getLabels().getDouble(i));\r\n        }\r\n        record = new CSVRecord(values);\r\n    }\r\n    return record;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.readSequenceVectors",
	"Comment": "this method loads previously saved sequencevectors model from inputstream",
	"Method": "SequenceVectors<T> readSequenceVectors(SequenceElementFactory<T> factory,File file,SequenceVectors<T> readSequenceVectors,SequenceElementFactory<T> factory,InputStream stream){\r\n    BufferedReader reader = new BufferedReader(new InputStreamReader(stream, \"UTF-8\"));\r\n    String line = reader.readLine();\r\n    VectorsConfiguration configuration = VectorsConfiguration.fromJson(new String(Base64.decodeBase64(line), \"UTF-8\"));\r\n    AbstractCache<T> vocabCache = new AbstractCache.Builder<T>().build();\r\n    List<INDArray> rows = new ArrayList();\r\n    while ((line = reader.readLine()) != null) {\r\n        if (line.isEmpty())\r\n            continue;\r\n        ElementPair pair = ElementPair.fromEncodedJson(line);\r\n        T element = factory.deserialize(pair.getObject());\r\n        rows.add(Nd4j.create(pair.getVector()));\r\n        vocabCache.addToken(element);\r\n        vocabCache.addWordToIndex(element.getIndex(), element.getLabel());\r\n    }\r\n    reader.close();\r\n    InMemoryLookupTable<T> lookupTable = (InMemoryLookupTable<T>) // fix: add vocab cache\r\n    new InMemoryLookupTable.Builder<T>().vectorLength(rows.get(0).columns()).cache(vocabCache).build();\r\n    INDArray syn0 = Nd4j.vstack(rows);\r\n    lookupTable.setSyn0(syn0);\r\n    SequenceVectors<T> vectors = new SequenceVectors.Builder<T>(configuration).vocabCache(vocabCache).lookupTable(lookupTable).resetModel(false).build();\r\n    return vectors;\r\n}"
}, {
	"Path": "org.datavec.api.transform.ui.HtmlAnalysis.createHtmlAnalysisString",
	"Comment": "render a data analysis object as a html file. this will produce a summary table, along charts fornumerical columns. the contents of the html file are returned as a string, which should be writtento a .html file.",
	"Method": "String createHtmlAnalysisString(DataAnalysis analysis){\r\n    Configuration cfg = new Configuration(new Version(2, 3, 23));\r\n    cfg.setClassForTemplateLoading(HtmlAnalysis.class, \"/templates/\");\r\n    cfg.setIncompatibleImprovements(new Version(2, 3, 23));\r\n    cfg.setDefaultEncoding(\"UTF-8\");\r\n    cfg.setLocale(Locale.US);\r\n    cfg.setTemplateExceptionHandler(TemplateExceptionHandler.RETHROW_HANDLER);\r\n    Map<String, Object> input = new HashMap();\r\n    ObjectMapper ret = new ObjectMapper();\r\n    ret.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);\r\n    ret.configure(SerializationFeature.FAIL_ON_EMPTY_BEANS, false);\r\n    ret.configure(MapperFeature.SORT_PROPERTIES_ALPHABETICALLY, true);\r\n    ret.enable(SerializationFeature.INDENT_OUTPUT);\r\n    List<ColumnAnalysis> caList = analysis.getColumnAnalysis();\r\n    Schema schema = analysis.getSchema();\r\n    SequenceDataAnalysis sda = null;\r\n    boolean hasSLA = false;\r\n    if (analysis instanceof SequenceDataAnalysis) {\r\n        sda = (SequenceDataAnalysis) analysis;\r\n        hasSLA = sda.getSequenceLengthAnalysis() != null;\r\n    }\r\n    int n = caList.size();\r\n    if (hasSLA) {\r\n        n++;\r\n    }\r\n    String[][] table = new String[n][3];\r\n    List<DivObject> divs = new ArrayList();\r\n    List<String> histogramDivNames = new ArrayList();\r\n    if (hasSLA) {\r\n        SequenceLengthAnalysis seqLength = sda.getSequenceLengthAnalysis();\r\n        String name = \"Sequence Lengths\";\r\n        table[0][0] = name;\r\n        table[0][1] = \"(Seq Length)\";\r\n        table[0][2] = seqLength.toString().replaceAll(\",\", \", \");\r\n        table[0][2] = table[0][2].replaceAll(\" -> \", \" : \");\r\n        double[] buckets = seqLength.getHistogramBuckets();\r\n        long[] counts = seqLength.getHistogramBucketCounts();\r\n        if (buckets != null) {\r\n            RenderableComponentHistogram.Builder histBuilder = new RenderableComponentHistogram.Builder();\r\n            for (int j = 0; j < counts.length; j++) {\r\n                histBuilder.addBin(buckets[j], buckets[j + 1], counts[j]);\r\n            }\r\n            histBuilder.margins(60, 60, 90, 20);\r\n            RenderableComponentHistogram hist = histBuilder.title(name).build();\r\n            String divName = \"histdiv_\" + name.replaceAll(\"\\\\W\", \"\");\r\n            divs.add(new DivObject(divName, ret.writeValueAsString(hist)));\r\n            histogramDivNames.add(divName);\r\n        }\r\n    }\r\n    for (int i = 0; i < caList.size(); i++) {\r\n        ColumnAnalysis ca = caList.get(i);\r\n        String name = schema.getName(i);\r\n        ColumnType type = schema.getType(i);\r\n        int idx = i + (sda != null && sda.getSequenceLengthAnalysis() != null ? 1 : 0);\r\n        table[idx][0] = name;\r\n        table[idx][1] = type.toString();\r\n        table[idx][2] = ca.toString().replaceAll(\",\", \", \");\r\n        table[idx][2] = table[idx][2].replaceAll(\" -> \", \" : \");\r\n        double[] buckets;\r\n        long[] counts;\r\n        switch(type) {\r\n            case String:\r\n                StringAnalysis sa = (StringAnalysis) ca;\r\n                buckets = sa.getHistogramBuckets();\r\n                counts = sa.getHistogramBucketCounts();\r\n                break;\r\n            case Integer:\r\n                IntegerAnalysis ia = (IntegerAnalysis) ca;\r\n                buckets = ia.getHistogramBuckets();\r\n                counts = ia.getHistogramBucketCounts();\r\n                break;\r\n            case Long:\r\n                LongAnalysis la = (LongAnalysis) ca;\r\n                buckets = la.getHistogramBuckets();\r\n                counts = la.getHistogramBucketCounts();\r\n                break;\r\n            case Double:\r\n                DoubleAnalysis da = (DoubleAnalysis) ca;\r\n                buckets = da.getHistogramBuckets();\r\n                counts = da.getHistogramBucketCounts();\r\n                break;\r\n            case NDArray:\r\n                NDArrayAnalysis na = (NDArrayAnalysis) ca;\r\n                buckets = na.getHistogramBuckets();\r\n                counts = na.getHistogramBucketCounts();\r\n                break;\r\n            case Categorical:\r\n            case Time:\r\n            case Bytes:\r\n                buckets = null;\r\n                counts = null;\r\n                break;\r\n            default:\r\n                throw new RuntimeException(\"Invalid/unknown column type: \" + type);\r\n        }\r\n        if (buckets != null) {\r\n            RenderableComponentHistogram.Builder histBuilder = new RenderableComponentHistogram.Builder();\r\n            for (int j = 0; j < counts.length; j++) {\r\n                histBuilder.addBin(buckets[j], buckets[j + 1], counts[j]);\r\n            }\r\n            histBuilder.margins(60, 60, 90, 20);\r\n            RenderableComponentHistogram hist = histBuilder.title(name).build();\r\n            String divName = \"histdiv_\" + name.replaceAll(\"\\\\W\", \"\");\r\n            divs.add(new DivObject(divName, ret.writeValueAsString(hist)));\r\n            histogramDivNames.add(divName);\r\n        }\r\n    }\r\n    RenderableComponentTable rct = new RenderableComponentTable.Builder().table(table).header(\"Column Name\", \"Column Type\", \"Column Analysis\").backgroundColor(\"#FFFFFF\").headerColor(\"#CCCCCC\").colWidthsPercent(20, 10, 70).border(1).padLeftPx(4).padRightPx(4).build();\r\n    divs.add(new DivObject(\"tablesource\", ret.writeValueAsString(rct)));\r\n    input.put(\"divs\", divs);\r\n    input.put(\"histogramIDs\", histogramDivNames);\r\n    DateTimeFormatter formatter = DateTimeFormat.forPattern(\"YYYY-MM-dd HH:mm:ss zzz\").withZone(DateTimeZone.UTC);\r\n    long currTime = System.currentTimeMillis();\r\n    String dateTime = formatter.print(currTime);\r\n    input.put(\"datetime\", dateTime);\r\n    Template template = cfg.getTemplate(\"analysis.ftl\");\r\n    Writer stringWriter = new StringWriter();\r\n    template.process(input, stringWriter);\r\n    return stringWriter.toString();\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.TrainingConfig.labelIdx",
	"Comment": "get the index of the label array that the specified variable is associated with",
	"Method": "int labelIdx(String s){\r\n    return dataSetLabelMapping.indexOf(s);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.SemgrexPattern.matcher",
	"Comment": "these get implemented in semgrex.coordinationmatcher and nodematcher",
	"Method": "SemgrexMatcher matcher(SemanticGraph sg,IndexedWord node,Map<String, IndexedWord> namesToNodes,Map<String, String> namesToRelations,VariableStrings variableStrings,boolean ignoreCase,SemgrexMatcher matcher,SemanticGraph sg,Alignment alignment,SemanticGraph sg_align,boolean hypToText,IndexedWord node,Map<String, IndexedWord> namesToNodes,Map<String, String> namesToRelations,VariableStrings variableStrings,boolean ignoreCase,SemgrexMatcher matcher,SemanticGraph sg,SemgrexMatcher matcher,SemanticGraph sg,Map<String, IndexedWord> variables,SemgrexMatcher matcher,SemanticGraph sg,boolean ignoreCase,SemgrexMatcher matcher,SemanticGraph hypGraph,Alignment alignment,SemanticGraph txtGraph,SemgrexMatcher matcher,SemanticGraph hypGraph,Alignment alignment,SemanticGraph txtGraph,boolean ignoreCase){\r\n    return matcher(hypGraph, alignment, txtGraph, true, hypGraph.getFirstRoot(), Generics.newHashMap(), Generics.newHashMap(), new VariableStrings(), ignoreCase);\r\n}"
}, {
	"Path": "org.datavec.audio.dsp.LinearInterpolation.interpolate",
	"Comment": "do interpolation on the samples according to the original and destinated sample rates",
	"Method": "short[] interpolate(int oldSampleRate,int newSampleRate,short[] samples){\r\n    if (oldSampleRate == newSampleRate) {\r\n        return samples;\r\n    }\r\n    int newLength = Math.round(((float) samples.length / oldSampleRate * newSampleRate));\r\n    float lengthMultiplier = (float) newLength / samples.length;\r\n    short[] interpolatedSamples = new short[newLength];\r\n    for (int i = 0; i < newLength; i++) {\r\n        float currentPosition = i / lengthMultiplier;\r\n        int nearestLeftPosition = (int) currentPosition;\r\n        int nearestRightPosition = nearestLeftPosition + 1;\r\n        if (nearestRightPosition >= samples.length) {\r\n            nearestRightPosition = samples.length - 1;\r\n        }\r\n        float slope = samples[nearestRightPosition] - samples[nearestLeftPosition];\r\n        float positionFromLeft = currentPosition - nearestLeftPosition;\r\n        interpolatedSamples[i] = (short) (slope * positionFromLeft + samples[nearestLeftPosition]);\r\n    }\r\n    return interpolatedSamples;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.wordstore.VocabularyHolder.updateHuffmanCodes",
	"Comment": "build binary tree ordered by counter.based on original w2v by google",
	"Method": "List<VocabularyWord> updateHuffmanCodes(){\r\n    int min1i;\r\n    int min2i;\r\n    int b;\r\n    int i;\r\n    List<VocabularyWord> vocab = this.words();\r\n    int[] count = new int[vocab.size() * 2 + 1];\r\n    int[] parent_node = new int[vocab.size() * 2 + 1];\r\n    byte[] binary = new byte[vocab.size() * 2 + 1];\r\n    for (int a = 0; a < vocab.size(); a++) count[a] = vocab.get(a).getCount();\r\n    for (int a = vocab.size(); a < vocab.size() * 2; a++) count[a] = Integer.MAX_VALUE;\r\n    int pos1 = vocab.size() - 1;\r\n    int pos2 = vocab.size();\r\n    for (int a = 0; a < vocab.size(); a++) {\r\n        if (pos1 >= 0) {\r\n            if (count[pos1] < count[pos2]) {\r\n                min1i = pos1;\r\n                pos1--;\r\n            } else {\r\n                min1i = pos2;\r\n                pos2++;\r\n            }\r\n        } else {\r\n            min1i = pos2;\r\n            pos2++;\r\n        }\r\n        if (pos1 >= 0) {\r\n            if (count[pos1] < count[pos2]) {\r\n                min2i = pos1;\r\n                pos1--;\r\n            } else {\r\n                min2i = pos2;\r\n                pos2++;\r\n            }\r\n        } else {\r\n            min2i = pos2;\r\n            pos2++;\r\n        }\r\n        count[vocab.size() + a] = count[min1i] + count[min2i];\r\n        parent_node[min1i] = vocab.size() + a;\r\n        parent_node[min2i] = vocab.size() + a;\r\n        binary[min2i] = 1;\r\n    }\r\n    byte[] code = new byte[MAX_CODE_LENGTH];\r\n    int[] point = new int[MAX_CODE_LENGTH];\r\n    for (int a = 0; a < vocab.size(); a++) {\r\n        b = a;\r\n        i = 0;\r\n        byte[] lcode = new byte[MAX_CODE_LENGTH];\r\n        int[] lpoint = new int[MAX_CODE_LENGTH];\r\n        while (true) {\r\n            code[i] = binary[b];\r\n            point[i] = b;\r\n            i++;\r\n            b = parent_node[b];\r\n            if (b == vocab.size() * 2 - 2)\r\n                break;\r\n        }\r\n        lpoint[0] = vocab.size() - 2;\r\n        for (b = 0; b < i; b++) {\r\n            lcode[i - b - 1] = code[b];\r\n            lpoint[i - b] = point[b] - vocab.size();\r\n        }\r\n        vocab.get(a).setHuffmanNode(new HuffmanNode(lcode, lpoint, a, (byte) i));\r\n    }\r\n    idxMap.clear();\r\n    for (VocabularyWord word : vocab) {\r\n        idxMap.put(word.getHuffmanNode().getIdx(), word);\r\n    }\r\n    return vocab;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.LossLayer.computeScoreForExamples",
	"Comment": "compute the score for each example individually, after labels and input have been set.",
	"Method": "INDArray computeScoreForExamples(double fullNetworkL1,double fullNetworkL2,LayerWorkspaceMgr workspaceMgr){\r\n    if (input == null || labels == null)\r\n        throw new IllegalStateException(\"Cannot calculate score without input and labels \" + layerId());\r\n    INDArray preOut = input;\r\n    ILossFunction lossFunction = layerConf().getLossFn();\r\n    INDArray scoreArray = lossFunction.computeScoreArray(getLabels2d(), preOut, layerConf().getActivationFn(), maskArray);\r\n    double l1l2 = fullNetworkL1 + fullNetworkL2;\r\n    if (l1l2 != 0.0) {\r\n        scoreArray.addi(l1l2);\r\n    }\r\n    return workspaceMgr.leverageTo(ArrayType.ACTIVATIONS, scoreArray);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.paragraphvectors.ParagraphVectorsTest.arraysSimilarity",
	"Comment": "left as reference implementation, before stuff was changed in w2v",
	"Method": "double arraysSimilarity(INDArray array1,INDArray array2){\r\n    if (array1.equals(array2))\r\n        return 1.0;\r\n    INDArray vector = Transforms.unitVec(array1);\r\n    INDArray vector2 = Transforms.unitVec(array2);\r\n    if (vector == null || vector2 == null)\r\n        return -1;\r\n    return Transforms.cosineSim(vector, vector2);\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.optimizedDotProduct",
	"Comment": "this method does not check entries for nan or infinity values in thedoubles returned. it also only iterates over the counter with the smallestnumber of keys to help speed up computation. pair this method withnormalizing your counters before hand and you have a reasonably quickimplementation of cosine.",
	"Method": "double optimizedDotProduct(Counter<E> c1,Counter<E> c2){\r\n    int size1 = c1.size();\r\n    int size2 = c2.size();\r\n    if (size1 < size2) {\r\n        return getDotProd(c1, c2);\r\n    } else {\r\n        return getDotProd(c2, c1);\r\n    }\r\n}"
}, {
	"Path": "org.datavec.arrow.ArrowConverter.toArrowWritablesSingle",
	"Comment": "return a singular record based on the convertedwritables result.",
	"Method": "List<Writable> toArrowWritablesSingle(List<FieldVector> fieldVectors,Schema schema){\r\n    return toArrowWritables(fieldVectors, schema).get(0);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.childPairs",
	"Comment": "returns a list of pairs of a relation name and the childindexedfeaturelabel that bears that relation.",
	"Method": "List<Pair<GrammaticalRelation, IndexedWord>> childPairs(IndexedWord vertex){\r\n    if (!containsVertex(vertex)) {\r\n        throw new IllegalArgumentException();\r\n    }\r\n    List<Pair<GrammaticalRelation, IndexedWord>> childPairs = Generics.newArrayList();\r\n    for (SemanticGraphEdge e : outgoingEdgeIterable(vertex)) {\r\n        childPairs.add(new Pair(e.getRelation(), e.getTarget()));\r\n    }\r\n    return childPairs;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.PropertiesUtils.getLong",
	"Comment": "load an integer property as a long.if the key is not present, returns defaultvalue.",
	"Method": "long getLong(Properties props,String key,long defaultValue){\r\n    String value = props.getProperty(key);\r\n    if (value != null) {\r\n        return Long.parseLong(value);\r\n    } else {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.categorical.CategoricalToIntegerTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return new String[] { columnName() };\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.putOrUpdateArrayForVarName",
	"Comment": "put the array if it does not exist for the given variable name, or update it if it does",
	"Method": "void putOrUpdateArrayForVarName(String varName,INDArray arr){\r\n    if (variableNameToArr.containsKey(varName)) {\r\n        updateArrayForVarName(varName, arr);\r\n    } else {\r\n        putArrayForVarName(varName, arr);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.QuoteAnnotator.xmlFreeText",
	"Comment": "helper method for creating version of document text without xml.",
	"Method": "String xmlFreeText(String documentText,Annotation annotation){\r\n    int firstTokenCharIndex = annotation.get(CoreAnnotations.TokensAnnotation.class).get(0).get(CoreAnnotations.CharacterOffsetBeginAnnotation.class);\r\n    String cleanedText = documentText.substring(0, firstTokenCharIndex).replaceAll(\"\\\\S\", \" \");\r\n    int tokenIndex = 0;\r\n    List<CoreLabel> tokens = annotation.get(CoreAnnotations.TokensAnnotation.class);\r\n    for (CoreLabel token : tokens) {\r\n        cleanedText += token.originalText();\r\n        tokenIndex += 1;\r\n        if (tokenIndex < tokens.size()) {\r\n            CoreLabel nextToken = tokens.get(tokenIndex);\r\n            int inBetweenStart = token.get(CoreAnnotations.CharacterOffsetEndAnnotation.class);\r\n            int inBetweenEnd = nextToken.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class);\r\n            String inBetweenTokenText = documentText.substring(inBetweenStart, inBetweenEnd);\r\n            inBetweenTokenText = inBetweenTokenText.replaceAll(\"\\\\S\", \" \");\r\n            cleanedText += inBetweenTokenText;\r\n        }\r\n    }\r\n    cleanedText += documentText.substring(cleanedText.length(), documentText.length()).replaceAll(\"\\\\S\", \" \");\r\n    return cleanedText;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.getVerticesWithoutParents",
	"Comment": "initially looks for nodes which have no incoming arcs. if there are any, itreturns a list of them. if not, it looks for nodes from which every othernode is reachable. if there are any, it returns a list of them. otherwise,it returns an empty list.",
	"Method": "List<IndexedWord> getVerticesWithoutParents(){\r\n    List<IndexedWord> result = new ArrayList();\r\n    for (IndexedWord v : vertexSet()) {\r\n        int inDegree = inDegree(v);\r\n        if (inDegree == 0) {\r\n            result.add(v);\r\n        }\r\n    }\r\n    Collections.sort(result);\r\n    return result;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.recurrent.KerasLSTM.getForgetBiasInitFromConfig",
	"Comment": "get lstm forget gate bias initialization from keras layer configuration.",
	"Method": "double getForgetBiasInitFromConfig(Map<String, Object> layerConfig,boolean train){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    String kerasForgetBiasInit;\r\n    if (innerConfig.containsKey(conf.getLAYER_FIELD_UNIT_FORGET_BIAS())) {\r\n        kerasForgetBiasInit = LSTM_FORGET_BIAS_INIT_ONE;\r\n    } else if (!innerConfig.containsKey(conf.getLAYER_FIELD_FORGET_BIAS_INIT())) {\r\n        throw new InvalidKerasConfigurationException(\"Keras LSTM layer config missing \" + conf.getLAYER_FIELD_FORGET_BIAS_INIT() + \" field\");\r\n    } else {\r\n        kerasForgetBiasInit = (String) innerConfig.get(conf.getLAYER_FIELD_FORGET_BIAS_INIT());\r\n    }\r\n    double init;\r\n    switch(kerasForgetBiasInit) {\r\n        case LSTM_FORGET_BIAS_INIT_ZERO:\r\n            init = 0.0;\r\n            break;\r\n        case LSTM_FORGET_BIAS_INIT_ONE:\r\n            init = 1.0;\r\n            break;\r\n        default:\r\n            if (train)\r\n                throw new UnsupportedKerasConfigurationException(\"Unsupported LSTM forget gate bias initialization: \" + kerasForgetBiasInit);\r\n            else {\r\n                init = 1.0;\r\n                log.warn(\"Unsupported LSTM forget gate bias initialization: \" + kerasForgetBiasInit + \" (using 1 instead)\");\r\n            }\r\n            break;\r\n    }\r\n    return init;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.learning.impl.elements.RandomUtils.nextBoolean",
	"Comment": "returns the next pseudorandom, uniformly distributed boolean valuefrom the given random sequence.",
	"Method": "boolean nextBoolean(boolean nextBoolean,Random random){\r\n    return random.nextBoolean();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.layers.samediff.SameDiffLambdaVertex.defineVertex",
	"Comment": "the definevertex method is used to define the foward pass for the vertex",
	"Method": "SDVariable defineVertex(SameDiff sameDiff,VertexInputs inputs,SDVariable defineVertex,SameDiff sameDiff,Map<String, SDVariable> layerInput,Map<String, SDVariable> paramTable){\r\n    VertexInputs vi = getInputs(sameDiff);\r\n    int i = 0;\r\n    if (vi.map.size() == 0 && layerInput.size() > 0) {\r\n        for (SDVariable v : layerInput.values()) {\r\n            vi.map.put(i++, v);\r\n        }\r\n    }\r\n    return defineVertex(sameDiff, getInputs(sameDiff));\r\n}"
}, {
	"Path": "org.deeplearning4j.zoo.BaseDL4JTest.getProfilingMode",
	"Comment": "override this to set the profiling mode for the tests defined in the child class",
	"Method": "OpExecutioner.ProfilingMode getProfilingMode(){\r\n    return OpExecutioner.ProfilingMode.SCOPE_PANIC;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.average",
	"Comment": "returns a new counter with counts averaged from the two given counters. theaverage counter will contain the union of keys in both source counters, andeach count will be the average of the two source counts for that key, whereas usual a missing count in one counter is treated as count 0.",
	"Method": "Counter<E> average(Counter<E> c1,Counter<E> c2){\r\n    Counter<E> average = c1.getFactory().create();\r\n    Set<E> allKeys = Generics.newHashSet(c1.keySet());\r\n    allKeys.addAll(c2.keySet());\r\n    for (E key : allKeys) {\r\n        average.setCount(key, (c1.getCount(key) + c2.getCount(key)) * 0.5);\r\n    }\r\n    return average;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.correlation",
	"Comment": "returns the correlation coefficient of two double vectors.",
	"Method": "double correlation(double[] residuals,double targetAttribute){\r\n    double[] predictedValues = new double[residuals.length];\r\n    for (int i = 0; i < predictedValues.length; i++) {\r\n        predictedValues[i] = targetAttribute[i] - residuals[i];\r\n    }\r\n    double ssErr = ssError(predictedValues, targetAttribute);\r\n    double total = ssTotal(residuals, targetAttribute);\r\n    return 1 - (ssErr / total);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.LogRecordHandler.signalEndTrack",
	"Comment": "signal the end of a track, i.e. that we have popped up to a higher level.",
	"Method": "List<Record> signalEndTrack(int newDepth,long timeEnded){\r\n    return EMPTY;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.HighlightUtils.addHighlight",
	"Comment": "highlight the given label from the first mouse event to the secondreturns true if the highlight was successful, false otherwise.",
	"Method": "boolean addHighlight(JTextField label,MouseEvent mouseEvent1,MouseEvent mouseEvent2){\r\n    FontMetrics fm = label.getFontMetrics(label.getFont());\r\n    int firstXpos = mouseEvent1.getX();\r\n    int lastXpos = mouseEvent2.getX();\r\n    int firstOffset = getCharOffset(fm, label.getText(), firstXpos);\r\n    int lastOffset = getCharOffset(fm, label.getText(), lastXpos);\r\n    if (lastOffset != firstOffset) {\r\n        if (firstOffset > lastOffset) {\r\n            int tmp = firstOffset;\r\n            firstOffset = lastOffset;\r\n            lastOffset = tmp;\r\n        }\r\n        try {\r\n            label.getHighlighter().removeAllHighlights();\r\n            label.getHighlighter().addHighlight(firstOffset, lastOffset, new DefaultHighlighter.DefaultHighlightPainter(Color.yellow));\r\n            return true;\r\n        } catch (BadLocationException e1) {\r\n            return false;\r\n        }\r\n    } else\r\n        return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.PreferencesPanel.alternateEncodingPrompt",
	"Comment": "prompts the user to enter a new encoding for loading tree files",
	"Method": "void alternateEncodingPrompt(String newDefaultEncoding){\r\n    String response = (String) JOptionPane.showInputDialog(this, \"Please enter a text encoding: \", \"Set Encoding...\", JOptionPane.QUESTION_MESSAGE, null, null, newDefaultEncoding);\r\n    FileTreeModel.setCurEncoding(response.trim());\r\n    setEncoding.setText(response.trim());\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.setScore",
	"Comment": "sets the score associated with the current node, if there is one.",
	"Method": "void setScore(double score){\r\n    throw new UnsupportedOperationException(\"You must use a tree type that implements scoring in order call setScore()\");\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.BasePretrainNetwork.getCorruptedInput",
	"Comment": "corrupts the given input by doing a binomial samplinggiven the corruption level",
	"Method": "INDArray getCorruptedInput(INDArray x,double corruptionLevel){\r\n    INDArray corrupted = Nd4j.getDistributions().createBinomial(1, 1 - corruptionLevel).sample(x.shape());\r\n    corrupted.muli(x);\r\n    return corrupted;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.labelProbabilities",
	"Comment": "returns the probabilities for each labelfor each example row wise",
	"Method": "INDArray labelProbabilities(INDArray examples){\r\n    List<INDArray> feed = feedForward(examples);\r\n    IOutputLayer o = (IOutputLayer) getOutputLayer();\r\n    return o.labelProbabilities(feed.get(feed.size() - 1));\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.isPreTerminal",
	"Comment": "return whether this node is a preterminal or not.a preterminal isdefined to be a node with one child which is itself a leaf.",
	"Method": "boolean isPreTerminal(){\r\n    Tree[] kids = children();\r\n    return (kids.length == 1) && (kids[0].isLeaf());\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.getChildWithReln",
	"Comment": "returns the first indexedfeaturelabel bearing a certain grammaticalrelation, or null if none.",
	"Method": "IndexedWord getChildWithReln(IndexedWord vertex,GrammaticalRelation reln){\r\n    if (vertex.equals(IndexedWord.NO_WORD))\r\n        return null;\r\n    if (!containsVertex(vertex))\r\n        throw new IllegalArgumentException();\r\n    for (SemanticGraphEdge edge : outgoingEdgeIterable(vertex)) {\r\n        if (edge.getRelation().equals(reln)) {\r\n            return edge.getTarget();\r\n        }\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.sequencevectors.sequence.Sequence.getElements",
	"Comment": "returns an ordered unmodifiable list of elements from this sequence",
	"Method": "List<T> getElements(){\r\n    return Collections.unmodifiableList(elements);\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.SequenceGibbsSampler.collectSamples",
	"Comment": "collects numsamples samples of sequences, from the distribution over sequences definedby the sequence model passed on construction.all samples collected are sampleinterval samples apart, in an attempt to reduceautocorrelation.",
	"Method": "List<int[]> collectSamples(SequenceModel model,int numSamples,int sampleInterval,List<int[]> collectSamples,SequenceModel model,int numSamples,int sampleInterval,int[] initialSequence){\r\n    if (verbose > 0)\r\n        log.info(\"Collecting samples\");\r\n    listener.setInitialSequence(initialSequence);\r\n    List<int[]> result = new ArrayList();\r\n    int[] sequence = initialSequence;\r\n    for (int i = 0; i < numSamples; i++) {\r\n        sequence = copy(sequence);\r\n        sampleSequenceRepeatedly(model, sequence, sampleInterval);\r\n        result.add(sequence);\r\n        if (verbose > 0)\r\n            log.info(\".\");\r\n        System.err.flush();\r\n    }\r\n    if (verbose > 1) {\r\n        log.info();\r\n        printSamples(result, System.err);\r\n    }\r\n    if (verbose > 0)\r\n        log.info(\"done.\");\r\n    return result;\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.Extractors.leftContext",
	"Comment": "find maximum left context of extractors. used in taginference to decide windows for dynamic programming.",
	"Method": "int leftContext(){\r\n    int max = 0;\r\n    for (Extractor extractor : v) {\r\n        int lf = extractor.leftContext();\r\n        if (lf > max) {\r\n            max = lf;\r\n        }\r\n    }\r\n    return max;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.toSortedList",
	"Comment": "a list of the keys in c, sorted from highest count to lowest.so note that the default is descending!",
	"Method": "List<E> toSortedList(Counter<E> c,List<E> toSortedList,Counter<E> c,boolean ascending){\r\n    List<E> l = new ArrayList(c.keySet());\r\n    Comparator<E> comp = ascending ? toComparator(c) : toComparatorDescending(c);\r\n    Collections.sort(l, comp);\r\n    return l;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArrayCoreMapTest.testObjectLoops",
	"Comment": "arraycoremap should be able to handle loops in its annotationswithout blowing up",
	"Method": "void testObjectLoops(){\r\n    ArrayCoreMap foo = new ArrayCoreMap();\r\n    foo.set(CoreAnnotations.TextAnnotation.class, \"foo\");\r\n    foo.set(CoreAnnotations.PartOfSpeechAnnotation.class, \"B\");\r\n    List<CoreMap> fooParagraph = new ArrayList<CoreMap>();\r\n    fooParagraph.add(foo);\r\n    ArrayCoreMap f1 = new ArrayCoreMap();\r\n    f1.set(CoreAnnotations.ParagraphsAnnotation.class, fooParagraph);\r\n    List<CoreMap> p1 = new ArrayList<CoreMap>();\r\n    p1.add(f1);\r\n    foo.set(CoreAnnotations.ParagraphsAnnotation.class, p1);\r\n    foo.toString();\r\n    foo.hashCode();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.layers.recurrent.Bidirectional.getUpdaterByParam",
	"Comment": "get the updater for the given parameter. typically the same updater will be used for all updaters, but thisis not necessarily the case",
	"Method": "IUpdater getUpdaterByParam(String paramName){\r\n    String sub = paramName.substring(1);\r\n    return fwd.getUpdaterByParam(sub);\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.fromProto",
	"Comment": "read a section coremap from its serialized form. requires the containing sentence to bepassed in along with the protocol buffer.",
	"Method": "CoreLabel fromProto(CoreNLPProtos.Token proto,CoreMap fromProto,CoreNLPProtos.Sentence proto,Annotation fromProto,CoreNLPProtos.Document proto,Tree fromProto,CoreNLPProtos.ParseTree proto,Language fromProto,CoreNLPProtos.Language lang,OperatorSpec fromProto,CoreNLPProtos.Operator operator,Polarity fromProto,CoreNLPProtos.Polarity polarity,SemanticGraph fromProto,CoreNLPProtos.DependencyGraph proto,List<CoreLabel> sentence,String docid,Optional<Annotation> document,SemanticGraph fromProto,CoreNLPProtos.DependencyGraph proto,List<CoreLabel> sentence,String docid,RelationTriple fromProto,CoreNLPProtos.RelationTriple proto,Annotation doc,int sentenceIndex,SentenceFragment fromProto,CoreNLPProtos.SentenceFragment fragment,SemanticGraph tree,HashMap<String, String> fromProto,CoreNLPProtos.MapStringString proto,HashMap<Integer, String> fromProto,CoreNLPProtos.MapIntString proto,CorefChain fromProto,CoreNLPProtos.CorefChain proto,Annotation partialDocument,SpeakerInfo fromProto,CoreNLPProtos.SpeakerInfo speakerInfo,Timex fromProto,CoreNLPProtos.Timex proto,EntityMention fromProto,CoreNLPProtos.Entity proto,CoreMap sentence,RelationMention fromProto,CoreNLPProtos.Relation proto,CoreMap sentence,Annotation fromProto,CoreNLPProtos.Quote quote,List<CoreLabel> tokens,CoreMap fromProto,CoreNLPProtos.NERMention mention,CoreMap fromProto,CoreNLPProtos.Section section,List<CoreMap> annotationSentences){\r\n    CoreMap map = new ArrayCoreMap();\r\n    map.set(CharacterOffsetBeginAnnotation.class, section.getCharBegin());\r\n    map.set(CharacterOffsetEndAnnotation.class, section.getCharEnd());\r\n    if (section.hasAuthor())\r\n        map.set(AuthorAnnotation.class, section.getAuthor());\r\n    if (section.hasDatetime())\r\n        map.set(SectionDateAnnotation.class, section.getDatetime());\r\n    ArrayList<CoreMap> sentencesList = new ArrayList();\r\n    for (int sentenceIndex : section.getSentenceIndexesList()) {\r\n        sentencesList.add(annotationSentences.get(sentenceIndex));\r\n    }\r\n    map.set(SentencesAnnotation.class, sentencesList);\r\n    map.set(QuotesAnnotation.class, new ArrayList());\r\n    for (CoreNLPProtos.Quote quote : section.getQuotesList()) {\r\n        int quoteCharStart = quote.getBegin();\r\n        int quoteCharEnd = quote.getEnd();\r\n        String quoteAuthor = null;\r\n        if (quote.hasAuthor()) {\r\n            quoteAuthor = quote.getAuthor();\r\n        }\r\n        CoreMap quoteCoreMap = new ArrayCoreMap();\r\n        quoteCoreMap.set(CharacterOffsetBeginAnnotation.class, quoteCharStart);\r\n        quoteCoreMap.set(CharacterOffsetEndAnnotation.class, quoteCharEnd);\r\n        quoteCoreMap.set(AuthorAnnotation.class, quoteAuthor);\r\n        map.get(QuotesAnnotation.class).add(quoteCoreMap);\r\n    }\r\n    if (section.hasAuthorCharBegin()) {\r\n        map.set(SectionAuthorCharacterOffsetBeginAnnotation.class, section.getAuthorCharBegin());\r\n    }\r\n    if (section.hasAuthorCharEnd()) {\r\n        map.set(SectionAuthorCharacterOffsetEndAnnotation.class, section.getAuthorCharEnd());\r\n    }\r\n    map.set(SectionTagAnnotation.class, fromProto(section.getXmlTag()));\r\n    return map;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.recurrent.RnnLossLayer.computeScoreForExamples",
	"Comment": "compute the score for each example individually, after labels and input have been set.",
	"Method": "INDArray computeScoreForExamples(double fullNetworkL1,double fullNetworkL2,LayerWorkspaceMgr workspaceMgr){\r\n    if (input == null || labels == null)\r\n        throw new IllegalStateException(\"Cannot calculate score without input and labels \" + layerId());\r\n    INDArray input2d = TimeSeriesUtils.reshape3dTo2d(input, workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n    INDArray labels2d = TimeSeriesUtils.reshape3dTo2d(labels, workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n    INDArray maskReshaped;\r\n    if (this.maskArray != null) {\r\n        if (this.maskArray.rank() == 3) {\r\n            maskReshaped = TimeSeriesUtils.reshapePerOutputTimeSeriesMaskTo2d(this.maskArray, workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n        } else {\r\n            maskReshaped = TimeSeriesUtils.reshapeTimeSeriesMaskToVector(this.maskArray, workspaceMgr, ArrayType.FF_WORKING_MEM);\r\n        }\r\n    } else {\r\n        maskReshaped = null;\r\n    }\r\n    ILossFunction lossFunction = layerConf().getLossFn();\r\n    INDArray scoreArray = lossFunction.computeScoreArray(labels2d, input2d, layerConf().getActivationFn(), maskReshaped);\r\n    INDArray scoreArrayTs = TimeSeriesUtils.reshapeVectorToTimeSeriesMask(scoreArray, (int) input.size(0));\r\n    INDArray summedScores = scoreArrayTs.sum(1);\r\n    double l1l2 = fullNetworkL1 + fullNetworkL2;\r\n    if (l1l2 != 0.0) {\r\n        summedScores.addi(l1l2);\r\n    }\r\n    return summedScores;\r\n}"
}, {
	"Path": "org.datavec.api.records.Buffer.toString",
	"Comment": "convert the byte buffer to a string an specific character encoding",
	"Method": "String toString(String toString,String charsetName){\r\n    return new String(this.get(), 0, this.getCount(), charsetName);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.LabeledScoredTreeNode.setScore",
	"Comment": "sets the score associated with the current node, if there is one",
	"Method": "void setScore(double score){\r\n    this.score = score;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.asCoreLabels",
	"Comment": "returns this sentence as a list of corelabels representing the sentence.note that, importantly, only the fields which have already been called will be populated inthe coremap!therefore, this method is generally not recommended.",
	"Method": "List<CoreLabel> asCoreLabels(Function<Sentence, Object> functions){\r\n    for (Function<Sentence, Object> function : functions) {\r\n        function.apply(this);\r\n    }\r\n    return asCoreMap().get(CoreAnnotations.TokensAnnotation.class);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArrayUtils.asImmutableSet",
	"Comment": "return an immutable set containing the same elements as the specified array. arrays with 0 or 1 elements are special cased to return the efficient small sets from the collections class.",
	"Method": "Set<T> asImmutableSet(T[] a){\r\n    if (a.length == 0) {\r\n        return Collections.emptySet();\r\n    } else if (a.length == 1) {\r\n        return Collections.singleton(a[0]);\r\n    } else {\r\n        return Collections.unmodifiableSet(Generics.newHashSet(Arrays.asList(a)));\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseSparseNDArrayCOO.indexesBinarySearch",
	"Comment": "return the position of the idx array into the indexes buffer between the lower and upper bound.",
	"Method": "int indexesBinarySearch(int lowerBound,int upperBound,int[] idx){\r\n    int min = lowerBound;\r\n    int max = upperBound;\r\n    int mid = (max + min) / 2;\r\n    int[] midIdx = getUnderlyingIndicesOf(mid).asInt();\r\n    if (Arrays.equals(idx, midIdx)) {\r\n        return mid;\r\n    }\r\n    if (ArrayUtil.lessThan(idx, midIdx)) {\r\n        max = mid;\r\n    }\r\n    if (ArrayUtil.greaterThan(idx, midIdx)) {\r\n        min = mid;\r\n    }\r\n    if (min == max) {\r\n        return -1;\r\n    }\r\n    return indexesBinarySearch(min, max, idx);\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.fromString",
	"Comment": "this will take a given string and separator and convert it to an equivalentdouble array.",
	"Method": "double[] fromString(String data,String separator){\r\n    String[] split = data.split(separator);\r\n    double[] ret = new double[split.length];\r\n    for (int i = 0; i < split.length; i++) {\r\n        ret[i] = Double.parseDouble(split[i]);\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.datavec.api.transform.sequence.comparator.BaseColumnComparator.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return columnNames();\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.clearAnnotatorPool",
	"Comment": "call this if you are no longer using stanfordcorenlp and want torelease the memory associated with the annotators.",
	"Method": "void clearAnnotatorPool(){\r\n    logger.warn(\"Clearing CoreNLP annotation pool; this should be unnecessary in production\");\r\n    GLOBAL_ANNOTATOR_CACHE.clear();\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLPServer.getDocument",
	"Comment": "reads the post contents of the request and parses it into an annotation object, ready to be annotated.this method can also read a serialized document, if the input format is set to be serialized.",
	"Method": "Annotation getDocument(Properties props,HttpExchange httpExchange){\r\n    String inputFormat = props.getProperty(\"inputFormat\", \"text\");\r\n    String date = props.getProperty(\"date\");\r\n    switch(inputFormat) {\r\n        case \"text\":\r\n            String defaultEncoding = this.strict ? \"ISO-8859-1\" : \"UTF-8\";\r\n            Headers h = httpExchange.getRequestHeaders();\r\n            String encoding;\r\n            if (h.containsKey(\"Content-type\")) {\r\n                String[] charsetPair = Arrays.stream(h.getFirst(\"Content-type\").split(\";\")).map(x -> x.split(\"=\")).filter(x -> x.length > 0 && \"charset\".equals(x[0])).findFirst().orElse(new String[] { \"charset\", defaultEncoding });\r\n                if (charsetPair.length == 2) {\r\n                    encoding = charsetPair[1];\r\n                } else {\r\n                    encoding = defaultEncoding;\r\n                }\r\n            } else {\r\n                encoding = defaultEncoding;\r\n            }\r\n            String text = IOUtils.slurpReader(IOUtils.encodedInputStreamReader(httpExchange.getRequestBody(), encoding));\r\n            text = text.replaceAll(\"%(?![0-9a-fA-F]{2})\", \"%\");\r\n            text = text.replaceAll(\"\\\\+\", \"+\");\r\n            text = URLDecoder.decode(text, encoding).trim();\r\n            Annotation annotation = new Annotation(text);\r\n            if (date != null) {\r\n                annotation.set(CoreAnnotations.DocDateAnnotation.class, date);\r\n            }\r\n            return annotation;\r\n        case \"serialized\":\r\n            String inputSerializerName = props.getProperty(\"inputSerializer\", ProtobufAnnotationSerializer.class.getName());\r\n            AnnotationSerializer serializer = MetaClass.create(inputSerializerName).createInstance();\r\n            Pair<Annotation, InputStream> pair = serializer.read(httpExchange.getRequestBody());\r\n            return pair.first;\r\n        default:\r\n            throw new IOException(\"Could not parse input format: \" + inputFormat);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.parallelism.RandomTests.testModelInitialParamsEquality1",
	"Comment": "in this test we check for equality of model params after initialization in different threads",
	"Method": "void testModelInitialParamsEquality1(){\r\n    final List<Model> models = new CopyOnWriteArrayList();\r\n    for (int i = 0; i < 4; i++) {\r\n        Thread thread = new Thread(new Runnable() {\r\n            @Override\r\n            public void run() {\r\n                // Training iterations as above\r\n                MultiLayerConfiguration conf = // .learningRateDecayPolicy(LearningRatePolicy.Inverse).lrPolicyDecayRate(0.001).lrPolicyPower(0.75)\r\n                new NeuralNetConfiguration.Builder().seed(119).l2(0.0005).weightInit(WeightInit.XAVIER).updater(new Nesterovs(0.01, 0.9)).trainingWorkspaceMode(WorkspaceMode.ENABLED).list().layer(0, // nIn and nOut specify depth. nIn here is the nChannels and nOut is the number of filters to be applied\r\n                new ConvolutionLayer.Builder(5, 5).nIn(1).stride(1, 1).nOut(20).activation(Activation.IDENTITY).build()).layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2).stride(2, 2).build()).layer(2, // Note that nIn need not be specified in later layers\r\n                new ConvolutionLayer.Builder(5, 5).stride(1, 1).nOut(50).activation(Activation.IDENTITY).build()).layer(3, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2).stride(2, 2).build()).layer(4, new DenseLayer.Builder().activation(Activation.RELU).nOut(500).build()).layer(5, // See note below\r\n                new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).nOut(10).activation(Activation.SOFTMAX).build()).setInputType(InputType.convolutionalFlat(28, 28, 1)).build();\r\n                MultiLayerNetwork network = new MultiLayerNetwork(conf);\r\n                network.init();\r\n                models.add(network);\r\n            }\r\n        });\r\n        thread.start();\r\n        thread.join();\r\n    }\r\n    for (int i = 0; i < models.size(); i++) {\r\n        assertEquals(models.get(0).params(), models.get(i).params());\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.parallelism.RandomTests.testModelInitialParamsEquality1",
	"Comment": "in this test we check for equality of model params after initialization in different threads",
	"Method": "void testModelInitialParamsEquality1(){\r\n    // Training iterations as above\r\n    MultiLayerConfiguration conf = // .learningRateDecayPolicy(LearningRatePolicy.Inverse).lrPolicyDecayRate(0.001).lrPolicyPower(0.75)\r\n    new NeuralNetConfiguration.Builder().seed(119).l2(0.0005).weightInit(WeightInit.XAVIER).updater(new Nesterovs(0.01, 0.9)).trainingWorkspaceMode(WorkspaceMode.ENABLED).list().layer(0, // nIn and nOut specify depth. nIn here is the nChannels and nOut is the number of filters to be applied\r\n    new ConvolutionLayer.Builder(5, 5).nIn(1).stride(1, 1).nOut(20).activation(Activation.IDENTITY).build()).layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2).stride(2, 2).build()).layer(2, // Note that nIn need not be specified in later layers\r\n    new ConvolutionLayer.Builder(5, 5).stride(1, 1).nOut(50).activation(Activation.IDENTITY).build()).layer(3, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2, 2).stride(2, 2).build()).layer(4, new DenseLayer.Builder().activation(Activation.RELU).nOut(500).build()).layer(5, // See note below\r\n    new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).nOut(10).activation(Activation.SOFTMAX).build()).setInputType(InputType.convolutionalFlat(28, 28, 1)).build();\r\n    MultiLayerNetwork network = new MultiLayerNetwork(conf);\r\n    network.init();\r\n    models.add(network);\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.DocumentPreprocessor.setSentenceDelimiter",
	"Comment": "make the processor assume that the document is already delimitedby the supplied parameter.",
	"Method": "void setSentenceDelimiter(String s){\r\n    sentenceDelimiter = s;\r\n}"
}, {
	"Path": "org.deeplearning4j.optimize.solvers.accumulation.EncodedGradientsAccumulator.setExternalSource",
	"Comment": "this method allows to pass external updates to accumulator, they will be populated across all workers using this gradientsaccumulator instance",
	"Method": "void setExternalSource(IndexedTail source){\r\n    this.externalSource = source;\r\n}"
}, {
	"Path": "org.nd4j.evaluation.EvaluationUtils.precision",
	"Comment": "calculate the precision from true positive and false positive counts",
	"Method": "double precision(long tpCount,long fpCount,double edgeCase){\r\n    if (tpCount == 0 && fpCount == 0) {\r\n        return edgeCase;\r\n    }\r\n    return tpCount / (double) (tpCount + fpCount);\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.MultiClassPrecisionRecallStats.getPrecisionDescription",
	"Comment": "returns a string summarizing precision that will print nicely.",
	"Method": "String getPrecisionDescription(int numDigits,String getPrecisionDescription,int numDigits,L label){\r\n    NumberFormat nf = NumberFormat.getNumberInstance();\r\n    nf.setMaximumFractionDigits(numDigits);\r\n    Triple<Double, Integer, Integer> prec = getPrecisionInfo(label);\r\n    return nf.format(prec.first()) + \"  (\" + prec.second() + \"/\" + (prec.second() + prec.third()) + \")\";\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.keysAbove",
	"Comment": "returns the set of keys whose counts are at or above the given threshold.this set may have 0 elements but will not be null.",
	"Method": "Set<E> keysAbove(int countThreshold){\r\n    Set<E> keys = Generics.newHashSet();\r\n    for (E key : map.keySet()) {\r\n        if (getIntCount(key) >= countThreshold) {\r\n            keys.add(key);\r\n        }\r\n    }\r\n    return keys;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Document.caseless",
	"Comment": "make this document caseless. that is, from now on, run the caseless modelson the document by default rather than the standard corenlp models.",
	"Method": "Document caseless(){\r\n    this.defaultProps = CASELESS_PROPS;\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseSparseNDArrayCSR.get",
	"Comment": "returns a subset of this array based on the specifiedindexes",
	"Method": "INDArray get(INDArrayIndex indexes){\r\n    if (indexes.length == 1 && indexes[0] instanceof NDArrayIndexAll || (indexes.length == 2 && (isRowVector() && indexes[0] instanceof PointIndex && indexes[0].offset() == 0 && indexes[1] instanceof NDArrayIndexAll || isColumnVector() && indexes[1] instanceof PointIndex && indexes[0].offset() == 0 && indexes[0] instanceof NDArrayIndexAll)))\r\n        return this;\r\n    indexes = NDArrayIndex.resolve(shapeInfoDataBuffer(), indexes);\r\n    ShapeOffsetResolution resolution = new ShapeOffsetResolution(this);\r\n    resolution.exec(indexes);\r\n    if (indexes.length < 1)\r\n        throw new IllegalStateException(\"Invalid index found of zero length\");\r\n    int[] shape = LongUtils.toInts(resolution.getShapes());\r\n    int numSpecifiedIndex = 0;\r\n    for (int i = 0; i < indexes.length; i++) if (indexes[i] instanceof SpecifiedIndex)\r\n        numSpecifiedIndex++;\r\n    if (shape != null && numSpecifiedIndex > 0) {\r\n        return null;\r\n    }\r\n    INDArray ret = subArray(resolution);\r\n    return ret;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.feedforward.autoencoder.recursive.Tree.depth",
	"Comment": "returns the distance between this nodeand the specified subnode",
	"Method": "int depth(int depth,Tree node){\r\n    Tree p = node.parent(this);\r\n    if (this == node) {\r\n        return 0;\r\n    }\r\n    if (p == null) {\r\n        return -1;\r\n    }\r\n    int depth = 1;\r\n    while (this != p) {\r\n        p = p.parent(this);\r\n        depth++;\r\n    }\r\n    return depth;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseSparseNDArrayCOO.get",
	"Comment": "returns a subset of this array based on the specifiedindexes",
	"Method": "INDArray get(INDArrayIndex indexes){\r\n    sort();\r\n    if (indexes.length == 1 && indexes[0] instanceof NDArrayIndexAll || (indexes.length == 2 && (isRowVector() && indexes[0] instanceof PointIndex && indexes[0].offset() == 0 && indexes[1] instanceof NDArrayIndexAll || isColumnVector() && indexes[1] instanceof PointIndex && indexes[0].offset() == 0 && indexes[0] instanceof NDArrayIndexAll)))\r\n        return this;\r\n    indexes = NDArrayIndex.resolve(shapeInfoDataBuffer(), indexes);\r\n    ShapeOffsetResolution resolution = new ShapeOffsetResolution(this);\r\n    resolution.exec(indexes);\r\n    if (indexes.length < 1)\r\n        throw new IllegalStateException(\"Invalid index found of zero length\");\r\n    long[] shape = resolution.getShapes();\r\n    int numSpecifiedIndex = 0;\r\n    for (int i = 0; i < indexes.length; i++) if (indexes[i] instanceof SpecifiedIndex)\r\n        numSpecifiedIndex++;\r\n    if (shape != null && numSpecifiedIndex > 0) {\r\n        Generator<List<List<Long>>> gen = SpecifiedIndex.iterateOverSparse(indexes);\r\n        INDArray ret = Nd4j.createSparseCOO(new double[] {}, new int[][] {}, shape);\r\n        int count = 0;\r\n        int maxValue = ArrayUtil.prod(shape());\r\n        while (count < maxValue) {\r\n            try {\r\n                List<List<Long>> next = gen.next();\r\n                List<Integer> coordsCombo = new ArrayList();\r\n                List<Integer> cooIdx = new ArrayList();\r\n                for (int i = 0; i < next.size(); i++) {\r\n                    if (next.get(i).size() != 2)\r\n                        throw new IllegalStateException(\"Illegal entry returned\");\r\n                    coordsCombo.add(next.get(i).get(0).intValue());\r\n                    cooIdx.add(next.get(i).get(1).intValue());\r\n                }\r\n                count++;\r\n                int[] idx = Ints.toArray(coordsCombo);\r\n                if (!isZero(idx)) {\r\n                    double val = getDouble(idx);\r\n                    ret.putScalar(filterOutFixedDimensions(resolution.getFixed(), cooIdx), val);\r\n                }\r\n            } catch (NoSuchElementException e) {\r\n                break;\r\n            }\r\n        }\r\n        return ret;\r\n    }\r\n    int numNewAxis = 0;\r\n    for (int i = 0; i < indexes.length; i++) if (indexes[i] instanceof NewAxis)\r\n        numNewAxis++;\r\n    if (numNewAxis != 0) {\r\n    }\r\n    INDArray ret = subArray(resolution);\r\n    return ret;\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.get",
	"Comment": "get the value of the name property. if no such propertyexists, then defaultvalue is returned.",
	"Method": "String get(String name,String get,String name,String defaultValue){\r\n    return substituteVars(getProps().getProperty(name, defaultValue));\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Timing.doing",
	"Comment": "print the start of timing message to stderr and start the timer.",
	"Method": "void doing(String str){\r\n    log.info(str + \" ... \");\r\n    start();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractTreebankLanguagePack.isPunctuationTag",
	"Comment": "accepts a string that is a punctuationtag name, and rejects everything else.",
	"Method": "boolean isPunctuationTag(String str){\r\n    return punctTagStringAcceptFilter.test(str);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArrayHeap.min",
	"Comment": "finds the object with the minimum key and returns it, withoutmodifying the heap.",
	"Method": "E min(){\r\n    HeapEntry<E> minEntry = indexToEntry.get(0);\r\n    return minEntry.object;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.LabeledScoredConstituent.setScore",
	"Comment": "sets the score associated with the current node, if there is one",
	"Method": "void setScore(double score){\r\n    this.score = score;\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.AsyncMultiDataSetIteratorTest.testVariableTimeSeries1",
	"Comment": "this test should be always run with double precision, without any exclusions",
	"Method": "void testVariableTimeSeries1(){\r\n    val iterator = new VariableMultiTimeseriesGenerator(1192, 1000, 32, 128, 10, 500, 10);\r\n    iterator.reset();\r\n    iterator.hasNext();\r\n    val amdsi = new AsyncMultiDataSetIterator(iterator, 2, true);\r\n    for (int e = 0; e < 10; e++) {\r\n        int cnt = 0;\r\n        while (amdsi.hasNext()) {\r\n            MultiDataSet mds = amdsi.next();\r\n            assertEquals(\"Failed on epoch \" + e + \"; iteration: \" + cnt + \";\", (double) cnt, mds.getFeatures()[0].meanNumber().doubleValue(), 1e-10);\r\n            assertEquals(\"Failed on epoch \" + e + \"; iteration: \" + cnt + \";\", (double) cnt + 0.25, mds.getLabels()[0].meanNumber().doubleValue(), 1e-10);\r\n            assertEquals(\"Failed on epoch \" + e + \"; iteration: \" + cnt + \";\", (double) cnt + 0.5, mds.getFeaturesMaskArrays()[0].meanNumber().doubleValue(), 1e-10);\r\n            assertEquals(\"Failed on epoch \" + e + \"; iteration: \" + cnt + \";\", (double) cnt + 0.75, mds.getLabelsMaskArrays()[0].meanNumber().doubleValue(), 1e-10);\r\n            cnt++;\r\n        }\r\n        amdsi.reset();\r\n        log.info(\"Epoch {} finished...\", e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.getDefaultAnnotatorPool",
	"Comment": "construct the default annotator pool, and save it as the static annotator poolfor corenlp.",
	"Method": "AnnotatorPool getDefaultAnnotatorPool(Properties inputProps,AnnotatorImplementations annotatorImplementation){\r\n    AnnotatorPool pool = AnnotatorPool.SINGLETON;\r\n    for (Map.Entry<String, BiFunction<Properties, AnnotatorImplementations, Annotator>> entry : getNamedAnnotators().entrySet()) {\r\n        AnnotatorSignature key = new AnnotatorSignature(entry.getKey(), PropertiesUtils.getSignature(entry.getKey(), inputProps));\r\n        pool.register(entry.getKey(), inputProps, GLOBAL_ANNOTATOR_CACHE.computeIfAbsent(key, (sig) -> Lazy.cache(() -> entry.getValue().apply(inputProps, annotatorImplementation))));\r\n    }\r\n    registerCustomAnnotators(pool, annotatorImplementation, inputProps);\r\n    return pool;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.functions.DifferentialFunction.isConfigProperties",
	"Comment": "returns true if the fields for this class should be looked up from a configuration class.",
	"Method": "boolean isConfigProperties(){\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Distribution.laplaceSmoothedDistribution",
	"Comment": "creates an laplace smoothed distribution from the given counter, ie adds one countto every item, including unseen ones, and divides by the total count.",
	"Method": "Distribution<E> laplaceSmoothedDistribution(Counter<E> counter,int numberOfKeys,Distribution<E> laplaceSmoothedDistribution,Counter<E> counter,int numberOfKeys,double lambda){\r\n    Distribution<E> norm = new Distribution();\r\n    norm.counter = new ClassicCounter();\r\n    double total = counter.totalCount();\r\n    double newTotal = total + (lambda * numberOfKeys);\r\n    double reservedMass = ((double) numberOfKeys - counter.size()) * lambda / newTotal;\r\n    if (verbose) {\r\n        log.info(((double) numberOfKeys - counter.size()) + \" * \" + lambda + \" / (\" + total + \" + ( \" + lambda + \" * \" + (double) numberOfKeys + \") )\");\r\n    }\r\n    norm.numberOfKeys = numberOfKeys;\r\n    norm.reservedMass = reservedMass;\r\n    if (verbose) {\r\n        log.info(\"reserved mass=\" + reservedMass);\r\n    }\r\n    for (E key : counter.keySet()) {\r\n        double count = counter.getCount(key);\r\n        norm.counter.setCount(key, (count + lambda) / newTotal);\r\n    }\r\n    if (verbose) {\r\n        log.info(\"unseenKeys=\" + (norm.numberOfKeys - norm.counter.size()) + \" seenKeys=\" + norm.counter.size() + \" reservedMass=\" + norm.reservedMass);\r\n        log.info(\"0 count prob: \" + lambda / newTotal);\r\n        log.info(\"1 count prob: \" + (1.0 + lambda) / newTotal);\r\n        log.info(\"2 count prob: \" + (2.0 + lambda) / newTotal);\r\n        log.info(\"3 count prob: \" + (3.0 + lambda) / newTotal);\r\n    }\r\n    return norm;\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.fitLabeledPoint",
	"Comment": "fit a multilayernetwork using spark mllib labeledpoint instances.this will convert the labeled points to the internal dl4j data format and train the model on that",
	"Method": "MultiLayerNetwork fitLabeledPoint(JavaRDD<LabeledPoint> rdd){\r\n    int nLayers = network.getLayerWiseConfigurations().getConfs().size();\r\n    FeedForwardLayer ffl = (FeedForwardLayer) network.getLayerWiseConfigurations().getConf(nLayers - 1).getLayer();\r\n    JavaRDD<DataSet> ds = MLLibUtil.fromLabeledPoint(sc, rdd, ffl.getNOut());\r\n    return fit(ds);\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.optimize.config.OptimizationConfiguration.toJson",
	"Comment": "return a json configuration of this optimization configuration",
	"Method": "String toJson(){\r\n    try {\r\n        return JsonMapper.getMapper().writeValueAsString(this);\r\n    } catch (JsonProcessingException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.SentenceAlgorithms.modeInSpan",
	"Comment": "select the most common element of the given type in the given span.this is useful for, e.g., finding the most likely ner span of a given span, or the mostlikely pos tag of a given span.null entries are removed.",
	"Method": "E modeInSpan(Span span,Function<Sentence, List<E>> selector){\r\n    if (!Span.fromValues(0, sentence.length()).contains(span)) {\r\n        throw new IllegalArgumentException(\"Span must be entirely contained in the sentence: \" + span + \" (sentence length=\" + sentence.length() + \")\");\r\n    }\r\n    Counter<E> candidates = new ClassicCounter();\r\n    for (int i : span) {\r\n        candidates.incrementCount(selector.apply(sentence).get(i));\r\n    }\r\n    candidates.remove(null);\r\n    return Counters.argmax(candidates);\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.time.DeriveColumnsFromTimeTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    String[] ret = new String[derivedColumns.size()];\r\n    for (int i = 0; i < ret.length; i++) ret[i] = derivedColumns.get(i).columnName;\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.XMLUtils.getTextContentFromTagsFromFile",
	"Comment": "returns the text content of all nodes in the given file with the given tag.",
	"Method": "List<String> getTextContentFromTagsFromFile(File f,String tag){\r\n    List<String> sents = Generics.newArrayList();\r\n    try {\r\n        sents = getTextContentFromTagsFromFileSAXException(f, tag);\r\n    } catch (SAXException e) {\r\n        log.warn(e);\r\n    }\r\n    return sents;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.intersection",
	"Comment": "returns a counter that is the intersection of c1 and c2. if both c1 and c2contain a key, the min of the two counts is used.",
	"Method": "Counter<E> intersection(Counter<E> c1,Counter<E> c2){\r\n    Counter<E> result = c1.getFactory().create();\r\n    for (E key : Sets.union(c1.keySet(), c2.keySet())) {\r\n        double count1 = c1.getCount(key);\r\n        double count2 = c2.getCount(key);\r\n        double minCount = (count1 < count2 ? count1 : count2);\r\n        if (minCount > 0) {\r\n            result.setCount(key, minCount);\r\n        }\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.LabeledScoredConstituent.score",
	"Comment": "returns the score associated with the current node, or nanif there is no score",
	"Method": "double score(){\r\n    return score;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.core.KerasReshape.getInputPreprocessor",
	"Comment": "gets appropriate dl4j inputpreprocessor for given inputtypes.",
	"Method": "InputPreProcessor getInputPreprocessor(InputType inputType){\r\n    if (inputType.length > 1)\r\n        throw new InvalidKerasConfigurationException(\"Keras Reshape layer accepts only one input (received \" + inputType.length + \")\");\r\n    InputPreProcessor preprocessor = null;\r\n    if (inputType[0] instanceof InputType.InputTypeConvolutional) {\r\n        InputType.InputTypeConvolutional it = (InputType.InputTypeConvolutional) inputType[0];\r\n        val inputShape = new long[] { it.getChannels(), it.getHeight(), it.getWidth() };\r\n        val dimOrder = getDimOrder();\r\n        if (dimOrder == DimOrder.THEANO || dimOrder == DimOrder.NONE && kerasMajorVersion == 1) {\r\n            if (targetShape.length == 2) {\r\n                targetShape = new long[] { targetShape[1], targetShape[0] };\r\n            } else {\r\n                targetShape = new long[] { targetShape[1], targetShape[0], targetShape[2] };\r\n            }\r\n            preprocessor = new ReshapePreprocessor(inputShape, targetShape);\r\n        } else {\r\n            if (inputShape[0] != targetShape[0])\r\n                targetShape = new long[] { targetShape[2], targetShape[0], targetShape[1] };\r\n            preprocessor = new ReshapePreprocessor(inputShape, targetShape);\r\n        }\r\n    } else if (inputType[0] instanceof InputType.InputTypeRecurrent) {\r\n        InputType.InputTypeRecurrent it = (InputType.InputTypeRecurrent) inputType[0];\r\n        val inputShape = new long[] { it.getSize(), it.getTimeSeriesLength() };\r\n        preprocessor = new ReshapePreprocessor(inputShape, this.targetShape);\r\n    } else if (inputType[0] instanceof InputType.InputTypeFeedForward) {\r\n        InputType.InputTypeFeedForward it = (InputType.InputTypeFeedForward) inputType[0];\r\n        val inputShape = new long[] { it.getSize() };\r\n        if (targetShape.length == 3) {\r\n            targetShape = targetShapeForDimOrder(inputShape, targetShape);\r\n        }\r\n        preprocessor = new ReshapePreprocessor(inputShape, this.targetShape);\r\n    }\r\n    return preprocessor;\r\n}"
}, {
	"Path": "org.deeplearning4j.text.tokenization.tokenizerfactory.PosUimaTokenizerFactory.getTokenPreProcessor",
	"Comment": "returns tokenpreprocessor set for this tokenizerfactory instance",
	"Method": "TokenPreProcess getTokenPreProcessor(){\r\n    return tokenPreProcess;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.variational.VariationalAutoencoder.addListeners",
	"Comment": "this method adds additional traininglistener to existing listeners",
	"Method": "void addListeners(TrainingListener listeners){\r\n    if (this.trainingListeners == null) {\r\n        setListeners(listeners);\r\n        return;\r\n    }\r\n    for (TrainingListener listener : listeners) trainingListeners.add(listener);\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.PTBTokenizerTest.testJacobEisensteinApostropheCase",
	"Comment": "these case check things still work at end of file that would normally have following contexts.",
	"Method": "void testJacobEisensteinApostropheCase(){\r\n    assertEquals(jeInputs.length, jeOutputs.length);\r\n    for (int i = 0; i < jeInputs.length; i++) {\r\n        StringReader reader = new StringReader(jeInputs[i]);\r\n        PTBTokenizer<Word> tokenizer = PTBTokenizer.newPTBTokenizer(reader);\r\n        List<Word> tokens = tokenizer.tokenize();\r\n        assertEquals(jeOutputs[i], tokens);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.MaxentTagger.simplifyLambda",
	"Comment": "searching the lambda array for 0 entries, removes them.thissaves a large chunk of space in the tagger models which are buildwith l1 regularization.after removing the zeros, go through the feature arrays andreindex the pointers into the lambda array.this saves some timelater on at runtime.",
	"Method": "void simplifyLambda(){\r\n    double[] lambda = getLambdaSolve().lambda;\r\n    int[] map = new int[lambda.length];\r\n    int current = 0;\r\n    for (int index = 0; index < lambda.length; ++index) {\r\n        if (lambda[index] == 0.0) {\r\n            map[index] = -1;\r\n        } else {\r\n            map[index] = current;\r\n            current++;\r\n        }\r\n    }\r\n    double[] condensedLambda = new double[current];\r\n    for (int i = 0; i < lambda.length; ++i) {\r\n        if (map[i] != -1) {\r\n            condensedLambda[map[i]] = lambda[i];\r\n        }\r\n    }\r\n    for (Map<String, int[]> featureMap : fAssociations) {\r\n        for (Map.Entry<String, int[]> entry : featureMap.entrySet()) {\r\n            int[] fAssociations = entry.getValue();\r\n            for (int index = 0; index < ySize; ++index) {\r\n                if (fAssociations[index] >= 0) {\r\n                    fAssociations[index] = map[fAssociations[index]];\r\n                }\r\n            }\r\n        }\r\n    }\r\n    prob = new LambdaSolveTagger(condensedLambda);\r\n}"
}, {
	"Path": "org.datavec.image.loader.Java2DNativeImageLoader.asBufferedImage",
	"Comment": "converts an indarray to a bufferedimage. only intended for images with rank 3.",
	"Method": "BufferedImage asBufferedImage(INDArray array,BufferedImage asBufferedImage,INDArray array,int dataType){\r\n    return converter2.convert(asFrame(array, dataType));\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.TreeFunctions.getLabeledTreeToCategoryWordTagTreeFunction",
	"Comment": "return a function that maps from stringlabel labeled trees tolabeledscoredtrees with a categorywordtag label.",
	"Method": "Function<Tree, Tree> getLabeledTreeToCategoryWordTagTreeFunction(){\r\n    return new LabeledTreeToCategoryWordTagTreeFunction();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Trees.localTreeAsCatList",
	"Comment": "returns the syntactic category of the tree as a list of the syntactic categories of the mother and the daughters",
	"Method": "List<String> localTreeAsCatList(Tree t){\r\n    List<String> l = new ArrayList(t.children().length + 1);\r\n    l.add(t.label().value());\r\n    for (int i = 0; i < t.children().length; i++) {\r\n        l.add(t.children()[i].label().value());\r\n    }\r\n    return l;\r\n}"
}, {
	"Path": "org.deeplearning4j.parallelism.ParallelWrapper.shutdown",
	"Comment": "this method causes all threads used for parallel training to stop",
	"Method": "void shutdown(){\r\n    try {\r\n        close();\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasModelBuilder.modelJsonFilename",
	"Comment": "set model architecture from file name pointing to model json string.",
	"Method": "KerasModelBuilder modelJsonFilename(String modelJsonFilename){\r\n    checkForExistence(modelJsonFilename);\r\n    this.modelJson = new String(Files.readAllBytes(Paths.get(modelJsonFilename)));\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.MaxentTagger.trainAndSaveModel",
	"Comment": "reads in the training corpus from a filename and trains the tagger",
	"Method": "void trainAndSaveModel(TaggerConfig config){\r\n    String modelName = config.getModel();\r\n    MaxentTagger maxentTagger = new MaxentTagger();\r\n    maxentTagger.init(config);\r\n    TaggerExperiments samples = new TaggerExperiments(config, maxentTagger);\r\n    TaggerFeatures feats = samples.getTaggerFeatures();\r\n    byte[][] fnumArr = samples.getFnumArr();\r\n    log.info(\"Samples from \" + config.getFile());\r\n    log.info(\"Number of features: \" + feats.size());\r\n    log.info(\"Tag set: \" + maxentTagger.tags.tagSet());\r\n    Problem p = new Problem(samples, feats);\r\n    LambdaSolveTagger prob = new LambdaSolveTagger(p, 0.0001, fnumArr);\r\n    maxentTagger.prob = prob;\r\n    if (config.getSearch().equals(\"owlqn\")) {\r\n        CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());\r\n        runner.solveL1(config.getRegL1());\r\n    } else if (config.getSearch().equals(\"owlqn2\")) {\r\n        CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());\r\n        runner.solveOWLQN2(config.getRegL1());\r\n    } else if (config.getSearch().equals(\"cg\")) {\r\n        CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());\r\n        runner.solveCG();\r\n    } else if (config.getSearch().equals(\"qn\")) {\r\n        CGRunner runner = new CGRunner(prob, config.getModel(), config.getSigmaSquared());\r\n        runner.solveQN();\r\n    } else {\r\n        prob.improvedIterative(config.getIterations());\r\n    }\r\n    if (prob.checkCorrectness()) {\r\n        log.info(\"Model is correct [empirical expec = model expec]\");\r\n    } else {\r\n        log.info(\"Model is not correct\");\r\n    }\r\n    maxentTagger.removeDeadRules();\r\n    maxentTagger.simplifyLambda();\r\n    maxentTagger.saveModel(modelName);\r\n    log.info(\"Extractors list:\");\r\n    log.info(maxentTagger.extractors.toString() + \"\\nrare\" + maxentTagger.extractorsRare.toString());\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.natlogPolarities",
	"Comment": "the natural logic notion of polarity for each token in a sentence.",
	"Method": "List<Polarity> natlogPolarities(Properties props,List<Polarity> natlogPolarities){\r\n    return natlogPolarities(this.defaultProps);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.spanish.SpanishXMLTreeReader.buildEllipticNode",
	"Comment": "build a parse tree node corresponding to an elliptic node in the parse xml.",
	"Method": "Tree buildEllipticNode(Node root){\r\n    Element eRoot = (Element) root;\r\n    String constituentStr = eRoot.getNodeName();\r\n    List<Tree> kids = new ArrayList();\r\n    Tree leafNode = treeFactory.newLeaf(SpanishTreeNormalizer.EMPTY_LEAF_VALUE);\r\n    if (leafNode.label() instanceof HasWord)\r\n        ((HasWord) leafNode.label()).setWord(SpanishTreeNormalizer.EMPTY_LEAF_VALUE);\r\n    kids.add(leafNode);\r\n    Tree t = treeFactory.newTreeNode(constituentStr, kids);\r\n    return t;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.TreeShapedStack.peek",
	"Comment": "returns the data in the top node of the stack.if there is nodata, eg the stack size is 0, an exception is thrown.",
	"Method": "T peek(){\r\n    if (size == 0) {\r\n        throw new EmptyStackException();\r\n    }\r\n    return data;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Trilean.isUnknown",
	"Comment": "returns true if this trilean is neither true or false, and false if it is either true or false.",
	"Method": "boolean isUnknown(){\r\n    return value == 2;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.BooleanLogRecordHandler.handle",
	"Comment": "for booleanlogrecordhandler, you should leave this alone and implement propagaterecord instead.",
	"Method": "List<Record> handle(Record record){\r\n    boolean keep = propagateRecord(record);\r\n    if (keep) {\r\n        ArrayList<Record> records = new ArrayList();\r\n        records.add(record);\r\n        return records;\r\n    } else {\r\n        return LogRecordHandler.EMPTY;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.sequencevectors.sequence.SequenceElement.getGradient",
	"Comment": "returns gradient for this specific element, at specific position",
	"Method": "double getGradient(int index,double g,double lr){\r\n    return 0.0;\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.string.StringListToCountsNDArrayTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return vocabulary.toArray(new String[vocabulary.size()]);\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.MultiClassPrecisionRecallStats.getRecallDescription",
	"Comment": "returns a string summarizing precision that will print nicely.",
	"Method": "String getRecallDescription(int numDigits,String getRecallDescription,int numDigits,L label){\r\n    NumberFormat nf = NumberFormat.getNumberInstance();\r\n    nf.setMaximumFractionDigits(numDigits);\r\n    Triple<Double, Integer, Integer> recall = getRecallInfo(label);\r\n    return nf.format(recall.first()) + \"  (\" + recall.second() + \"/\" + (recall.second() + recall.third()) + \")\";\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.sequence.SequenceDifferenceTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return new String[] { columnName() };\r\n}"
}, {
	"Path": "org.datavec.image.transform.WarpImageTransform.doTransform",
	"Comment": "takes an image and returns a transformed image.uses the random object in the case of random transformations.",
	"Method": "ImageWritable doTransform(ImageWritable image,Random random){\r\n    if (image == null) {\r\n        return null;\r\n    }\r\n    Mat mat = converter.convert(image.getFrame());\r\n    Point2f src = new Point2f(4);\r\n    Point2f dst = new Point2f(4);\r\n    src.put(0, 0, mat.cols(), 0, mat.cols(), mat.rows(), 0, mat.rows());\r\n    for (int i = 0; i < 8; i++) {\r\n        dst.put(i, src.get(i) + deltas[i] * (random != null ? 2 * random.nextFloat() - 1 : 1));\r\n    }\r\n    Mat result = new Mat();\r\n    M = getPerspectiveTransform(src, dst);\r\n    warpPerspective(mat, result, M, mat.size(), interMode, borderMode, borderValue);\r\n    return new ImageWritable(converter.convert(result));\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraPennTreeNormalizer.cleanUpLabel",
	"Comment": "remove things like hyphened functional tags and equals from theend of a node label.",
	"Method": "String cleanUpLabel(String label){\r\n    if (nodeCleanup == 1) {\r\n        return tlp.categoryAndFunction(label);\r\n    } else if (nodeCleanup == 2) {\r\n        return tlp.basicCategory(label);\r\n    }\r\n    return label;\r\n}"
}, {
	"Path": "com.atilika.kuromoji.TokenBase.isUser",
	"Comment": "predicate indicating whether this token is included is from the user dictionaryif a token is contained both in the user dictionary and standard dictionary, this method will return true",
	"Method": "boolean isUser(){\r\n    return type == Type.USER;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.layers.misc.ElementWiseMultiplicationLayer.getMemoryReport",
	"Comment": "this is a report of the estimated memory consumption for the given layer",
	"Method": "LayerMemoryReport getMemoryReport(InputType inputType){\r\n    InputType outputType = getOutputType(-1, inputType);\r\n    val numParams = initializer().numParams(this);\r\n    val updaterStateSize = (int) getIUpdater().stateSize(numParams);\r\n    int trainSizeFixed = 0;\r\n    int trainSizeVariable = 0;\r\n    if (getIDropout() != null) {\r\n        if (false) {\r\n            trainSizeVariable += 0;\r\n        } else {\r\n            trainSizeVariable += inputType.arrayElementsPerExample();\r\n        }\r\n    }\r\n    trainSizeVariable += outputType.arrayElementsPerExample();\r\n    return // No additional memory (beyond activations) for inference\r\n    new LayerMemoryReport.Builder(layerName, ElementWiseMultiplicationLayer.class, inputType, outputType).standardMemory(numParams, updaterStateSize).workingMemory(// No additional memory (beyond activations) for inference\r\n    0, // No additional memory (beyond activations) for inference\r\n    0, trainSizeFixed, // No caching in DenseLayer\r\n    trainSizeVariable).cacheMemory(MemoryReport.CACHE_MODE_ALL_ZEROS, MemoryReport.CACHE_MODE_ALL_ZEROS).build();\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.WikidictAnnotator.main",
	"Comment": "a debugging method to try entity linking sentences from the console.",
	"Method": "void main(String[] args){\r\n    Properties props = StringUtils.argsToProperties(args);\r\n    props.setProperty(\"annotators\", \"tokenize,ssplit,pos,lemma,ner,entitymentions,entitylink\");\r\n    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\r\n    IOUtils.console(\"sentence> \", line -> {\r\n        Annotation ann = new Annotation(line);\r\n        pipeline.annotate(ann);\r\n        List<CoreLabel> tokens = ann.get(CoreAnnotations.SentencesAnnotation.class).get(0).get(CoreAnnotations.TokensAnnotation.class);\r\n        System.err.println(StringUtils.join(tokens.stream().map(x -> x.get(CoreAnnotations.WikipediaEntityAnnotation.class)), \"  \"));\r\n    });\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.learning.impl.sequence.DM.inferSequence",
	"Comment": "this method does training on previously unseen paragraph, and returns inferred vector",
	"Method": "INDArray inferSequence(Sequence<T> sequence,long nr,double learningRate,double minLearningRate,int iterations){\r\n    AtomicLong nextRandom = new AtomicLong(nr);\r\n    if (sequence.isEmpty())\r\n        return null;\r\n    Random random = Nd4j.getRandomFactory().getNewRandomInstance(configuration.getSeed() * sequence.hashCode(), lookupTable.layerSize() + 1);\r\n    INDArray ret = Nd4j.rand(new int[] { 1, lookupTable.layerSize() }, random).subi(0.5).divi(lookupTable.layerSize());\r\n    for (int iter = 0; iter < iterations; iter++) {\r\n        for (int i = 0; i < sequence.size(); i++) {\r\n            nextRandom.set(Math.abs(nextRandom.get() * 25214903917L + 11));\r\n            dm(i, sequence, (int) nextRandom.get() % window, nextRandom, learningRate, null, true, ret);\r\n        }\r\n        learningRate = ((learningRate - minLearningRate) / (iterations - iter)) + minLearningRate;\r\n    }\r\n    finish();\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.asCollection",
	"Comment": "creates a new collection from the given collectionfactory, and adds all of the objectsreturned by the given iterator.",
	"Method": "Collection<E> asCollection(Iterator<? extends E> iter,CollectionFactory<E> cf){\r\n    Collection<E> c = cf.newCollection();\r\n    return addAll(iter, c);\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.TaggerExperiments.getFeaturesNew",
	"Comment": "this method uses and deletes a file tempxxxxxx.x in the current directory!",
	"Method": "void getFeaturesNew(){\r\n    try {\r\n        log.info(\"TaggerExperiments.getFeaturesNew: initializing fnumArr.\");\r\n        fnumArr = new byte[xSize][ySize];\r\n        File hFile = File.createTempFile(\"temp\", \".x\", new File(\"./\"));\r\n        RandomAccessFile hF = new RandomAccessFile(hFile, \"rw\");\r\n        log.info(\"  length of sTemplates keys: \" + sTemplates.size());\r\n        log.info(\"getFeaturesNew adding features ...\");\r\n        int current = 0;\r\n        int numFeats = 0;\r\n        final boolean VERBOSE = false;\r\n        for (FeatureKey fK : sTemplates) {\r\n            int numF = fK.num;\r\n            int[] xValues;\r\n            Pair<Integer, String> wT = new Pair(numF, fK.val);\r\n            xValues = tFeature.getXValues(wT);\r\n            if (xValues == null) {\r\n                log.info(\"  xValues is null: \" + fK);\r\n                continue;\r\n            }\r\n            int numEvidence = 0;\r\n            int y = maxentTagger.tags.getIndex(fK.tag);\r\n            for (int xValue : xValues) {\r\n                if (maxentTagger.occurringTagsOnly) {\r\n                    String word = ExtractorFrames.cWord.extract(tHistories.getHistory(xValue));\r\n                    if (maxentTagger.dict.getCount(word, fK.tag) == 0) {\r\n                        continue;\r\n                    }\r\n                }\r\n                if (maxentTagger.possibleTagsOnly) {\r\n                    String word = ExtractorFrames.cWord.extract(tHistories.getHistory(xValue));\r\n                    String[] tags = maxentTagger.dict.getTags(word);\r\n                    Set<String> s = Generics.newHashSet(Arrays.asList(maxentTagger.tags.deterministicallyExpandTags(tags)));\r\n                    if (DEBUG)\r\n                        System.err.printf(\"possible tags for %s: %s\\n\", word, Arrays.toString(s.toArray()));\r\n                    if (!s.contains(fK.tag))\r\n                        continue;\r\n                }\r\n                numEvidence += this.px[xValue];\r\n            }\r\n            if (populated(numF, numEvidence)) {\r\n                int[] positions = tFeature.getPositions(fK);\r\n                if (maxentTagger.occurringTagsOnly || maxentTagger.possibleTagsOnly) {\r\n                    positions = null;\r\n                }\r\n                if (positions == null) {\r\n                    int numElements = 0;\r\n                    for (int x : xValues) {\r\n                        if (maxentTagger.occurringTagsOnly) {\r\n                            String word = ExtractorFrames.cWord.extract(tHistories.getHistory(x));\r\n                            if (maxentTagger.dict.getCount(word, fK.tag) == 0) {\r\n                                continue;\r\n                            }\r\n                        }\r\n                        if (maxentTagger.possibleTagsOnly) {\r\n                            String word = ExtractorFrames.cWord.extract(tHistories.getHistory(x));\r\n                            String[] tags = maxentTagger.dict.getTags(word);\r\n                            Set<String> s = Generics.newHashSet(Arrays.asList(maxentTagger.tags.deterministicallyExpandTags(tags)));\r\n                            if (!s.contains(fK.tag))\r\n                                continue;\r\n                        }\r\n                        numElements++;\r\n                        hF.writeInt(x);\r\n                        fnumArr[x][y]++;\r\n                    }\r\n                    TaggerFeature tF = new TaggerFeature(current, current + numElements - 1, fK, maxentTagger.getTagIndex(fK.tag), this);\r\n                    tFeature.addPositions(current, current + numElements - 1, fK);\r\n                    current = current + numElements;\r\n                    feats.add(tF);\r\n                    if (VERBOSE) {\r\n                        log.info(\"  added feature with key \" + fK + \" has support \" + numElements);\r\n                    }\r\n                } else {\r\n                    for (int x : xValues) {\r\n                        fnumArr[x][y]++;\r\n                    }\r\n                    TaggerFeature tF = new TaggerFeature(positions[0], positions[1], fK, maxentTagger.getTagIndex(fK.tag), this);\r\n                    feats.add(tF);\r\n                    if (VERBOSE) {\r\n                        log.info(\"  added feature with key \" + fK + \" has support \" + xValues.length);\r\n                    }\r\n                }\r\n                if (maxentTagger.fAssociations.size() <= fK.num) {\r\n                    for (int i = maxentTagger.fAssociations.size(); i <= fK.num; ++i) {\r\n                        maxentTagger.fAssociations.add(Generics.<String, int[]>newHashMap());\r\n                    }\r\n                }\r\n                Map<String, int[]> fValueAssociations = maxentTagger.fAssociations.get(fK.num);\r\n                int[] fTagAssociations = fValueAssociations.get(fK.val);\r\n                if (fTagAssociations == null) {\r\n                    fTagAssociations = new int[ySize];\r\n                    for (int i = 0; i < ySize; ++i) {\r\n                        fTagAssociations[i] = -1;\r\n                    }\r\n                    fValueAssociations.put(fK.val, fTagAssociations);\r\n                }\r\n                fTagAssociations[maxentTagger.tags.getIndex(fK.tag)] = numFeats;\r\n                numFeats++;\r\n            }\r\n        }\r\n        tFeature.release();\r\n        feats.xIndexed = new int[current];\r\n        hF.seek(0);\r\n        int current1 = 0;\r\n        while (current1 < current) {\r\n            feats.xIndexed[current1] = hF.readInt();\r\n            current1++;\r\n        }\r\n        log.info(\"  total feats: \" + sTemplates.size() + \", populated: \" + numFeats);\r\n        hF.close();\r\n        hFile.delete();\r\n        int max = 0;\r\n        int maxGt = 0;\r\n        int numZeros = 0;\r\n        for (int x = 0; x < xSize; x++) {\r\n            int numGt = 0;\r\n            for (int y = 0; y < ySize; y++) {\r\n                if (fnumArr[x][y] > 0) {\r\n                    numGt++;\r\n                    if (max < fnumArr[x][y]) {\r\n                        max = fnumArr[x][y];\r\n                    }\r\n                } else {\r\n                    numZeros++;\r\n                }\r\n            }\r\n            if (maxGt < numGt) {\r\n                maxGt = numGt;\r\n            }\r\n        }\r\n        log.info(\"  Max features per x,y pair: \" + max);\r\n        log.info(\"  Max non-zero y values for an x: \" + maxGt);\r\n        log.info(\"  Number of non-zero feature x,y pairs: \" + (xSize * ySize - numZeros));\r\n        log.info(\"  Number of zero feature x,y pairs: \" + numZeros);\r\n        log.info(\"end getFeaturesNew.\");\r\n    } catch (Exception e) {\r\n        throw new RuntimeIOException(e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.XMLUtils.getTagElementTriplesFromFileSAXException",
	"Comment": "returns the elements in the given file with the given tag associated withthe text content of the two previous siblings and two next siblings.",
	"Method": "List<Triple<String, Element, String>> getTagElementTriplesFromFileSAXException(File f,String tag){\r\n    return getTagElementTriplesFromFileNumBoundedSAXException(f, tag, 2);\r\n}"
}, {
	"Path": "org.deeplearning4j.integration.BaseDL4JTest.getDataType",
	"Comment": "override this to set the datatype of the tests defined in the child class",
	"Method": "DataBuffer.Type getDataType(){\r\n    return DataBuffer.Type.FLOAT;\r\n}"
}, {
	"Path": "org.datavec.api.util.ReflectionUtils.copy",
	"Comment": "make a copy of the writable object using serialization to a buffer",
	"Method": "T copy(Configuration conf,T src,T dst){\r\n    CopyInCopyOutBuffer buffer = cloneBuffers.get();\r\n    buffer.outBuffer.reset();\r\n    SerializationFactory factory = getFactory(conf);\r\n    Class<T> cls = (Class<T>) src.getClass();\r\n    Serializer<T> serializer = factory.getSerializer(cls);\r\n    serializer.open(buffer.outBuffer);\r\n    serializer.serialize(src);\r\n    buffer.moveData();\r\n    Deserializer<T> deserializer = factory.getDeserializer(cls);\r\n    deserializer.open(buffer.inBuffer);\r\n    dst = deserializer.deserialize(dst);\r\n    return dst;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasLayerUtils.getTimeDistributedLayerConfig",
	"Comment": "extract inner layer config from timedistributed configuration and mergeit into the outer config.",
	"Method": "Map<String, Object> getTimeDistributedLayerConfig(Map<String, Object> layerConfig,KerasLayerConfiguration conf){\r\n    if (!layerConfig.containsKey(conf.getLAYER_FIELD_CLASS_NAME()))\r\n        throw new InvalidKerasConfigurationException(\"Field \" + conf.getLAYER_FIELD_CLASS_NAME() + \" missing from layer config\");\r\n    if (!layerConfig.get(conf.getLAYER_FIELD_CLASS_NAME()).equals(conf.getLAYER_CLASS_NAME_TIME_DISTRIBUTED()))\r\n        throw new InvalidKerasConfigurationException(\"Expected \" + conf.getLAYER_CLASS_NAME_TIME_DISTRIBUTED() + \" layer, found \" + layerConfig.get(conf.getLAYER_FIELD_CLASS_NAME()));\r\n    if (!layerConfig.containsKey(conf.getLAYER_FIELD_CONFIG()))\r\n        throw new InvalidKerasConfigurationException(\"Field \" + conf.getLAYER_FIELD_CONFIG() + \" missing from layer config\");\r\n    Map<String, Object> outerConfig = getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    Map<String, Object> innerLayer = (Map<String, Object>) outerConfig.get(conf.getLAYER_FIELD_LAYER());\r\n    layerConfig.put(conf.getLAYER_FIELD_CLASS_NAME(), innerLayer.get(conf.getLAYER_FIELD_CLASS_NAME()));\r\n    layerConfig.put(conf.getLAYER_FIELD_NAME(), innerLayer.get(conf.getLAYER_FIELD_CLASS_NAME()));\r\n    Map<String, Object> innerConfig = getInnerLayerConfigFromConfig(innerLayer, conf);\r\n    outerConfig.putAll(innerConfig);\r\n    outerConfig.remove(conf.getLAYER_FIELD_LAYER());\r\n    return layerConfig;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.toString",
	"Comment": "returns a string representation of the contents of calling tostringon each element of the given iterable, joining the elements togetherwith the given glue.",
	"Method": "String toString(Iterable<E> iter,String glue){\r\n    StringBuilder builder = new StringBuilder();\r\n    for (Iterator<E> it = iter.iterator(); it.hasNext(); ) {\r\n        builder.append(it.next());\r\n        if (it.hasNext()) {\r\n            builder.append(glue);\r\n        }\r\n    }\r\n    return builder.toString();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArrayCoreMap.hashCode",
	"Comment": "returns a composite hashcode over all the keys and values currentlystored in the map.because they may change over time, this classis not appropriate for use as map keys.",
	"Method": "int hashCode(){\r\n    IdentityHashSet<CoreMap> calledSet = hashCodeCalled.get();\r\n    boolean createdCalledSet = (calledSet == null);\r\n    if (createdCalledSet) {\r\n        calledSet = new IdentityHashSet();\r\n        hashCodeCalled.set(calledSet);\r\n    }\r\n    if (calledSet.contains(this)) {\r\n        return 0;\r\n    }\r\n    calledSet.add(this);\r\n    int keysCode = 0;\r\n    int valuesCode = 0;\r\n    for (int i = 0; i < size; i++) {\r\n        keysCode += (i < keys.length && values[i] != null ? keys[i].hashCode() : 0);\r\n        valuesCode += (i < values.length && values[i] != null ? values[i].hashCode() : 0);\r\n    }\r\n    if (createdCalledSet) {\r\n        hashCodeCalled.set(null);\r\n    } else {\r\n        calledSet.remove(this);\r\n    }\r\n    return keysCode * 37 + valuesCode;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractTreebankLanguagePack.sentenceFinalPunctuationTagAcceptFilter",
	"Comment": "returns a filter that accepts a string that is a sentence endpunctuation tag, and rejects everything else.",
	"Method": "Predicate<String> sentenceFinalPunctuationTagAcceptFilter(){\r\n    return sFPunctTagStringAcceptFilter;\r\n}"
}, {
	"Path": "org.datavec.camel.component.DataVecProducer.inputFromExchange",
	"Comment": "stub, still need to fill out more of the end point yet..endpoint will likely be initialized with a split",
	"Method": "InputSplit inputFromExchange(Exchange exchange){\r\n    return marshaller.getSplit(exchange);\r\n}"
}, {
	"Path": "org.datavec.image.loader.NativeImageLoader.asFrame",
	"Comment": "converts an indarray to a javacv frame. only intended for images with rank 3.",
	"Method": "Frame asFrame(INDArray array,Frame asFrame,INDArray array,int dataType){\r\n    return converter.convert(asMat(array, OpenCVFrameConverter.getMatDepth(dataType)));\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.PTBTokenizer.factory",
	"Comment": "get a tokenizerfactory that does penn treebank tokenization. this is now the recommended factory method to use.",
	"Method": "TokenizerFactory<Word> factory(TokenizerFactory<CoreLabel> factory,boolean tokenizeNLs,boolean invertible,TokenizerFactory<T> factory,LexedTokenFactory<T> factory,String options){\r\n    return new PTBTokenizerFactory(factory, options);\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.getProperties",
	"Comment": "fetches the properties object used to construct this annotator.",
	"Method": "Properties getProperties(){\r\n    return properties;\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.AsyncShieldMultiDataSetIterator.setPreProcessor",
	"Comment": "set the preprocessor to be applied to each multidataset, before each multidataset is returned.",
	"Method": "void setPreProcessor(MultiDataSetPreProcessor preProcessor){\r\n    backingIterator.setPreProcessor(preProcessor);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CollectionUtils.filterAsList",
	"Comment": "filters the objects in the collection according to the given filter and returns a list.",
	"Method": "List<T> filterAsList(Collection<? extends T> original,Predicate<? super T> f){\r\n    List<T> transformed = new ArrayList();\r\n    for (T t : original) {\r\n        if (f.test(t)) {\r\n            transformed.add(t);\r\n        }\r\n    }\r\n    return transformed;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.convolutional.KerasConvolutionUtils.getConvolutionModeFromConfig",
	"Comment": "get convolution border mode from keras layer configuration.",
	"Method": "ConvolutionMode getConvolutionModeFromConfig(Map<String, Object> layerConfig,KerasLayerConfiguration conf){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (!innerConfig.containsKey(conf.getLAYER_FIELD_BORDER_MODE()))\r\n        throw new InvalidKerasConfigurationException(\"Could not determine convolution border mode: no \" + conf.getLAYER_FIELD_BORDER_MODE() + \" field found\");\r\n    String borderMode = (String) innerConfig.get(conf.getLAYER_FIELD_BORDER_MODE());\r\n    ConvolutionMode convolutionMode;\r\n    if (borderMode.equals(conf.getLAYER_BORDER_MODE_SAME())) {\r\n        convolutionMode = ConvolutionMode.Same;\r\n    } else if (borderMode.equals(conf.getLAYER_BORDER_MODE_VALID()) || borderMode.equals(conf.getLAYER_BORDER_MODE_FULL())) {\r\n        convolutionMode = ConvolutionMode.Truncate;\r\n    } else {\r\n        throw new UnsupportedKerasConfigurationException(\"Unsupported convolution border mode: \" + borderMode);\r\n    }\r\n    return convolutionMode;\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.ChunkAnnotationUtils.fixChunkTokenBoundaries",
	"Comment": "give an list of character offsets for chunk, fix tokenization so tokenization occurs atboundary of chunks.",
	"Method": "boolean fixChunkTokenBoundaries(CoreMap docAnnotation,List<IntPair> chunkCharOffsets){\r\n    String text = docAnnotation.get(CoreAnnotations.TextAnnotation.class);\r\n    List<CoreLabel> tokens = docAnnotation.get(CoreAnnotations.TokensAnnotation.class);\r\n    List<CoreLabel> output = new ArrayList(tokens.size());\r\n    int i = 0;\r\n    CoreLabel token = tokens.get(i);\r\n    for (IntPair offsets : chunkCharOffsets) {\r\n        assert (token.beginPosition() >= 0);\r\n        assert (token.endPosition() >= 0);\r\n        int offsetBegin = offsets.getSource();\r\n        int offsetEnd = offsets.getTarget();\r\n        while (offsetBegin < token.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class) || offsetBegin >= token.get(CoreAnnotations.CharacterOffsetEndAnnotation.class)) {\r\n            output.add(token);\r\n            i++;\r\n            if (i >= tokens.size()) {\r\n                return false;\r\n            }\r\n            token = tokens.get(i);\r\n        }\r\n        while (offsetEnd > token.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class)) {\r\n            if (offsetBegin > token.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class)) {\r\n                if (offsetEnd < token.get(CoreAnnotations.CharacterOffsetEndAnnotation.class)) {\r\n                    output.add(tokenFactory.makeToken(text.substring(token.beginPosition(), offsetBegin), token.beginPosition(), offsetBegin - token.beginPosition()));\r\n                    output.add(tokenFactory.makeToken(text.substring(offsetBegin, offsetEnd), offsetBegin, offsetEnd - offsetBegin));\r\n                    output.add(tokenFactory.makeToken(text.substring(offsetEnd, token.endPosition()), offsetEnd, token.endPosition() - offsetEnd));\r\n                } else {\r\n                    output.add(tokenFactory.makeToken(text.substring(token.beginPosition(), offsetBegin), token.beginPosition(), offsetBegin - token.beginPosition()));\r\n                    output.add(tokenFactory.makeToken(text.substring(offsetBegin, token.endPosition()), offsetBegin, token.endPosition() - offsetBegin));\r\n                }\r\n            } else if (offsetEnd < token.get(CoreAnnotations.CharacterOffsetEndAnnotation.class)) {\r\n                output.add(tokenFactory.makeToken(text.substring(token.beginPosition(), offsetEnd), token.beginPosition(), offsetEnd - token.beginPosition()));\r\n                output.add(tokenFactory.makeToken(text.substring(offsetEnd, token.endPosition()), offsetEnd, token.endPosition() - offsetEnd));\r\n            } else {\r\n                output.add(token);\r\n            }\r\n            i++;\r\n            if (i >= tokens.size()) {\r\n                return false;\r\n            }\r\n            token = tokens.get(i);\r\n        }\r\n    }\r\n    for (; i < tokens.size(); i++) {\r\n        token = tokens.get(i);\r\n        output.add(token);\r\n    }\r\n    docAnnotation.set(CoreAnnotations.TokensAnnotation.class, output);\r\n    return true;\r\n}"
}, {
	"Path": "org.deeplearning4j.nearestneighbor.client.NearestNeighborsClient.addAuthHeader",
	"Comment": "add the specified authentication header to the specified httprequest",
	"Method": "HttpRequest addAuthHeader(HttpRequest request){\r\n    if (authToken != null) {\r\n        request.header(\"authorization\", \"Bearer \" + authToken);\r\n    }\r\n    return request;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.addListeners",
	"Comment": "this method adds additional traininglistener to existing listeners",
	"Method": "void addListeners(TrainingListener listeners){\r\n    Collections.addAll(trainingListeners, listeners);\r\n    if (solver != null) {\r\n        solver.setListeners(this.trainingListeners);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.CustomAnnotationSerializer.saveToken",
	"Comment": "saves one individual sentence token, in a simple tabular format, in the style of conll",
	"Method": "void saveToken(CoreLabel token,boolean haveExplicitAntecedent,PrintWriter pw){\r\n    String word = token.get(CoreAnnotations.TextAnnotation.class);\r\n    if (word == null) {\r\n        word = token.get(CoreAnnotations.ValueAnnotation.class);\r\n    }\r\n    if (word != null) {\r\n        word = word.replaceAll(\"\\\\s+\", SPACE_HOLDER);\r\n        pw.print(word);\r\n    }\r\n    pw.print(\"\\t\");\r\n    String lemma = token.get(CoreAnnotations.LemmaAnnotation.class);\r\n    if (lemma != null) {\r\n        lemma = lemma.replaceAll(\"\\\\s+\", SPACE_HOLDER);\r\n        pw.print(lemma);\r\n    }\r\n    pw.print(\"\\t\");\r\n    String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);\r\n    if (pos != null)\r\n        pw.print(pos);\r\n    pw.print(\"\\t\");\r\n    String ner = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);\r\n    if (ner != null)\r\n        pw.print(ner);\r\n    pw.print(\"\\t\");\r\n    String normNer = token.get(CoreAnnotations.NormalizedNamedEntityTagAnnotation.class);\r\n    if (normNer != null)\r\n        pw.print(normNer);\r\n    pw.print(\"\\t\");\r\n    Integer charBegin = token.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class);\r\n    if (charBegin != null)\r\n        pw.print(charBegin);\r\n    pw.print(\"\\t\");\r\n    Integer charEnd = token.get(CoreAnnotations.CharacterOffsetEndAnnotation.class);\r\n    if (charEnd != null)\r\n        pw.print(charEnd);\r\n    if (haveExplicitAntecedent) {\r\n        String aa = token.get(CoreAnnotations.AntecedentAnnotation.class);\r\n        if (aa != null) {\r\n            pw.print(\"\\t\");\r\n            aa = aa.replaceAll(\"\\\\s+\", SPACE_HOLDER);\r\n            pw.print(aa);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.addVarNameForImport",
	"Comment": "marks a variable name as imported.this is used in conjunction with modelimport to ensure immutabilitywhen referencing graph variablesmapped from an external source.",
	"Method": "void addVarNameForImport(String varName){\r\n    importedVarName.add(varName);\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.column.ReorderColumnsTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return newOrder.toArray(new String[newOrder.size()]);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Interval.includesBegin",
	"Comment": "returns whether the start endpoint is included in the interval",
	"Method": "boolean includesBegin(){\r\n    return ((flags & INTERVAL_OPEN_BEGIN) == 0);\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.Morpha.stem",
	"Comment": "delete del letters from end of token, and append string add to give stem.return affix as the affix of the word.",
	"Method": "String stem(int del,String add,String affix){\r\n    int stem_length = yylength() - del;\r\n    int i = 0;\r\n    String result = yytext().substring(0, stem_length);\r\n    if (option(change_case)) {\r\n        result = result.toLowerCase();\r\n    }\r\n    if (!(add.length() == 0))\r\n        result += add;\r\n    if (option(print_affixes)) {\r\n        result += (\"+\" + affix);\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.spanish.SpanishTreeNormalizer.expandCliticPronounsInner",
	"Comment": "expand clitic pronouns on verbs matching the given pattern.",
	"Method": "Tree expandCliticPronounsInner(Tree t,TregexPattern pattern){\r\n    TregexMatcher matcher = pattern.matcher(t);\r\n    while (matcher.find()) {\r\n        Tree verbNode = matcher.getNode(\"vb\");\r\n        String verb = verbNode.value();\r\n        if (!SpanishVerbStripper.isStrippable(verb))\r\n            continue;\r\n        SpanishVerbStripper.StrippedVerb split = verbStripper.separatePronouns(verb);\r\n        if (split == null)\r\n            continue;\r\n        StringBuilder clauseYieldBuilder = new StringBuilder();\r\n        for (Label label : matcher.getNode(\"clause\").yield()) clauseYieldBuilder.append(label.value()).append(\" \");\r\n        String clauseYield = clauseYieldBuilder.toString();\r\n        clauseYield = clauseYield.substring(0, clauseYield.length() - 1);\r\n        List<String> pronouns = split.getPronouns();\r\n        for (int i = pronouns.size() - 1; i >= 0; i--) {\r\n            String pronoun = pronouns.get(i);\r\n            String newTreeStr = null;\r\n            if (AnCoraPronounDisambiguator.isAmbiguous(pronoun)) {\r\n                AnCoraPronounDisambiguator.PersonalPronounType type = AnCoraPronounDisambiguator.disambiguatePersonalPronoun(split, i, clauseYield);\r\n                switch(type) {\r\n                    case OBJECT:\r\n                        newTreeStr = \"(sn (grup.nom (pp000000 %s)))\";\r\n                        break;\r\n                    case REFLEXIVE:\r\n                        newTreeStr = \"(morfema.pronominal (pp000000 %s))\";\r\n                        break;\r\n                    case UNKNOWN:\r\n                        newTreeStr = \"(PRONOUN? (pp000000 %s))\";\r\n                        break;\r\n                }\r\n            } else {\r\n                newTreeStr = \"(sn (grup.nom (pp000000 %s)))\";\r\n            }\r\n            String patternString = \"[insert \" + String.format(newTreeStr, pronoun) + \" $- target]\";\r\n            TsurgeonPattern insertPattern = Tsurgeon.parseOperation(patternString);\r\n            t = insertPattern.matcher().evaluate(t, matcher);\r\n        }\r\n        TsurgeonPattern relabelOperation = Tsurgeon.parseOperation(String.format(\"[relabel vb /%s/]\", split.getStem()));\r\n        t = relabelOperation.matcher().evaluate(t, matcher);\r\n    }\r\n    return t;\r\n}"
}, {
	"Path": "org.datavec.api.transform.TransformProcess.transformRawStringsToInput",
	"Comment": "based on the input schema,map raw string values to the appropriatewritable",
	"Method": "List<Writable> transformRawStringsToInput(String values){\r\n    return transformRawStringsToInputList(Arrays.asList(values));\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Trees.rightEdge",
	"Comment": "returns the positional index of the right edge of a treet within a given root, as defined by the size of the yieldof all material preceding t plus all the materialcontained in t.",
	"Method": "int rightEdge(Tree t,Tree root,boolean rightEdge,Tree t,Tree t1,MutableInteger i){\r\n    if (t == t1) {\r\n        return true;\r\n    } else if (t1.isLeaf()) {\r\n        int j = t1.yield().size();\r\n        i.set(i.intValue() - j);\r\n        return false;\r\n    } else {\r\n        Tree[] kids = t1.children();\r\n        for (int j = kids.length - 1; j >= 0; j--) {\r\n            if (rightEdge(t, kids[j], i)) {\r\n                return true;\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n}"
}, {
	"Path": "org.datavec.image.transform.PipelineImageTransform.doTransform",
	"Comment": "takes an image and executes a pipeline of combined transforms.",
	"Method": "ImageWritable doTransform(ImageWritable image,Random random){\r\n    if (shuffle) {\r\n        Collections.shuffle(imageTransforms);\r\n    }\r\n    currentTransforms.clear();\r\n    for (Pair<ImageTransform, Double> tuple : imageTransforms) {\r\n        if (tuple.getSecond() == 1.0 || rng.nextDouble() < tuple.getSecond()) {\r\n            currentTransforms.add(tuple.getFirst());\r\n            image = random != null ? tuple.getFirst().transform(image, random) : tuple.getFirst().transform(image);\r\n        }\r\n    }\r\n    return image;\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.AsyncShieldDataSetIterator.next",
	"Comment": "like the standard next method but allows acustomizable number of examples returned",
	"Method": "DataSet next(int num,DataSet next){\r\n    return backingIterator.next();\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.makeGraphFromNodes",
	"Comment": "given a collection of nodes from srcgraph, generates a newsemanticgraph based off the subset represented by those nodes.this uses the same vertices as in the original graph, whichallows for equality and comparisons between the two graphs.",
	"Method": "SemanticGraph makeGraphFromNodes(Collection<IndexedWord> nodes,SemanticGraph srcGraph){\r\n    if (nodes.size() == 1) {\r\n        SemanticGraph retSg = new SemanticGraph();\r\n        for (IndexedWord node : nodes) retSg.addVertex(node);\r\n        return retSg;\r\n    }\r\n    if (nodes.isEmpty()) {\r\n        return null;\r\n    }\r\n    List<SemanticGraphEdge> edges = new ArrayList();\r\n    for (IndexedWord nodeG : nodes) {\r\n        for (IndexedWord nodeD : nodes) {\r\n            Collection<SemanticGraphEdge> existingEdges = srcGraph.getAllEdges(nodeG, nodeD);\r\n            if (existingEdges != null) {\r\n                edges.addAll(existingEdges);\r\n            }\r\n        }\r\n    }\r\n    return SemanticGraphFactory.makeFromEdges(edges);\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.DocumentPreprocessor.setTagDelimiter",
	"Comment": "split tags from tokens. the tag will be placed in the tagannotation ofthe returned label.note that for strings that contain two or more instances of the tag delimiter,the last instance is treated as the split point.the tag delimiter should not contain any characters that must be escaped in a javaregex.",
	"Method": "void setTagDelimiter(String s){\r\n    tagDelimiter = s;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.setGradientForVariableName",
	"Comment": "assign a sdvariable to represent the gradient of the sdvariable with the specified name",
	"Method": "void setGradientForVariableName(String variableName,SDVariable variable){\r\n    if (variable == null) {\r\n        throw new ND4JIllegalStateException(\"Unable to set null gradient for variable name \" + variableName);\r\n    }\r\n    gradients.put(variableName, variable);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.GrammaticalRelation.isAncestor",
	"Comment": "returns whether this is equal to or an ancestor of gr in the grammatical relations hierarchy.",
	"Method": "boolean isAncestor(GrammaticalRelation gr){\r\n    while (gr != null) {\r\n        if (this.equals(gr)) {\r\n            return true;\r\n        }\r\n        gr = gr.parent;\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.parameterserver.pw.SharedTrainingWrapper.attachDS",
	"Comment": "this method registers given iterable in virtualdatasetiterator",
	"Method": "void attachDS(Iterator<DataSet> iterator){\r\n    log.debug(\"Attaching thread...\");\r\n    if (iteratorDataSetCount.get() == null)\r\n        iteratorDataSetCount.set(new AtomicInteger(0));\r\n    AtomicInteger count = iteratorDataSetCount.get();\r\n    count.set(0);\r\n    VirtualIterator<DataSet> wrapped = new VirtualIterator(new CountingIterator(iterator, count));\r\n    BlockingObserver obs = new BlockingObserver(exceptionEncountered);\r\n    wrapped.addObserver(obs);\r\n    iteratorsDS.add(wrapped);\r\n    observer.set(obs);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.flow.FlowPath.setRewindPosition",
	"Comment": "this method allows to set position for next rewind within graph",
	"Method": "void setRewindPosition(String frameName,int position){\r\n    frames.get(frameName).setRewindPosition(position);\r\n}"
}, {
	"Path": "edu.stanford.nlp.sentiment.SentimentModel.getVocabWord",
	"Comment": "get the known vocabulary word associated with the given word.",
	"Method": "String getVocabWord(String word){\r\n    if (op.lowercaseWordVectors) {\r\n        word = word.toLowerCase();\r\n    }\r\n    if (wordVectors.containsKey(word)) {\r\n        return word;\r\n    }\r\n    return UNKNOWN_WORD;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.FrozenLayerWithBackpropTest.testFrozenLayerVsSgd",
	"Comment": "frozen layer should have same results as a layer with sgd updater with learning rate set to 0",
	"Method": "void testFrozenLayerVsSgd(){\r\n    DataSet randomData = new DataSet(Nd4j.rand(100, 4, 12345), Nd4j.rand(100, 1, 12345));\r\n    MultiLayerConfiguration confSgd = new NeuralNetConfiguration.Builder().seed(12345).weightInit(WeightInit.XAVIER).updater(new Sgd(2)).list().layer(0, new DenseLayer.Builder().nIn(4).nOut(3).build()).layer(1, new DenseLayer.Builder().updater(new Sgd(0.0)).biasUpdater(new Sgd(0.0)).nIn(3).nOut(4).build()).layer(2, new DenseLayer.Builder().updater(new Sgd(0.0)).biasUpdater(new Sgd(0.0)).nIn(4).nOut(2).build()).layer(3, new OutputLayer.Builder(LossFunctions.LossFunction.MSE).updater(new Sgd(0.0)).biasUpdater(new Sgd(0.0)).activation(Activation.TANH).nIn(2).nOut(1).build()).build();\r\n    MultiLayerConfiguration confFrozen = new NeuralNetConfiguration.Builder().seed(12345).weightInit(WeightInit.XAVIER).updater(new Sgd(2)).list().layer(0, new DenseLayer.Builder().nIn(4).nOut(3).build()).layer(1, new org.deeplearning4j.nn.conf.layers.misc.FrozenLayerWithBackprop(new DenseLayer.Builder().nIn(3).nOut(4).build())).layer(2, new org.deeplearning4j.nn.conf.layers.misc.FrozenLayerWithBackprop(new DenseLayer.Builder().nIn(4).nOut(2).build())).layer(3, new org.deeplearning4j.nn.conf.layers.misc.FrozenLayerWithBackprop(new OutputLayer.Builder(LossFunctions.LossFunction.MSE).activation(Activation.TANH).nIn(2).nOut(1).build())).build();\r\n    MultiLayerNetwork frozenNetwork = new MultiLayerNetwork(confFrozen);\r\n    frozenNetwork.init();\r\n    INDArray unfrozenLayerParams = frozenNetwork.getLayer(0).params().dup();\r\n    INDArray frozenLayerParams1 = frozenNetwork.getLayer(1).params().dup();\r\n    INDArray frozenLayerParams2 = frozenNetwork.getLayer(2).params().dup();\r\n    INDArray frozenOutputLayerParams = frozenNetwork.getLayer(3).params().dup();\r\n    MultiLayerNetwork sgdNetwork = new MultiLayerNetwork(confSgd);\r\n    sgdNetwork.init();\r\n    INDArray unfrozenSgdLayerParams = sgdNetwork.getLayer(0).params().dup();\r\n    INDArray frozenSgdLayerParams1 = sgdNetwork.getLayer(1).params().dup();\r\n    INDArray frozenSgdLayerParams2 = sgdNetwork.getLayer(2).params().dup();\r\n    INDArray frozenSgdOutputLayerParams = sgdNetwork.getLayer(3).params().dup();\r\n    for (int i = 0; i < 100; i++) {\r\n        frozenNetwork.fit(randomData);\r\n    }\r\n    for (int i = 0; i < 100; i++) {\r\n        sgdNetwork.fit(randomData);\r\n    }\r\n    assertEquals(frozenNetwork.getLayer(0).params(), sgdNetwork.getLayer(0).params());\r\n    assertEquals(frozenNetwork.getLayer(1).params(), sgdNetwork.getLayer(1).params());\r\n    assertEquals(frozenNetwork.getLayer(2).params(), sgdNetwork.getLayer(2).params());\r\n    assertEquals(frozenNetwork.getLayer(3).params(), sgdNetwork.getLayer(3).params());\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.eye",
	"Comment": "generate an identity matrix with the specified number of rows and columns.",
	"Method": "SDVariable eye(int rows,SDVariable eye,String name,int rows,SDVariable eye,int rows,int cols,SDVariable eye,String name,int rows,int cols,SDVariable eye,int rows,int cols,int batchDimension,SDVariable eye,String name,int rows,int cols,int batchDimension,SDVariable eye,String name,SDVariable rows,SDVariable cols,SDVariable batchDimension,SDVariable eye,SDVariable rows,SDVariable cols,SDVariable batchDimension,SDVariable eye,String name,SDVariable rows,SDVariable cols,SDVariable eye,SDVariable rows,SDVariable cols,SDVariable eye,String name,SDVariable rows,SDVariable eye,SDVariable rows){\r\n    SDVariable eye = new Eye(this, rows).outputVariables()[0];\r\n    return updateVariableNameAndReference(eye, null);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.LabeledScoredTreeNode.setLabel",
	"Comment": "sets the label associated with the current node, if there is one.",
	"Method": "void setLabel(Label label){\r\n    this.label = label;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.TregexGUI.macOSXRegistration",
	"Comment": "this is based heavily on the apple sample code for dealing with this issue",
	"Method": "void macOSXRegistration(){\r\n    if (isMacOSX()) {\r\n        try {\r\n            Class<?> osxAdapter = ClassLoader.getSystemClassLoader().loadClass(\"edu.stanford.nlp.trees.tregex.gui.OSXAdapter\");\r\n            Class<?>[] defArgs = { TregexGUI.class };\r\n            Method registerMethod = osxAdapter.getDeclaredMethod(\"registerMacOSXApplication\", defArgs);\r\n            if (registerMethod != null) {\r\n                Object[] args = { this };\r\n                registerMethod.invoke(osxAdapter, args);\r\n            }\r\n            defArgs[0] = boolean.class;\r\n            Method prefsEnableMethod = osxAdapter.getDeclaredMethod(\"enablePrefs\", defArgs);\r\n            if (prefsEnableMethod != null) {\r\n                Object[] args = { Boolean.TRUE };\r\n                prefsEnableMethod.invoke(osxAdapter, args);\r\n            }\r\n        } catch (NoClassDefFoundError e) {\r\n            log.info(\"This version of Mac OS X does not support the Apple EAWT.  Application Menu handling has been disabled (\" + e + \")\");\r\n        } catch (ClassNotFoundException e) {\r\n            log.info(\"This version of Mac OS X does not support the Apple EAWT.  Application Menu handling has been disabled (\" + e + \")\");\r\n        } catch (Exception e) {\r\n            log.info(\"Exception while loading the OSXAdapter:\");\r\n            e.printStackTrace();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.tuebadz.TueBaDZPennTreeNormalizer.normalizeNonterminal",
	"Comment": "normalizes a nonterminal contents.this implementation strips functional tags, etc. and interns thenonterminal.",
	"Method": "String normalizeNonterminal(String category){\r\n    return cleanUpLabel(category).intern();\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.FactoredSequenceListener.setInitialSequence",
	"Comment": "informs this sequence model that the value of the whole sequence is initialized to sequence",
	"Method": "void setInitialSequence(int[] sequence){\r\n    if (models != null) {\r\n        for (SequenceListener model : models) model.setInitialSequence(sequence);\r\n        return;\r\n    }\r\n    model1.setInitialSequence(sequence);\r\n    model2.setInitialSequence(sequence);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.addAll",
	"Comment": "adds all of the objects returned by the given iterator into the given collection.",
	"Method": "Collection<T> addAll(Iterator<? extends T> iter,Collection<T> c){\r\n    while (iter.hasNext()) {\r\n        c.add(iter.next());\r\n    }\r\n    return c;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ConfusionMatrix.sortKeys",
	"Comment": "returns the current set of unique labels, sorted by their string order.",
	"Method": "List<U> sortKeys(){\r\n    Set<U> labels = uniqueLabels();\r\n    if (labels.size() == 0) {\r\n        return Collections.emptyList();\r\n    }\r\n    boolean comparable = true;\r\n    for (U label : labels) {\r\n        if (!(label instanceof Comparable)) {\r\n            comparable = false;\r\n            break;\r\n        }\r\n    }\r\n    if (comparable) {\r\n        List<Comparable<Object>> sorted = Generics.newArrayList();\r\n        for (U label : labels) {\r\n            sorted.add(ErasureUtils.<Comparable<Object>>uncheckedCast(label));\r\n        }\r\n        Collections.sort(sorted);\r\n        List<U> ret = Generics.newArrayList();\r\n        for (Object o : sorted) {\r\n            ret.add(ErasureUtils.<U>uncheckedCast(o));\r\n        }\r\n        return ret;\r\n    } else {\r\n        ArrayList<String> names = new ArrayList();\r\n        HashMap<String, U> lookup = new HashMap();\r\n        for (U label : labels) {\r\n            names.add(label.toString());\r\n            lookup.put(label.toString(), label);\r\n        }\r\n        Collections.sort(names);\r\n        ArrayList<U> ret = new ArrayList();\r\n        for (String name : names) {\r\n            ret.add(lookup.get(name));\r\n        }\r\n        return ret;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.PropertiesUtils.getInt",
	"Comment": "load an integer property.if the key is not present, returns defaultvalue.",
	"Method": "int getInt(Properties props,String key,int getInt,Properties props,String key,int defaultValue){\r\n    String value = props.getProperty(key);\r\n    if (value != null) {\r\n        return Integer.parseInt(value);\r\n    } else {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator.loadFromMetaData",
	"Comment": "load a multiple examples to a dataset, using the provided recordmetadata instances.",
	"Method": "DataSet loadFromMetaData(RecordMetaData recordMetaData,DataSet loadFromMetaData,List<RecordMetaData> list){\r\n    if (underlying == null) {\r\n        Record r = recordReader.loadFromMetaData(list.get(0));\r\n        initializeUnderlying(r);\r\n    }\r\n    List<RecordMetaData> l = new ArrayList(list.size());\r\n    for (RecordMetaData m : list) {\r\n        l.add(new RecordMetaDataComposableMap(Collections.singletonMap(READER_KEY, m)));\r\n    }\r\n    MultiDataSet m = underlying.loadFromMetaData(l);\r\n    return mdsToDataSet(m);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.Ssurgeon.getFirstChildElement",
	"Comment": "returns the first child whose node type is element under the given element.",
	"Method": "Element getFirstChildElement(Element element){\r\n    try {\r\n        NodeList nodeList = element.getChildNodes();\r\n        for (int i = 0; i < nodeList.getLength(); i++) {\r\n            Node node = nodeList.item(i);\r\n            if (node.getNodeType() == Node.ELEMENT_NODE)\r\n                return (Element) node;\r\n        }\r\n    } catch (Exception e) {\r\n        log.warning(\"Error getting first child Element for element=\" + element + \", exception=\" + e);\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.randomBinomial",
	"Comment": "generate a new random sdvariable, where values are randomly sampled according to a binomial distribution,with the specified number of trials and probability.",
	"Method": "SDVariable randomBinomial(int nTrials,double p,long shape,SDVariable randomBinomial,String name,int nTrials,double p,long shape){\r\n    SDVariable ret = f().randomBinomial(nTrials, p, shape);\r\n    return updateVariableNameAndReference(ret, name);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.flow.FlowPath.ensureNodeStateExists",
	"Comment": "this method checks if nodestate was created for specified graph node",
	"Method": "void ensureNodeStateExists(String nodeName){\r\n    if (!states.containsKey(nodeName))\r\n        states.put(nodeName, new NodeState(nodeName));\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.randomprojection.RPUtils.getAllCandidates",
	"Comment": "get the search candidates as indices given the inputand similarity function",
	"Method": "INDArray getAllCandidates(INDArray x,List<RPTree> trees,String similarityFunction){\r\n    List<Integer> candidates = getCandidates(x, trees, similarityFunction);\r\n    Collections.sort(candidates);\r\n    int prevIdx = -1;\r\n    int idxCount = 0;\r\n    List<Pair<Integer, Integer>> scores = new ArrayList();\r\n    for (int i = 0; i < candidates.size(); i++) {\r\n        if (candidates.get(i) == prevIdx) {\r\n            idxCount++;\r\n        } else if (prevIdx != -1) {\r\n            scores.add(Pair.of(idxCount, prevIdx));\r\n            idxCount = 1;\r\n        }\r\n        prevIdx = i;\r\n    }\r\n    scores.add(Pair.of(idxCount, prevIdx));\r\n    INDArray arr = Nd4j.create(scores.size());\r\n    for (int i = 0; i < scores.size(); i++) {\r\n        arr.putScalar(i, scores.get(i).getSecond());\r\n    }\r\n    return arr;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.flow.FlowPath.getRewindPosition",
	"Comment": "this method returns planned position within graph for next rewind.",
	"Method": "int getRewindPosition(String frameName){\r\n    return frames.get(frameName).getRewindPosition();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.normalization.KerasBatchNormalization.getEpsFromConfig",
	"Comment": "get batchnormalization epsilon parameter from keras layer configuration.",
	"Method": "double getEpsFromConfig(Map<String, Object> layerConfig){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (!innerConfig.containsKey(LAYER_FIELD_EPSILON))\r\n        throw new InvalidKerasConfigurationException(\"Keras BatchNorm layer config missing \" + LAYER_FIELD_EPSILON + \" field\");\r\n    return (double) innerConfig.get(LAYER_FIELD_EPSILON);\r\n}"
}, {
	"Path": "org.datavec.spark.transform.DataFrames.toDataFrame",
	"Comment": "creates a data frame from a collection of writablesrdd given a schema",
	"Method": "DataRowsFacade toDataFrame(Schema schema,JavaRDD<List<Writable>> data){\r\n    JavaSparkContext sc = new JavaSparkContext(data.context());\r\n    SQLContext sqlContext = new SQLContext(sc);\r\n    JavaRDD<Row> rows = data.map(new ToRow(schema));\r\n    return dataRows(sqlContext.createDataFrame(rows, fromSchema(schema)));\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.CoreQuote.speakerEntityMention",
	"Comment": "retrieve the entity mention corresponding to the speaker if there is one",
	"Method": "Optional<CoreEntityMention> speakerEntityMention(){\r\n    return this.speakerEntityMention;\r\n}"
}, {
	"Path": "org.deeplearning4j.zoo.util.imagenet.ImageNetLabels.getLabel",
	"Comment": "returns the description of tne nth class in the 1000 classes of imagenet.",
	"Method": "String getLabel(int n){\r\n    return predictionLabels.get(n);\r\n}"
}, {
	"Path": "edu.stanford.nlp.patterns.TextAnnotationPatternsInterface.main",
	"Comment": "application method to run the server runs in an infinite looplistening on port 9898.when a connection is requested, itspawns a new thread to do the servicing and immediately returnsto listening.the server keeps a unique client number for eachclient that connects just to show interesting loggingmessages.it is certainly not necessary to do this.",
	"Method": "void main(String[] args){\r\n    System.out.println(\"The modeling server is running.\");\r\n    int clientNumber = 0;\r\n    ServerSocket listener = new ServerSocket(9898);\r\n    try {\r\n        while (true) {\r\n            new PerformActionUpdateModel(listener.accept(), clientNumber++).start();\r\n        }\r\n    } finally {\r\n        listener.close();\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.parameterserver.networking.v2.WiredEncodingHandler.sendMessage",
	"Comment": "this method sends given message to all registered recipients",
	"Method": "void sendMessage(INDArray message,int iterationNumber,int epochNumber){\r\n    try (MemoryWorkspace wsO = Nd4j.getMemoryManager().scopeOutOfWorkspaces()) {\r\n        long updateId = updatesCounter.getAndIncrement();\r\n        val m = message.unsafeDuplication();\r\n        ModelParameterServer.getInstance().sendUpdate(m, iterationNumber, epochNumber);\r\n    }\r\n    super.sendMessage(message, iterationNumber, epochNumber);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.normalization.KerasBatchNormalization.getGammaRegularizerFromConfig",
	"Comment": "get batchnormalization gamma regularizer from keras layer configuration. currently unsupported.",
	"Method": "void getGammaRegularizerFromConfig(Map<String, Object> layerConfig,boolean enforceTrainingConfig){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (innerConfig.get(LAYER_FIELD_GAMMA_REGULARIZER) != null) {\r\n        if (enforceTrainingConfig)\r\n            throw new UnsupportedKerasConfigurationException(\"Regularization for BatchNormalization gamma parameter not supported\");\r\n        else\r\n            log.warn(\"Regularization for BatchNormalization gamma parameter not supported...ignoring.\");\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ops.custom.ScatterUpdate.isInplaceCall",
	"Comment": "this method returns true if op is supposed to be executed inplace",
	"Method": "boolean isInplaceCall(){\r\n    return op.isInplaceCall();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.AbstractLayer.addListeners",
	"Comment": "this method adds additional traininglistener to existing listeners",
	"Method": "void addListeners(TrainingListener listeners){\r\n    if (this.trainingListeners == null) {\r\n        setListeners(listeners);\r\n        return;\r\n    }\r\n    Collections.addAll(trainingListeners, listeners);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Trilean.toBooleanOrNull",
	"Comment": "convert this trilean to a boolean, or null if the value is not known.",
	"Method": "Boolean toBooleanOrNull(){\r\n    switch(value) {\r\n        case 1:\r\n            return true;\r\n        case 0:\r\n            return false;\r\n        case 2:\r\n            return null;\r\n        default:\r\n            throw new IllegalStateException(\"Something went very very wrong.\");\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Trilean.isTrue",
	"Comment": "returns true if this trilean is true, and false if it is false or unknown.",
	"Method": "boolean isTrue(){\r\n    return value == 1;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.pennchinese.ChineseSemanticHeadFinder.ruleChanges",
	"Comment": "makes modifications of head finder rules to better fit with semantic notions of heads.",
	"Method": "void ruleChanges(){\r\n    nonTerminalInfo.put(\"VP\", new String[][] { { \"left\", \"VP\", \"VCD\", \"VPT\", \"VV\", \"VCP\", \"VA\", \"VE\", \"VC\", \"IP\", \"VSB\", \"VCP\", \"VRD\", \"VNV\" }, leftExceptPunct });\r\n    nonTerminalInfo.put(\"CP\", new String[][] { { \"right\", \"CP\", \"IP\", \"VP\" }, rightExceptPunct });\r\n    nonTerminalInfo.put(\"DNP\", new String[][] { { \"leftdis\", \"NP\" } });\r\n    nonTerminalInfo.put(\"DVP\", new String[][] { { \"leftdis\", \"VP\", \"ADVP\" } });\r\n    nonTerminalInfo.put(\"LST\", new String[][] { { \"right\", \"CD\", \"NP\", \"QP\", \"PU\" } });\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.AnnotatorPool.clear",
	"Comment": "clear this pool, and unmount all the annotators mounted on it.",
	"Method": "void clear(){\r\n    synchronized (this.cachedAnnotators) {\r\n        for (Map.Entry<String, CachedAnnotator> entry : new HashSet(this.cachedAnnotators.entrySet())) {\r\n            Optional.ofNullable(entry.getValue()).flatMap(ann -> Optional.ofNullable(ann.annotator.getIfDefined())).ifPresent(Annotator::unmount);\r\n            this.cachedAnnotators.remove(entry.getKey());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiffOpExecutioner.getTADManager",
	"Comment": "this method returns tadmanager instance used for this opexecutioner",
	"Method": "TADManager getTADManager(){\r\n    return backendExecutioner.getTADManager();\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.TestSentence.printUnknown",
	"Comment": "this method should be called after the sentence has been tagged.for every unknown word, this method prints the 3 most probable tagsto the file pfu.",
	"Method": "void printUnknown(int numSent,PrintFile pfu){\r\n    NumberFormat nf = new DecimalFormat(\"0.0000\");\r\n    int numTags = maxentTagger.numTags();\r\n    double[][][] probabilities = new double[size][kBestSize][numTags];\r\n    calculateProbs(probabilities);\r\n    for (int current = 0; current < size; current++) {\r\n        if (maxentTagger.dict.isUnknown(sent.get(current))) {\r\n            pfu.print(sent.get(current));\r\n            pfu.print(':');\r\n            pfu.print(numSent);\r\n            double[] probs = new double[3];\r\n            String[] tag3 = new String[3];\r\n            getTop3(probabilities, current, probs, tag3);\r\n            for (int i = 0; i < 3; i++) {\r\n                if (probs[i] > Double.NEGATIVE_INFINITY) {\r\n                    pfu.print('\\t');\r\n                    pfu.print(tag3[i]);\r\n                    pfu.print(' ');\r\n                    pfu.print(nf.format(Math.exp(probs[i])));\r\n                }\r\n            }\r\n            int rank;\r\n            String correctTag = toNice(this.correctTags[current]);\r\n            for (rank = 0; rank < 3; rank++) {\r\n                if (correctTag.equals(tag3[rank])) {\r\n                    break;\r\n                }\r\n            }\r\n            pfu.print('\\t');\r\n            switch(rank) {\r\n                case 0:\r\n                    pfu.print(\"Correct\");\r\n                    break;\r\n                case 1:\r\n                    pfu.print(\"2nd\");\r\n                    break;\r\n                case 2:\r\n                    pfu.print(\"3rd\");\r\n                    break;\r\n                default:\r\n                    pfu.print(\"Not top 3\");\r\n            }\r\n            pfu.println();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.parameterserver.networking.v1.SilentTrainingDriver.finishTraining",
	"Comment": "this method is used on master only, applies buffered updates to params",
	"Method": "void finishTraining(long originatorId,long taskId){\r\n    if (params != null && stepFunction != null) {\r\n        if (hasSomething.get()) {\r\n            stepFunction.step(params, updates);\r\n            updates.assign(0.0);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.transform.ui.HtmlSequencePlotting.createHtmlSequencePlotFile",
	"Comment": "create a html file with plots for the given sequence and write it to a file.",
	"Method": "void createHtmlSequencePlotFile(String title,Schema schema,List<List<Writable>> sequence,File output){\r\n    String s = createHtmlSequencePlots(title, schema, sequence);\r\n    FileUtils.writeStringToFile(output, s);\r\n}"
}, {
	"Path": "org.datavec.spark.transform.model.SingleCSVRecord.fromRow",
	"Comment": "instantiate a csv record from a vectorgiven either an input dataset and aone hot matrix, the index will be appended tothe end of the record, or for regressionit will append all values in the labels",
	"Method": "SingleCSVRecord fromRow(DataSet row){\r\n    if (!row.getFeatures().isVector() && !row.getFeatures().isScalar())\r\n        throw new IllegalArgumentException(\"Passed in dataset must represent a scalar or vector\");\r\n    if (!row.getLabels().isVector() && !row.getLabels().isScalar())\r\n        throw new IllegalArgumentException(\"Passed in dataset labels must be a scalar or vector\");\r\n    SingleCSVRecord record;\r\n    int idx = 0;\r\n    if (row.getLabels().sumNumber().doubleValue() == 1.0) {\r\n        String[] values = new String[row.getFeatures().columns() + 1];\r\n        for (int i = 0; i < row.getFeatures().length(); i++) {\r\n            values[idx++] = String.valueOf(row.getFeatures().getDouble(i));\r\n        }\r\n        int maxIdx = 0;\r\n        for (int i = 0; i < row.getLabels().length(); i++) {\r\n            if (row.getLabels().getDouble(maxIdx) < row.getLabels().getDouble(i)) {\r\n                maxIdx = i;\r\n            }\r\n        }\r\n        values[idx++] = String.valueOf(maxIdx);\r\n        record = new SingleCSVRecord(values);\r\n    } else {\r\n        String[] values = new String[row.getFeatures().columns() + row.getLabels().columns()];\r\n        for (int i = 0; i < row.getFeatures().length(); i++) {\r\n            values[idx++] = String.valueOf(row.getFeatures().getDouble(i));\r\n        }\r\n        for (int i = 0; i < row.getLabels().length(); i++) {\r\n            values[idx++] = String.valueOf(row.getLabels().getDouble(i));\r\n        }\r\n        record = new SingleCSVRecord(values);\r\n    }\r\n    return record;\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.TokensRegexNERAnnotator.checkPosTags",
	"Comment": "that allows for better matching because unmatched sequences will be eliminated at match time",
	"Method": "boolean checkPosTags(List<CoreLabel> tokens,int start,int end){\r\n    if (validPosPattern != null || atLeastOneValidPosPattern(validPosPatternList)) {\r\n        switch(posMatchType) {\r\n            case MATCH_ONE_TOKEN_PHRASE_ONLY:\r\n                if (tokens.size() > 1) {\r\n                    return true;\r\n                }\r\n            case MATCH_AT_LEAST_ONE_TOKEN:\r\n                for (int i = start; i < end; i++) {\r\n                    CoreLabel token = tokens.get(i);\r\n                    String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);\r\n                    if (pos != null && validPosPattern != null && validPosPattern.matcher(pos).matches()) {\r\n                        return true;\r\n                    } else if (pos != null) {\r\n                        for (Pattern pattern : validPosPatternList) {\r\n                            if (pattern != null && pattern.matcher(pos).matches()) {\r\n                                return true;\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n                return false;\r\n            case MATCH_ALL_TOKENS:\r\n                return true;\r\n            default:\r\n                return true;\r\n        }\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Interval.toValidInterval",
	"Comment": "create an interval with the specified endpoints, reordering them as needed,using the specified flags",
	"Method": "Interval<E> toValidInterval(E a,E b,Interval<E> toValidInterval,E a,E b,int flags){\r\n    int comp = a.compareTo(b);\r\n    if (comp <= 0) {\r\n        return new Interval(a, b, flags);\r\n    } else {\r\n        return new Interval(b, a, flags);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ops.aggregates.Batch.getBatches",
	"Comment": "helper method to create batch from list of aggregates, for cases when list of aggregates is higher then batchlimit",
	"Method": "List<Batch<U>> getBatches(List<U> list,List<Batch<U>> getBatches,List<U> list,int partitionSize){\r\n    List<List<U>> partitions = Lists.partition(list, partitionSize);\r\n    List<Batch<U>> split = new ArrayList();\r\n    for (List<U> partition : partitions) {\r\n        split.add(new Batch<U>(partition));\r\n    }\r\n    return split;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.util.IETestUtils.mkWord",
	"Comment": "create a dummy word, just with a given word at a given index.mostly useful for making semantic graphs.",
	"Method": "CoreLabel mkWord(String gloss,int index){\r\n    CoreLabel w = new CoreLabel();\r\n    w.setWord(gloss);\r\n    w.setValue(gloss);\r\n    if (index >= 0) {\r\n        w.setIndex(index);\r\n    }\r\n    return w;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraLexer.yytext",
	"Comment": "returns the text matched by the current regular expression.",
	"Method": "String yytext(){\r\n    return new String(zzBuffer, zzStartRead, zzMarkedPos - zzStartRead);\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.parse.ParseDoubleTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return inputSchema.getColumnNames().toArray(new String[inputSchema.numColumns()]);\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.datavec.RecordReaderMultiDataSetIterator.loadFromMetaData",
	"Comment": "load a multiple sequence examples to a dataset, using the provided recordmetadata instances.",
	"Method": "MultiDataSet loadFromMetaData(RecordMetaData recordMetaData,MultiDataSet loadFromMetaData,List<RecordMetaData> list){\r\n    Map<String, List<List<Writable>>> nextRRVals = new HashMap();\r\n    Map<String, List<List<List<Writable>>>> nextSeqRRVals = new HashMap();\r\n    List<RecordMetaDataComposableMap> nextMetas = (collectMetaData ? new ArrayList<RecordMetaDataComposableMap>() : null);\r\n    for (Map.Entry<String, RecordReader> entry : recordReaders.entrySet()) {\r\n        RecordReader rr = entry.getValue();\r\n        List<RecordMetaData> thisRRMeta = new ArrayList();\r\n        for (RecordMetaData m : list) {\r\n            RecordMetaDataComposableMap m2 = (RecordMetaDataComposableMap) m;\r\n            thisRRMeta.add(m2.getMeta().get(entry.getKey()));\r\n        }\r\n        List<Record> fromMeta = rr.loadFromMetaData(thisRRMeta);\r\n        List<List<Writable>> writables = new ArrayList(list.size());\r\n        for (Record r : fromMeta) {\r\n            writables.add(r.getRecord());\r\n        }\r\n        nextRRVals.put(entry.getKey(), writables);\r\n    }\r\n    for (Map.Entry<String, SequenceRecordReader> entry : sequenceRecordReaders.entrySet()) {\r\n        SequenceRecordReader rr = entry.getValue();\r\n        List<RecordMetaData> thisRRMeta = new ArrayList();\r\n        for (RecordMetaData m : list) {\r\n            RecordMetaDataComposableMap m2 = (RecordMetaDataComposableMap) m;\r\n            thisRRMeta.add(m2.getMeta().get(entry.getKey()));\r\n        }\r\n        List<SequenceRecord> fromMeta = rr.loadSequenceFromMetaData(thisRRMeta);\r\n        List<List<List<Writable>>> writables = new ArrayList(list.size());\r\n        for (SequenceRecord r : fromMeta) {\r\n            writables.add(r.getSequenceRecord());\r\n        }\r\n        nextSeqRRVals.put(entry.getKey(), writables);\r\n    }\r\n    return nextMultiDataSet(nextRRVals, null, nextSeqRRVals, nextMetas);\r\n}"
}, {
	"Path": "org.datavec.api.transform.condition.column.BaseColumnCondition.transform",
	"Comment": "get the output schema for this transformation, given an input schema",
	"Method": "Schema transform(Schema inputSchema){\r\n    return inputSchema;\r\n}"
}, {
	"Path": "edu.stanford.nlp.time.Timex.getDate",
	"Comment": "gets the calendar matching the year, month and day of this timex.",
	"Method": "Calendar getDate(){\r\n    if (Pattern.matches(\"\\\\d\\\\d\\\\d\\\\d-\\\\d\\\\d-\\\\d\\\\d\", this.val)) {\r\n        int year = Integer.parseInt(this.val.substring(0, 4));\r\n        int month = Integer.parseInt(this.val.substring(5, 7));\r\n        int day = Integer.parseInt(this.val.substring(8, 10));\r\n        return makeCalendar(year, month, day);\r\n    } else if (Pattern.matches(\"\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d\", this.val)) {\r\n        int year = Integer.parseInt(this.val.substring(0, 4));\r\n        int month = Integer.parseInt(this.val.substring(4, 6));\r\n        int day = Integer.parseInt(this.val.substring(6, 8));\r\n        return makeCalendar(year, month, day);\r\n    }\r\n    throw new UnsupportedOperationException(String.format(\"%s is not a fully specified date\", this));\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraHeadFinder.determineNonTrivialHead",
	"Comment": "called by determinehead and may be overridden in subclasses if special treatment is necessary for particular categories.",
	"Method": "Tree determineNonTrivialHead(Tree t,Tree parent){\r\n    Tree theHead = null;\r\n    String motherCat = basicCategory(t.label().value());\r\n    if (motherCat.startsWith(\"@\")) {\r\n        motherCat = motherCat.substring(1);\r\n    }\r\n    if (DEBUG) {\r\n        log.info(\"Looking for head of \" + t.label() + \"; value is |\" + t.label().value() + \"|, \" + \" baseCat is |\" + motherCat + \"|\");\r\n    }\r\n    String[][] how = nonTerminalInfo.get(motherCat);\r\n    if (how == null) {\r\n        if (DEBUG) {\r\n            log.info(\"Warning: No rule found for \" + motherCat + \" (first char: \" + motherCat.charAt(0) + \")\");\r\n            log.info(\"Known nonterms are: \" + nonTerminalInfo.keySet());\r\n        }\r\n        if (defaultRule != null) {\r\n            if (DEBUG) {\r\n                log.info(\"  Using defaultRule\");\r\n            }\r\n            return traverseLocate(t.children(), defaultRule, true);\r\n        } else {\r\n            return null;\r\n        }\r\n    }\r\n    for (int i = 0; i < how.length; i++) {\r\n        boolean deflt = (i == how.length - 1);\r\n        theHead = traverseLocate(t.children(), how[i], deflt);\r\n        if (theHead != null) {\r\n            break;\r\n        }\r\n    }\r\n    if (DEBUG) {\r\n        log.info(\"  Chose \" + theHead.label());\r\n    }\r\n    return theHead;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.insertSpecificIntoList",
	"Comment": "inserts the given specific portion of an uncollapsed relation back into thetargetlist",
	"Method": "void insertSpecificIntoList(String specific,IndexedWord relnTgtNode,List<IndexedWord> tgtList){\r\n    int currIndex = tgtList.indexOf(relnTgtNode);\r\n    Set<IndexedWord> descendants = descendants(relnTgtNode);\r\n    IndexedWord specificNode = new IndexedWord();\r\n    specificNode.set(CoreAnnotations.LemmaAnnotation.class, specific);\r\n    specificNode.set(CoreAnnotations.TextAnnotation.class, specific);\r\n    specificNode.set(CoreAnnotations.OriginalTextAnnotation.class, specific);\r\n    while ((currIndex >= 1) && descendants.contains(tgtList.get(currIndex - 1))) {\r\n        currIndex--;\r\n    }\r\n    tgtList.add(currIndex, specificNode);\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.PTB2TextLexer.yytext",
	"Comment": "returns the text matched by the current regular expression.",
	"Method": "String yytext(){\r\n    return new String(zzBuffer, zzStartRead, zzMarkedPos - zzStartRead);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.glove.AbstractCoOccurrences.getCoOccurrenceCount",
	"Comment": "this method returns cooccurrence distance weights for two sequenceelements",
	"Method": "double getCoOccurrenceCount(T element1,T element2){\r\n    return coOccurrenceCounts.getCount(element1, element2);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Lazy.of",
	"Comment": "create a lazy value from the given provider.the provider is only called once on initialization.",
	"Method": "Lazy<E> of(Supplier<E> fn){\r\n    return new Lazy<E>() {\r\n        @Override\r\n        protected E compute() {\r\n            return fn.get();\r\n        }\r\n        @Override\r\n        public boolean isCache() {\r\n            return false;\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Lazy.of",
	"Comment": "create a lazy value from the given provider.the provider is only called once on initialization.",
	"Method": "Lazy<E> of(Supplier<E> fn){\r\n    return fn.get();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Lazy.of",
	"Comment": "create a lazy value from the given provider.the provider is only called once on initialization.",
	"Method": "Lazy<E> of(Supplier<E> fn){\r\n    return false;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.execAndEndResult",
	"Comment": "execute the specified ops and return the output of the last one",
	"Method": "INDArray execAndEndResult(List<DifferentialFunction> ops,INDArray execAndEndResult,INDArray execAndEndResult,int outputIndex,INDArray execAndEndResult,String functionName){\r\n    return sameDiffFunctionInstances.get(functionName).execAndEndResult();\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.util.CollectionUtils.getUnique",
	"Comment": "returns a list containing only unique values in a collection",
	"Method": "List<T> getUnique(Collection<T> collection){\r\n    HashSet<T> set = new HashSet();\r\n    List<T> out = new ArrayList();\r\n    for (T t : collection) {\r\n        if (!set.contains(t)) {\r\n            out.add(t);\r\n            set.add(t);\r\n        }\r\n    }\r\n    return out;\r\n}"
}, {
	"Path": "org.deeplearning4j.util.Convolution1DUtils.validateConvolutionModePadding",
	"Comment": "check that the convolution mode is consistent with the padding specification",
	"Method": "void validateConvolutionModePadding(ConvolutionMode mode,int padding){\r\n    if (mode == ConvolutionMode.Same) {\r\n        boolean nullPadding = true;\r\n        if (padding != 0)\r\n            nullPadding = false;\r\n        if (!nullPadding)\r\n            throw new IllegalArgumentException(\"Padding cannot be used when using the `same' convolution mode\");\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.WordsToSentencesAnnotatorTest.testDatelineSeparation",
	"Comment": "test whether you can separate off a dateline as a separate sentence using ssplit.boundarymultitokenregex.",
	"Method": "void testDatelineSeparation(){\r\n    Properties props = PropertiesUtils.asProperties(\"annotators\", \"tokenize, cleanxml, ssplit\", \"tokenize.language\", \"en\", \"ssplit.newlineIsSentenceBreak\", \"two\", \"ssplit.boundaryMultiTokenRegex\", \"( /\\\\*NL\\\\*/ /\\\\p{Lu}[-\\\\p{L}]+/+ /,/ ( /[-\\\\p{L}]+/+ /,/ )? \" + \"/\\\\p{Lu}\\\\p{Ll}{2,5}\\\\.?/ /[1-3]?[0-9]/ /-LRB-/ /\\\\p{Lu}\\\\p{L}+/ /-RRB-/ /--/ | \" + \"/\\\\*NL\\\\*/ /\\\\p{Lu}[-\\\\p{Lu}]+/+ ( /,/ /[-\\\\p{L}]+/+ )? /-/ )\");\r\n    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\r\n    assertEquals(\"Bad test data\", dateLineTexts.length, dateLineTokens.length);\r\n    for (int i = 0; i < dateLineTexts.length; i++) {\r\n        Annotation document1 = new Annotation(dateLineTexts[i]);\r\n        pipeline.annotate(document1);\r\n        List<CoreMap> sentences = document1.get(CoreAnnotations.SentencesAnnotation.class);\r\n        assertEquals(\"For \" + dateLineTexts[i] + \" annotation is \" + document1, 2, sentences.size());\r\n        List<CoreLabel> sentenceOneTokens = sentences.get(0).get(CoreAnnotations.TokensAnnotation.class);\r\n        String sentenceOne = SentenceUtils.listToString(sentenceOneTokens);\r\n        assertEquals(\"Bad tokens in dateline\", dateLineTokens[i], sentenceOne);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.entrySet",
	"Comment": "returns a view of the doubles in this map.can be safely modified.",
	"Method": "Set<Map.Entry<E, Double>> entrySet(){\r\n    return new AbstractSet<Map.Entry<E, Double>>() {\r\n        @Override\r\n        public Iterator<Entry<E, Double>> iterator() {\r\n            return new Iterator<Entry<E, Double>>() {\r\n                final Iterator<Entry<E, MutableInteger>> inner = map.entrySet().iterator();\r\n                public boolean hasNext() {\r\n                    return inner.hasNext();\r\n                }\r\n                public Entry<E, Double> next() {\r\n                    return new Map.Entry<E, Double>() {\r\n                        final Entry<E, MutableInteger> e = inner.next();\r\n                        public E getKey() {\r\n                            return e.getKey();\r\n                        }\r\n                        public Double getValue() {\r\n                            return e.getValue().doubleValue();\r\n                        }\r\n                        public Double setValue(Double value) {\r\n                            final double old = e.getValue().doubleValue();\r\n                            e.getValue().set(value.intValue());\r\n                            totalCount = totalCount - (int) old + value.intValue();\r\n                            return old;\r\n                        }\r\n                    };\r\n                }\r\n                public void remove() {\r\n                    throw new UnsupportedOperationException();\r\n                }\r\n            };\r\n        }\r\n        @Override\r\n        public int size() {\r\n            return map.size();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.entrySet",
	"Comment": "returns a view of the doubles in this map.can be safely modified.",
	"Method": "Set<Map.Entry<E, Double>> entrySet(){\r\n    return new Iterator<Entry<E, Double>>() {\r\n        final Iterator<Entry<E, MutableInteger>> inner = map.entrySet().iterator();\r\n        public boolean hasNext() {\r\n            return inner.hasNext();\r\n        }\r\n        public Entry<E, Double> next() {\r\n            return new Map.Entry<E, Double>() {\r\n                final Entry<E, MutableInteger> e = inner.next();\r\n                public E getKey() {\r\n                    return e.getKey();\r\n                }\r\n                public Double getValue() {\r\n                    return e.getValue().doubleValue();\r\n                }\r\n                public Double setValue(Double value) {\r\n                    final double old = e.getValue().doubleValue();\r\n                    e.getValue().set(value.intValue());\r\n                    totalCount = totalCount - (int) old + value.intValue();\r\n                    return old;\r\n                }\r\n            };\r\n        }\r\n        public void remove() {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.entrySet",
	"Comment": "returns a view of the doubles in this map.can be safely modified.",
	"Method": "Set<Map.Entry<E, Double>> entrySet(){\r\n    return inner.hasNext();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.entrySet",
	"Comment": "returns a view of the doubles in this map.can be safely modified.",
	"Method": "Set<Map.Entry<E, Double>> entrySet(){\r\n    return new Map.Entry<E, Double>() {\r\n        final Entry<E, MutableInteger> e = inner.next();\r\n        public E getKey() {\r\n            return e.getKey();\r\n        }\r\n        public Double getValue() {\r\n            return e.getValue().doubleValue();\r\n        }\r\n        public Double setValue(Double value) {\r\n            final double old = e.getValue().doubleValue();\r\n            e.getValue().set(value.intValue());\r\n            totalCount = totalCount - (int) old + value.intValue();\r\n            return old;\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.entrySet",
	"Comment": "returns a view of the doubles in this map.can be safely modified.",
	"Method": "Set<Map.Entry<E, Double>> entrySet(){\r\n    return e.getKey();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.entrySet",
	"Comment": "returns a view of the doubles in this map.can be safely modified.",
	"Method": "Set<Map.Entry<E, Double>> entrySet(){\r\n    return e.getValue().doubleValue();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.entrySet",
	"Comment": "returns a view of the doubles in this map.can be safely modified.",
	"Method": "Set<Map.Entry<E, Double>> entrySet(){\r\n    final double old = e.getValue().doubleValue();\r\n    e.getValue().set(value.intValue());\r\n    totalCount = totalCount - (int) old + value.intValue();\r\n    return old;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.entrySet",
	"Comment": "returns a view of the doubles in this map.can be safely modified.",
	"Method": "Set<Map.Entry<E, Double>> entrySet(){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.entrySet",
	"Comment": "returns a view of the doubles in this map.can be safely modified.",
	"Method": "Set<Map.Entry<E, Double>> entrySet(){\r\n    return map.size();\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.yieldSpan",
	"Comment": "returns the span of the subtree yield of this node. that is, the span of all the nodes under it.in the case of projective graphs, the words in this span are also the yield of the constituent rootedat this node.",
	"Method": "Pair<Integer, Integer> yieldSpan(IndexedWord word){\r\n    int min = Integer.MAX_VALUE;\r\n    int max = Integer.MIN_VALUE;\r\n    Stack<IndexedWord> fringe = new Stack();\r\n    fringe.push(word);\r\n    while (!fringe.isEmpty()) {\r\n        IndexedWord parent = fringe.pop();\r\n        min = Math.min(min, parent.index() - 1);\r\n        max = Math.max(max, parent.index());\r\n        for (SemanticGraphEdge edge : outgoingEdgeIterable(parent)) {\r\n            if (!edge.isExtra()) {\r\n                fringe.push(edge.getDependent());\r\n            }\r\n        }\r\n    }\r\n    return Pair.makePair(min, max);\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.fitContinuousLabeledPoint",
	"Comment": "fits a multilayernetwork using spark mllib labeledpoint instancesthis will convert labeled points that have continuous labels used for regression to the internaldl4j data format and train the model on that",
	"Method": "MultiLayerNetwork fitContinuousLabeledPoint(JavaRDD<LabeledPoint> rdd){\r\n    return fit(MLLibUtil.fromContinuousLabeledPoint(sc, rdd));\r\n}"
}, {
	"Path": "edu.stanford.nlp.sentiment.SentimentPipeline.outputTreeScores",
	"Comment": "outputs the scores from the tree.counts the tree nodes thesame as setindexlabels.",
	"Method": "int outputTreeScores(PrintStream out,Tree tree,int index){\r\n    if (tree.isLeaf()) {\r\n        return index;\r\n    }\r\n    out.print(\"  \" + index + ':');\r\n    SimpleMatrix vector = RNNCoreAnnotations.getPredictions(tree);\r\n    for (int i = 0; i < vector.getNumElements(); ++i) {\r\n        out.print(\"  \" + NF.format(vector.get(i)));\r\n    }\r\n    out.println();\r\n    index++;\r\n    for (Tree child : tree.children()) {\r\n        index = outputTreeScores(out, child, index);\r\n    }\r\n    return index;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.remove",
	"Comment": "removes the given key from this counter. its count will now be 0 and itwill no longer be considered previously seen.",
	"Method": "double remove(E key){\r\n    totalCount -= getCount(key);\r\n    MutableInteger val = map.remove(key);\r\n    if (val == null) {\r\n        return Double.NaN;\r\n    } else {\r\n        return val.doubleValue();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.HashIndex.addAll",
	"Comment": "adds every member of collection to the index. does nothing for members already in the index.",
	"Method": "boolean addAll(Collection<? extends E> c){\r\n    boolean changed = false;\r\n    for (E element : c) {\r\n        changed |= add(element);\r\n    }\r\n    return changed;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.maxIndex",
	"Comment": "returns index of maximum element in a givenarray of doubles. first maximum is returned.",
	"Method": "int maxIndex(double[] doubles){\r\n    double maximum = 0;\r\n    int maxIndex = 0;\r\n    for (int i = 0; i < doubles.length; i++) {\r\n        if ((i == 0) || (doubles[i] > maximum)) {\r\n            maxIndex = i;\r\n            maximum = doubles[i];\r\n        }\r\n    }\r\n    return maxIndex;\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.ExtractorFramesRare.naacl2003Conjunctions",
	"Comment": "this provides the conjunction of various features as rare words features.",
	"Method": "Extractor[] naacl2003Conjunctions(){\r\n    Extractor[] newW = new Extractor[24];\r\n    newW[0] = new ExtractorsConjunction(cWordUppCase, cWordSuff1);\r\n    newW[1] = new ExtractorsConjunction(cWordUppCase, cWordSuff2);\r\n    newW[2] = new ExtractorsConjunction(cWordUppCase, cWordSuff3);\r\n    newW[3] = new ExtractorsConjunction(cWordUppCase, cWordSuff4);\r\n    newW[4] = new ExtractorsConjunction(cNoLower, cWordSuff1);\r\n    newW[5] = new ExtractorsConjunction(cNoLower, cWordSuff2);\r\n    newW[6] = new ExtractorsConjunction(cNoLower, cWordSuff3);\r\n    newW[7] = new ExtractorsConjunction(cNoLower, cWordSuff4);\r\n    Extractor cMidSentence = new ExtractorMidSentenceCap();\r\n    newW[8] = new ExtractorsConjunction(cMidSentence, cWordSuff1);\r\n    newW[9] = new ExtractorsConjunction(cMidSentence, cWordSuff2);\r\n    newW[10] = new ExtractorsConjunction(cMidSentence, cWordSuff3);\r\n    newW[11] = new ExtractorsConjunction(cMidSentence, cWordSuff4);\r\n    Extractor cWordStartUCase = new ExtractorStartSentenceCap();\r\n    newW[12] = new ExtractorsConjunction(cWordStartUCase, cWordSuff1);\r\n    newW[13] = new ExtractorsConjunction(cWordStartUCase, cWordSuff2);\r\n    newW[14] = new ExtractorsConjunction(cWordStartUCase, cWordSuff3);\r\n    newW[15] = new ExtractorsConjunction(cWordStartUCase, cWordSuff4);\r\n    Extractor cWordMidUCase = new ExtractorMidSentenceCapC();\r\n    newW[16] = new ExtractorsConjunction(cWordMidUCase, cWordSuff1);\r\n    newW[17] = new ExtractorsConjunction(cWordMidUCase, cWordSuff2);\r\n    newW[18] = new ExtractorsConjunction(cWordMidUCase, cWordSuff3);\r\n    newW[19] = new ExtractorsConjunction(cWordMidUCase, cWordSuff4);\r\n    newW[20] = new ExtractorsConjunction(cCapDist, cWordSuff1);\r\n    newW[21] = new ExtractorsConjunction(cCapDist, cWordSuff2);\r\n    newW[22] = new ExtractorsConjunction(cCapDist, cWordSuff3);\r\n    newW[23] = new ExtractorsConjunction(cCapDist, cWordSuff4);\r\n    return newW;\r\n}"
}, {
	"Path": "com.atilika.kuromoji.TokenizerBase.debugLattice",
	"Comment": "writes the viterbi lattice for the provided text to an output streamthe output is written in dot format.this method is not thread safe",
	"Method": "void debugLattice(OutputStream outputStream,String text){\r\n    ViterbiLattice lattice = viterbiBuilder.build(text);\r\n    outputStream.write(viterbiFormatter.format(lattice).getBytes(StandardCharsets.UTF_8));\r\n    outputStream.flush();\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLPClient.shutdown",
	"Comment": "good practice to call after you are done with this object.shuts down the queue of annotations to run and the associated threads.if this is not called, any job which has been scheduled but not run will becancelled.",
	"Method": "void shutdown(){\r\n    scheduler.stateLock.lock();\r\n    try {\r\n        while (!scheduler.queue.isEmpty() || scheduler.freeAnnotators.size() != scheduler.backends.size()) {\r\n            scheduler.shouldShutdown.await(5, TimeUnit.SECONDS);\r\n        }\r\n        scheduler.doRun = false;\r\n        scheduler.enqueued.signalAll();\r\n    } finally {\r\n        scheduler.stateLock.unlock();\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.sequencevectors.sequence.SequenceElement.hashCode",
	"Comment": "hashcode method override should be properly implemented for any extended class, otherwise it will be based on label hashcode",
	"Method": "int hashCode(){\r\n    if (this.getLabel() == null)\r\n        throw new IllegalStateException(\"Label should not be null\");\r\n    return this.getLabel().hashCode();\r\n}"
}, {
	"Path": "org.datavec.api.transform.condition.column.BaseColumnCondition.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return columnNames();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.GrammaticalFunctionTreeNormalizer.normalizeNonterminal",
	"Comment": "normalizes a nonterminal contents.this implementation strips functional tags, etc. and interns thenonterminal.",
	"Method": "String normalizeNonterminal(String category){\r\n    return cleanUpLabel(category).intern();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.tfLogScale",
	"Comment": "returns a new counter which is the input counter with log tf scaling",
	"Method": "C tfLogScale(C c,double base){\r\n    C scaled = (C) c.getFactory().create();\r\n    for (E key : c.keySet()) {\r\n        double cnt = c.getCount(key);\r\n        double scaledCnt = 0.0;\r\n        if (cnt > 0) {\r\n            scaledCnt = 1.0 + SloppyMath.log(cnt, base);\r\n        }\r\n        scaled.setCount(key, scaledCnt);\r\n    }\r\n    return scaled;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.TwoDimensionalMapTest.testBasicIterator",
	"Comment": "test that basic operations on a twodimensionalmap iterator work.",
	"Method": "void testBasicIterator(){\r\n    TwoDimensionalMap<String, String, String> map = new TwoDimensionalMap();\r\n    Iterator<TwoDimensionalMap.Entry<String, String, String>> mapIterator = map.iterator();\r\n    assertFalse(mapIterator.hasNext());\r\n    map.put(\"A\", \"B\", \"C\");\r\n    mapIterator = map.iterator();\r\n    assertTrue(mapIterator.hasNext());\r\n    TwoDimensionalMap.Entry<String, String, String> entry = mapIterator.next();\r\n    assertEquals(\"A\", entry.getFirstKey());\r\n    assertEquals(\"B\", entry.getSecondKey());\r\n    assertEquals(\"C\", entry.getValue());\r\n    assertFalse(mapIterator.hasNext());\r\n    map.put(\"A\", \"E\", \"F\");\r\n    map.put(\"D\", \"E\", \"F\");\r\n    map.put(\"G\", \"H\", \"I\");\r\n    map.put(\"J\", \"K\", \"L\");\r\n    assertEquals(5, map.size());\r\n    int count = 0;\r\n    Set<String> firstKeys = new HashSet();\r\n    Set<String> values = new HashSet();\r\n    for (TwoDimensionalMap.Entry<String, String, String> e : map) {\r\n        ++count;\r\n        firstKeys.add(e.getFirstKey());\r\n        values.add(e.getValue());\r\n    }\r\n    assertTrue(firstKeys.contains(\"A\"));\r\n    assertTrue(firstKeys.contains(\"D\"));\r\n    assertTrue(firstKeys.contains(\"G\"));\r\n    assertTrue(firstKeys.contains(\"J\"));\r\n    assertTrue(values.contains(\"C\"));\r\n    assertTrue(values.contains(\"F\"));\r\n    assertTrue(values.contains(\"I\"));\r\n    assertTrue(values.contains(\"L\"));\r\n    assertEquals(5, count);\r\n    assertEquals(4, firstKeys.size());\r\n    assertEquals(4, values.size());\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.dynamicStitch",
	"Comment": "dynamically merge the specified input arrays into a single array, using the specified indices",
	"Method": "SDVariable dynamicStitch(SDVariable[] indices,SDVariable[] x,SDVariable dynamicStitch,String name,SDVariable[] indices,SDVariable[] x){\r\n    SDVariable ret = f().dynamicStitch(indices, x);\r\n    return updateVariableNameAndReference(ret, name);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.convolutional.KerasConvolution1D.getInputPreprocessor",
	"Comment": "gets appropriate dl4j inputpreprocessor for given inputtypes.",
	"Method": "InputPreProcessor getInputPreprocessor(InputType inputType){\r\n    if (inputType.length > 1)\r\n        throw new InvalidKerasConfigurationException(\"Keras LSTM layer accepts only one input (received \" + inputType.length + \")\");\r\n    return InputTypeUtil.getPreprocessorForInputTypeRnnLayers(inputType[0], layerName);\r\n}"
}, {
	"Path": "org.deeplearning4j.text.documentiterator.SimpleLabelAwareIterator.reset",
	"Comment": "this methods resets labelawareiterator by creating new iterator from iterable internally",
	"Method": "void reset(){\r\n    if (underlyingIterable != null)\r\n        this.currentIterator = this.underlyingIterable.iterator();\r\n    else\r\n        throw new UnsupportedOperationException(\"You can't use reset() method for Iterator<> based instance, please provide Iterable<> instead, or avoid reset()\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Timing.startDoing",
	"Comment": "print the start of timing message to stderr and start the timer.",
	"Method": "void startDoing(String str){\r\n    log.info(str + \" ... \");\r\n    startTime();\r\n}"
}, {
	"Path": "org.datavec.api.transform.rank.CalculateSortedRank.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    List<String> columnNames = inputSchema.getColumnNames();\r\n    columnNames.add(newColumnName);\r\n    return columnNames.toArray(new String[columnNames.size()]);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.BaseOutputLayer.f1Score",
	"Comment": "returns the f1 score for the given examples.think of this to be like a percentage right.the higher the number the more it got right.this is on a scale from 0 to 1.",
	"Method": "double f1Score(DataSet data,double f1Score,INDArray examples,INDArray labels){\r\n    Evaluation eval = new Evaluation();\r\n    eval.eval(labels, labelProbabilities(examples));\r\n    return eval.f1();\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getPattern",
	"Comment": "get the value of the name property as a pattern.if no such property is specified, or if the specified value is not a validpattern, then defaultvalue is returned.",
	"Method": "Pattern getPattern(String name,Pattern defaultValue){\r\n    String valString = get(name);\r\n    if (null == valString || \"\".equals(valString)) {\r\n        return defaultValue;\r\n    }\r\n    try {\r\n        return Pattern.compile(valString);\r\n    } catch (PatternSyntaxException pse) {\r\n        LOG.warn(\"Regular expression '\" + valString + \"' for property '\" + name + \"' not valid. Using default\", pse);\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.wordstore.VocabularyHolder.setScavengerActivationThreshold",
	"Comment": "this method is needed only for unit tests and should not be available in public scope.it sets the vocab size ratio, at wich dynamic scavenger will be activated",
	"Method": "void setScavengerActivationThreshold(int threshold){\r\n    this.scavengerThreshold = threshold;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.rawToken",
	"Comment": "a helper to get the raw protobuf builder for a given token.primarily useful for cache checks.",
	"Method": "CoreNLPProtos.Token.Builder rawToken(int i){\r\n    return tokensBuilders.get(i);\r\n}"
}, {
	"Path": "org.deeplearning4j.text.corpora.treeparser.TreeParser.getTrees",
	"Comment": "gets trees from text.first a sentence segmenter is used to segment the training examples in to sentences.sentences are then turned in to trees and returned.",
	"Method": "List<Tree> getTrees(String text,SentencePreProcessor preProcessor,List<Tree> getTrees,String text){\r\n    CAS c = pool.getCas();\r\n    c.setDocumentText(text);\r\n    tokenizer.process(c);\r\n    List<Tree> ret = new ArrayList();\r\n    CAS c2 = pool.getCas();\r\n    for (Sentence sentence : JCasUtil.select(c.getJCas(), Sentence.class)) {\r\n        List<String> tokens = new ArrayList();\r\n        for (Token t : JCasUtil.selectCovered(Token.class, sentence)) tokens.add(t.getCoveredText());\r\n        c2.setDocumentText(sentence.getCoveredText());\r\n        tokenizer.process(c2);\r\n        parser.process(c2);\r\n        TopTreebankNode node = JCasUtil.selectSingle(c2.getJCas(), TopTreebankNode.class);\r\n        log.info(\"Tree bank parse \" + node.getTreebankParse());\r\n        for (TreebankNode node2 : JCasUtil.select(c2.getJCas(), TreebankNode.class)) {\r\n            log.info(\"Node val \" + node2.getNodeValue() + \" and label \" + node2.getNodeType() + \" and tags was \" + node2.getNodeTags());\r\n        }\r\n        ret.add(TreeFactory.buildTree(node));\r\n        c2.reset();\r\n    }\r\n    pool.releaseCas(c);\r\n    pool.releaseCas(c2);\r\n    return ret;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasModelBuilder.trainingYamlInputStream",
	"Comment": "provide training configuration as file input stream from yaml",
	"Method": "KerasModelBuilder trainingYamlInputStream(InputStream trainingYamlInputStream){\r\n    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\r\n    IOUtils.copy(trainingYamlInputStream, byteArrayOutputStream);\r\n    this.trainingYaml = new String(byteArrayOutputStream.toByteArray());\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Timing.end",
	"Comment": "print the timing done message with elapsed time in x.y seconds.restart the timer too.",
	"Method": "void end(String msg){\r\n    long elapsed = System.nanoTime() - start;\r\n    log.info(msg + \" done [\" + nf.format(((double) elapsed) / SECOND_DIVISOR) + \" sec].\");\r\n    this.start();\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.Evaluation.classCount",
	"Comment": "returns the number of times the given labelhas actually occurred",
	"Method": "int classCount(Integer clazz){\r\n    return confusion().getActualTotal(clazz);\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.LabeledChunkIdentifier.isStartOfChunk",
	"Comment": "returns whether a chunk started between the previous and current token",
	"Method": "boolean isStartOfChunk(String prevTag,String prevType,String curTag,String curType,boolean isStartOfChunk,LabelTagType prev,LabelTagType cur){\r\n    if (prev == null) {\r\n        return isStartOfChunk(\"O\", \"O\", cur.tag, cur.type);\r\n    } else {\r\n        return isStartOfChunk(prev.tag, prev.type, cur.tag, cur.type);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.ocnn.OCNNOutputLayer.computeScoreForExamples",
	"Comment": "compute the score for each example individually, after labels and input have been set.",
	"Method": "INDArray computeScoreForExamples(double fullNetworkL1,double fullNetworkL2,LayerWorkspaceMgr workspaceMgr){\r\n    if (input == null || labels == null)\r\n        throw new IllegalStateException(\"Cannot calculate score without input and labels \" + layerId());\r\n    INDArray preOut = preOutput2d(false, workspaceMgr);\r\n    ILossFunction lossFunction = layerConf().getLossFn();\r\n    INDArray scoreArray = lossFunction.computeScoreArray(getLabels2d(workspaceMgr, ArrayType.FF_WORKING_MEM), preOut, layerConf().getActivationFn(), maskArray);\r\n    INDArray summedScores = scoreArray.sum(1);\r\n    double l1l2 = fullNetworkL1 + fullNetworkL2;\r\n    if (l1l2 != 0.0) {\r\n        summedScores.addi(l1l2);\r\n    }\r\n    return summedScores;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.toBiggestValuesFirstString",
	"Comment": "todo this method seems badly written. it should exploit topk printing of priorityqueue",
	"Method": "String toBiggestValuesFirstString(Counter<E> c,String toBiggestValuesFirstString,Counter<E> c,int k,String toBiggestValuesFirstString,Counter<Integer> c,int k,Index<T> index){\r\n    PriorityQueue<Integer> pq = toPriorityQueue(c);\r\n    PriorityQueue<T> largestK = new BinaryHeapPriorityQueue();\r\n    while (largestK.size() < k && !pq.isEmpty()) {\r\n        double firstScore = pq.getPriority(pq.getFirst());\r\n        int first = pq.removeFirst();\r\n        largestK.changePriority(index.get(first), firstScore);\r\n    }\r\n    return largestK.toString();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.pennchinese.ChineseEnglishWordMap.getInstance",
	"Comment": "a method for getting a singleton instance of this class.in general, you should use this method rather than the constructor,since each instance of the class is a large data file in memory.",
	"Method": "ChineseEnglishWordMap getInstance(){\r\n    return SingletonHolder.INSTANCE;\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.WordsToSentencesAnnotatorTest.testSpanishDatelineSeparation",
	"Comment": "test whether you can separate off a dateline as a separate sentence using ssplit.boundarymultitokenregex.",
	"Method": "void testSpanishDatelineSeparation(){\r\n    Properties props = PropertiesUtils.asProperties(\"annotators\", \"tokenize, cleanxml, ssplit\", \"tokenize.language\", \"es\", \"tokenize.options\", \"tokenizeNLs,ptb3Escaping=true\", \"ssplit.newlineIsSentenceBreak\", \"two\", \"ssplit.boundaryMultiTokenRegex\", \"/\\\\*NL\\\\*/ /\\\\p{Lu}[-\\\\p{L}]+/+ ( /,/  /[-\\\\p{L}]+/+ )? \" + \"( /,/ /[1-3]?[0-9]/ /\\\\p{Ll}{3,3}/ )? /=LRB=/ /\\\\p{Lu}\\\\p{L}+/ /=RRB=/ /--/\");\r\n    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\r\n    assertEquals(\"Bad test data\", dateLineSpanishTexts.length, dateLineSpanishTokens.length);\r\n    for (int i = 0; i < dateLineSpanishTexts.length; i++) {\r\n        Annotation document1 = new Annotation(dateLineSpanishTexts[i]);\r\n        pipeline.annotate(document1);\r\n        List<CoreMap> sentences = document1.get(CoreAnnotations.SentencesAnnotation.class);\r\n        assertEquals(\"For \" + dateLineSpanishTexts[i] + \" annotation is \" + document1, 2, sentences.size());\r\n        List<CoreLabel> sentenceOneTokens = sentences.get(0).get(CoreAnnotations.TokensAnnotation.class);\r\n        String sentenceOne = SentenceUtils.listToString(sentenceOneTokens);\r\n        assertEquals(\"Bad tokens in dateline\", dateLineSpanishTokens[i], sentenceOne);\r\n    }\r\n}"
}, {
	"Path": "com.atilika.kuromoji.TokenizerBase.debugTokenize",
	"Comment": "tokenizes the provided text and outputs the corresponding viterbi lattice and the viterbi path to the provided output streamthe output is written in dot format.this method is not thread safe",
	"Method": "void debugTokenize(OutputStream outputStream,String text){\r\n    ViterbiLattice lattice = viterbiBuilder.build(text);\r\n    List<ViterbiNode> bestPath = viterbiSearcher.search(lattice);\r\n    outputStream.write(viterbiFormatter.format(lattice, bestPath).getBytes(StandardCharsets.UTF_8));\r\n    outputStream.flush();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.asCounter",
	"Comment": "takes in a collection of something and makes a counter, incrementing oncefor each object in the collection.",
	"Method": "Counter<E> asCounter(Collection<E> c,Counter<E> asCounter,FixedPrioritiesPriorityQueue<E> p){\r\n    FixedPrioritiesPriorityQueue<E> pq = p.clone();\r\n    ClassicCounter<E> counter = new ClassicCounter();\r\n    while (pq.hasNext()) {\r\n        double priority = pq.getPriority();\r\n        E element = pq.next();\r\n        counter.incrementCount(element, priority);\r\n    }\r\n    return counter;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.paragraphvectors.ParagraphVectors.similarityToLabel",
	"Comment": "this method returns similarity of the document to specific label, based on mean value",
	"Method": "double similarityToLabel(String rawText,String label,double similarityToLabel,LabelledDocument document,String label,double similarityToLabel,List<VocabWord> document,String label){\r\n    if (document.isEmpty())\r\n        throw new IllegalStateException(\"Document has no words inside\");\r\n    INDArray docMean = inferVector(document);\r\n    INDArray otherVec = getWordVectorMatrix(label);\r\n    double sim = Transforms.cosineSim(docMean, otherVec);\r\n    return sim;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.MatchesPanel.isEmpty",
	"Comment": "determine whether any trees are in the matches panel at this time",
	"Method": "boolean isEmpty(){\r\n    return ((DefaultListModel) list.getModel()).isEmpty();\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.LanguageInfo.main",
	"Comment": "go through all of the paths via reflection, and print them out in a tsv format.this is useful for command line scripts.",
	"Method": "void main(String[] args){\r\n    for (Field field : LanguageInfo.class.getFields()) {\r\n        System.out.println(field.getName() + \"\\t\" + field.get(null));\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.transformWithValuesAdd",
	"Comment": "returns the counter with keys modified according to function f. if two keys are same after the transformation, their values get added up.",
	"Method": "Counter<T2> transformWithValuesAdd(Counter<T1> c,Function<T1, T2> f){\r\n    Counter<T2> c2 = new ClassicCounter();\r\n    for (T1 key : c.keySet()) {\r\n        c2.incrementCount(f.apply(key), c.getCount(key));\r\n    }\r\n    return c2;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FixedPrioritiesPriorityQueue.next",
	"Comment": "returns the element in the queue with highest priority, and pops it fromthe queue.",
	"Method": "E next(){\r\n    return removeFirst();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.LabeledScoredTreeNode.children",
	"Comment": "returns an array of children for the current node, or nullif it is a leaf.",
	"Method": "Tree[] children(){\r\n    return daughterTrees;\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.parameterserver.networking.v1.WiredEncodingHandler.sendMessage",
	"Comment": "this method sends given message to all registered recipients",
	"Method": "void sendMessage(INDArray message,int iterationNumber,int epochNumber){\r\n    try (MemoryWorkspace wsO = Nd4j.getMemoryManager().scopeOutOfWorkspaces()) {\r\n        long updateId = updatesCounter.getAndIncrement();\r\n        VoidParameterServer.getInstance().execDistributedImmediately(new SilentUpdatesMessage(message.unsafeDuplication(), updateId));\r\n    }\r\n    super.sendMessage(message, iterationNumber, epochNumber);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.outgoingEdgesWithReln",
	"Comment": "checks for outgoing edges of the node, in the given graph, which containthe given relation.relations are matched on if they are grammaticalrelationobjects or strings.",
	"Method": "List<SemanticGraphEdge> outgoingEdgesWithReln(IndexedWord node,SemanticGraph sg,GrammaticalRelation reln){\r\n    return edgesWithReln(sg.outgoingEdgeIterable(node), reln);\r\n}"
}, {
	"Path": "edu.stanford.nlp.patterns.SPIEDServlet.init",
	"Comment": "set the properties to the paths they appear at on the servlet.see build.xml for where these paths get copied.",
	"Method": "void init(){\r\n    testPropertiesFile = getServletContext().getRealPath(\"/WEB-INF/data/test.properties\");\r\n    modelNametoDirName = new HashMap();\r\n    modelNametoDirName.put(\"food\", \"food\");\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.flow.FlowPath.planRewind",
	"Comment": "this method announces future rewind of graph execution to specified position",
	"Method": "void planRewind(String frameName,boolean reallyPlan){\r\n    frames.get(frameName).setRewindPlanned(reallyPlan);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.getShapeForVarName",
	"Comment": "get the shape for the given vertex id.note that if an array is defined, it will use the shape of the array instead.a shapean array should not be defined at the same time.this wastes memory. the internal map used for tracking shapes for particularvertex ids should also delete redundant shapes stored to avoid redundant sources of information.",
	"Method": "long[] getShapeForVarName(String varName){\r\n    if (variableNameToArr.containsKey(varName)) {\r\n        return variableNameToArr.get(varName).shape();\r\n    }\r\n    return variableNameToShape.get(varName);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.TregexTest.testChinese",
	"Comment": "test a pattern with chinese characters in it, just to make surethat also works",
	"Method": "void testChinese(){\r\n    TregexPattern pattern = TregexPattern.compile(\"DEG|DEC < ?\");\r\n    runTest(\"DEG|DEC < ?\", \"(DEG (? 1))\", \"(DEG (? 1))\");\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.summary",
	"Comment": "string detailing the architecture of the multilayernetwork.will also display activation size when given an input type.columns are layerindex with layer type, nin, nout, total number of parameters, shapes of the parameters, input activation shape, output activation shapewill also give information about frozen layers, if any.",
	"Method": "String summary(String summary,InputType inputType){\r\n    StringBuilder ret = new StringBuilder();\r\n    ret.append(\"\\n\");\r\n    List<String[]> lines = new ArrayList();\r\n    if (inputType == null) {\r\n        lines.add(new String[] { \"LayerName (LayerType)\", \"nIn,nOut\", \"TotalParams\", \"ParamsShape\" });\r\n    } else {\r\n        lines.add(new String[] { \"LayerName (LayerType)\", \"nIn,nOut\", \"TotalParams\", \"ParamsShape\", \"InputShape\", \"OutputShape\" });\r\n    }\r\n    int[] maxLength = new int[inputType == null ? 4 : 6];\r\n    String[] header = lines.get(0);\r\n    for (int i = 0; i < header.length; i++) {\r\n        maxLength[i] = header[i].length();\r\n    }\r\n    int frozenParams = 0;\r\n    for (org.deeplearning4j.nn.api.Layer currentLayer : getLayers()) {\r\n        String name = currentLayer.conf().getLayer().getLayerName();\r\n        if (name == null) {\r\n            name = String.valueOf(currentLayer.getIndex());\r\n        }\r\n        String paramShape = \"-\";\r\n        String in = \"-\";\r\n        String out = \"-\";\r\n        String[] classNameArr = currentLayer.getClass().getName().split(\"\\\\.\");\r\n        String className = classNameArr[classNameArr.length - 1];\r\n        String paramCount = String.valueOf(currentLayer.numParams());\r\n        String inShape = \"\";\r\n        String outShape = \"\";\r\n        InputPreProcessor preProcessor;\r\n        InputType outType;\r\n        if (inputType != null) {\r\n            preProcessor = getLayerWiseConfigurations().getInputPreProcess(currentLayer.getIndex());\r\n            inShape = inputType.toString();\r\n            if (preProcessor != null) {\r\n                inputType = preProcessor.getOutputType(inputType);\r\n                inShape += \"--> \" + inputType.toString();\r\n            }\r\n            outType = currentLayer.conf().getLayer().getOutputType(currentLayer.getIndex(), inputType);\r\n            outShape = outType.toString();\r\n            inputType = outType;\r\n        }\r\n        if (currentLayer.numParams() > 0) {\r\n            paramShape = \"\";\r\n            if (currentLayer instanceof BidirectionalLayer) {\r\n                BidirectionalLayer bi = (BidirectionalLayer) currentLayer;\r\n                in = String.valueOf(((Bidirectional) bi.conf().getLayer()).getNIn());\r\n                out = String.valueOf(((Bidirectional) bi.conf().getLayer()).getNOut());\r\n            } else {\r\n                try {\r\n                    in = String.valueOf(((FeedForwardLayer) currentLayer.conf().getLayer()).getNIn());\r\n                    out = String.valueOf(((FeedForwardLayer) currentLayer.conf().getLayer()).getNOut());\r\n                } catch (Exception e) {\r\n                }\r\n            }\r\n            Set<String> paraNames = currentLayer.paramTable().keySet();\r\n            for (String aP : paraNames) {\r\n                String paramS = ArrayUtils.toString(currentLayer.paramTable().get(aP).shape());\r\n                paramShape += aP + \":\" + paramS + \", \";\r\n            }\r\n            paramShape = paramShape.subSequence(0, paramShape.lastIndexOf(\",\")).toString();\r\n        }\r\n        if (currentLayer instanceof FrozenLayer) {\r\n            frozenParams += currentLayer.numParams();\r\n            classNameArr = ((FrozenLayer) currentLayer).getInsideLayer().getClass().getName().split(\"\\\\.\");\r\n            className = \"Frozen \" + classNameArr[classNameArr.length - 1];\r\n        }\r\n        String[] line;\r\n        if (inputType == null) {\r\n            line = new String[] { name + \" (\" + className + \")\", in + \",\" + out, paramCount, paramShape };\r\n        } else {\r\n            line = new String[] { name + \" (\" + className + \")\", in + \",\" + out, paramCount, paramShape, inShape, outShape };\r\n        }\r\n        for (int i = 0; i < line.length; i++) {\r\n            maxLength[i] = Math.max(maxLength[i], line[i] == null ? 0 : line[i].length());\r\n        }\r\n        lines.add(line);\r\n    }\r\n    StringBuilder sbFormat = new StringBuilder();\r\n    int totalLength = 0;\r\n    int pos = 0;\r\n    for (int length : maxLength) {\r\n        int currLength;\r\n        if (pos++ == maxLength.length - 1) {\r\n            currLength = length;\r\n        } else {\r\n            currLength = length + 3;\r\n        }\r\n        sbFormat.append(\"%-\").append(currLength).append(\"s\");\r\n        totalLength += currLength;\r\n    }\r\n    sbFormat.append(\"\\n\");\r\n    String format = sbFormat.toString();\r\n    ret.append(StringUtils.repeat(\"=\", totalLength)).append(\"\\n\");\r\n    boolean first = true;\r\n    for (String[] line : lines) {\r\n        String formatted = String.format(format, (Object[]) line);\r\n        ret.append(formatted);\r\n        if (first) {\r\n            ret.append(StringUtils.repeat(\"=\", totalLength)).append(\"\\n\");\r\n            first = false;\r\n        }\r\n    }\r\n    ret.append(StringUtils.repeat(\"-\", totalLength));\r\n    ret.append(String.format(\"\\n0s %d\", \"Total Parameters: \", params().length()));\r\n    ret.append(String.format(\"\\n0s %d\", \"Trainable Parameters: \", params().length() - frozenParams));\r\n    ret.append(String.format(\"\\n0s %d\", \"Frozen Parameters: \", frozenParams));\r\n    ret.append(\"\\n\");\r\n    ret.append(StringUtils.repeat(\"=\", totalLength));\r\n    ret.append(\"\\n\");\r\n    return ret.toString();\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.setCollectTrainingStats",
	"Comment": "set whether training statistics should be collected for debugging purposes. statistics collection is disabled by default",
	"Method": "void setCollectTrainingStats(boolean collectTrainingStats){\r\n    trainingMaster.setCollectTrainingStats(collectTrainingStats);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.semgrexFromGraph",
	"Comment": "given a set of edges that form a rooted and connected graph, returns a semgrex patterncorresponding to it.",
	"Method": "String semgrexFromGraph(SemanticGraph sg,boolean matchTag,boolean matchWord,Map<IndexedWord, String> nodeNameMap,String semgrexFromGraph,SemanticGraph sg,Collection<IndexedWord> wildcardNodes,boolean useTag,boolean useWord,Map<IndexedWord, String> nodeNameMap,String semgrexFromGraph,SemanticGraph sg,Collection<IndexedWord> wildcardNodes,Map<IndexedWord, String> nodeNameMap,Function<IndexedWord, String> wordTransformation,String semgrexFromGraph,Iterable<SemanticGraphEdge> edges,boolean matchTag,boolean matchWord,Map<IndexedWord, String> nodeNameMap){\r\n    SemanticGraph sg = SemanticGraphFactory.makeFromEdges(edges);\r\n    return semgrexFromGraph(sg, matchTag, matchWord, nodeNameMap);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CollectionUtils.mode",
	"Comment": "returns the mode in the collection.if the collection has multiple modes, this method picks onearbitrarily.",
	"Method": "T mode(Collection<T> values){\r\n    Set<T> modes = modes(values);\r\n    return modes.iterator().next();\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.get",
	"Comment": "returns a subset of this array based on the specifiedindexes",
	"Method": "INDArray get(INDArray indices,INDArray get,List<List<Integer>> indices,INDArray get,INDArrayIndex indexes){\r\n    Nd4j.getCompressor().autoDecompress(this);\r\n    if (indexes.length > rank()) {\r\n        int numNonNewAxis = 0;\r\n        for (int i = 0; i < indexes.length; i++) {\r\n            if (!(indexes[i] instanceof NewAxis))\r\n                numNonNewAxis++;\r\n        }\r\n        if (numNonNewAxis > rank()) {\r\n            throw new IllegalArgumentException(\"Too many indices for array. Number of indexes must be <= rank()\");\r\n        }\r\n    }\r\n    if (indexes.length == 1 && indexes[0] instanceof NDArrayIndexAll || (indexes.length == 2 && (isRowVector() && indexes[0] instanceof PointIndex && indexes[0].offset() == 0 && indexes[1] instanceof NDArrayIndexAll || isColumnVector() && indexes[1] instanceof PointIndex && indexes[0].offset() == 0 && indexes[0] instanceof NDArrayIndexAll)))\r\n        return this;\r\n    indexes = NDArrayIndex.resolve(shapeInfoDataBuffer(), indexes);\r\n    ShapeOffsetResolution resolution = new ShapeOffsetResolution(this);\r\n    resolution.exec(indexes);\r\n    if (indexes.length < 1)\r\n        throw new IllegalStateException(\"Invalid index found of zero length\");\r\n    int[] shape = LongUtils.toInts(resolution.getShapes());\r\n    int numSpecifiedIndex = 0;\r\n    for (int i = 0; i < indexes.length; i++) if (indexes[i] instanceof SpecifiedIndex)\r\n        numSpecifiedIndex++;\r\n    if (shape != null && numSpecifiedIndex > 0) {\r\n        Generator<List<List<Long>>> gen = SpecifiedIndex.iterate(indexes);\r\n        INDArray ret = Nd4j.create(shape, 'c');\r\n        int count = 0;\r\n        while (true) {\r\n            try {\r\n                List<List<Long>> next = gen.next();\r\n                List<Long> coordsCombo = new ArrayList();\r\n                for (int i = 0; i < next.size(); i++) {\r\n                    if (next.get(i).size() > 1)\r\n                        throw new IllegalStateException(\"Illegal entry returned\");\r\n                    coordsCombo.add(next.get(i).get(0));\r\n                }\r\n                ret.putScalar(count++, getDouble(Ints.toArray(coordsCombo)));\r\n            } catch (NoSuchElementException e) {\r\n                break;\r\n            }\r\n            if (count >= ret.length())\r\n                break;\r\n        }\r\n        return ret;\r\n    }\r\n    INDArray ret = subArray(resolution);\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.TwoDimensionalMap.addAll",
	"Comment": "adds all of the entries in the other map, performingfunction on them to transform the values",
	"Method": "void addAll(TwoDimensionalMap<? extends K1, ? extends K2, ? extends V2> other,Function<V2, V> function){\r\n    for (TwoDimensionalMap.Entry<? extends K1, ? extends K2, ? extends V2> entry : other) {\r\n        put(entry.getFirstKey(), entry.getSecondKey(), function.apply(entry.getValue()));\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.getCommonAncestor",
	"Comment": "returns the least common ancestor. we only search as high as grandparents.we return null if no common parent or grandparent is found. any of theinput words can also be the answer if one is the parent or grandparent ofother, or if the input words are the same.",
	"Method": "IndexedWord getCommonAncestor(IndexedWord v1,IndexedWord v2){\r\n    if (v1.equals(v2)) {\r\n        return v1;\r\n    }\r\n    if (this.isAncestor(v1, v2) >= 1) {\r\n        return v2;\r\n    }\r\n    if (this.isAncestor(v2, v1) >= 1) {\r\n        return v1;\r\n    }\r\n    Set<IndexedWord> v1Parents = this.getParents(v1);\r\n    Set<IndexedWord> v2Parents = this.getParents(v2);\r\n    Set<IndexedWord> v1GrandParents = wordMapFactory.newSet();\r\n    Set<IndexedWord> v2GrandParents = wordMapFactory.newSet();\r\n    for (IndexedWord v1Parent : v1Parents) {\r\n        if (v2Parents.contains(v1Parent)) {\r\n            return v1Parent;\r\n        }\r\n        v1GrandParents.addAll(this.getParents(v1Parent));\r\n    }\r\n    for (IndexedWord v1GrandParent : v1GrandParents) {\r\n        if (v2Parents.contains(v1GrandParent)) {\r\n            return v1GrandParent;\r\n        }\r\n    }\r\n    for (IndexedWord v2Parent : v2Parents) {\r\n        v2GrandParents.addAll(this.getParents(v2Parent));\r\n    }\r\n    for (IndexedWord v2GrandParent : v2GrandParents) {\r\n        if (v1Parents.contains(v2GrandParent)) {\r\n            return v2GrandParent;\r\n        }\r\n        if (v1GrandParents.contains(v2GrandParent)) {\r\n            return v2GrandParent;\r\n        }\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.mnist.MnistManager.writeImageToPpm",
	"Comment": "writes the given image in the given file using the ppm data format.",
	"Method": "void writeImageToPpm(int[][] image,String ppmFileName){\r\n    try (BufferedWriter ppmOut = new BufferedWriter(new FileWriter(ppmFileName))) {\r\n        int rows = image.length;\r\n        int cols = image[0].length;\r\n        ppmOut.write(\"P3\\n\");\r\n        ppmOut.write(\"\" + rows + \" \" + cols + \" 255\\n\");\r\n        for (int i = 0; i < rows; i++) {\r\n            StringBuilder s = new StringBuilder();\r\n            for (int j = 0; j < cols; j++) {\r\n                s.append(image[i][j] + \" \" + image[i][j] + \" \" + image[i][j] + \"  \");\r\n            }\r\n            ppmOut.write(s.toString());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProto",
	"Comment": "serialize the given relation mention to the corresponding protocol buffer.",
	"Method": "CoreNLPProtos.Token toProto(CoreLabel coreLabel,CoreNLPProtos.Sentence toProto,CoreMap sentence,CoreNLPProtos.Document toProto,Annotation doc,CoreNLPProtos.ParseTree toProto,Tree parseTree,CoreNLPProtos.DependencyGraph toProto,SemanticGraph graph,CoreNLPProtos.CorefChain toProto,CorefChain chain,CoreNLPProtos.Mention toProto,Mention mention,CoreNLPProtos.SpeakerInfo toProto,SpeakerInfo speakerInfo,CoreNLPProtos.Timex toProto,Timex timex,CoreNLPProtos.Entity toProto,EntityMention ent,CoreNLPProtos.Relation toProto,RelationMention rel,CoreNLPProtos.Language toProto,Language lang,CoreNLPProtos.Operator toProto,OperatorSpec op,CoreNLPProtos.Polarity toProto,Polarity pol,CoreNLPProtos.SentenceFragment toProto,SentenceFragment fragment,CoreNLPProtos.RelationTriple toProto,RelationTriple triple){\r\n    CoreNLPProtos.RelationTriple.Builder builder = CoreNLPProtos.RelationTriple.newBuilder().setSubject(triple.subjectGloss()).setRelation(triple.relationGloss()).setObject(triple.objectGloss()).setConfidence(triple.confidence).addAllSubjectTokens(triple.subject.stream().map(token -> CoreNLPProtos.TokenLocation.newBuilder().setSentenceIndex(token.sentIndex()).setTokenIndex(token.index() - 1).build()).collect(Collectors.toList())).addAllRelationTokens(triple.relation.size() == 1 && triple.relation.get(0).get(IndexAnnotation.class) == null ? Collections.emptyList() : triple.relation.stream().map(token -> CoreNLPProtos.TokenLocation.newBuilder().setSentenceIndex(token.sentIndex()).setTokenIndex(token.index() - 1).build()).collect(Collectors.toList())).addAllObjectTokens(triple.object.stream().map(token -> CoreNLPProtos.TokenLocation.newBuilder().setSentenceIndex(token.sentIndex()).setTokenIndex(token.index() - 1).build()).collect(Collectors.toList()));\r\n    Optional<SemanticGraph> treeOptional = triple.asDependencyTree();\r\n    treeOptional.ifPresent(semanticGraph -> builder.setTree(toProto(semanticGraph)));\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseSparseNDArrayCOO.getIndicesOf",
	"Comment": "returns the indices of the element of the given index in the array context",
	"Method": "DataBuffer getIndicesOf(int i){\r\n    int from = underlyingRank() * i;\r\n    int to = from + underlyingRank();\r\n    int[] arr = new int[rank];\r\n    int j = 0;\r\n    int k = 0;\r\n    for (int dim = 0; dim < rank; dim++) {\r\n        if (k < hiddenDimensions().length && hiddenDimensions()[k] == j) {\r\n            arr[dim] = 0;\r\n            k++;\r\n        } else {\r\n            arr[dim] = indices.getInt(j);\r\n            j++;\r\n        }\r\n    }\r\n    return Nd4j.getDataBufferFactory().createInt(arr);\r\n}"
}, {
	"Path": "com.atilika.kuromoji.trie.DoubleArrayTrie.findBase",
	"Comment": "find base value for current node, which contains input nodes. they are children of current node.set default base value , which is one, at the index of each input node.",
	"Method": "int findBase(int index,List<Trie.Node> nodes){\r\n    int base = baseBuffer.get(index);\r\n    if (base < 0) {\r\n        return base;\r\n    }\r\n    while (true) {\r\n        boolean collision = false;\r\n        for (Trie.Node node : nodes) {\r\n            int nextIndex = index + base + node.getKey();\r\n            maxBaseCheckIndex = Math.max(maxBaseCheckIndex, nextIndex);\r\n            if (baseBuffer.capacity() <= nextIndex) {\r\n                extendBuffers(nextIndex);\r\n            }\r\n            if (baseBuffer.get(nextIndex) != 0) {\r\n                base++;\r\n                collision = true;\r\n                break;\r\n            }\r\n        }\r\n        if (!collision) {\r\n            break;\r\n        }\r\n    }\r\n    for (Trie.Node node : nodes) {\r\n        baseBuffer.put(index + base + node.getKey(), node.getKey() == TERMINATING_CHARACTER ? -1 : 1);\r\n    }\r\n    return base;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.blas.impl.BaseLevel2.trsv",
	"Comment": "trsv solves a system of linear equations whose coefficients are in a triangular matrix.",
	"Method": "void trsv(char order,char Uplo,char TransA,char Diag,INDArray A,INDArray X){\r\n    if (Nd4j.getExecutioner().getProfilingMode() == OpExecutioner.ProfilingMode.ALL)\r\n        OpProfiler.getInstance().processBlasCall(false, A, X);\r\n    if (X.data().dataType() == DataBuffer.Type.DOUBLE) {\r\n        DefaultOpExecutioner.validateDataType(DataBuffer.Type.DOUBLE, A, X);\r\n        dtrsv(order, Uplo, TransA, Diag, (int) A.length(), A, (int) A.size(0), X, X.majorStride());\r\n    } else {\r\n        DefaultOpExecutioner.validateDataType(DataBuffer.Type.FLOAT, A, X);\r\n        strsv(order, Uplo, TransA, Diag, (int) A.length(), A, (int) A.size(0), X, X.majorStride());\r\n    }\r\n    OpExecutionerUtil.checkForAny(X);\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.blas.impl.BaseLevel1.iamin",
	"Comment": "finds the element of a vector that has the minimum absolute value.",
	"Method": "int iamin(INDArray arr){\r\n    if (arr.isSparse()) {\r\n        return Nd4j.getSparseBlasWrapper().level1().iamin(arr);\r\n    } else {\r\n        throw new UnsupportedOperationException();\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.aws.ec2.provision.HostProvisioner.uploadForDeployment",
	"Comment": "creates the directory for the file if necessary and uploads the file",
	"Method": "void uploadForDeployment(String from,String to){\r\n    File fromFile = new File(from);\r\n    if (!to.isEmpty() && fromFile.isDirectory())\r\n        mkDir(to);\r\n    else\r\n        upload(from, to);\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.mnist.MnistLabelFile.readLabels",
	"Comment": "read the specified number of labels from the current position",
	"Method": "int[] readLabels(int num){\r\n    int[] out = new int[num];\r\n    for (int i = 0; i < num; i++) out[i] = readLabel();\r\n    return out;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasInitilizationUtils.getWeightInitFromConfig",
	"Comment": "get weight initialization from keras layer configuration.",
	"Method": "Pair<WeightInit, Distribution> getWeightInitFromConfig(Map<String, Object> layerConfig,String initField,boolean enforceTrainingConfig,KerasLayerConfiguration conf,int kerasMajorVersion){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (!innerConfig.containsKey(initField))\r\n        throw new InvalidKerasConfigurationException(\"Keras layer is missing \" + initField + \" field\");\r\n    String kerasInit;\r\n    Map<String, Object> initMap;\r\n    if (kerasMajorVersion != 2) {\r\n        kerasInit = (String) innerConfig.get(initField);\r\n        initMap = innerConfig;\r\n    } else {\r\n        @SuppressWarnings(\"unchecked\")\r\n        Map<String, Object> fullInitMap = (HashMap) innerConfig.get(initField);\r\n        initMap = (HashMap) fullInitMap.get(\"config\");\r\n        if (fullInitMap.containsKey(\"class_name\")) {\r\n            kerasInit = (String) fullInitMap.get(\"class_name\");\r\n        } else {\r\n            throw new UnsupportedKerasConfigurationException(\"Incomplete initialization class\");\r\n        }\r\n    }\r\n    Pair<WeightInit, Distribution> init;\r\n    try {\r\n        init = mapWeightInitialization(kerasInit, conf, initMap, kerasMajorVersion);\r\n    } catch (UnsupportedKerasConfigurationException e) {\r\n        if (enforceTrainingConfig)\r\n            throw e;\r\n        else {\r\n            init = new Pair(WeightInit.XAVIER, null);\r\n            log.warn(\"Unknown weight initializer \" + kerasInit + \" (Using XAVIER instead).\");\r\n        }\r\n    }\r\n    return init;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.constituentsNodes",
	"Comment": "same as int constituents but just puts the span as an intpairin the corelabel of the nodes.",
	"Method": "int constituentsNodes(int left){\r\n    if (isLeaf()) {\r\n        if (label() instanceof CoreLabel) {\r\n            ((CoreLabel) label()).set(CoreAnnotations.SpanAnnotation.class, new IntPair(left, left));\r\n        } else {\r\n            throw new UnsupportedOperationException(\"Can only set spans on trees which use CoreLabel\");\r\n        }\r\n        return (left + 1);\r\n    }\r\n    int position = left;\r\n    Tree[] kids = children();\r\n    for (Tree kid : kids) position = kid.constituentsNodes(position);\r\n    if (label() instanceof CoreLabel) {\r\n        ((CoreLabel) label()).set(CoreAnnotations.SpanAnnotation.class, new IntPair(left, position - 1));\r\n    } else {\r\n        throw new UnsupportedOperationException(\"Can only set spans on trees which use CoreLabel\");\r\n    }\r\n    return position;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.layers.Layer.resetLayerDefaultConfig",
	"Comment": "reset the learning related configs of the layer to default. when instantiated with a global neural network configurationthe parameters specified in the neural network configuration will be used.for internal use with the transfer learning api. users should not have to call this method directly.",
	"Method": "void resetLayerDefaultConfig(){\r\n    this.iDropout = null;\r\n    this.constraints = null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.TwoDimensionalMap.iterator",
	"Comment": "iterate over the map using the iterator and entry inner classes.",
	"Method": "Iterator<Entry<K1, K2, V>> iterator(){\r\n    return new TwoDimensionalMapIterator(this);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Filters.collectionRejectFilter",
	"Comment": "the collectionrejectfilter rejects a certain collection.",
	"Method": "Predicate<E> collectionRejectFilter(E[] objs,Predicate<E> collectionRejectFilter,Collection<E> objs){\r\n    return new CollectionAcceptFilter(objs, false);\r\n}"
}, {
	"Path": "org.deeplearning4j.optimize.listeners.CheckpointListener.getFileForCheckpoint",
	"Comment": "get the model file for the given checkpoint number. checkpoint model file must exist",
	"Method": "File getFileForCheckpoint(Checkpoint checkpoint,File getFileForCheckpoint,int checkpointNum){\r\n    if (checkpointNum < 0) {\r\n        throw new IllegalArgumentException(\"Invalid checkpoint number: \" + checkpointNum);\r\n    }\r\n    File f = null;\r\n    for (String s : MODEL_TYPES) {\r\n        f = new File(rootDir, getFileName(checkpointNum, s));\r\n        if (f.exists()) {\r\n            return f;\r\n        }\r\n    }\r\n    throw new IllegalStateException(\"Model file for checkpoint \" + checkpointNum + \" does not exist\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractTreebankLanguagePack.isLabelAnnotationIntroducingCharacter",
	"Comment": "say whether this character is an annotation introducingcharacter.",
	"Method": "boolean isLabelAnnotationIntroducingCharacter(char ch){\r\n    char[] cutChars = labelAnnotationIntroducingCharacters();\r\n    for (char cutChar : cutChars) {\r\n        if (ch == cutChar) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.toSortedListWithCounts",
	"Comment": "a list of the keys in c, sorted by the given comparator, paired withcounts.",
	"Method": "List<Pair<E, Double>> toSortedListWithCounts(Counter<E> c,List<Pair<E, Double>> toSortedListWithCounts,Counter<E> c,Comparator<Pair<E, Double>> comparator){\r\n    List<Pair<E, Double>> l = new ArrayList(c.size());\r\n    for (E e : c.keySet()) {\r\n        l.add(new Pair(e, c.getCount(e)));\r\n    }\r\n    Collections.sort(l, comparator);\r\n    return l;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.TwoDimensionalMapTest.testTreeMapIterator",
	"Comment": "now that we know the mapfactory constructor should work and theiterator should work, we can really test both by using a treemapand checking that the iterated elements are sorted",
	"Method": "void testTreeMapIterator(){\r\n    TwoDimensionalMap<String, String, String> map = new TwoDimensionalMap(MapFactory.<String, Map<String, String>>treeMapFactory(), MapFactory.<String, String>treeMapFactory());\r\n    map.put(\"A\", \"B\", \"C\");\r\n    map.put(\"Z\", \"Y\", \"X\");\r\n    map.put(\"Z\", \"B\", \"C\");\r\n    map.put(\"A\", \"Y\", \"X\");\r\n    map.put(\"D\", \"D\", \"D\");\r\n    map.put(\"D\", \"F\", \"E\");\r\n    map.put(\"K\", \"G\", \"B\");\r\n    map.put(\"G\", \"F\", \"E\");\r\n    map.put(\"D\", \"D\", \"E\");\r\n    assertEquals(8, map.size());\r\n    Iterator<TwoDimensionalMap.Entry<String, String, String>> mapIterator = map.iterator();\r\n    TwoDimensionalMap.Entry<String, String, String> entry = mapIterator.next();\r\n    assertEquals(\"A\", entry.getFirstKey());\r\n    assertEquals(\"B\", entry.getSecondKey());\r\n    assertEquals(\"C\", entry.getValue());\r\n    entry = mapIterator.next();\r\n    assertEquals(\"A\", entry.getFirstKey());\r\n    assertEquals(\"Y\", entry.getSecondKey());\r\n    assertEquals(\"X\", entry.getValue());\r\n    entry = mapIterator.next();\r\n    assertEquals(\"D\", entry.getFirstKey());\r\n    assertEquals(\"D\", entry.getSecondKey());\r\n    assertEquals(\"E\", entry.getValue());\r\n    entry = mapIterator.next();\r\n    assertEquals(\"D\", entry.getFirstKey());\r\n    assertEquals(\"F\", entry.getSecondKey());\r\n    assertEquals(\"E\", entry.getValue());\r\n    entry = mapIterator.next();\r\n    assertEquals(\"G\", entry.getFirstKey());\r\n    assertEquals(\"F\", entry.getSecondKey());\r\n    assertEquals(\"E\", entry.getValue());\r\n    entry = mapIterator.next();\r\n    assertEquals(\"K\", entry.getFirstKey());\r\n    assertEquals(\"G\", entry.getSecondKey());\r\n    assertEquals(\"B\", entry.getValue());\r\n    entry = mapIterator.next();\r\n    assertEquals(\"Z\", entry.getFirstKey());\r\n    assertEquals(\"B\", entry.getSecondKey());\r\n    assertEquals(\"C\", entry.getValue());\r\n    entry = mapIterator.next();\r\n    assertEquals(\"Z\", entry.getFirstKey());\r\n    assertEquals(\"Y\", entry.getSecondKey());\r\n    assertEquals(\"X\", entry.getValue());\r\n    assertFalse(mapIterator.hasNext());\r\n    Iterator<String> valueIterator = map.valueIterator();\r\n    assertTrue(valueIterator.hasNext());\r\n    assertEquals(\"C\", valueIterator.next());\r\n    assertEquals(\"X\", valueIterator.next());\r\n    assertEquals(\"E\", valueIterator.next());\r\n    assertEquals(\"E\", valueIterator.next());\r\n    assertEquals(\"E\", valueIterator.next());\r\n    assertEquals(\"B\", valueIterator.next());\r\n    assertEquals(\"C\", valueIterator.next());\r\n    assertEquals(\"X\", valueIterator.next());\r\n    assertFalse(valueIterator.hasNext());\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.asArrayList",
	"Comment": "creates an arraylist containing all of the objects returned by the given iterator.",
	"Method": "ArrayList<T> asArrayList(Iterator<? extends T> iter){\r\n    ArrayList<T> al = new ArrayList();\r\n    return (ArrayList<T>) addAll(iter, al);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.OutputHandler.style",
	"Comment": "style a particular string segment, according to a color and style",
	"Method": "StringBuilder style(StringBuilder b,String line,Color color,Style style){\r\n    if (color != Color.NONE || style != Style.NONE) {\r\n        if (Redwood.supportsAnsi && this.supportsAnsi()) {\r\n            b.append(color.ansiCode);\r\n            b.append(style.ansiCode);\r\n        }\r\n        b.append(line);\r\n        if (Redwood.supportsAnsi && this.supportsAnsi()) {\r\n            b.append(\"\\033[0m\");\r\n        }\r\n    } else {\r\n        b.append(line);\r\n    }\r\n    return b;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sentiment.SentimentModel.modelFromMatrices",
	"Comment": "given single matrices and sets of options, create thecorresponding sentimentmodel.useful for creating a java versionof a model trained in some other manner, such as using theoriginal matlab code.",
	"Method": "SentimentModel modelFromMatrices(SimpleMatrix W,SimpleMatrix Wcat,SimpleTensor Wt,Map<String, SimpleMatrix> wordVectors,RNNOptions op){\r\n    if (!op.combineClassification || !op.simplifiedModel) {\r\n        throw new IllegalArgumentException(\"Can only create a model using this method if combineClassification and simplifiedModel are turned on\");\r\n    }\r\n    TwoDimensionalMap<String, String, SimpleMatrix> binaryTransform = TwoDimensionalMap.treeMap();\r\n    binaryTransform.put(\"\", \"\", W);\r\n    TwoDimensionalMap<String, String, SimpleTensor> binaryTensors = TwoDimensionalMap.treeMap();\r\n    binaryTensors.put(\"\", \"\", Wt);\r\n    TwoDimensionalMap<String, String, SimpleMatrix> binaryClassification = TwoDimensionalMap.treeMap();\r\n    Map<String, SimpleMatrix> unaryClassification = Generics.newTreeMap();\r\n    unaryClassification.put(\"\", Wcat);\r\n    return new SentimentModel(binaryTransform, binaryTensors, binaryClassification, unaryClassification, wordVectors, op);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.wordstore.inmemory.AbstractCache.importVocabulary",
	"Comment": "this method imports all elements from vocabcache passed as argumentif element already exists,",
	"Method": "void importVocabulary(VocabCache<T> vocabCache){\r\n    for (T element : vocabCache.vocabWords()) {\r\n        this.addToken(element);\r\n    }\r\n    this.documentsCounter.addAndGet(vocabCache.totalNumberOfDocs());\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraPennLexer.yytext",
	"Comment": "returns the text matched by the current regular expression.",
	"Method": "String yytext(){\r\n    return new String(zzBuffer, zzStartRead, zzMarkedPos - zzStartRead);\r\n}"
}, {
	"Path": "org.deeplearning4j.zoo.BaseDL4JTest.getDataType",
	"Comment": "override this to set the datatype of the tests defined in the child class",
	"Method": "DataBuffer.Type getDataType(){\r\n    return DataBuffer.Type.FLOAT;\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.CorefAnnotator.findBestCoreferentEntityMention",
	"Comment": "helper method to find the longest entity mention that is coreferent to an entity mention after coref has been run...match an entity mention to a coref mention, go through all of the coref mentions and find the one with the longest matching entity mention, return that entity mention",
	"Method": "Optional<CoreMap> findBestCoreferentEntityMention(CoreMap em,Annotation ann){\r\n    Function<Optional<CoreMap>, Integer> lengthOfOptionalEntityMention = (v) -> v.isPresent() ? v.get().get(CoreAnnotations.TextAnnotation.class).length() : -1;\r\n    Optional<CoreMap> bestCoreferentEntityMention = Optional.empty();\r\n    int entityMentionIndex = em.get(CoreAnnotations.EntityMentionIndexAnnotation.class);\r\n    Optional<Integer> matchingCorefMentionIndex = Optional.ofNullable(ann.get(CoreAnnotations.EntityMentionToCorefMentionMappingAnnotation.class).get(entityMentionIndex));\r\n    Optional<Mention> matchingCorefMention = matchingCorefMentionIndex.isPresent() ? Optional.of(ann.get(CorefCoreAnnotations.CorefMentionsAnnotation.class).get(matchingCorefMentionIndex.get())) : Optional.empty();\r\n    if (matchingCorefMention.isPresent()) {\r\n        Optional<CorefChain> matchingCorefChain = Optional.ofNullable(ann.get(CorefCoreAnnotations.CorefChainAnnotation.class).get(matchingCorefMention.get().corefClusterID));\r\n        List<CorefMention> corefMentionsInTextualOrder = matchingCorefChain.isPresent() ? matchingCorefChain.get().getMentionsInTextualOrder() : new ArrayList<CorefMention>();\r\n        for (CorefMention cm : corefMentionsInTextualOrder) {\r\n            Optional<Integer> candidateCoreferentEntityMentionIndex = Optional.ofNullable(ann.get(CoreAnnotations.CorefMentionToEntityMentionMappingAnnotation.class).get(cm.mentionID));\r\n            Optional<CoreMap> candidateCoreferentEntityMention = candidateCoreferentEntityMentionIndex.isPresent() ? Optional.ofNullable(ann.get(CoreAnnotations.MentionsAnnotation.class).get(candidateCoreferentEntityMentionIndex.get())) : Optional.empty();\r\n            if (lengthOfOptionalEntityMention.apply(candidateCoreferentEntityMention) > lengthOfOptionalEntityMention.apply(bestCoreferentEntityMention)) {\r\n                bestCoreferentEntityMention = candidateCoreferentEntityMention;\r\n            }\r\n        }\r\n    }\r\n    return bestCoreferentEntityMention;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.algorithms",
	"Comment": "return a class that can perform common algorithms on this sentence.",
	"Method": "SentenceAlgorithms algorithms(){\r\n    return new SentenceAlgorithms(this);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.functions.DifferentialFunctionFactory.matchConditionCount",
	"Comment": "returns a count of the number of elements that satisfy the condition",
	"Method": "SDVariable matchConditionCount(SDVariable in,Condition condition,boolean keepDims,int dimensions){\r\n    return new MatchCondition(sameDiff(), in, condition, keepDims, dimensions).outputVariable();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Interval.includesEnd",
	"Comment": "returns whether the end endpoint is included in the interval",
	"Method": "boolean includesEnd(){\r\n    return ((flags & INTERVAL_OPEN_END) == 0);\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.impl.graph.SparkComputationGraph.fitPathsMultiDataSet",
	"Comment": "fit the network using a list of paths for serialized multidataset objects.",
	"Method": "ComputationGraph fitPathsMultiDataSet(JavaRDD<String> paths){\r\n    return fitPaths(paths, new SerializedMultiDataSetLoader());\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.toDotFormat",
	"Comment": "returns a dot format digraph with the given name.nodes will be labeled with the word and edges will be labeledwith the dependency.",
	"Method": "String toDotFormat(String toDotFormat,String graphname,String toDotFormat,String graphname,CoreLabel.OutputFormat indexedWordFormat){\r\n    StringBuilder output = new StringBuilder();\r\n    output.append(\"digraph \" + graphname + \" {\\n\");\r\n    for (IndexedWord word : graph.getAllVertices()) {\r\n        output.append(\"  N_\" + word.index() + \" [label=\\\"\" + word.toString(indexedWordFormat) + \"\\\"];\\n\");\r\n    }\r\n    for (SemanticGraphEdge edge : graph.edgeIterable()) {\r\n        output.append(\"  N_\" + edge.getSource().index() + \" -> N_\" + edge.getTarget().index() + \" [label=\\\"\" + edge.getRelation() + \"\\\"];\\n\");\r\n    }\r\n    output.append(\"}\\n\");\r\n    return output.toString();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.AbstractLayer.params",
	"Comment": "returns the parameters of the neural network as a flattened row vector",
	"Method": "INDArray params(){\r\n    return null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Trilean.isKnown",
	"Comment": "returns true if this trilean is either true or false, and false if it is unknown.",
	"Method": "boolean isKnown(){\r\n    return value != 2;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.toDecimal",
	"Comment": "this will convert the given binary string to a decimal basedinteger",
	"Method": "int toDecimal(String binary){\r\n    long num = Long.parseLong(binary);\r\n    long rem;\r\n    while (num > 0) {\r\n        rem = num % 10;\r\n        num = num / 10;\r\n        if (rem != 0 && rem != 1) {\r\n            System.out.println(\"This is not a binary number.\");\r\n            System.out.println(\"Please try once again.\");\r\n            return -1;\r\n        }\r\n    }\r\n    return Integer.parseInt(binary, 2);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.TregexTest.testComplex",
	"Comment": "more complex tests, often based on examples from our source code",
	"Method": "void testComplex(){\r\n    String testPattern = \"S < (NP=m1 $.. (VP < ((/VB/ < /^(am|are|is|was|were|'m|'re|'s|be)$/) $.. NP=m2)))\";\r\n    String testTree = \"(S (NP (NP (DT The) (JJ next) (NN stop)) (PP (IN on) (NP (DT the) (NN itinerary)))) (VP (VBD was) (NP (NP (NNP Chad)) (, ,) (SBAR (WHADVP (WRB where)) (S (NP (NNP Chen)) (VP (VBD dined) (PP (IN with) (NP (NP (NNP Chad) (POS \\'s)) (NNP President) (NNP Idris) (NNP Debi)))))))) (. .))\";\r\n    runTest(testPattern, \"(ROOT \" + testTree + \")\", testTree);\r\n    testTree = \"(S (NP (NNP Chen) (NNP Shui) (HYPH -) (NNP bian)) (VP (VBZ is) (NP (NP (DT the) (JJ first) (NML (NNP ROC) (NN president))) (SBAR (S (ADVP (RB ever)) (VP (TO to) (VP (VB travel) (PP (IN to) (NP (JJ western) (NNP Africa))))))))) (. .))\";\r\n    runTest(testPattern, \"(ROOT \" + testTree + \")\", testTree);\r\n    testTree = \"(ROOT (S (NP (PRP$ My) (NN dog)) (VP (VBZ is) (VP (VBG eating) (NP (DT a) (NN sausage)))) (. .)))\";\r\n    runTest(testPattern, testTree);\r\n    testTree = \"(ROOT (S (NP (PRP He)) (VP (MD will) (VP (VB be) (ADVP (RB here) (RB soon)))) (. .)))\";\r\n    runTest(testPattern, testTree);\r\n    testPattern = \"/^NP(?:-TMP|-ADV)?$/=m1 < (NP=m2 $- /^,$/ $-- NP=m3 !$ CC|CONJP)\";\r\n    testTree = \"(ROOT (S (NP (NP (NP (NP (DT The) (NNP ROC) (POS \\'s)) (NN ambassador)) (PP (IN to) (NP (NNP Nicaragua)))) (, ,) (NP (NNP Antonio) (NNP Tsai)) (, ,)) (ADVP (RB bluntly)) (VP (VBD argued) (PP (IN in) (NP (NP (DT a) (NN briefing)) (PP (IN with) (NP (NNP Chen))))) (SBAR (IN that) (S (NP (NP (NP (NNP Taiwan) (POS \\'s)) (JJ foreign) (NN assistance)) (PP (IN to) (NP (NNP Nicaragua)))) (VP (VBD was) (VP (VBG being) (ADJP (JJ misused))))))) (. .)))\";\r\n    String expectedResult = \"(NP (NP (NP (NP (DT The) (NNP ROC) (POS 's)) (NN ambassador)) (PP (IN to) (NP (NNP Nicaragua)))) (, ,) (NP (NNP Antonio) (NNP Tsai)) (, ,))\";\r\n    runTest(testPattern, testTree, expectedResult);\r\n    testTree = \"(ROOT (S (PP (IN In) (NP (NP (DT the) (NN opinion)) (PP (IN of) (NP (NP (NNP Norman) (NNP Hsu)) (, ,) (NP (NP (NN vice) (NN president)) (PP (IN of) (NP (NP (DT a) (NNS foods) (NN company)) (SBAR (WHNP (WHNP (WP$ whose) (NN family)) (PP (IN of) (NP (CD four)))) (S (VP (VBD had) (VP (VBN spent) (NP (QP (DT a) (JJ few)) (NNS years)) (PP (IN in) (NP (NNP New) (NNP Zealand))) (PP (IN before) (S (VP (VBG moving) (PP (IN to) (NP (NNP Dongguan))))))))))))))))) (, ,) (`` \\\") (NP (NP (DT The) (JJ first) (NN thing)) (VP (TO to) (VP (VB do)))) (VP (VBZ is) (S (VP (VB ask) (NP (DT the) (NNS children)) (NP (PRP$ their) (NN reason)) (PP (IN for) (S (VP (VBG saying) (NP (JJ such) (NNS things)))))))) (. .)))\";\r\n    expectedResult = \"(NP (NP (NNP Norman) (NNP Hsu)) (, ,) (NP (NP (NN vice) (NN president)) (PP (IN of) (NP (NP (DT a) (NNS foods) (NN company)) (SBAR (WHNP (WHNP (WP$ whose) (NN family)) (PP (IN of) (NP (CD four)))) (S (VP (VBD had) (VP (VBN spent) (NP (QP (DT a) (JJ few)) (NNS years)) (PP (IN in) (NP (NNP New) (NNP Zealand))) (PP (IN before) (S (VP (VBG moving) (PP (IN to) (NP (NNP Dongguan))))))))))))))\";\r\n    runTest(testPattern, testTree, expectedResult);\r\n    testTree = \"(ROOT (S (NP (NP (NNP Banana)) (, ,) (NP (NN orange)) (, ,) (CC and) (NP (NN apple))) (VP (VBP are) (NP (NNS fruits))) (. .)))\";\r\n    runTest(testPattern, testTree);\r\n    testTree = \"(ROOT (S (NP (PRP He)) (, ,) (ADVP (RB however)) (, ,) (VP (VBZ does) (RB not) (VP (VB look) (ADJP (JJ fine)))) (. .)))\";\r\n    runTest(testPattern, testTree);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiffOpExecutioner.thresholdDecode",
	"Comment": "this method decodes thresholds array, and puts it into target array",
	"Method": "INDArray thresholdDecode(INDArray encoded,INDArray target){\r\n    return backendExecutioner.thresholdDecode(encoded, target);\r\n}"
}, {
	"Path": "org.datavec.api.transform.TransformProcess.transformRawStringsToInputList",
	"Comment": "based on the input schema,map raw string values to the appropriatewritable",
	"Method": "List<Writable> transformRawStringsToInputList(List<String> values){\r\n    List<Writable> ret = new ArrayList();\r\n    if (values.size() != initialSchema.numColumns())\r\n        throw new IllegalArgumentException(String.format(\"Number of values %d does not match the number of input columns %d for schema\", values.size(), initialSchema.numColumns()));\r\n    for (int i = 0; i < values.size(); i++) {\r\n        switch(initialSchema.getType(i)) {\r\n            case String:\r\n                ret.add(new Text(values.get(i)));\r\n                break;\r\n            case Integer:\r\n                ret.add(new IntWritable(Integer.parseInt(values.get(i))));\r\n                break;\r\n            case Double:\r\n                ret.add(new DoubleWritable(Double.parseDouble(values.get(i))));\r\n                break;\r\n            case Float:\r\n                ret.add(new FloatWritable(Float.parseFloat(values.get(i))));\r\n                break;\r\n            case Categorical:\r\n                ret.add(new Text(values.get(i)));\r\n                break;\r\n            case Boolean:\r\n                ret.add(new BooleanWritable(Boolean.parseBoolean(values.get(i))));\r\n                break;\r\n            case Time:\r\n                break;\r\n            case Long:\r\n                ret.add(new LongWritable(Long.parseLong(values.get(i))));\r\n        }\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.OSXAdapter.handleAbout",
	"Comment": "functionality from the main app, as if it came over from another platform.",
	"Method": "void handleAbout(ApplicationEvent ae){\r\n    if (mainApp != null) {\r\n        ae.setHandled(true);\r\n        mainApp.about();\r\n    } else {\r\n        throw new IllegalStateException(\"handleAbout: TregexGUI instance detached from listener\");\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.paragraphvectors.ParagraphVectors.inferVector",
	"Comment": "this method calculates inferred vector for given list of words, with default parameters for learning rate and iterations",
	"Method": "INDArray inferVector(String text,double learningRate,double minLearningRate,int iterations,INDArray inferVector,LabelledDocument document,double learningRate,double minLearningRate,int iterations,INDArray inferVector,List<VocabWord> document,double learningRate,double minLearningRate,int iterations,INDArray inferVector,String text,INDArray inferVector,LabelledDocument document,INDArray inferVector,List<VocabWord> document){\r\n    return inferVector(document, this.learningRate.get(), this.minLearningRate, this.numEpochs * this.numIterations);\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.parameterserver.networking.v1.SilentTrainingDriver.getUpdatesBuffer",
	"Comment": "this method is viable only at spark workers, master node will always have empty buffer here by design",
	"Method": "IndexedTail getUpdatesBuffer(){\r\n    return updatesBuffer;\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLPServer.respondBadInput",
	"Comment": "a helper function to respond to a request with an error specifically indicatingbad input from the user.",
	"Method": "void respondBadInput(String response,HttpExchange httpExchange){\r\n    httpExchange.getResponseHeaders().add(\"Content-type\", \"text/plain\");\r\n    httpExchange.sendResponseHeaders(HTTP_BAD_REQUEST, response.length());\r\n    httpExchange.getResponseBody().write(response.getBytes());\r\n    httpExchange.close();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.wrappers.KerasBidirectional.getUnderlyingRecurrentLayer",
	"Comment": "return the underlying recurrent layer of this bidirectional layer",
	"Method": "Layer getUnderlyingRecurrentLayer(){\r\n    return kerasRnnlayer.getLayer();\r\n}"
}, {
	"Path": "org.nd4j.evaluation.EvaluationUtils.falsePositiveRate",
	"Comment": "calculate the false positive rate from the false positive count and true negative count",
	"Method": "double falsePositiveRate(long fpCount,long tnCount,double edgeCase){\r\n    if (fpCount == 0 && tnCount == 0) {\r\n        return edgeCase;\r\n    }\r\n    return fpCount / (double) (fpCount + tnCount);\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.EvaluationBinary.matthewsCorrelation",
	"Comment": "calculate the matthews correlation coefficient for the specified output",
	"Method": "double matthewsCorrelation(int outputNum){\r\n    assertIndex(outputNum);\r\n    return EvaluationUtils.matthewsCorrelation(truePositives(outputNum), falsePositives(outputNum), falseNegatives(outputNum), trueNegatives(outputNum));\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.VisibilityHandler.alsoHide",
	"Comment": "show all the channels currently being printed, with the exceptionof this new one",
	"Method": "boolean alsoHide(Object filter){\r\n    switch(this.defaultState) {\r\n        case HIDE_ALL:\r\n            return this.deltaPool.remove(filter);\r\n        case SHOW_ALL:\r\n            return this.deltaPool.add(filter);\r\n        default:\r\n            throw new IllegalStateException(\"Unknown default state setting: \" + this.defaultState);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.argmax",
	"Comment": "argmax array reduction operation, optionally along specified dimensions.output values are the index of the maximum value of each slice along the specified dimension",
	"Method": "SDVariable argmax(SDVariable in,int dimensions,SDVariable argmax,SDVariable in,boolean keepDims,int dimensions,SDVariable argmax,String name,SDVariable in,int dimensions,SDVariable argmax,String name,SDVariable in,boolean keepDims,int dimensions){\r\n    SDVariable ret = f().argmax(in, keepDims, dimensions);\r\n    return updateVariableNameAndReference(ret, name);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.loader.WordVectorSerializer.readWord2VecFromText",
	"Comment": "this method allows you to read paragraphvectors from externally originated vectors and syn1.so, technically this method is compatible with any other w2v implementation",
	"Method": "Word2Vec readWord2VecFromText(File vectors,File hs,File h_codes,File h_points,VectorsConfiguration configuration){\r\n    Pair<InMemoryLookupTable, VocabCache> pair = loadTxt(vectors);\r\n    InMemoryLookupTable lookupTable = pair.getFirst();\r\n    lookupTable.setNegative(configuration.getNegative());\r\n    if (configuration.getNegative() > 0)\r\n        lookupTable.initNegative();\r\n    VocabCache<VocabWord> vocab = (VocabCache<VocabWord>) pair.getSecond();\r\n    BufferedReader reader = new BufferedReader(new FileReader(hs));\r\n    String line = null;\r\n    List<INDArray> rows = new ArrayList();\r\n    while ((line = reader.readLine()) != null) {\r\n        String[] split = line.split(\" \");\r\n        double[] array = new double[split.length];\r\n        for (int i = 0; i < split.length; i++) {\r\n            array[i] = Double.parseDouble(split[i]);\r\n        }\r\n        rows.add(Nd4j.create(array));\r\n    }\r\n    reader.close();\r\n    if (!rows.isEmpty()) {\r\n        INDArray syn1 = Nd4j.vstack(rows);\r\n        lookupTable.setSyn1(syn1);\r\n    }\r\n    reader = new BufferedReader(new FileReader(h_points));\r\n    while ((line = reader.readLine()) != null) {\r\n        String[] split = line.split(\" \");\r\n        VocabWord word = vocab.wordFor(decodeB64(split[0]));\r\n        List<Integer> points = new ArrayList();\r\n        for (int i = 1; i < split.length; i++) {\r\n            points.add(Integer.parseInt(split[i]));\r\n        }\r\n        word.setPoints(points);\r\n    }\r\n    reader.close();\r\n    reader = new BufferedReader(new FileReader(h_codes));\r\n    while ((line = reader.readLine()) != null) {\r\n        String[] split = line.split(\" \");\r\n        VocabWord word = vocab.wordFor(decodeB64(split[0]));\r\n        List<Byte> codes = new ArrayList();\r\n        for (int i = 1; i < split.length; i++) {\r\n            codes.add(Byte.parseByte(split[i]));\r\n        }\r\n        word.setCodes(codes);\r\n        word.setCodeLength((short) codes.size());\r\n    }\r\n    reader.close();\r\n    Word2Vec.Builder builder = new Word2Vec.Builder(configuration).vocabCache(vocab).lookupTable(lookupTable).resetModel(false);\r\n    TokenizerFactory factory = getTokenizerFactory(configuration);\r\n    if (factory != null)\r\n        builder.tokenizerFactory(factory);\r\n    Word2Vec w2v = builder.build();\r\n    return w2v;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.leverageOrDetach",
	"Comment": "this method detaches indarray from current workspace, and attaches it to workspace with a given id, if a workspacewith the given id is open and active.if the workspace does not exist, or is not active, the array is detached from any workspaces.",
	"Method": "INDArray leverageOrDetach(String id){\r\n    if (!isAttached()) {\r\n        return this;\r\n    }\r\n    if (!Nd4j.getWorkspaceManager().checkIfWorkspaceExistsAndActive(id)) {\r\n        return detach();\r\n    }\r\n    return leverageTo(id);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.TreeFunctions.main",
	"Comment": "this method just tests the functionality of the included transformers.",
	"Method": "void main(String[] args){\r\n    Tree stringyTree = null;\r\n    try {\r\n        stringyTree = (new PennTreeReader(new StringReader(\"(S (VP (VBZ Try) (NP (DT this))) (. .))\"), new LabeledScoredTreeFactory(new StringLabelFactory()))).readTree();\r\n    } catch (IOException e) {\r\n    }\r\n    System.out.println(stringyTree);\r\n    Function<Tree, Tree> a = getLabeledTreeToCategoryWordTagTreeFunction();\r\n    Tree adaptyTree = a.apply(stringyTree);\r\n    System.out.println(adaptyTree);\r\n    adaptyTree.percolateHeads(new CollinsHeadFinder());\r\n    System.out.println(adaptyTree);\r\n    Function<Tree, Tree> b = getLabeledTreeToStringLabeledTreeFunction();\r\n    Tree stringLabelTree = b.apply(adaptyTree);\r\n    System.out.println(stringLabelTree);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.Ssurgeon.getTagText",
	"Comment": "for the given element, returns the text for the first child element withthe given tag.",
	"Method": "String getTagText(Element element,String tag){\r\n    try {\r\n        Element firstElt = getFirstTag(element, tag);\r\n        if (firstElt == null)\r\n            return \"\";\r\n        return getEltText(firstElt);\r\n    } catch (Exception e) {\r\n        log.warning(\"Exception thrown attempting to get tag text for tag=\" + tag + \", from element=\" + element);\r\n    }\r\n    return \"\";\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.util.MLLibUtil.fromLabeledPoint",
	"Comment": "convert an rddof labeled pointbased on the specified batch sizein to data set",
	"Method": "JavaRDD<DataSet> fromLabeledPoint(JavaRDD<LabeledPoint> data,long numPossibleLabels,long batchSize,JavaRDD<DataSet> fromLabeledPoint,JavaSparkContext sc,JavaRDD<LabeledPoint> data,long numPossibleLabels,JavaRDD<DataSet> fromLabeledPoint,JavaRDD<LabeledPoint> data,long numPossibleLabels,JavaRDD<DataSet> fromLabeledPoint,JavaRDD<LabeledPoint> data,long numPossibleLabels,boolean preCache,List<DataSet> fromLabeledPoint,List<LabeledPoint> labeledPoints,long numPossibleLabels,DataSet fromLabeledPoint,LabeledPoint point,long numPossibleLabels){\r\n    Vector features = point.features();\r\n    double label = point.label();\r\n    return new DataSet(Nd4j.create(features.toArray()), FeatureUtil.toOutcomeVector((int) label, (int) numPossibleLabels));\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.getChild",
	"Comment": "return the child at some daughter index.the children are numberedstarting with an index of 0.",
	"Method": "Tree getChild(int i){\r\n    Tree[] kids = children();\r\n    return kids[i];\r\n}"
}, {
	"Path": "org.datavec.arrow.ArrowConverter.toArrowColumns",
	"Comment": "given a buffer allocator and datavec schema,convert the passed in batch of recordsto a set of arrow columns",
	"Method": "List<FieldVector> toArrowColumns(BufferAllocator bufferAllocator,Schema schema,List<List<Writable>> dataVecRecord){\r\n    int numRows = dataVecRecord.size();\r\n    List<FieldVector> ret = createFieldVectors(bufferAllocator, schema, numRows);\r\n    for (int j = 0; j < schema.numColumns(); j++) {\r\n        FieldVector fieldVector = ret.get(j);\r\n        int row = 0;\r\n        for (List<Writable> record : dataVecRecord) {\r\n            Writable writable = record.get(j);\r\n            setValue(schema.getType(j), fieldVector, writable, row);\r\n            row++;\r\n        }\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.deeplearning4j.text.movingwindow.ContextLabelRetriever.stringWithLabels",
	"Comment": "returns a stripped sentence with the indices of wordswith certain kinds of labels.",
	"Method": "Pair<String, MultiDimensionalMap<Integer, Integer, String>> stringWithLabels(String sentence,TokenizerFactory tokenizerFactory){\r\n    MultiDimensionalMap<Integer, Integer, String> map = MultiDimensionalMap.newHashBackedMap();\r\n    Tokenizer t = tokenizerFactory.create(sentence);\r\n    List<String> currTokens = new ArrayList();\r\n    String currLabel = null;\r\n    String endLabel = null;\r\n    List<Pair<String, List<String>>> tokensWithSameLabel = new ArrayList();\r\n    while (t.hasMoreTokens()) {\r\n        String token = t.nextToken();\r\n        if (token.matches(BEGIN_LABEL)) {\r\n            if (endLabel != null)\r\n                throw new IllegalStateException(\"Tried parsing sentence; found an end label when the begin label has not been cleared\");\r\n            currLabel = token;\r\n            if (!currTokens.isEmpty()) {\r\n                tokensWithSameLabel.add(new Pair(\"NONE\", (List<String>) new ArrayList(currTokens)));\r\n                currTokens.clear();\r\n            }\r\n        } else if (token.matches(END_LABEL)) {\r\n            if (currLabel == null)\r\n                throw new IllegalStateException(\"Found an ending label with no matching begin label\");\r\n            endLabel = token;\r\n        } else\r\n            currTokens.add(token);\r\n        if (currLabel != null && endLabel != null) {\r\n            currLabel = currLabel.replaceAll(\"[<>/]\", \"\");\r\n            endLabel = endLabel.replaceAll(\"[<>/]\", \"\");\r\n            Preconditions.checkState(!currLabel.isEmpty(), \"Current label is empty!\");\r\n            Preconditions.checkState(!endLabel.isEmpty(), \"End label is empty!\");\r\n            Preconditions.checkState(currLabel.equals(endLabel), \"Current label begin and end did not match for the parse. Was: %s ending with %s\", currLabel, endLabel);\r\n            tokensWithSameLabel.add(new Pair(currLabel, (List<String>) new ArrayList(currTokens)));\r\n            currTokens.clear();\r\n            currLabel = null;\r\n            endLabel = null;\r\n        }\r\n    }\r\n    if (!currTokens.isEmpty()) {\r\n        tokensWithSameLabel.add(new Pair(\"none\", (List<String>) new ArrayList(currTokens)));\r\n        currTokens.clear();\r\n    }\r\n    StringBuilder strippedSentence = new StringBuilder();\r\n    for (Pair<String, List<String>> tokensWithLabel : tokensWithSameLabel) {\r\n        String joinedSentence = StringUtils.join(\" \", tokensWithLabel.getSecond());\r\n        if (!(strippedSentence.length() < 1))\r\n            strippedSentence.append(\" \");\r\n        strippedSentence.append(joinedSentence);\r\n        int begin = strippedSentence.toString().indexOf(joinedSentence);\r\n        int end = begin + joinedSentence.length();\r\n        map.put(begin, end, tokensWithLabel.getFirst());\r\n    }\r\n    return new Pair(strippedSentence.toString(), map);\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.maxInPlace",
	"Comment": "places the maximum of first and second keys values in the first counter.",
	"Method": "void maxInPlace(Counter<E> target,Counter<E> other){\r\n    for (E e : CollectionUtils.union(other.keySet(), target.keySet())) {\r\n        target.setCount(e, Math.max(target.getCount(e), other.getCount(e)));\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.learning.impl.elements.SkipGram.configure",
	"Comment": "skipgram initialization over given vocabulary and weightlookuptable",
	"Method": "void configure(VocabCache<T> vocabCache,WeightLookupTable<T> lookupTable,VectorsConfiguration configuration){\r\n    this.vocabCache = vocabCache;\r\n    this.lookupTable = lookupTable;\r\n    this.configuration = configuration;\r\n    if (configuration.getNegative() > 0) {\r\n        if (((InMemoryLookupTable<T>) lookupTable).getSyn1Neg() == null) {\r\n            log.info(\"Initializing syn1Neg...\");\r\n            ((InMemoryLookupTable<T>) lookupTable).setUseHS(configuration.isUseHierarchicSoftmax());\r\n            ((InMemoryLookupTable<T>) lookupTable).setNegative(configuration.getNegative());\r\n            ((InMemoryLookupTable<T>) lookupTable).resetWeights(false);\r\n        }\r\n    }\r\n    this.expTable = new DeviceLocalNDArray(Nd4j.create(((InMemoryLookupTable<T>) lookupTable).getExpTable()));\r\n    this.syn0 = new DeviceLocalNDArray(((InMemoryLookupTable<T>) lookupTable).getSyn0());\r\n    this.syn1 = new DeviceLocalNDArray(((InMemoryLookupTable<T>) lookupTable).getSyn1());\r\n    this.syn1Neg = new DeviceLocalNDArray(((InMemoryLookupTable<T>) lookupTable).getSyn1Neg());\r\n    this.table = new DeviceLocalNDArray(((InMemoryLookupTable<T>) lookupTable).getTable());\r\n    this.window = configuration.getWindow();\r\n    this.useAdaGrad = configuration.isUseAdaGrad();\r\n    this.negative = configuration.getNegative();\r\n    this.sampling = configuration.getSampling();\r\n    this.variableWindows = configuration.getVariableWindows();\r\n    this.vectorLength = configuration.getLayersSize();\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.shell",
	"Comment": "runs as either a filter or as an interactive shell where input text is processed with the given pipeline.the default case is to treat each line as a document. this can be altered with the property.",
	"Method": "void shell(){\r\n    AnnotationOutputter.Options options = AnnotationOutputter.getOptions(properties);\r\n    String encoding = getEncoding();\r\n    BufferedReader r = new BufferedReader(IOUtils.encodedInputStreamReader(System.in, encoding));\r\n    boolean isTty = System.console() != null;\r\n    boolean oneDocument = Boolean.parseBoolean(properties.getProperty(\"isOneDocument\"));\r\n    if (isTty) {\r\n        System.err.println(\"Entering interactive shell. Type q RETURN or EOF to quit.\");\r\n    }\r\n    while (true) {\r\n        if (isTty) {\r\n            System.err.print(\"NLP> \");\r\n        }\r\n        String line;\r\n        if (oneDocument) {\r\n            line = IOUtils.slurpReader(r);\r\n        } else {\r\n            line = r.readLine();\r\n        }\r\n        if (line == null || isTty && line.equalsIgnoreCase(\"q\")) {\r\n            break;\r\n        }\r\n        if (!line.isEmpty()) {\r\n            Annotation anno = process(line);\r\n            outputAnnotation(System.out, anno, properties, options);\r\n        }\r\n        if (oneDocument) {\r\n            break;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.Redwood.captureSystemStreams",
	"Comment": "captures system.out and system.err and redirects themto redwood logging.",
	"Method": "void captureSystemStreams(boolean captureOut,boolean captureErr){\r\n    if (captureOut) {\r\n        System.setOut(new RedwoodPrintStream(STDOUT, realSysOut));\r\n    } else {\r\n        System.setOut(realSysOut);\r\n    }\r\n    if (captureErr) {\r\n        System.setErr(new RedwoodPrintStream(STDERR, realSysErr));\r\n    } else {\r\n        System.setErr(realSysErr);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.recurrent.KerasLSTM.getGateActivationFromConfig",
	"Comment": "get lstm gate activation function from keras layer configuration.",
	"Method": "IActivation getGateActivationFromConfig(Map<String, Object> layerConfig){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (!innerConfig.containsKey(conf.getLAYER_FIELD_INNER_ACTIVATION()))\r\n        throw new InvalidKerasConfigurationException(\"Keras LSTM layer config missing \" + conf.getLAYER_FIELD_INNER_ACTIVATION() + \" field\");\r\n    return mapToIActivation((String) innerConfig.get(conf.getLAYER_FIELD_INNER_ACTIVATION()), conf);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.getChildrenWithReln",
	"Comment": "returns a set of all children bearing a certain grammatical relation, oran empty set if none.",
	"Method": "Set<IndexedWord> getChildrenWithReln(IndexedWord vertex,GrammaticalRelation reln){\r\n    if (vertex.equals(IndexedWord.NO_WORD))\r\n        return Collections.emptySet();\r\n    if (!containsVertex(vertex))\r\n        throw new IllegalArgumentException();\r\n    Set<IndexedWord> childList = wordMapFactory.newSet();\r\n    for (SemanticGraphEdge edge : outgoingEdgeIterable(vertex)) {\r\n        if (edge.getRelation().equals(reln)) {\r\n            childList.add(edge.getTarget());\r\n        }\r\n    }\r\n    return childList;\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getChar",
	"Comment": "get the char value of the name property, null ifno such property exists.values are processed for variable expansionbefore being returned.",
	"Method": "char getChar(String name,char getChar,String name,char defaultValue){\r\n    return getProps().getProperty(name, String.valueOf(defaultValue)).charAt(0);\r\n}"
}, {
	"Path": "org.datavec.api.io.WritableUtils.clone",
	"Comment": "make a copy of a writable object using serialization to a buffer.",
	"Method": "T clone(T orig,Configuration conf){\r\n    try {\r\n        @SuppressWarnings(\"unchecked\")\r\n        T newInst = ReflectionUtils.newInstance((Class<T>) orig.getClass(), conf);\r\n        ReflectionUtils.copy(conf, orig, newInst);\r\n        return newInst;\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(\"Error writing/reading clone buffer\", e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.concurrent.AtomicDouble.compareAndSet",
	"Comment": "atomically sets the value to the given updated valueif the current value is bitwise equalto the expected value.",
	"Method": "boolean compareAndSet(double expect,double update){\r\n    return updater.compareAndSet(this, doubleToRawLongBits(expect), doubleToRawLongBits(update));\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.asHashSet",
	"Comment": "creates a hashset containing all of the objects returned by the given iterator.",
	"Method": "HashSet<T> asHashSet(Iterator<? extends T> iter){\r\n    HashSet<T> hs = new HashSet();\r\n    return (HashSet<T>) addAll(iter, hs);\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.inference.TableFactorTest.subsetToSupersetAssignments",
	"Comment": "convenience function to construct a subset to superset assignment map. each subset assignment will be mappingto a large number of superset assignments.",
	"Method": "Map<List<Integer>, List<int[]>> subsetToSupersetAssignments(TableFactor superset,TableFactor subset){\r\n    Map<List<Integer>, List<int[]>> subsetToSupersets = new HashMap();\r\n    for (int[] assignment : subset) {\r\n        List<Integer> subsetAssignmentList = Arrays.stream(assignment).boxed().collect(Collectors.toList());\r\n        List<int[]> supersetAssignments = new ArrayList();\r\n        for (int[] supersetAssignment : superset) {\r\n            if (Arrays.equals(assignment, subsetAssignment(supersetAssignment, superset, subset))) {\r\n                supersetAssignments.add(supersetAssignment);\r\n            }\r\n        }\r\n        subsetToSupersets.put(subsetAssignmentList, supersetAssignments);\r\n    }\r\n    return subsetToSupersets;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.wordvectors.WordVectorsImpl.getWordVectors",
	"Comment": "this method returns 2d array, where each row represents corresponding label",
	"Method": "INDArray getWordVectors(Collection<String> labels){\r\n    int[] indexes = new int[labels.size()];\r\n    int cnt = 0;\r\n    for (String label : labels) {\r\n        if (vocab.containsWord(label)) {\r\n            indexes[cnt] = vocab.indexOf(label);\r\n        } else\r\n            indexes[cnt] = -1;\r\n        cnt++;\r\n    }\r\n    while (ArrayUtils.contains(indexes, -1)) {\r\n        indexes = ArrayUtils.removeElement(indexes, -1);\r\n    }\r\n    INDArray result = Nd4j.pullRows(lookupTable.getWeights(), 1, indexes);\r\n    return result;\r\n}"
}, {
	"Path": "edu.stanford.nlp.swing.FontDetector.supportedFonts",
	"Comment": "returns which fonts on the system can display the sample string.",
	"Method": "List<Font> supportedFonts(int language){\r\n    if (language < 0 || language > NUM_LANGUAGES) {\r\n        throw new IllegalArgumentException();\r\n    }\r\n    List<Font> fonts = new ArrayList();\r\n    Font[] systemFonts = GraphicsEnvironment.getLocalGraphicsEnvironment().getAllFonts();\r\n    for (Font systemFont : systemFonts) {\r\n        boolean canDisplay = true;\r\n        for (int j = 0; j < unicodeRanges[language].length; j++) {\r\n            if (systemFont.canDisplayUpTo(unicodeRanges[language][j]) != -1) {\r\n                canDisplay = false;\r\n                break;\r\n            }\r\n        }\r\n        if (canDisplay) {\r\n            fonts.add(systemFont);\r\n        }\r\n    }\r\n    return fonts;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.PropertiesUtils.getBool",
	"Comment": "load a boolean property.if the key is not present, returns defaultvalue.",
	"Method": "boolean getBool(Properties props,String key,boolean getBool,Properties props,String key,boolean defaultValue){\r\n    String value = props.getProperty(key);\r\n    if (value != null) {\r\n        return Boolean.parseBoolean(value);\r\n    } else {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.TokenizerAnnotator.setTokenBeginTokenEnd",
	"Comment": "helper method to set the tokenbeginannotation and tokenendannotation of every token.",
	"Method": "void setTokenBeginTokenEnd(List<CoreLabel> tokensList){\r\n    int tokenIndex = 0;\r\n    for (CoreLabel token : tokensList) {\r\n        token.set(CoreAnnotations.TokenBeginAnnotation.class, tokenIndex);\r\n        token.set(CoreAnnotations.TokenEndAnnotation.class, tokenIndex + 1);\r\n        tokenIndex++;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphFactory.generateCCProcessedDependencies",
	"Comment": "produces a ccprocessed semanticgraph with optional extras.",
	"Method": "SemanticGraph generateCCProcessedDependencies(Tree tree,SemanticGraph generateCCProcessedDependencies,GrammaticalStructure gs,SemanticGraph generateCCProcessedDependencies,GrammaticalStructure gs,GrammaticalStructure.Extras extras){\r\n    return makeFromTree(gs, Mode.CCPROCESSED, extras, null);\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.constructAnnotatorPool",
	"Comment": "construct the default annotator pool from the passed in properties, and overwriting annotators which have changedsince the last call.",
	"Method": "AnnotatorPool constructAnnotatorPool(Properties inputProps,AnnotatorImplementations annotatorImplementation){\r\n    AnnotatorPool pool = new AnnotatorPool();\r\n    for (Map.Entry<String, BiFunction<Properties, AnnotatorImplementations, Annotator>> entry : getNamedAnnotators().entrySet()) {\r\n        AnnotatorSignature key = new AnnotatorSignature(entry.getKey(), PropertiesUtils.getSignature(entry.getKey(), inputProps));\r\n        pool.register(entry.getKey(), inputProps, GLOBAL_ANNOTATOR_CACHE.computeIfAbsent(key, (sig) -> Lazy.cache(() -> entry.getValue().apply(inputProps, annotatorImplementation))));\r\n    }\r\n    registerCustomAnnotators(pool, annotatorImplementation, inputProps);\r\n    return pool;\r\n}"
}, {
	"Path": "org.nd4j.imports.graphmapper.tf.TFGraphMapper.getNodeName",
	"Comment": "map a tensorflow node nameto the samediff equivalentfor import",
	"Method": "String getNodeName(String name){\r\n    String ret = name;\r\n    if (ret.startsWith(\"^\"))\r\n        ret = ret.substring(1);\r\n    if (ret.endsWith(\"/read\")) {\r\n        ret = ret.replace(\"/read\", \"\");\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CollectionUtils.asSet",
	"Comment": "returns a new set containing all the objects in the specified array.",
	"Method": "Set<T> asSet(T o){\r\n    return Generics.newHashSet(Arrays.asList(o));\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.JFlexDummyLexer.yytext",
	"Comment": "returns the text matched by the current regular expression.",
	"Method": "String yytext(){\r\n    return new String(zzBuffer, zzStartRead, zzMarkedPos - zzStartRead);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.pennchinese.CHTBLexer.yytext",
	"Comment": "returns the text matched by the current regular expression.",
	"Method": "String yytext(){\r\n    return new String(zzBuffer, zzStartRead, zzMarkedPos - zzStartRead);\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.EvaluationBinary.totalCount",
	"Comment": "get the total number of values for the specified column, accounting for any masking",
	"Method": "int totalCount(int outputNum){\r\n    assertIndex(outputNum);\r\n    return countTruePositive[outputNum] + countTrueNegative[outputNum] + countFalseNegative[outputNum] + countFalsePositive[outputNum];\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.EnglishGrammaticalStructureTest.testNonCollapsedSeparator",
	"Comment": "tests printing of the extra dependencies after the basic ones.",
	"Method": "void testNonCollapsedSeparator(){\r\n    String[] testTrees = { \"(ROOT (S (NP (PRP I)) (VP (VBP like) (S (VP (TO to) (VP (VB swim))))) (. .)))\" };\r\n    String[] testAnswers = { \"nsubj(like-2, I-1)\\n\" + \"root(ROOT-0, like-2)\\n\" + \"aux(swim-4, to-3)\\n\" + \"xcomp(like-2, swim-4)\\n\" + \"======\\n\" + \"nsubj(swim-4, I-1)\\n\" };\r\n    assertEquals(\"Test array lengths mismatch!\", testTrees.length, testAnswers.length);\r\n    TreeReaderFactory trf = new PennTreeReaderFactory();\r\n    for (int i = 0; i < testTrees.length; i++) {\r\n        String testTree = testTrees[i];\r\n        String testAnswer = testAnswers[i];\r\n        Tree tree = Tree.valueOf(testTree, trf);\r\n        GrammaticalStructure gs = new EnglishGrammaticalStructure(tree);\r\n        assertEquals(\"Unexpected basic dependencies for tree \" + testTree, testAnswer, GrammaticalStructureConversionUtils.dependenciesToString(gs, gs.allTypedDependencies(), tree, false, true, false));\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.sequence.SequenceMovingWindowReduceTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return columnNames();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.dotProductInPlace",
	"Comment": "multiplies every count in target by the corresponding value in the termcounter.",
	"Method": "void dotProductInPlace(Counter<E> target,Counter<E> term){\r\n    for (E key : target.keySet()) {\r\n        target.setCount(key, target.getCount(key) * term.getCount(key));\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.feedforward.autoencoder.recursive.Tree.getLeaves",
	"Comment": "gets the leaves of the tree.all leaves nodes are returned as a listordered by the natural left to right order of the tree.null values,if any, are inserted into the list like any other value.",
	"Method": "List<T> getLeaves(List<T> getLeaves,List<T> list){\r\n    if (isLeaf()) {\r\n        list.add((T) this);\r\n    } else {\r\n        for (Tree kid : children()) {\r\n            kid.getLeaves(list);\r\n        }\r\n    }\r\n    return list;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.spaceToBatch",
	"Comment": "convolution 2d layer space to batch operation on 4d input.increases input batch dimension by rearranging data from spatial dimensions into batch dimension",
	"Method": "SDVariable spaceToBatch(SDVariable x,int[] blocks,int[][] padding,SDVariable spaceToBatch,String name,SDVariable x,int[] blocks,int[][] padding){\r\n    SDVariable ret = f().spaceToBatch(x, blocks, padding);\r\n    return updateVariableNameAndReference(ret, name);\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.Evaluation.f1",
	"Comment": "calculate the average f1 score across all classes, using macro or micro averaging",
	"Method": "double f1(int classLabel,double f1,double f1,EvaluationAveraging averaging){\r\n    return fBeta(1.0, averaging);\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseSparseNDArrayCOO.createSparseOffsets",
	"Comment": "compute the sparse offsets of the view we are getting, for each dimension according to the original ndarray",
	"Method": "long[] createSparseOffsets(long offset){\r\n    int underlyingRank = sparseOffsets().length;\r\n    long[] newOffsets = new long[rank()];\r\n    List<Long> shapeList = Longs.asList(shape());\r\n    int penultimate = rank() - 1;\r\n    for (int i = 0; i < penultimate; i++) {\r\n        long prod = ArrayUtil.prodLong(shapeList.subList(i + 1, rank()));\r\n        newOffsets[i] = offset / prod;\r\n        offset = offset - newOffsets[i] * prod;\r\n    }\r\n    newOffsets[rank() - 1] = offset % shape()[rank() - 1];\r\n    long[] finalOffsets = new long[underlyingRank];\r\n    int dimNotFixed = 0;\r\n    for (int dim = 0; dim < underlyingRank; dim++) {\r\n        if (flags()[dim] == 1) {\r\n            finalOffsets[dim] = sparseOffsets()[dim];\r\n        } else {\r\n            finalOffsets[dim] = newOffsets[dimNotFixed] + sparseOffsets()[dim];\r\n            dimNotFixed++;\r\n        }\r\n    }\r\n    return finalOffsets;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.FilePanel.setTreeReaderFactory",
	"Comment": "sets a new tree reader factory for reading trees from files in this panel.since this may make some fileswith trees unable to be read, clearfiles indicates if all current files should be removed from the panel.",
	"Method": "void setTreeReaderFactory(TreeReaderFactory trf){\r\n    treeModel.setTRF(trf);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.paragraphvectors.ParagraphVectorsTest.testParagraphVectorsVocabBuilding1",
	"Comment": "this test checks, how vocab is built using sentenceiterator provided, without labels.",
	"Method": "void testParagraphVectorsVocabBuilding1(){\r\n    ClassPathResource resource = new ClassPathResource(\"/big/raw_sentences.txt\");\r\n    File file = resource.getFile();\r\n    SentenceIterator iter = new BasicLineIterator(file);\r\n    int numberOfLines = 0;\r\n    while (iter.hasNext()) {\r\n        iter.nextSentence();\r\n        numberOfLines++;\r\n    }\r\n    iter.reset();\r\n    InMemoryLookupCache cache = new InMemoryLookupCache(false);\r\n    TokenizerFactory t = new DefaultTokenizerFactory();\r\n    t.setTokenPreProcessor(new CommonPreprocessor());\r\n    ParagraphVectors vec = // .labelsGenerator(source)\r\n    new ParagraphVectors.Builder().minWordFrequency(1).iterations(5).layerSize(100).windowSize(5).iterate(iter).vocabCache(cache).tokenizerFactory(t).build();\r\n    vec.buildVocab();\r\n    LabelsSource source = vec.getLabelsSource();\r\n    log.info(\"Number of lines in corpus: \" + numberOfLines);\r\n    assertEquals(numberOfLines, source.getLabels().size());\r\n    assertEquals(97162, source.getLabels().size());\r\n    assertNotEquals(null, cache);\r\n    assertEquals(97406, cache.numWords());\r\n    assertEquals(244, cache.numWords() - source.getLabels().size());\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.TwoDimensionalSet.iterator",
	"Comment": "iterate over the map using the iterator and entry inner classes.",
	"Method": "Iterator<Pair<K1, K2>> iterator(){\r\n    return new TwoDimensionalSetIterator(this);\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.WordToSentenceProcessor.isForcedEndToken",
	"Comment": "this is a sort of hacked in other way to end sentences. tokens with the forcedsentenceendannotation set to true will also end a sentence.",
	"Method": "boolean isForcedEndToken(Object o){\r\n    if (o instanceof CoreMap) {\r\n        Boolean forcedEndValue = ((CoreMap) o).get(CoreAnnotations.ForcedSentenceEndAnnotation.class);\r\n        String originalText = ((CoreMap) o).get(CoreAnnotations.OriginalTextAnnotation.class);\r\n        return (forcedEndValue != null && forcedEndValue) || (originalText != null && originalText.equals(\"?\"));\r\n    } else {\r\n        return false;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.optimize.solvers.BaseOptimizer.optimize",
	"Comment": "todo add flag to allow retaining state between mini batches and when to apply updates",
	"Method": "boolean optimize(LayerWorkspaceMgr workspaceMgr){\r\n    INDArray gradient;\r\n    INDArray searchDirection;\r\n    INDArray parameters;\r\n    Pair<Gradient, Double> pair = gradientAndScore(workspaceMgr);\r\n    if (searchState.isEmpty()) {\r\n        searchState.put(GRADIENT_KEY, pair.getFirst().gradient());\r\n        try (MemoryWorkspace ws = Nd4j.getWorkspaceManager().scopeOutOfWorkspaces()) {\r\n            setupSearchState(pair);\r\n        }\r\n    } else {\r\n        searchState.put(GRADIENT_KEY, pair.getFirst().gradient());\r\n    }\r\n    try (MemoryWorkspace ws = Nd4j.getWorkspaceManager().scopeOutOfWorkspaces()) {\r\n        preProcessLine();\r\n    }\r\n    gradient = (INDArray) searchState.get(GRADIENT_KEY);\r\n    searchDirection = (INDArray) searchState.get(SEARCH_DIR);\r\n    parameters = (INDArray) searchState.get(PARAMS_KEY);\r\n    try {\r\n        step = lineMaximizer.optimize(parameters, gradient, searchDirection, workspaceMgr);\r\n    } catch (InvalidStepException e) {\r\n        log.warn(\"Invalid step...continuing another iteration: {}\", e.getMessage());\r\n        step = 0.0;\r\n    }\r\n    if (step != 0.0) {\r\n        stepFunction.step(parameters, searchDirection, step);\r\n        model.setParams(parameters);\r\n    } else {\r\n        log.debug(\"Step size returned by line search is 0.0.\");\r\n    }\r\n    pair = gradientAndScore(workspaceMgr);\r\n    try (MemoryWorkspace ws = Nd4j.getWorkspaceManager().scopeOutOfWorkspaces()) {\r\n        postStep(pair.getFirst().gradient());\r\n    }\r\n    int iterationCount = BaseOptimizer.getIterationCount(model);\r\n    int epochCount = BaseOptimizer.getEpochCount(model);\r\n    try (MemoryWorkspace workspace = Nd4j.getMemoryManager().scopeOutOfWorkspaces()) {\r\n        for (TrainingListener listener : trainingListeners) listener.iterationDone(model, iterationCount, epochCount);\r\n    }\r\n    incrementIterationCount(model, 1);\r\n    applyConstraints(model);\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.updateTokens",
	"Comment": "update each token in the sentence with the given information.",
	"Method": "void updateTokens(List<CoreLabel> tokens,Consumer<Pair<CoreNLPProtos.Token.Builder, E>> setter,Function<CoreLabel, E> getter){\r\n    synchronized (this.impl) {\r\n        for (int i = 0; i < tokens.size(); ++i) {\r\n            E value = getter.apply(tokens.get(i));\r\n            if (value != null) {\r\n                setter.accept(Pair.makePair(tokensBuilders.get(i), value));\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.KerasModel.getComputationGraph",
	"Comment": "build a computationgraph from this keras model configuration and import weights.",
	"Method": "ComputationGraph getComputationGraph(ComputationGraph getComputationGraph,boolean importWeights){\r\n    ComputationGraph model = new ComputationGraph(getComputationGraphConfiguration());\r\n    model.init();\r\n    if (importWeights)\r\n        model = (ComputationGraph) KerasModelUtils.copyWeightsToModel(model, this.layers);\r\n    return model;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.toSortedString",
	"Comment": "returns a string representation of a counter, displaying the keys and theircounts in decreasing order of count. at most k keys are displayed.",
	"Method": "String toSortedString(Counter<T> counter,int k,String itemFormat,String joiner,String wrapperFormat,String toSortedString,Counter<T> counter,int k,String itemFormat,String joiner){\r\n    return toSortedString(counter, k, itemFormat, joiner, \"%s\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.tryFile",
	"Comment": "checks for the existence of the block associated with the key",
	"Method": "boolean tryFile(Object key){\r\n    try {\r\n        return hash2file(key.hashCode(), false).exists();\r\n    } catch (IOException e) {\r\n        throw throwSafe(e);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.optimize.solvers.accumulation.EncodedGradientsAccumulatorTest.testStore1",
	"Comment": "this test ensures, that memory amount assigned to buffer is enough for any number of updates",
	"Method": "void testStore1(){\r\n    int numParams = 100000;\r\n    int[] workers = new int[] { 2, 4, 8 };\r\n    for (int numWorkers : workers) {\r\n        EncodingHandler handler = new EncodingHandler(new FixedThresholdAlgorithm(1e-3), null, null, false);\r\n        val bufferSize = EncodedGradientsAccumulator.getOptimalBufferSize(numParams, numWorkers, 2);\r\n        log.info(\"Workers: {}; Buffer size: {} bytes\", numWorkers, bufferSize);\r\n        EncodedGradientsAccumulator accumulator = new EncodedGradientsAccumulator(numWorkers, handler, bufferSize, 2, null, false);\r\n        for (int e = 10; e < numParams / 10; e++) {\r\n            INDArray encoded = handler.encodeUpdates(0, 0, getGradients(numParams, e, 2e-3));\r\n            accumulator.receiveUpdate(encoded);\r\n            for (int i = 0; i < accumulator.messages.size(); i++) {\r\n                accumulator.messages.get(i).clear();\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.functions.DifferentialFunction.resolvePropertiesFromSameDiffBeforeExecution",
	"Comment": "resolve properties and arguments right before execution ofthis operation.",
	"Method": "void resolvePropertiesFromSameDiffBeforeExecution(){\r\n    val properties = sameDiff.propertiesToResolveForFunction(this);\r\n    val fields = DifferentialFunctionClassHolder.getInstance().getFieldsForFunction(this);\r\n    val currentFields = this.propertiesForFunction();\r\n    for (val property : properties) {\r\n        if (!fields.containsKey(property))\r\n            continue;\r\n        val var = sameDiff.getVarNameForFieldAndFunction(this, property);\r\n        val fieldType = fields.get(property);\r\n        val varArr = sameDiff.getArrForVarName(var);\r\n        if (currentFields.containsKey(property)) {\r\n            continue;\r\n        }\r\n        if (varArr == null) {\r\n            throw new ND4JIllegalStateException(\"Unable to set null array!\");\r\n        }\r\n        if (fieldType.getType().equals(int[].class)) {\r\n            setValueFor(fieldType, varArr.data().asInt());\r\n        } else if (fieldType.equals(double[].class)) {\r\n            setValueFor(fieldType, varArr.data().asDouble());\r\n        } else if (fieldType.equals(int.class)) {\r\n            setValueFor(fieldType, varArr.getInt(0));\r\n        } else if (fieldType.equals(double.class)) {\r\n            setValueFor(fieldType, varArr.getDouble(0));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.transform.metadata.CategoricalMetaData.isValid",
	"Comment": "is the given object valid for this column,given the column type and anyrestrictions given by thecolumnmetadata object?",
	"Method": "boolean isValid(Writable writable,boolean isValid,Object input){\r\n    return stateNamesSet.contains(input.toString());\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArraySetTest.testAdd",
	"Comment": "tests the set add function.note that add is probably already tested by the combination ofsetup and testequals.",
	"Method": "void testAdd(){\r\n    assertTrue(set.contains(5));\r\n    assertFalse(set.contains(4));\r\n    for (int i = 0; i < 11; ++i) {\r\n        set.add(i);\r\n    }\r\n    assertEquals(11, set.size());\r\n    assertTrue(set.contains(5));\r\n    assertTrue(set.contains(4));\r\n}"
}, {
	"Path": "org.deeplearning4j.text.documentiterator.LabelsSource.storeLabel",
	"Comment": "this method is intended for storing labels retrieved from external sources.",
	"Method": "void storeLabel(String label){\r\n    if (labels == null)\r\n        labels = new ArrayList();\r\n    if (!uniq.contains(label)) {\r\n        uniq.add(label);\r\n        labels.add(label);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraHeadFinder.isLabelAnnotationIntroducingCharacter",
	"Comment": "say whether this character is an annotation introducingcharacter.",
	"Method": "boolean isLabelAnnotationIntroducingCharacter(char ch){\r\n    char[] cutChars = tlp.labelAnnotationIntroducingCharacters();\r\n    for (char cutChar : cutChars) {\r\n        if (ch == cutChar) {\r\n            return true;\r\n        }\r\n    }\r\n    if (ch == '-')\r\n        return true;\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Maps.addAll",
	"Comment": "adds all of the keys in from to to,applying function to the values to transform themfrom v2 to v1.",
	"Method": "void addAll(Map<K, V1> to,Map<K, V2> from,Function<V2, V1> function){\r\n    for (Map.Entry<K, V2> entry : from.entrySet()) {\r\n        to.put(entry.getKey(), function.apply(entry.getValue()));\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.PreferencesPanel.makeColorButton",
	"Comment": "makes a color choosing button that displays only an icon with a square of the given color",
	"Method": "JButton makeColorButton(String promptText,Color iconColor,JPanel parent){\r\n    final ColorIcon icon = new ColorIcon(iconColor);\r\n    final JButton button = new JButton(icon);\r\n    button.addActionListener(arg0 -> {\r\n        Color newColor = JColorChooser.showDialog(parent, promptText, icon.getColor());\r\n        if (newColor != null) {\r\n            icon.setColor(newColor);\r\n            parent.repaint();\r\n        }\r\n    });\r\n    return button;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.paragraphvectors.ParagraphVectors.predictSeveral",
	"Comment": "predict several labels based on the document.computes a similarity wrt the mean of therepresentation of words in the document",
	"Method": "Collection<String> predictSeveral(LabelledDocument document,int limit,Collection<String> predictSeveral,String rawText,int limit,Collection<String> predictSeveral,List<VocabWord> document,int limit){\r\n    if (document.isEmpty())\r\n        throw new IllegalStateException(\"Document has no words inside\");\r\n    INDArray docMean = inferVector(document);\r\n    Counter<String> distances = new Counter();\r\n    for (String s : labelsSource.getLabels()) {\r\n        INDArray otherVec = getWordVectorMatrix(s);\r\n        double sim = Transforms.cosineSim(docMean, otherVec);\r\n        log.debug(\"Similarity inside: [\" + s + \"] -> \" + sim);\r\n        distances.incrementCount(s, (float) sim);\r\n    }\r\n    return distances.keySetSorted().subList(0, limit);\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Distribution.getDistributionFromLogValues",
	"Comment": "creates a distribution from the given counter, ie makes an internalcopy of the counter and divides all counts by the total count.",
	"Method": "Distribution<E> getDistributionFromLogValues(Counter<E> counter){\r\n    Counter<E> c = new ClassicCounter();\r\n    double max = Counters.max(counter);\r\n    for (E key : counter.keySet()) {\r\n        double count = Math.exp(counter.getCount(key) - max);\r\n        c.setCount(key, count);\r\n    }\r\n    return getDistribution(c);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.recurrent.KerasRnnUtils.getUnrollRecurrentLayer",
	"Comment": "get unroll parameter to decide whether to unroll rnn with bptt or not.",
	"Method": "boolean getUnrollRecurrentLayer(KerasLayerConfiguration conf,Map<String, Object> layerConfig){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (!innerConfig.containsKey(conf.getLAYER_FIELD_UNROLL()))\r\n        throw new InvalidKerasConfigurationException(\"Keras LSTM layer config missing \" + conf.getLAYER_FIELD_UNROLL() + \" field\");\r\n    return (boolean) innerConfig.get(conf.getLAYER_FIELD_UNROLL());\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.argmin",
	"Comment": "argmin array reduction operation, optionally along specified dimensions.output values are the index of the minimum value of each slice along the specified dimension",
	"Method": "SDVariable argmin(SDVariable in,int dimensions,SDVariable argmin,SDVariable in,boolean keepDims,int dimensions,SDVariable argmin,String name,SDVariable in,int dimensions,SDVariable argmin,String name,SDVariable in,boolean keepDims,int dimensions){\r\n    SDVariable ret = f().argmin(in, keepDims, dimensions);\r\n    return updateVariableNameAndReference(ret, name);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.SemanticHeadFinder.hasPassiveProgressiveAuxiliary",
	"Comment": "now overly complex so it deals with coordinations.maybe change this class to use tregrex?f",
	"Method": "boolean hasPassiveProgressiveAuxiliary(Tree[] kids){\r\n    if (DEBUG) {\r\n        log.info(\"Checking for passive/progressive auxiliary\");\r\n    }\r\n    boolean foundPassiveVP = false;\r\n    boolean foundPassiveAux = false;\r\n    for (Tree kid : kids) {\r\n        if (DEBUG) {\r\n            log.info(\"  checking in \" + kid);\r\n        }\r\n        if (isVerbalAuxiliary(kid, passiveAuxiliaries, false)) {\r\n            foundPassiveAux = true;\r\n        } else if (kid.isPhrasal()) {\r\n            Label kidLabel = kid.label();\r\n            String cat = null;\r\n            if (kidLabel instanceof HasCategory) {\r\n                cat = ((HasCategory) kidLabel).category();\r\n            }\r\n            if (cat == null) {\r\n                cat = kid.value();\r\n            }\r\n            if (!cat.startsWith(\"VP\")) {\r\n                continue;\r\n            }\r\n            if (DEBUG) {\r\n                log.info(\"hasPassiveProgressiveAuxiliary found VP\");\r\n            }\r\n            Tree[] kidkids = kid.children();\r\n            boolean foundParticipleInVp = false;\r\n            for (Tree kidkid : kidkids) {\r\n                if (DEBUG) {\r\n                    log.info(\"  hasPassiveProgressiveAuxiliary examining \" + kidkid);\r\n                }\r\n                if (kidkid.isPreTerminal()) {\r\n                    Label kidkidLabel = kidkid.label();\r\n                    String tag = null;\r\n                    if (kidkidLabel instanceof HasTag) {\r\n                        tag = ((HasTag) kidkidLabel).tag();\r\n                    }\r\n                    if (tag == null) {\r\n                        tag = kidkid.value();\r\n                    }\r\n                    if (\"VBN\".equals(tag) || \"VBG\".equals(tag) || \"VBD\".equals(tag)) {\r\n                        foundPassiveVP = true;\r\n                        if (DEBUG) {\r\n                            log.info(\"hasPassiveAuxiliary found VBN/VBG/VBD VP\");\r\n                        }\r\n                        break;\r\n                    } else if (\"CC\".equals(tag) && foundParticipleInVp) {\r\n                        foundPassiveVP = true;\r\n                        if (DEBUG) {\r\n                            log.info(\"hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CC\");\r\n                        }\r\n                        break;\r\n                    }\r\n                } else if (kidkid.isPhrasal()) {\r\n                    String catcat = null;\r\n                    if (kidLabel instanceof HasCategory) {\r\n                        catcat = ((HasCategory) kidLabel).category();\r\n                    }\r\n                    if (catcat == null) {\r\n                        catcat = kid.value();\r\n                    }\r\n                    if (\"VP\".equals(catcat)) {\r\n                        if (DEBUG) {\r\n                            log.info(\"hasPassiveAuxiliary found (VP (VP)), recursing\");\r\n                        }\r\n                        foundParticipleInVp = vpContainsParticiple(kidkid);\r\n                    } else if ((\"CONJP\".equals(catcat) || \"PRN\".equals(catcat)) && foundParticipleInVp) {\r\n                        foundPassiveVP = true;\r\n                        if (DEBUG) {\r\n                            log.info(\"hasPassiveAuxiliary [coordination] found (VP (VP[VBN/VBG/VBD] CONJP\");\r\n                        }\r\n                        break;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        if (foundPassiveAux && foundPassiveVP) {\r\n            break;\r\n        }\r\n    }\r\n    if (DEBUG) {\r\n        log.info(\"hasPassiveProgressiveAuxiliary returns \" + (foundPassiveAux && foundPassiveVP));\r\n    }\r\n    return foundPassiveAux && foundPassiveVP;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.feedforward.autoencoder.recursive.Tree.connect",
	"Comment": "connects the given treesand sets the parents of the children",
	"Method": "void connect(List<Tree> children){\r\n    this.children = children;\r\n    for (Tree t : children) t.setParent(this);\r\n}"
}, {
	"Path": "edu.stanford.nlp.sentiment.SentimentPipeline.outputTreeVectors",
	"Comment": "outputs the vectors from the tree.counts the tree nodes thesame as setindexlabels.",
	"Method": "int outputTreeVectors(PrintStream out,Tree tree,int index){\r\n    if (tree.isLeaf()) {\r\n        return index;\r\n    }\r\n    out.print(\"  \" + index + ':');\r\n    SimpleMatrix vector = RNNCoreAnnotations.getNodeVector(tree);\r\n    for (int i = 0; i < vector.getNumElements(); ++i) {\r\n        out.print(\"  \" + NF.format(vector.get(i)));\r\n    }\r\n    out.println();\r\n    index++;\r\n    for (Tree child : tree.children()) {\r\n        index = outputTreeVectors(out, child, index);\r\n    }\r\n    return index;\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator.getLabelsArray",
	"Comment": "get the label assignments for the given set as a character array.",
	"Method": "char[] getLabelsArray(Set dataSet){\r\n    switch(dataSet) {\r\n        case COMPLETE:\r\n            return LABELS_COMPLETE;\r\n        case MERGE:\r\n            return LABELS_MERGE;\r\n        case BALANCED:\r\n            return LABELS_BALANCED;\r\n        case LETTERS:\r\n            return LABELS_LETTERS;\r\n        case DIGITS:\r\n        case MNIST:\r\n            return LABELS_DIGITS;\r\n        default:\r\n            throw new UnsupportedOperationException(\"Unknown Set: \" + dataSet);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.powNormalized",
	"Comment": "returns a counter where each element corresponds to the normalized count ofthe corresponding element in c raised to the given power.",
	"Method": "Counter<E> powNormalized(Counter<E> c,double temp){\r\n    Counter<E> d = c.getFactory().create();\r\n    double total = c.totalCount();\r\n    for (E e : c.keySet()) {\r\n        d.setCount(e, Math.pow(c.getCount(e) / total, temp));\r\n    }\r\n    return d;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.tuebadz.TueBaDZPennTreeNormalizer.normalizeTerminal",
	"Comment": "normalizes a leaf contents.this implementation interns the leaf.",
	"Method": "String normalizeTerminal(String leaf){\r\n    return leaf.intern();\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.hasArgs",
	"Comment": "returns true if this function already has defined arguments",
	"Method": "boolean hasArgs(DifferentialFunction function){\r\n    String[] vertexIdArgs = incomingArgsReverse.get(function.getOwnName());\r\n    return vertexIdArgs != null && vertexIdArgs.length > 0;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.functions.DifferentialFunctionFactory.matchCondition",
	"Comment": "returns a boolean mask of equal shape to the input, where the condition is satisfied",
	"Method": "SDVariable matchCondition(SDVariable in,Condition condition){\r\n    return new MatchConditionTransform(sameDiff(), in, condition).outputVariable();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.BaseOutputLayer.computeScoreForExamples",
	"Comment": "compute the score for each example individually, after labels and input have been set.",
	"Method": "INDArray computeScoreForExamples(double fullNetworkL1,double fullNetworkL2,LayerWorkspaceMgr workspaceMgr){\r\n    if (input == null || labels == null)\r\n        throw new IllegalStateException(\"Cannot calculate score without input and labels \" + layerId());\r\n    INDArray preOut = preOutput2d(false, workspaceMgr);\r\n    ILossFunction lossFunction = layerConf().getLossFn();\r\n    INDArray scoreArray = lossFunction.computeScoreArray(getLabels2d(workspaceMgr, ArrayType.FF_WORKING_MEM), preOut, layerConf().getActivationFn(), maskArray);\r\n    double l1l2 = fullNetworkL1 + fullNetworkL2;\r\n    if (l1l2 != 0.0) {\r\n        scoreArray.addi(l1l2);\r\n    }\r\n    return workspaceMgr.leverageTo(ArrayType.ACTIVATIONS, scoreArray);\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.determinationCoefficient",
	"Comment": "this returns the determination coefficient of two vectors given a length",
	"Method": "double determinationCoefficient(double[] y1,double[] y2,int n){\r\n    return Math.pow(correlation(y1, y2), 2);\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.ROCMultiClass.merge",
	"Comment": "merge this rocmulticlass instance with another.this rocmulticlass instance is modified, by adding the stats from the other instance.",
	"Method": "void merge(ROCMultiClass other){\r\n    if (this.underlying == null) {\r\n        this.underlying = other.underlying;\r\n        return;\r\n    } else if (other.underlying == null) {\r\n        return;\r\n    }\r\n    if (underlying.length != other.underlying.length) {\r\n        throw new UnsupportedOperationException(\"Cannot merge ROCBinary: this expects \" + underlying.length + \"outputs, other expects \" + other.underlying.length + \" outputs\");\r\n    }\r\n    for (int i = 0; i < underlying.length; i++) {\r\n        this.underlying[i].merge(other.underlying[i]);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.data.DataSetIteratorFactoryProvider.testData",
	"Comment": "get training data given some parameters for the data. data parameters mapis used to specify things like batchsize data preprocessing",
	"Method": "DataSetIteratorFactory testData(Map<String, Object> dataParameters){\r\n    return create(dataParameters);\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.getNamedAnnotators",
	"Comment": "this function defines the list of named annotators in corenlp, along with how to constructthem.",
	"Method": "Map<String, BiFunction<Properties, AnnotatorImplementations, Annotator>> getNamedAnnotators(){\r\n    Map<String, BiFunction<Properties, AnnotatorImplementations, Annotator>> pool = new HashMap();\r\n    pool.put(STANFORD_TOKENIZE, (props, impl) -> impl.tokenizer(props));\r\n    pool.put(STANFORD_CLEAN_XML, (props, impl) -> impl.cleanXML(props));\r\n    pool.put(STANFORD_SSPLIT, (props, impl) -> impl.wordToSentences(props));\r\n    pool.put(STANFORD_POS, (props, impl) -> impl.posTagger(props));\r\n    pool.put(STANFORD_LEMMA, (props, impl) -> impl.morpha(props, false));\r\n    pool.put(STANFORD_NER, (props, impl) -> impl.ner(props));\r\n    pool.put(STANFORD_TOKENSREGEX, (props, impl) -> impl.tokensregex(props, STANFORD_TOKENSREGEX));\r\n    pool.put(STANFORD_REGEXNER, (props, impl) -> impl.tokensRegexNER(props, STANFORD_REGEXNER));\r\n    pool.put(STANFORD_ENTITY_MENTIONS, (props, impl) -> impl.entityMentions(props, STANFORD_ENTITY_MENTIONS));\r\n    pool.put(STANFORD_GENDER, (props, impl) -> impl.gender(props, STANFORD_GENDER));\r\n    pool.put(STANFORD_TRUECASE, (props, impl) -> impl.trueCase(props));\r\n    pool.put(STANFORD_PARSE, (props, impl) -> impl.parse(props));\r\n    pool.put(STANFORD_COREF_MENTION, (props, impl) -> impl.corefMention(props));\r\n    pool.put(STANFORD_DETERMINISTIC_COREF, (props, impl) -> impl.dcoref(props));\r\n    pool.put(STANFORD_COREF, (props, impl) -> impl.coref(props));\r\n    pool.put(STANFORD_RELATION, (props, impl) -> impl.relations(props));\r\n    pool.put(STANFORD_SENTIMENT, (props, impl) -> impl.sentiment(props, STANFORD_SENTIMENT));\r\n    pool.put(STANFORD_COLUMN_DATA_CLASSIFIER, (props, impl) -> impl.columnData(props));\r\n    pool.put(STANFORD_DEPENDENCIES, (props, impl) -> impl.dependencies(props));\r\n    pool.put(STANFORD_NATLOG, (props, impl) -> impl.natlog(props));\r\n    pool.put(STANFORD_OPENIE, (props, impl) -> impl.openie(props));\r\n    pool.put(STANFORD_QUOTE, (props, impl) -> impl.quote(props));\r\n    pool.put(STANFORD_QUOTE_ATTRIBUTION, (props, impl) -> impl.quoteattribution(props));\r\n    pool.put(STANFORD_UD_FEATURES, (props, impl) -> impl.udfeats(props));\r\n    pool.put(STANFORD_LINK, (props, impl) -> impl.link(props));\r\n    pool.put(STANFORD_KBP, (props, impl) -> impl.kbp(props));\r\n    return pool;\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.setStrings",
	"Comment": "set the array of string values for the name property asas comma delimited values.",
	"Method": "void setStrings(String name,String values){\r\n    set(name, StringUtils.arrayToString(values));\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.blas.impl.BaseLevel1.asum",
	"Comment": "computes the sum of magnitudes of all vector elements or, for a complex vector x, the sum",
	"Method": "double asum(INDArray arr,double asum,long n,DataBuffer x,int offsetX,int incrX){\r\n    if (supportsDataBufferL1Ops()) {\r\n        if (x.dataType() == DataBuffer.Type.FLOAT) {\r\n            return sasum(n, x, offsetX, incrX);\r\n        } else if (x.dataType() == DataBuffer.Type.DOUBLE) {\r\n            return dasum(n, x, offsetX, incrX);\r\n        } else {\r\n            return hasum(n, x, offsetX, incrX);\r\n        }\r\n    } else {\r\n        long[] shapex = { 1, n };\r\n        long[] stridex = { incrX, incrX };\r\n        INDArray arrX = Nd4j.create(x, shapex, stridex, offsetX, 'c');\r\n        return asum(arrX);\r\n    }\r\n}"
}, {
	"Path": "org.datavec.nlp.movingwindow.Windows.windows",
	"Comment": "constructs a list of window of size windowsize.note that padding for each window is created as well.",
	"Method": "List<Window> windows(InputStream words,int windowSize,List<Window> windows,InputStream words,TokenizerFactory tokenizerFactory,int windowSize,List<Window> windows,String words,int windowSize,List<Window> windows,String words,TokenizerFactory tokenizerFactory,int windowSize,List<Window> windows,String words,List<Window> windows,String words,TokenizerFactory tokenizerFactory,List<Window> windows,List<String> words,int windowSize){\r\n    List<Window> ret = new ArrayList();\r\n    for (int i = 0; i < words.size(); i++) ret.add(windowForWordInPosition(windowSize, i, words));\r\n    return ret;\r\n}"
}, {
	"Path": "org.datavec.image.loader.ImageLoader.toRaveledTensor",
	"Comment": "convert an image in to a raveled tensor ofthe bgr values of the image",
	"Method": "INDArray toRaveledTensor(File file,INDArray toRaveledTensor,InputStream is,INDArray toRaveledTensor,BufferedImage image){\r\n    try {\r\n        image = scalingIfNeed(image, false);\r\n        return toINDArrayBGR(image).ravel();\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(\"Unable to load image\", e);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.validation.OpValidation.excludeFromTfImportCoverage",
	"Comment": "these ops are excluded from tf import test coverage, for various reasons",
	"Method": "Set<String> excludeFromTfImportCoverage(){\r\n    List<String> list = // Can be excluded because \"Reverse_v2\" is synonym that TF uses with tf.reverse(...); ReverseV2 is also Java op that is synonym for same op\r\n    Arrays.asList(\"Reverse\", \"LogSigmoid\", \"HardSigmoid\", \"SpaceToBatch\", \"BatchToSpace\", \"Pad\", \"TopK\", \"InTopK\", \"BatchMatrixDeterminant\", \"BatchMatrixDiagPart\", \"BatchMatrixDiag\", \"BatchMatrixBandPart\", \"BatchMatrixInverse\", \"BatchMatrixSetDiag\", \"BatchMatrixSolve\", \"BatchMatrixSolveLs\", \"BatchMatrixTriangularSolve\", \"BatchSelfAdjointEig\", \"BatchSelfAdjointEigV2\", \"BatchSvd\", // This is a thing, but quite different from our op: https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/bayesflow/entropy/entropy_shannon\r\n    \"HardTanh\", // This is a thing, but quite different from our op: https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/bayesflow/entropy/entropy_shannon\r\n    \"Swish\", // This is a thing, but quite different from our op: https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/bayesflow/entropy/entropy_shannon\r\n    \"RDiv\", // This is a thing, but quite different from our op: https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/bayesflow/entropy/entropy_shannon\r\n    \"DivScalar\", // This is a thing, but quite different from our op: https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/bayesflow/entropy/entropy_shannon\r\n    \"LogX\", // This is a thing, but quite different from our op: https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/bayesflow/entropy/entropy_shannon\r\n    \"RationalTanh\", // This is a thing, but quite different from our op: https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/bayesflow/entropy/entropy_shannon\r\n    \"absargmax\", \"absargmin\", \"entropy_shannon\", \"count_zero\");\r\n    return new HashSet(list);\r\n}"
}, {
	"Path": "org.deeplearning4j.ui.weights.ConvolutionalIterationListener.rasterizeConvoLayers",
	"Comment": "we visualize set of tensors as vertically aligned set of patches",
	"Method": "BufferedImage rasterizeConvoLayers(List<INDArray> tensors3D,BufferedImage sourceImage){\r\n    long width = 0;\r\n    long height = 0;\r\n    int border = 1;\r\n    int padding_row = 2;\r\n    int padding_col = 80;\r\n    val shape = tensors3D.get(0).shape();\r\n    val numImages = shape[0];\r\n    height = (shape[2]);\r\n    width = (shape[1]);\r\n    int maxHeight = 0;\r\n    int totalWidth = 0;\r\n    int iOffset = 1;\r\n    Orientation orientation = Orientation.LANDSCAPE;\r\n    if (tensors3D.size() > 3) {\r\n        orientation = Orientation.PORTRAIT;\r\n    }\r\n    List<BufferedImage> images = new ArrayList();\r\n    for (int layer = 0; layer < tensors3D.size(); layer++) {\r\n        INDArray tad = tensors3D.get(layer);\r\n        int zoomed = 0;\r\n        BufferedImage image = null;\r\n        if (orientation == Orientation.LANDSCAPE) {\r\n            maxHeight = (int) ((height + (border * 2) + padding_row) * numImages);\r\n            image = renderMultipleImagesLandscape(tad, maxHeight, (int) width, (int) height);\r\n            totalWidth += image.getWidth() + padding_col;\r\n        } else if (orientation == Orientation.PORTRAIT) {\r\n            totalWidth = (int) ((width + (border * 2) + padding_row) * numImages);\r\n            image = renderMultipleImagesPortrait(tad, totalWidth, (int) width, (int) height);\r\n            maxHeight += image.getHeight() + padding_col;\r\n        }\r\n        images.add(image);\r\n    }\r\n    if (orientation == Orientation.LANDSCAPE) {\r\n        totalWidth += padding_col * 2;\r\n    } else if (orientation == Orientation.PORTRAIT) {\r\n        maxHeight += padding_col * 2;\r\n        maxHeight += sourceImage.getHeight() + (padding_col * 2);\r\n    }\r\n    BufferedImage output = new BufferedImage(totalWidth, maxHeight, BufferedImage.TYPE_INT_RGB);\r\n    Graphics2D graphics2D = output.createGraphics();\r\n    graphics2D.setPaint(bgColor);\r\n    graphics2D.fillRect(0, 0, output.getWidth(), output.getHeight());\r\n    BufferedImage singleArrow = null;\r\n    BufferedImage multipleArrows = null;\r\n    try {\r\n        if (orientation == Orientation.LANDSCAPE) {\r\n            try {\r\n                ClassPathResource resource = new ClassPathResource(\"arrow_sing.PNG\");\r\n                ClassPathResource resource2 = new ClassPathResource(\"arrow_mul.PNG\");\r\n                singleArrow = ImageIO.read(resource.getInputStream());\r\n                multipleArrows = ImageIO.read(resource2.getInputStream());\r\n            } catch (Exception e) {\r\n            }\r\n            graphics2D.drawImage(sourceImage, (padding_col / 2) - (sourceImage.getWidth() / 2), (maxHeight / 2) - (sourceImage.getHeight() / 2), null);\r\n            graphics2D.setPaint(borderColor);\r\n            graphics2D.drawRect((padding_col / 2) - (sourceImage.getWidth() / 2), (maxHeight / 2) - (sourceImage.getHeight() / 2), sourceImage.getWidth(), sourceImage.getHeight());\r\n            iOffset += sourceImage.getWidth();\r\n            if (singleArrow != null)\r\n                graphics2D.drawImage(singleArrow, iOffset + (padding_col / 2) - (singleArrow.getWidth() / 2), (maxHeight / 2) - (singleArrow.getHeight() / 2), null);\r\n        } else {\r\n            try {\r\n                ClassPathResource resource = new ClassPathResource(\"arrow_singi.PNG\");\r\n                ClassPathResource resource2 = new ClassPathResource(\"arrow_muli.PNG\");\r\n                singleArrow = ImageIO.read(resource.getInputStream());\r\n                multipleArrows = ImageIO.read(resource2.getInputStream());\r\n            } catch (Exception e) {\r\n            }\r\n            graphics2D.drawImage(sourceImage, (totalWidth / 2) - (sourceImage.getWidth() / 2), (padding_col / 2) - (sourceImage.getHeight() / 2), null);\r\n            graphics2D.setPaint(borderColor);\r\n            graphics2D.drawRect((totalWidth / 2) - (sourceImage.getWidth() / 2), (padding_col / 2) - (sourceImage.getHeight() / 2), sourceImage.getWidth(), sourceImage.getHeight());\r\n            iOffset += sourceImage.getHeight();\r\n            if (singleArrow != null)\r\n                graphics2D.drawImage(singleArrow, (totalWidth / 2) - (singleArrow.getWidth() / 2), iOffset + (padding_col / 2) - (singleArrow.getHeight() / 2), null);\r\n        }\r\n        iOffset += padding_col;\r\n    } catch (Exception e) {\r\n    }\r\n    for (int i = 0; i < images.size(); i++) {\r\n        BufferedImage curImage = images.get(i);\r\n        if (orientation == Orientation.LANDSCAPE) {\r\n            graphics2D.drawImage(curImage, iOffset, 1, null);\r\n            iOffset += curImage.getWidth() + padding_col;\r\n            if (singleArrow != null && multipleArrows != null) {\r\n                if (i < images.size() - 1) {\r\n                    if (multipleArrows != null)\r\n                        graphics2D.drawImage(multipleArrows, iOffset - (padding_col / 2) - (multipleArrows.getWidth() / 2), (maxHeight / 2) - (multipleArrows.getHeight() / 2), null);\r\n                } else {\r\n                }\r\n            }\r\n        } else if (orientation == Orientation.PORTRAIT) {\r\n            graphics2D.drawImage(curImage, 1, iOffset, null);\r\n            iOffset += curImage.getHeight() + padding_col;\r\n            if (singleArrow != null && multipleArrows != null) {\r\n                if (i < images.size() - 1) {\r\n                    if (multipleArrows != null)\r\n                        graphics2D.drawImage(multipleArrows, (totalWidth / 2) - (multipleArrows.getWidth() / 2), iOffset - (padding_col / 2) - (multipleArrows.getHeight() / 2), null);\r\n                } else {\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return output;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.pennchinese.ChineseTreebankLanguagePack.isPunctuationTag",
	"Comment": "accepts a string that is a punctuationtag name, and rejects everything else.",
	"Method": "boolean isPunctuationTag(String str){\r\n    return str.equals(\"PU\");\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.setClassLoader",
	"Comment": "set the class loader that will be used to load the various objects.",
	"Method": "void setClassLoader(ClassLoader classLoader){\r\n    this.classLoader = classLoader;\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.EvaluationBinary.stats",
	"Comment": "get a string representation of the evaluationbinary class, using the specified precision",
	"Method": "String stats(String stats,int printPrecision){\r\n    StringBuilder sb = new StringBuilder();\r\n    int maxLabelsLength = 15;\r\n    if (labels != null) {\r\n        for (String s : labels) {\r\n            maxLabelsLength = Math.max(s.length(), maxLabelsLength);\r\n        }\r\n    }\r\n    String subPattern = \"%-12.\" + printPrecision + \"f\";\r\n    // Label\r\n    String pattern = \"%-\" + (maxLabelsLength + 5) + \"s\" + subPattern + subPattern + subPattern + subPattern + \"%-8d%-7d%-7d%-7d%-7d\";\r\n    String patternHeader = \"%-\" + (maxLabelsLength + 5) + \"s%-12s%-12s%-12s%-12s%-8s%-7s%-7s%-7s%-7s\";\r\n    List<String> headerNames = Arrays.asList(\"Label\", \"Accuracy\", \"F1\", \"Precision\", \"Recall\", \"Total\", \"TP\", \"TN\", \"FP\", \"FN\");\r\n    if (rocBinary != null) {\r\n        patternHeader += \"%-12s\";\r\n        pattern += subPattern;\r\n        headerNames = new ArrayList(headerNames);\r\n        headerNames.add(\"AUC\");\r\n    }\r\n    String header = String.format(patternHeader, headerNames.toArray());\r\n    sb.append(header);\r\n    if (countTrueNegative != null) {\r\n        for (int i = 0; i < countTrueNegative.length; i++) {\r\n            int totalCount = totalCount(i);\r\n            double acc = accuracy(i);\r\n            double f1 = f1(i);\r\n            double precision = precision(i);\r\n            double recall = recall(i);\r\n            String label = (labels == null ? String.valueOf(i) : labels.get(i));\r\n            List<Object> args = Arrays.<Object>asList(label, acc, f1, precision, recall, totalCount, truePositives(i), trueNegatives(i), falsePositives(i), falseNegatives(i));\r\n            if (rocBinary != null) {\r\n                args = new ArrayList(args);\r\n                args.add(rocBinary.calculateAUC(i));\r\n            }\r\n            sb.append(\"\\n\").append(String.format(pattern, args.toArray()));\r\n        }\r\n        if (decisionThreshold != null) {\r\n            sb.append(\"\\nPer-output decision thresholds: \").append(Arrays.toString(decisionThreshold.dup().data().asFloat()));\r\n        }\r\n    } else {\r\n        sb.append(\"\\n-- No Data --\\n\");\r\n    }\r\n    return sb.toString();\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.util.MLLibUtil.fromBinary",
	"Comment": "convert a traditional sc.binaryfilesin to something usable for machine learning",
	"Method": "JavaRDD<LabeledPoint> fromBinary(JavaPairRDD<String, PortableDataStream> binaryFiles,RecordReader reader,JavaRDD<LabeledPoint> fromBinary,JavaRDD<Tuple2<String, PortableDataStream>> binaryFiles,RecordReader reader){\r\n    return fromBinary(JavaPairRDD.fromJavaRDD(binaryFiles), reader);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.SemgrexTest.testComplicatedGraph",
	"Comment": "test that governors, dependents, ancestors, descendants are allreturned with multiplicity 1 if there are multiple paths to thesame node.",
	"Method": "void testComplicatedGraph(){\r\n    SemanticGraph graph = makeComplicatedGraph();\r\n    runTest(\"{} < {word:A}\", graph, \"B\", \"C\", \"D\");\r\n    runTest(\"{} > {word:E}\", graph, \"B\", \"C\", \"D\");\r\n    runTest(\"{} > {word:J}\", graph, \"I\");\r\n    runTest(\"{} < {word:E}\", graph, \"F\", \"G\", \"I\");\r\n    runTest(\"{} < {word:I}\", graph, \"J\");\r\n    runTest(\"{} << {word:A}\", graph, \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\");\r\n    runTest(\"{} << {word:B}\", graph, \"E\", \"F\", \"G\", \"H\", \"I\", \"J\");\r\n    runTest(\"{} << {word:C}\", graph, \"E\", \"F\", \"G\", \"H\", \"I\", \"J\");\r\n    runTest(\"{} << {word:D}\", graph, \"E\", \"F\", \"G\", \"H\", \"I\", \"J\");\r\n    runTest(\"{} << {word:E}\", graph, \"F\", \"G\", \"H\", \"I\", \"J\");\r\n    runTest(\"{} << {word:F}\", graph, \"H\", \"I\", \"J\");\r\n    runTest(\"{} << {word:G}\", graph, \"H\", \"I\", \"J\");\r\n    runTest(\"{} << {word:H}\", graph, \"I\", \"J\");\r\n    runTest(\"{} << {word:I}\", graph, \"J\");\r\n    runTest(\"{} << {word:J}\", graph);\r\n    runTest(\"{} << {word:K}\", graph);\r\n    runTest(\"{} >> {word:A}\", graph);\r\n    runTest(\"{} >> {word:B}\", graph, \"A\");\r\n    runTest(\"{} >> {word:C}\", graph, \"A\");\r\n    runTest(\"{} >> {word:D}\", graph, \"A\");\r\n    runTest(\"{} >> {word:E}\", graph, \"A\", \"B\", \"C\", \"D\");\r\n    runTest(\"{} >> {word:F}\", graph, \"A\", \"B\", \"C\", \"D\", \"E\");\r\n    runTest(\"{} >> {word:G}\", graph, \"A\", \"B\", \"C\", \"D\", \"E\");\r\n    runTest(\"{} >> {word:H}\", graph, \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\");\r\n    runTest(\"{} >> {word:I}\", graph, \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\");\r\n    runTest(\"{} >> {word:J}\", graph, \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\");\r\n    runTest(\"{} >> {word:K}\", graph);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Pair.stringIntern",
	"Comment": "if first and second are strings, then this returns an mutableinternedpairwhere the strings have been interned, and if this pair is serializedand then deserialized, first and second are interned upondeserialization.",
	"Method": "Pair<String, String> stringIntern(Pair<String, String> p){\r\n    return new MutableInternedPair(p);\r\n}"
}, {
	"Path": "org.deeplearning4j.plot.Tsne.hBeta",
	"Comment": "computes a gaussian kernelgiven a vector of squared distance distances",
	"Method": "Pair<Double, INDArray> hBeta(INDArray d,double beta){\r\n    INDArray P = exp(d.neg().muli(beta));\r\n    double sumP = P.sumNumber().doubleValue();\r\n    double logSumP = FastMath.log(sumP);\r\n    Double H = logSumP + ((beta * (d.mul(P).sumNumber().doubleValue())) / sumP);\r\n    P.divi(sumP);\r\n    return new Pair(H, P);\r\n}"
}, {
	"Path": "edu.stanford.nlp.sentiment.SentimentUtils.readTreesWithLabels",
	"Comment": "given a file name, reads in those trees and returns them as a list",
	"Method": "List<Tree> readTreesWithLabels(String path,Class<? extends CoreAnnotation<Integer>> annotationClass){\r\n    List<Tree> trees = Generics.newArrayList();\r\n    MemoryTreebank treebank = new MemoryTreebank(\"utf-8\");\r\n    treebank.loadPath(path, null);\r\n    for (Tree tree : treebank) {\r\n        attachLabels(tree, annotationClass);\r\n        trees.add(tree);\r\n    }\r\n    return trees;\r\n}"
}, {
	"Path": "com.atilika.kuromoji.viterbi.ViterbiBuilder.isAcceptableCandidate",
	"Comment": "check whether a candidate for a glue node is acceptable.the candidate should be as short as possible, but long enough to overlap with the inserted user entry",
	"Method": "boolean isAcceptableCandidate(int targetLength,ViterbiNode glueBase,ViterbiNode candidate){\r\n    return (glueBase == null || candidate.getSurface().length() < glueBase.getSurface().length()) && candidate.getSurface().length() >= targetLength;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.PTB2TextLexer.yypushback",
	"Comment": "pushes the specified amount of characters back into the input stream.they will be read again by then next call of the scanning method",
	"Method": "void yypushback(int number){\r\n    if (number > yylength())\r\n        zzScanError(ZZ_PUSHBACK_2BIG);\r\n    zzMarkedPos -= number;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.isUniformDistribution",
	"Comment": "check if this counter is a uniform distribution.that is, it should sum to 1.0, and every value should be equal to every other value.",
	"Method": "boolean isUniformDistribution(Counter<E> distribution,double tolerance){\r\n    double value = Double.NaN;\r\n    double totalCount = 0.0;\r\n    for (double val : distribution.values()) {\r\n        if (Double.isNaN(value)) {\r\n            value = val;\r\n        }\r\n        if (Math.abs(val - value) > tolerance) {\r\n            return false;\r\n        }\r\n        totalCount += val;\r\n    }\r\n    return Math.abs(totalCount - 1.0) < tolerance;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.FeatureFactory.addAllInterningAndSuffixing",
	"Comment": "makes more complete feature names out of partial feature names, by adding a suffix to the string feature name, adding results to an accumulator",
	"Method": "void addAllInterningAndSuffixing(Collection<String> accumulator,Collection<String> addend,String suffix){\r\n    boolean nonNullSuffix = suffix != null && !suffix.isEmpty();\r\n    if (nonNullSuffix) {\r\n        suffix = '|' + suffix;\r\n    }\r\n    for (String feat : addend) {\r\n        if (nonNullSuffix) {\r\n            feat = feat.concat(suffix);\r\n        }\r\n        accumulator.add(feat);\r\n    }\r\n}"
}, {
	"Path": "org.datavec.spark.transform.Normalization.aggregate",
	"Comment": "aggregate based on an arbitrary listof aggregation and grouping functions",
	"Method": "List<Row> aggregate(DataRowsFacade data,String[] columns,String[] functions){\r\n    String[] rest = new String[columns.length - 1];\r\n    for (int i = 0; i < rest.length; i++) rest[i] = columns[i + 1];\r\n    List<Row> rows = new ArrayList();\r\n    for (String op : functions) {\r\n        Map<String, String> expressions = new ListOrderedMap();\r\n        for (String s : columns) {\r\n            expressions.put(s, op);\r\n        }\r\n        DataRowsFacade aggregated = dataRows(data.get().agg(expressions));\r\n        String[] columns2 = aggregated.get().columns();\r\n        Map<String, String> opReplace = new TreeMap();\r\n        for (String s : columns2) {\r\n            if (s.contains(\"min(\") || s.contains(\"max(\"))\r\n                opReplace.put(s, s.replace(op, \"\").replaceAll(\"[()]\", \"\"));\r\n            else if (s.contains(\"avg\")) {\r\n                opReplace.put(s, s.replace(\"avg\", \"\").replaceAll(\"[()]\", \"\"));\r\n            } else {\r\n                opReplace.put(s, s.replace(op, \"\").replaceAll(\"[()]\", \"\"));\r\n            }\r\n        }\r\n        DataRowsFacade rearranged = null;\r\n        for (Map.Entry<String, String> entries : opReplace.entrySet()) {\r\n            if (rearranged == null) {\r\n                rearranged = dataRows(aggregated.get().withColumnRenamed(entries.getKey(), entries.getValue()));\r\n            } else\r\n                rearranged = dataRows(rearranged.get().withColumnRenamed(entries.getKey(), entries.getValue()));\r\n        }\r\n        rearranged = dataRows(rearranged.get().select(DataFrames.toColumns(columns)));\r\n        rows.addAll(rearranged.get().collectAsList());\r\n    }\r\n    return rows;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sentiment.SentimentPipeline.setIndexLabels",
	"Comment": "sets the labels on the tree to be the indices of the nodes.starts counting at the root and does a postorder traversal.",
	"Method": "int setIndexLabels(Tree tree,int index){\r\n    if (tree.isLeaf()) {\r\n        return index;\r\n    }\r\n    tree.label().setValue(Integer.toString(index));\r\n    index++;\r\n    for (Tree child : tree.children()) {\r\n        index = setIndexLabels(child, index);\r\n    }\r\n    return index;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.SemanticHeadFinder.determineNonTrivialHead",
	"Comment": "determine which daughter of the current parse tree is thehead.it assumes that the daughters already have had theirheads determined.uses special rule for vp heads",
	"Method": "Tree determineNonTrivialHead(Tree t,Tree parent){\r\n    String motherCat = tlp.basicCategory(t.label().value());\r\n    if (DEBUG) {\r\n        log.info(\"At \" + motherCat + \", my parent is \" + parent);\r\n    }\r\n    if (motherCat.equals(\"CONJP\")) {\r\n        for (TregexPattern pattern : headOfConjpTregex) {\r\n            TregexMatcher matcher = pattern.matcher(t);\r\n            if (matcher.matchesAt(t)) {\r\n                return matcher.getNode(\"head\");\r\n            }\r\n        }\r\n    }\r\n    if (motherCat.equals(\"SBARQ\") || motherCat.equals(\"SINV\")) {\r\n        if (!makeCopulaHead) {\r\n            for (TregexPattern pattern : headOfCopulaTregex) {\r\n                TregexMatcher matcher = pattern.matcher(t);\r\n                if (matcher.matchesAt(t)) {\r\n                    return matcher.getNode(\"head\");\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if ((motherCat.equals(\"VP\") || motherCat.equals(\"SQ\") || motherCat.equals(\"SINV\"))) {\r\n        Tree[] kids = t.children();\r\n        if (DEBUG) {\r\n            log.info(\"Semantic head finder: at VP\");\r\n            log.info(\"Class is \" + t.getClass().getName());\r\n            t.pennPrint(System.err);\r\n        }\r\n        Tree[] tmpFilteredChildren = null;\r\n        if (hasVerbalAuxiliary(kids, verbalAuxiliaries, true) || hasPassiveProgressiveAuxiliary(kids)) {\r\n            String[] how;\r\n            if (hasVerbalAuxiliary(kids, copulars, true)) {\r\n                how = new String[] { \"left\", \"VP\", \"ADJP\" };\r\n            } else {\r\n                how = new String[] { \"left\", \"VP\" };\r\n            }\r\n            if (tmpFilteredChildren == null) {\r\n                tmpFilteredChildren = ArrayUtils.filter(kids, REMOVE_TMP_AND_ADV);\r\n            }\r\n            Tree pti = traverseLocate(tmpFilteredChildren, how, false);\r\n            if (DEBUG) {\r\n                log.info(\"Determined head (case 1) for \" + t.value() + \" is: \" + pti);\r\n            }\r\n            if (pti != null) {\r\n                return pti;\r\n            }\r\n        }\r\n        if (hasVerbalAuxiliary(kids, copulars, false) && !isExistential(t, parent) && !isWHQ(t, parent)) {\r\n            String[] how;\r\n            if (motherCat.equals(\"SQ\")) {\r\n                how = new String[] { \"right\", \"VP\", \"ADJP\", \"NP\", \"WHADJP\", \"WHNP\" };\r\n            } else {\r\n                how = new String[] { \"left\", \"VP\", \"ADJP\", \"NP\", \"WHADJP\", \"WHNP\" };\r\n            }\r\n            if (tmpFilteredChildren == null) {\r\n                tmpFilteredChildren = ArrayUtils.filter(kids, REMOVE_TMP_AND_ADV);\r\n            }\r\n            Tree pti = traverseLocate(tmpFilteredChildren, how, false);\r\n            if (motherCat.equals(\"SQ\") && pti != null && pti.label() != null && pti.label().value().startsWith(\"NP\")) {\r\n                boolean foundAnotherNp = false;\r\n                for (Tree kid : kids) {\r\n                    if (kid == pti) {\r\n                        break;\r\n                    } else if (kid.label() != null && kid.label().value().startsWith(\"NP\")) {\r\n                        foundAnotherNp = true;\r\n                        break;\r\n                    }\r\n                }\r\n                if (!foundAnotherNp) {\r\n                    pti = null;\r\n                }\r\n            }\r\n            if (DEBUG) {\r\n                log.info(\"Determined head (case 2) for \" + t.value() + \" is: \" + pti);\r\n            }\r\n            if (pti != null) {\r\n                return pti;\r\n            } else {\r\n                if (DEBUG) {\r\n                    log.info(\"------\");\r\n                    log.info(\"SemanticHeadFinder failed to reassign head for\");\r\n                    t.pennPrint(System.err);\r\n                    log.info(\"------\");\r\n                }\r\n            }\r\n        }\r\n    }\r\n    Tree hd = super.determineNonTrivialHead(t, parent);\r\n    if (DEBUG) {\r\n        log.info(\"Determined head (case 3) for \" + t.value() + \" is: \" + hd);\r\n    }\r\n    return hd;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.core.KerasFlatten.getInputPreprocessor",
	"Comment": "gets appropriate dl4j inputpreprocessor for given inputtypes.",
	"Method": "InputPreProcessor getInputPreprocessor(InputType inputType){\r\n    if (inputType.length > 1)\r\n        throw new InvalidKerasConfigurationException(\"Keras Flatten layer accepts only one input (received \" + inputType.length + \")\");\r\n    InputPreProcessor preprocessor = null;\r\n    if (inputType[0] instanceof InputTypeConvolutional) {\r\n        InputTypeConvolutional it = (InputTypeConvolutional) inputType[0];\r\n        switch(this.getDimOrder()) {\r\n            case NONE:\r\n            case THEANO:\r\n                preprocessor = new CnnToFeedForwardPreProcessor(it.getHeight(), it.getWidth(), it.getChannels());\r\n                break;\r\n            case TENSORFLOW:\r\n                preprocessor = new TensorFlowCnnToFeedForwardPreProcessor(it.getHeight(), it.getWidth(), it.getChannels());\r\n                break;\r\n            default:\r\n                throw new InvalidKerasConfigurationException(\"Unknown Keras backend \" + this.getDimOrder());\r\n        }\r\n    } else if (inputType[0] instanceof InputType.InputTypeRecurrent) {\r\n        InputType.InputTypeRecurrent it = (InputType.InputTypeRecurrent) inputType[0];\r\n        preprocessor = new KerasFlattenRnnPreprocessor(it.getSize(), it.getTimeSeriesLength());\r\n    } else if (inputType[0] instanceof InputType.InputTypeFeedForward) {\r\n        InputType.InputTypeFeedForward it = (InputType.InputTypeFeedForward) inputType[0];\r\n        val inputShape = new long[] { it.getSize() };\r\n        preprocessor = new ReshapePreprocessor(inputShape, inputShape);\r\n    }\r\n    return preprocessor;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.diffEdges",
	"Comment": "given two iterable sequences of edges, returns a pair containing the set ofedges in the first graph not in the second, and edges in the second not in the first.edge equality is determined using an object that implements isemanticgraphedgeeql.",
	"Method": "EdgeDiffResult diffEdges(Collection<SemanticGraphEdge> edges1,Collection<SemanticGraphEdge> edges2,SemanticGraph sg1,SemanticGraph sg2,ISemanticGraphEdgeEql compareObj){\r\n    Set<SemanticGraphEdge> remainingEdges1 = Generics.newHashSet();\r\n    Set<SemanticGraphEdge> remainingEdges2 = Generics.newHashSet();\r\n    Set<SemanticGraphEdge> sameEdges = Generics.newHashSet();\r\n    ArrayList<SemanticGraphEdge> edges2Cache = new ArrayList(edges2);\r\n    edge1Loop: for (SemanticGraphEdge edge1 : edges1) {\r\n        for (SemanticGraphEdge edge2 : edges2Cache) {\r\n            if (compareObj.equals(edge1, edge2, sg1, sg2)) {\r\n                sameEdges.add(edge1);\r\n                edges2Cache.remove(edge2);\r\n                continue edge1Loop;\r\n            }\r\n        }\r\n        remainingEdges1.add(edge1);\r\n    }\r\n    ArrayList<SemanticGraphEdge> edges1Cache = new ArrayList(edges1);\r\n    edge2Loop: for (SemanticGraphEdge edge2 : edges2) {\r\n        for (SemanticGraphEdge edge1 : edges1) {\r\n            if (compareObj.equals(edge1, edge2, sg1, sg2)) {\r\n                edges1Cache.remove(edge1);\r\n                continue edge2Loop;\r\n            }\r\n        }\r\n        remainingEdges2.add(edge2);\r\n    }\r\n    return new EdgeDiffResult(sameEdges, remainingEdges1, remainingEdges2);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.functions.DifferentialFunction.diff",
	"Comment": "perform automatic differentiationwrt the input variables",
	"Method": "List<SDVariable> diff(List<SDVariable> i_v1){\r\n    List<SDVariable> vals = doDiff(i_v1);\r\n    if (vals == null) {\r\n        throw new IllegalStateException(\"Error executing diff operation: doDiff returned null for op: \" + this.opName());\r\n    }\r\n    val outputVars = args();\r\n    boolean copied = false;\r\n    for (int i = 0; i < vals.size(); i++) {\r\n        SDVariable var = outputVars[i];\r\n        SDVariable grad = var.getGradient();\r\n        if (grad != null) {\r\n            if (!copied) {\r\n                vals = new ArrayList(vals);\r\n                copied = true;\r\n            }\r\n            SDVariable gradVar = f().add(grad, vals.get(i));\r\n            vals.set(i, gradVar);\r\n            sameDiff.setGradientForVariableName(var.getVarName(), gradVar);\r\n        } else {\r\n            SDVariable gradVar = vals.get(i);\r\n            sameDiff.updateVariableNameAndReference(gradVar, var.getVarName() + \"-grad\");\r\n            sameDiff.setGradientForVariableName(var.getVarName(), gradVar);\r\n            sameDiff.setForwardVariableForVarName(gradVar.getVarName(), var);\r\n        }\r\n    }\r\n    return vals;\r\n}"
}, {
	"Path": "edu.stanford.nlp.wordseg.ChineseSegmenterFeatureFactory.getCliqueFeatures",
	"Comment": "extracts all the features from the input data at a certain index.",
	"Method": "Collection<String> getCliqueFeatures(PaddedList<IN> cInfo,int loc,Clique clique){\r\n    Collection<String> features = Generics.newHashSet();\r\n    if (clique == cliqueC) {\r\n        addAllInterningAndSuffixing(features, featuresC(cInfo, loc), \"C\");\r\n    } else if (clique == cliqueCpC) {\r\n        addAllInterningAndSuffixing(features, featuresCpC(cInfo, loc), \"CpC\");\r\n        addAllInterningAndSuffixing(features, featuresCnC(cInfo, loc - 1), \"CnC\");\r\n    }\r\n    return features;\r\n}"
}, {
	"Path": "org.datavec.api.transform.metadata.LongMetaData.isValid",
	"Comment": "is the given object valid for this column,given the column type and anyrestrictions given by thecolumnmetadata object?",
	"Method": "boolean isValid(Writable writable,boolean isValid,Object input){\r\n    long value;\r\n    try {\r\n        value = Long.parseLong(input.toString());\r\n    } catch (NumberFormatException e) {\r\n        return false;\r\n    }\r\n    if (minAllowedValue != null && value < minAllowedValue)\r\n        return false;\r\n    if (maxAllowedValue != null && value > maxAllowedValue)\r\n        return false;\r\n    return true;\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.util.MLLibUtil.pointOf",
	"Comment": "returns a labeled point of the writableswhere the final item is the point and the rest of the items arefeatures",
	"Method": "LabeledPoint pointOf(Collection<Writable> writables){\r\n    double[] ret = new double[writables.size() - 1];\r\n    int count = 0;\r\n    double target = 0;\r\n    for (Writable w : writables) {\r\n        if (count < writables.size() - 1)\r\n            ret[count++] = Float.parseFloat(w.toString());\r\n        else\r\n            target = Float.parseFloat(w.toString());\r\n    }\r\n    if (target < 0)\r\n        throw new IllegalStateException(\"Target must be >= 0\");\r\n    return new LabeledPoint(target, Vectors.dense(ret));\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.DocumentPreprocessorTest.testXMLElementInText",
	"Comment": "yeah... a bug that failed this test bug not the notintext test was part of the preprocessor for a while.",
	"Method": "void testXMLElementInText(){\r\n    String TAG_IN_TEXT = \"<xml><wood>There are many trees in the woods<\/wood><\/xml>\";\r\n    compareXMLResults(TAG_IN_TEXT, \"wood\", \"There are many trees in the woods\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    final IdentityHashMap<CM, Boolean> references = new IdentityHashMap();\r\n    for (CM map : coremaps) {\r\n        references.put(map, true);\r\n    }\r\n    final Set<Map.Entry<CM, V>> entrySet = new AbstractSet<Map.Entry<CM, V>>() {\r\n        @Override\r\n        public Iterator<Map.Entry<CM, V>> iterator() {\r\n            return new Iterator<Map.Entry<CM, V>>() {\r\n                Iterator<CM> it = coremaps.iterator();\r\n                CM last = null;\r\n                public boolean hasNext() {\r\n                    return it.hasNext();\r\n                }\r\n                public Map.Entry<CM, V> next() {\r\n                    final CM next = it.next();\r\n                    last = next;\r\n                    return new Map.Entry<CM, V>() {\r\n                        public CM getKey() {\r\n                            return next;\r\n                        }\r\n                        public V getValue() {\r\n                            return next.get(valueKey);\r\n                        }\r\n                        public V setValue(V value) {\r\n                            return next.set(valueKey, value);\r\n                        }\r\n                    };\r\n                }\r\n                public void remove() {\r\n                    references.remove(last);\r\n                    it.remove();\r\n                }\r\n            };\r\n        }\r\n        @Override\r\n        public int size() {\r\n            return coremaps.size();\r\n        }\r\n    };\r\n    return new AbstractMap<CM, V>() {\r\n        @Override\r\n        public int size() {\r\n            return coremaps.size();\r\n        }\r\n        @Override\r\n        public boolean containsKey(Object key) {\r\n            return coremaps.contains(key);\r\n        }\r\n        @Override\r\n        public V get(Object key) {\r\n            if (!references.containsKey(key)) {\r\n                return null;\r\n            }\r\n            return ((CoreMap) key).get(valueKey);\r\n        }\r\n        @Override\r\n        public V put(CM key, V value) {\r\n            if (!references.containsKey(key)) {\r\n                coremaps.add(key);\r\n                references.put(key, true);\r\n            }\r\n            return key.set(valueKey, value);\r\n        }\r\n        @Override\r\n        public V remove(Object key) {\r\n            if (!references.containsKey(key)) {\r\n                return null;\r\n            }\r\n            return coremaps.remove(key) ? ((CoreMap) key).get(valueKey) : null;\r\n        }\r\n        @Override\r\n        public Set<Map.Entry<CM, V>> entrySet() {\r\n            return entrySet;\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    return new Iterator<Map.Entry<CM, V>>() {\r\n        Iterator<CM> it = coremaps.iterator();\r\n        CM last = null;\r\n        public boolean hasNext() {\r\n            return it.hasNext();\r\n        }\r\n        public Map.Entry<CM, V> next() {\r\n            final CM next = it.next();\r\n            last = next;\r\n            return new Map.Entry<CM, V>() {\r\n                public CM getKey() {\r\n                    return next;\r\n                }\r\n                public V getValue() {\r\n                    return next.get(valueKey);\r\n                }\r\n                public V setValue(V value) {\r\n                    return next.set(valueKey, value);\r\n                }\r\n            };\r\n        }\r\n        public void remove() {\r\n            references.remove(last);\r\n            it.remove();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    return it.hasNext();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    final CM next = it.next();\r\n    last = next;\r\n    return new Map.Entry<CM, V>() {\r\n        public CM getKey() {\r\n            return next;\r\n        }\r\n        public V getValue() {\r\n            return next.get(valueKey);\r\n        }\r\n        public V setValue(V value) {\r\n            return next.set(valueKey, value);\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    return next;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    return next.get(valueKey);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    return next.set(valueKey, value);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    references.remove(last);\r\n    it.remove();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    return coremaps.size();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    return coremaps.size();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    return coremaps.contains(key);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    if (!references.containsKey(key)) {\r\n        return null;\r\n    }\r\n    return ((CoreMap) key).get(valueKey);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    if (!references.containsKey(key)) {\r\n        coremaps.add(key);\r\n        references.put(key, true);\r\n    }\r\n    return key.set(valueKey, value);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    if (!references.containsKey(key)) {\r\n        return null;\r\n    }\r\n    return coremaps.remove(key) ? ((CoreMap) key).get(valueKey) : null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.asMap",
	"Comment": "returns a view of a collection of coremaps as a map from each coremap tothe value it stores under valuekey. changes to the map are propagateddirectly to the coremaps in the collection and to the collection itself inthe case of removal operations.keys added or removed from the givencollection by anything other than the returned map will leave the mapin an undefined state.",
	"Method": "Map<CM, V> asMap(COLL coremaps,Class<? extends TypesafeMap.Key<V>> valueKey){\r\n    return entrySet;\r\n}"
}, {
	"Path": "org.datavec.api.transform.condition.BooleanCondition.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return conditions[0].outputColumnNames();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Timing.toSecondsString",
	"Comment": "format with one decimal place elapsed milliseconds in seconds.",
	"Method": "String toSecondsString(String toSecondsString,long elapsed){\r\n    return nf.format(((double) elapsed) / MILLISECONDS_TO_SECONDS);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.shape",
	"Comment": "returns the shape of the specified sdvariable as a 1d sdvariable",
	"Method": "SDVariable shape(SDVariable input,SDVariable shape,String name,SDVariable input){\r\n    SDVariable ret = f().shape(input);\r\n    return updateVariableNameAndReference(ret, name);\r\n}"
}, {
	"Path": "org.datavec.api.transform.schema.Schema.infer",
	"Comment": "infers a schema based on the record.the column names are based on indexing.",
	"Method": "Schema infer(List<Writable> record){\r\n    Schema.Builder builder = new Schema.Builder();\r\n    for (int i = 0; i < record.size(); i++) {\r\n        if (record.get(i) instanceof DoubleWritable)\r\n            builder.addColumnDouble(String.valueOf(i));\r\n        else if (record.get(i) instanceof IntWritable)\r\n            builder.addColumnInteger(String.valueOf(i));\r\n        else if (record.get(i) instanceof LongWritable)\r\n            builder.addColumnLong(String.valueOf(i));\r\n        else if (record.get(i) instanceof FloatWritable)\r\n            builder.addColumnFloat(String.valueOf(i));\r\n        else if (record.get(i) instanceof Text) {\r\n            builder.addColumnString(String.valueOf(i));\r\n        } else\r\n            throw new IllegalStateException(\"Illegal writable for infering schema of type \" + record.get(i).getClass().toString() + \" with record \" + record);\r\n    }\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.equalsWithEps",
	"Comment": "this method allows you to compare indarray against other indarray, with variable eps",
	"Method": "boolean equalsWithEps(Object o,double eps){\r\n    Nd4j.getCompressor().autoDecompress(this);\r\n    if (o == null)\r\n        return false;\r\n    if (!(o instanceof INDArray))\r\n        return false;\r\n    INDArray n = (INDArray) o;\r\n    if (n.isSparse()) {\r\n        return n.equals(this);\r\n    }\r\n    if (this.length() != n.length())\r\n        return false;\r\n    if (this.isEmpty() != n.isEmpty())\r\n        return false;\r\n    if (this.isEmpty() && n.isEmpty())\r\n        return true;\r\n    if (isScalar() && n.isScalar()) {\r\n        if (data.dataType() == DataBuffer.Type.FLOAT) {\r\n            double val = getDouble(0);\r\n            double val2 = n.getDouble(0);\r\n            if (Double.isNaN(val) != Double.isNaN(val2))\r\n                return false;\r\n            return Math.abs(val - val2) < eps;\r\n        } else {\r\n            double val = getDouble(0);\r\n            double val2 = n.getDouble(0);\r\n            if (Double.isNaN(val) != Double.isNaN(val2))\r\n                return false;\r\n            return Math.abs(val - val2) < eps;\r\n        }\r\n    } else if (isVector() && n.isVector()) {\r\n        EqualsWithEps op = new EqualsWithEps(this, n, eps);\r\n        Nd4j.getExecutioner().exec(op);\r\n        double diff = op.getFinalResult().doubleValue();\r\n        return diff < 0.5;\r\n    }\r\n    if (!Arrays.equals(this.shape(), n.shape()))\r\n        return false;\r\n    if (!Shape.shapeEquals(shape(), n.shape())) {\r\n        return false;\r\n    }\r\n    if (slices() != n.slices())\r\n        return false;\r\n    if (n.ordering() == ordering()) {\r\n        EqualsWithEps op = new EqualsWithEps(this, n, eps);\r\n        Nd4j.getExecutioner().exec(op);\r\n        double diff = op.getFinalResult().doubleValue();\r\n        return diff < 0.5;\r\n    } else {\r\n        EqualsWithEps op = new EqualsWithEps(this, n, eps);\r\n        Nd4j.getExecutioner().exec(op);\r\n        double diff = op.getFinalResult().doubleValue();\r\n        return diff < 0.5;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.ComputationGraphSpace.fromYaml",
	"Comment": "instantiate a computation graph spacefrom a raw yaml string",
	"Method": "ComputationGraphSpace fromYaml(String yaml){\r\n    try {\r\n        return YamlMapper.getMapper().readValue(yaml, ComputationGraphSpace.class);\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.records.reader.impl.JDBCRecordReaderTest.testResetForwardOnlyShouldFail",
	"Comment": "resetting the record reader when initialized as forward only should fail",
	"Method": "void testResetForwardOnlyShouldFail(){\r\n    try (JDBCRecordReader reader = new JDBCRecordReader(\"SELECT * FROM Coffee\", dataSource)) {\r\n        Configuration conf = new Configuration();\r\n        conf.setInt(JDBCRecordReader.JDBC_RESULTSET_TYPE, ResultSet.TYPE_FORWARD_ONLY);\r\n        reader.initialize(conf, null);\r\n        reader.next();\r\n        reader.reset();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractCollinsHeadFinder.findRightDisHead",
	"Comment": "from right, but search for any of the categories, not by category in turn",
	"Method": "int findRightDisHead(Tree[] daughterTrees,String[] how){\r\n    for (int headIdx = daughterTrees.length - 1; headIdx >= 0; headIdx--) {\r\n        String childCat = tlp.basicCategory(daughterTrees[headIdx].label().value());\r\n        for (int i = 1; i < how.length; i++) {\r\n            if (how[i].equals(childCat)) {\r\n                return headIdx;\r\n            }\r\n        }\r\n    }\r\n    return -1;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.RedwoodConfiguration.output",
	"Comment": "determine where, in the end, console output should go.the default is stdout.",
	"Method": "RedwoodConfiguration output(String method){\r\n    if (method.equalsIgnoreCase(\"stdout\") || method.equalsIgnoreCase(\"out\")) {\r\n        edu.stanford.nlp.util.logging.JavaUtilLoggingAdaptor.adapt();\r\n        this.outputHandler = Redwood.ConsoleHandler.out();\r\n    } else if (method.equalsIgnoreCase(\"stderr\") || method.equalsIgnoreCase(\"err\")) {\r\n        edu.stanford.nlp.util.logging.JavaUtilLoggingAdaptor.adapt();\r\n        this.outputHandler = Redwood.ConsoleHandler.err();\r\n    } else if (method.equalsIgnoreCase(\"java.util.logging\")) {\r\n        edu.stanford.nlp.util.logging.JavaUtilLoggingAdaptor.adapt();\r\n        this.outputHandler = RedirectOutputHandler.fromJavaUtilLogging(Logger.getLogger(\"``error``\"));\r\n    } else {\r\n        throw new IllegalArgumentException(\"Unknown value for log.method\");\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.siblings",
	"Comment": "returns the siblings of this tree node.the siblings are allchildren of the parent of this node except this node.",
	"Method": "List<Tree> siblings(Tree root){\r\n    Tree parent = parent(root);\r\n    if (parent == null) {\r\n        return null;\r\n    }\r\n    List<Tree> siblings = parent.getChildrenAsList();\r\n    siblings.remove(this);\r\n    return siblings;\r\n}"
}, {
	"Path": "org.deeplearning4j.ui.WebReporter.postReport",
	"Comment": "this method immediately sends ui report to specified target using post request",
	"Method": "void postReport(WebTarget target,Entity entity){\r\n    Response resp = target.request(MediaType.APPLICATION_JSON).accept(MediaType.APPLICATION_JSON).post(entity);\r\n    log.debug(\"{}\", resp);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.addListeners",
	"Comment": "this method adds additional traininglistener to existing listeners",
	"Method": "void addListeners(TrainingListener listeners){\r\n    if (this.trainingListeners == null) {\r\n        setListeners(listeners);\r\n        return;\r\n    } else {\r\n        List<TrainingListener> newListeners = new ArrayList(this.trainingListeners);\r\n        Collections.addAll(newListeners, listeners);\r\n        setListeners(newListeners);\r\n    }\r\n    if (solver != null) {\r\n        solver.setListeners(this.trainingListeners);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.ud.UniversalDependenciesFeatureAnnotator.getRelAndIntPronFeatures",
	"Comment": "extracts features from relative and interrogative pronouns.",
	"Method": "HashMap<String, String> getRelAndIntPronFeatures(SemanticGraph sg,IndexedWord word){\r\n    HashMap<String, String> features = new HashMap();\r\n    if (word.tag().startsWith(\"W\")) {\r\n        boolean isRel = false;\r\n        IndexedWord parent = sg.getParent(word);\r\n        if (parent != null) {\r\n            IndexedWord parentParent = sg.getParent(parent);\r\n            if (parentParent != null) {\r\n                SemanticGraphEdge edge = sg.getEdge(parentParent, parent);\r\n                isRel = edge.getRelation().equals(UniversalEnglishGrammaticalRelations.RELATIVE_CLAUSE_MODIFIER);\r\n            }\r\n        }\r\n        if (isRel) {\r\n            features.put(\"PronType\", \"Rel\");\r\n        } else {\r\n            if (word.value().equalsIgnoreCase(\"that\")) {\r\n                features.put(\"PronType\", \"Dem\");\r\n            } else {\r\n                features.put(\"PronType\", \"Int\");\r\n            }\r\n        }\r\n    }\r\n    return features;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasModelBuilder.inputShape",
	"Comment": "provide input shape for keras models that have been compiled without one. dl4jneeds this shape information on import to infer shapes of later layers and doshape validation.",
	"Method": "KerasModelBuilder inputShape(int[] inputShape){\r\n    this.inputShape = inputShape;\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.HashableCoreMap.hashCode",
	"Comment": "provides a hash code based on the immutable keys and values providedto the constructor.",
	"Method": "int hashCode(){\r\n    return hashcode;\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.reloadConfiguration",
	"Comment": "reload configuration from previously added resources.this method will clear all the configuration read from the addedresources, and final parameters. this will make the resources tobe read again before accessing the values. values that are addedvia set methods will overlay values read from the resources.",
	"Method": "void reloadConfiguration(){\r\n    properties = null;\r\n    finalParameters.clear();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.keysAbove",
	"Comment": "returns the set of keys whose counts are at or above the given threshold.this set may have 0 elements but will not be null.",
	"Method": "Set<E> keysAbove(Counter<E> c,double countThreshold){\r\n    Set<E> keys = Generics.newHashSet();\r\n    for (E key : c.keySet()) {\r\n        if (c.getCount(key) >= countThreshold) {\r\n            keys.add(key);\r\n        }\r\n    }\r\n    return (keys);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.backpropGradient",
	"Comment": "calculate the gradient of the network with respect to some external errors.note that this is typically used for things like reinforcement learning, not typical networks that includean outputlayer or rnnoutputlayer",
	"Method": "Gradient backpropGradient(INDArray epsilons){\r\n    if (epsilons == null || epsilons.length != numOutputArrays)\r\n        throw new IllegalArgumentException(\"Invalid input: must have epsilons length equal to number of output arrays\");\r\n    try {\r\n        calcBackpropGradients(true, configuration.getBackpropType() == BackpropType.TruncatedBPTT, epsilons);\r\n        return gradient;\r\n    } catch (OutOfMemoryError e) {\r\n        CrashReportingUtil.writeMemoryCrashDump(this, e);\r\n        throw e;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.learning.impl.elements.RandomUtils.nextDouble",
	"Comment": "returns the next pseudorandom, uniformly distributed float valuebetween 0.0 and 1.0 from the given randomsequence.",
	"Method": "double nextDouble(double nextDouble,Random random){\r\n    return random.nextDouble();\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.Evaluation.falsePositiveRate",
	"Comment": "calculate the average false positive rate across all classes. can specify whether macro or micro averaging should be used",
	"Method": "double falsePositiveRate(int classLabel,double falsePositiveRate,int classLabel,double edgeCase,double falsePositiveRate,double falsePositiveRate,EvaluationAveraging averaging){\r\n    int nClasses = confusion().getClasses().size();\r\n    if (averaging == EvaluationAveraging.Macro) {\r\n        double macroFPR = 0.0;\r\n        for (int i = 0; i < nClasses; i++) {\r\n            macroFPR += falsePositiveRate(i);\r\n        }\r\n        macroFPR /= nClasses;\r\n        return macroFPR;\r\n    } else if (averaging == EvaluationAveraging.Micro) {\r\n        long fpCount = 0;\r\n        long tnCount = 0;\r\n        for (int i = 0; i < nClasses; i++) {\r\n            fpCount += falsePositives.getCount(i);\r\n            tnCount += trueNegatives.getCount(i);\r\n        }\r\n        return EvaluationUtils.falsePositiveRate(fpCount, tnCount, DEFAULT_EDGE_VALUE);\r\n    } else {\r\n        throw new UnsupportedOperationException(\"Unknown averaging approach: \" + averaging);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.transform",
	"Comment": "transformed view of the given iterable.returns the outputof the given function when applied to each element of theiterable.",
	"Method": "Iterable<V> transform(Iterable<K> iterable,Function<? super K, ? extends V> function){\r\n    return () -> {\r\n        return new Iterator<V>() {\r\n            Iterator<K> inner = iterable.iterator();\r\n            public boolean hasNext() {\r\n                return inner.hasNext();\r\n            }\r\n            public V next() {\r\n                return function.apply(inner.next());\r\n            }\r\n            public void remove() {\r\n                inner.remove();\r\n            }\r\n        };\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.transform",
	"Comment": "transformed view of the given iterable.returns the outputof the given function when applied to each element of theiterable.",
	"Method": "Iterable<V> transform(Iterable<K> iterable,Function<? super K, ? extends V> function){\r\n    return inner.hasNext();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.transform",
	"Comment": "transformed view of the given iterable.returns the outputof the given function when applied to each element of theiterable.",
	"Method": "Iterable<V> transform(Iterable<K> iterable,Function<? super K, ? extends V> function){\r\n    return function.apply(inner.next());\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.transform",
	"Comment": "transformed view of the given iterable.returns the outputof the given function when applied to each element of theiterable.",
	"Method": "Iterable<V> transform(Iterable<K> iterable,Function<? super K, ? extends V> function){\r\n    inner.remove();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArrayUtils.removeAt",
	"Comment": "removes the element at the specified index from the array, and returnsa new array containing the remaining elements.if index isinvalid, returns array unchanged.uses reflection to determinethe type of the array and returns an array of the appropriate type.",
	"Method": "double[] removeAt(double[] array,int index,Object[] removeAt,Object[] array,int index){\r\n    if (array == null) {\r\n        return null;\r\n    }\r\n    if (index < 0 || index >= array.length) {\r\n        return array;\r\n    }\r\n    Object[] retVal = (Object[]) Array.newInstance(array[0].getClass(), array.length - 1);\r\n    for (int i = 0; i < array.length; i++) {\r\n        if (i < index) {\r\n            retVal[i] = array[i];\r\n        } else if (i > index) {\r\n            retVal[i - 1] = array[i];\r\n        }\r\n    }\r\n    return retVal;\r\n}"
}, {
	"Path": "org.datavec.local.transforms.LocalTransformExecutor.convertWritableInputToString",
	"Comment": "convert a string time series tothe proper writable set based on the schema.note that this does not use arrow.this just uses normal writable objects.",
	"Method": "List<List<String>> convertWritableInputToString(List<List<Writable>> stringInput,Schema schema){\r\n    List<List<String>> ret = new ArrayList();\r\n    List<List<String>> timeStepAdd = new ArrayList();\r\n    for (int j = 0; j < stringInput.size(); j++) {\r\n        List<Writable> record = stringInput.get(j);\r\n        List<String> recordAdd = new ArrayList();\r\n        for (int k = 0; k < record.size(); k++) {\r\n            recordAdd.add(record.get(k).toString());\r\n        }\r\n        timeStepAdd.add(recordAdd);\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.Morpha.yytext",
	"Comment": "returns the text matched by the current regular expression.",
	"Method": "String yytext(){\r\n    return new String(zzBuffer, zzStartRead, zzMarkedPos - zzStartRead);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArrayCoreMap.keySetNotNull",
	"Comment": "return a set of keys such that the value of that key is not null.",
	"Method": "Set<Class<?>> keySetNotNull(){\r\n    Set<Class<?>> mapKeys = new IdentityHashSet();\r\n    for (int i = 0; i < size(); ++i) {\r\n        if (values[i] != null) {\r\n            mapKeys.add(this.keys[i]);\r\n        }\r\n    }\r\n    return mapKeys;\r\n}"
}, {
	"Path": "org.datavec.api.transform.filter.InvalidNumColumns.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return inputSchema.getColumnNames().toArray(new String[inputSchema.numColumns()]);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ConfusionMatrix.setLeftPadSize",
	"Comment": "this sets the lefthand side pad width for displaying the text table.",
	"Method": "void setLeftPadSize(int newPadSize){\r\n    this.leftPadSize = newPadSize;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.sequencevectors.sequence.Sequence.addSequenceLabel",
	"Comment": "adds sequence label. in this case sequence will have multiple labels",
	"Method": "void addSequenceLabel(T label){\r\n    this.labels.add(label);\r\n    if (this.label == null)\r\n        this.label = label;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.execution.input.Operands.addArgument",
	"Comment": "this method allows to pass array to the node identified by numeric id",
	"Method": "Operands addArgument(String id,INDArray array,Operands addArgument,int id,INDArray array,Operands addArgument,int id,int index,INDArray array,Operands addArgument,String name,int id,int index,INDArray array){\r\n    map.put(NodeDescriptor.builder().name(name).id(id).index(index).build(), array);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.MatchesPanel.getMatchedSentences",
	"Comment": "returns all currently displayed sentences in plain text form.",
	"Method": "String getMatchedSentences(){\r\n    StringBuilder sb = new StringBuilder();\r\n    for (int i = 0, sz = list.getModel().getSize(); i < sz; i++) {\r\n        String t = ((TreeFromFile) list.getModel().getElementAt(i)).getLabel().getText();\r\n        sb.append(t);\r\n        sb.append(\"\\n\");\r\n    }\r\n    return sb.toString();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.MultiClassAccuracyStats.numCorrect",
	"Comment": "how many correct do we have if we return the most confident num recall ones",
	"Method": "int numCorrect(int recall){\r\n    int correct = 0;\r\n    for (int j = scores.length - 1; j >= scores.length - recall; j--) {\r\n        if (isCorrect[j]) {\r\n            correct++;\r\n        }\r\n    }\r\n    return correct;\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.util.CountClosedTags.addTaggedWords",
	"Comment": "given a line, split it into tagged words and add each word tothe given tagwordmap",
	"Method": "void addTaggedWords(List<TaggedWord> line,Map<String, Set<String>> tagWordMap){\r\n    for (TaggedWord taggedWord : line) {\r\n        String word = taggedWord.word();\r\n        String tag = taggedWord.tag();\r\n        if (closedTags == null || closedTags.contains(tag)) {\r\n            if (!tagWordMap.containsKey(tag)) {\r\n                tagWordMap.put(tag, new TreeSet());\r\n            }\r\n            tagWordMap.get(tag).add(word);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Interval.contains",
	"Comment": "checks whether the point p is contained inside this interval.",
	"Method": "boolean contains(E p,boolean contains,Interval<E> other){\r\n    boolean containsOtherBegin = (other.includesBegin()) ? contains(other.getBegin()) : containsOpen(other.getBegin());\r\n    boolean containsOtherEnd = (other.includesEnd()) ? contains(other.getEnd()) : containsOpen(other.getEnd());\r\n    return (containsOtherBegin && containsOtherEnd);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.weights.WeightInitUtil.reshapeWeights",
	"Comment": "reshape the parameters view, without modifying the paramsview array values.",
	"Method": "INDArray reshapeWeights(int[] shape,INDArray paramsView,INDArray reshapeWeights,long[] shape,INDArray paramsView,INDArray reshapeWeights,int[] shape,INDArray paramsView,char flatteningOrder,INDArray reshapeWeights,long[] shape,INDArray paramsView,char flatteningOrder){\r\n    return paramsView.reshape(flatteningOrder, shape);\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getInt",
	"Comment": "get the value of the name property as an int.if no such property exists, or if the specified value is not a validint, then defaultvalue is returned.",
	"Method": "int getInt(String name,int defaultValue){\r\n    String valueString = get(name);\r\n    if (valueString == null)\r\n        return defaultValue;\r\n    try {\r\n        String hexString = getHexDigits(valueString);\r\n        if (hexString != null) {\r\n            return Integer.parseInt(hexString, 16);\r\n        }\r\n        return Integer.parseInt(valueString);\r\n    } catch (NumberFormatException e) {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.layers.LayerValidation.assertNInNOutSet",
	"Comment": "asserts that the layer nin and nout values are set for the layer",
	"Method": "void assertNInNOutSet(String layerType,String layerName,long layerIndex,long nIn,long nOut){\r\n    if (nIn <= 0 || nOut <= 0) {\r\n        if (layerName == null)\r\n            layerName = \"(name not set)\";\r\n        throw new DL4JInvalidConfigException(layerType + \" (index=\" + layerIndex + \", name=\" + layerName + \") nIn=\" + nIn + \", nOut=\" + nOut + \"; nIn and nOut must be > 0\");\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.hash2file",
	"Comment": "returns a file corresponding to a hash code, ensuring it exists first",
	"Method": "File hash2file(int hashCode,boolean create){\r\n    File candidate = canonicalFile.intern(new File(cacheDir.getCanonicalPath() + File.separator + fileRoot(hashCode) + \".block.ser.gz\").getCanonicalFile());\r\n    if (create) {\r\n        robustCreateFile(candidate);\r\n    }\r\n    return candidate;\r\n}"
}, {
	"Path": "org.nd4j.evaluation.EvaluationUtils.recall",
	"Comment": "calculate the recall from true positive and false negative counts",
	"Method": "double recall(long tpCount,long fnCount,double edgeCase){\r\n    if (tpCount == 0 && fnCount == 0) {\r\n        return edgeCase;\r\n    }\r\n    return tpCount / (double) (tpCount + fnCount);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.tabuDescendants",
	"Comment": "finds the descendents of the given node in graph, avoiding the given set of nodes",
	"Method": "Set<IndexedWord> tabuDescendants(SemanticGraph sg,IndexedWord vertex,Collection<IndexedWord> tabu,Set<IndexedWord> tabuDescendants,SemanticGraph sg,IndexedWord vertex,Collection<IndexedWord> tabu,Collection<GrammaticalRelation> tabuRelns){\r\n    if (!sg.containsVertex(vertex)) {\r\n        throw new IllegalArgumentException();\r\n    }\r\n    Set<IndexedWord> descendantSet = Generics.newHashSet();\r\n    tabuDescendantsHelper(sg, vertex, descendantSet, tabu, tabuRelns, (Predicate<IndexedWord>) null);\r\n    return descendantSet;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.Morpha.loadVerbStemSet",
	"Comment": "loads a list of words from the array and stores them in a hashset.",
	"Method": "Set<String> loadVerbStemSet(String[] verbStems){\r\n    Set<String> set = Generics.newHashSet(verbStems.length);\r\n    Collections.addAll(set, verbStems);\r\n    return Collections.unmodifiableSet(set);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FixedPrioritiesPriorityQueue.clone",
	"Comment": "returns a clone of this priority queue.modifications to one will notaffect modifications to the other.",
	"Method": "FixedPrioritiesPriorityQueue<E> clone(){\r\n    FixedPrioritiesPriorityQueue<E> clonePQ;\r\n    try {\r\n        clonePQ = ErasureUtils.uncheckedCast(super.clone());\r\n    } catch (CloneNotSupportedException cnse) {\r\n        throw new AssertionError(\"Should be able to clone.\");\r\n    }\r\n    clonePQ.elements = new ArrayList(capacity);\r\n    clonePQ.priorities = new double[capacity];\r\n    if (size() > 0) {\r\n        clonePQ.elements.addAll(elements);\r\n        System.arraycopy(priorities, 0, clonePQ.priorities, 0, size());\r\n    }\r\n    return clonePQ;\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.util.ClassPathResource.getInputStream",
	"Comment": "returns requested classpathresource as inputstream object",
	"Method": "InputStream getInputStream(){\r\n    URL url = this.getUrl();\r\n    if (isJarURL(url)) {\r\n        try {\r\n            url = extractActualUrl(url);\r\n            ZipFile zipFile = new ZipFile(url.getFile());\r\n            ZipEntry entry = zipFile.getEntry(this.resourceName);\r\n            if (entry == null) {\r\n                if (this.resourceName.startsWith(\"/\")) {\r\n                    entry = zipFile.getEntry(this.resourceName.replaceFirst(\"/\", \"\"));\r\n                    if (entry == null) {\r\n                        throw new FileNotFoundException(\"Resource \" + this.resourceName + \" not found\");\r\n                    }\r\n                } else\r\n                    throw new FileNotFoundException(\"Resource \" + this.resourceName + \" not found\");\r\n            }\r\n            InputStream stream = zipFile.getInputStream(entry);\r\n            return stream;\r\n        } catch (Exception e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    } else {\r\n        File srcFile = this.getFile();\r\n        return new FileInputStream(srcFile);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.KerasModel.getComputationGraphConfiguration",
	"Comment": "configure a computationgraph from this keras model configuration.",
	"Method": "ComputationGraphConfiguration getComputationGraphConfiguration(){\r\n    if (!this.className.equals(config.getFieldClassNameModel()) && !this.className.equals(config.getFieldClassNameSequential()))\r\n        throw new InvalidKerasConfigurationException(\"Keras model class name \" + this.className + \" incompatible with ComputationGraph\");\r\n    NeuralNetConfiguration.Builder modelBuilder = new NeuralNetConfiguration.Builder();\r\n    if (optimizer != null) {\r\n        modelBuilder.updater(optimizer);\r\n    }\r\n    ComputationGraphConfiguration.GraphBuilder graphBuilder = modelBuilder.graphBuilder();\r\n    graphBuilder.allowDisconnected(true);\r\n    String[] inputLayerNameArray = new String[this.inputLayerNames.size()];\r\n    this.inputLayerNames.toArray(inputLayerNameArray);\r\n    graphBuilder.addInputs(inputLayerNameArray);\r\n    List<InputType> inputTypeList = new ArrayList();\r\n    for (String inputLayerName : this.inputLayerNames) inputTypeList.add(this.layers.get(inputLayerName).getOutputType());\r\n    InputType[] inputTypes = new InputType[inputTypeList.size()];\r\n    inputTypeList.toArray(inputTypes);\r\n    graphBuilder.setInputTypes(inputTypes);\r\n    String[] outputLayerNameArray = new String[this.outputLayerNames.size()];\r\n    this.outputLayerNames.toArray(outputLayerNameArray);\r\n    graphBuilder.setOutputs(outputLayerNameArray);\r\n    Map<String, InputPreProcessor> preprocessors = new HashMap();\r\n    for (KerasLayer layer : this.layersOrdered) {\r\n        List<String> inboundLayerNames = layer.getInboundLayerNames();\r\n        String[] inboundLayerNamesArray = new String[inboundLayerNames.size()];\r\n        inboundLayerNames.toArray(inboundLayerNamesArray);\r\n        List<InputType> inboundTypeList = new ArrayList();\r\n        for (String layerName : inboundLayerNames) inboundTypeList.add(this.outputTypes.get(layerName));\r\n        InputType[] inboundTypeArray = new InputType[inboundTypeList.size()];\r\n        inboundTypeList.toArray(inboundTypeArray);\r\n        InputPreProcessor preprocessor = layer.getInputPreprocessor(inboundTypeArray);\r\n        if (layer.isLayer()) {\r\n            if (preprocessor != null)\r\n                preprocessors.put(layer.getLayerName(), preprocessor);\r\n            graphBuilder.addLayer(layer.getLayerName(), layer.getLayer(), inboundLayerNamesArray);\r\n        } else if (layer.isVertex()) {\r\n            if (preprocessor != null)\r\n                preprocessors.put(layer.getLayerName(), preprocessor);\r\n            graphBuilder.addVertex(layer.getLayerName(), layer.getVertex(), inboundLayerNamesArray);\r\n        } else if (layer.isInputPreProcessor()) {\r\n            if (preprocessor == null)\r\n                throw new UnsupportedKerasConfigurationException(\"Layer \" + layer.getLayerName() + \" could not be mapped to Layer, Vertex, or InputPreProcessor\");\r\n            graphBuilder.addVertex(layer.getLayerName(), new PreprocessorVertex(preprocessor), inboundLayerNamesArray);\r\n        }\r\n    }\r\n    graphBuilder.setInputPreProcessors(preprocessors);\r\n    if (this.useTruncatedBPTT && this.truncatedBPTT > 0)\r\n        graphBuilder.backpropType(BackpropType.TruncatedBPTT).tBPTTForwardLength(truncatedBPTT).tBPTTBackwardLength(truncatedBPTT);\r\n    else\r\n        graphBuilder.backpropType(BackpropType.Standard);\r\n    return graphBuilder.build();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Distribution.distributionFromLogisticCounter",
	"Comment": "maps a counter representing the linear weights of a multiclasslogistic regression model to the probabilities of each class.",
	"Method": "Distribution<E> distributionFromLogisticCounter(Counter<E> cntr){\r\n    double expSum = 0.0;\r\n    int numKeys = 0;\r\n    for (E key : cntr.keySet()) {\r\n        expSum += Math.exp(cntr.getCount(key));\r\n        numKeys++;\r\n    }\r\n    Distribution<E> probs = new Distribution();\r\n    probs.counter = new ClassicCounter();\r\n    probs.reservedMass = 0.0;\r\n    probs.numberOfKeys = numKeys;\r\n    for (E key : cntr.keySet()) {\r\n        probs.counter.setCount(key, Math.exp(cntr.getCount(key)) / expSum);\r\n    }\r\n    return probs;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.mergeCoords",
	"Comment": "this will merge the coordinates of the given coordinate system.",
	"Method": "double[] mergeCoords(double[] x,double[] y,List<Double> mergeCoords,List<Double> x,List<Double> y){\r\n    if (x.size() != y.size())\r\n        throw new IllegalArgumentException(\"Sample sizes must be the same for each data applyTransformToDestination.\");\r\n    List<Double> ret = new ArrayList();\r\n    for (int i = 0; i < x.size(); i++) {\r\n        ret.add(x.get(i));\r\n        ret.add(y.get(i));\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiffOpExecutioner.commit",
	"Comment": "this method ensures all operations that supposed to be executed at this moment, are executed and finished.",
	"Method": "void commit(){\r\n    backendExecutioner.commit();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.convolutional.KerasConvolutionUtils.getPaddingFromConfig",
	"Comment": "get padding and cropping configurations from keras layer configuration.",
	"Method": "int[] getPaddingFromConfig(Map<String, Object> layerConfig,KerasLayerConfiguration conf,String layerField,int dimension){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (!innerConfig.containsKey(layerField))\r\n        throw new InvalidKerasConfigurationException(\"Field \" + layerField + \" not found in Keras cropping or padding layer\");\r\n    int[] padding;\r\n    if (dimension >= 2) {\r\n        List<Integer> paddingList;\r\n        try {\r\n            List paddingNoCast = (List) innerConfig.get(layerField);\r\n            boolean isNested;\r\n            try {\r\n                @SuppressWarnings(\"unchecked\")\r\n                List<Integer> firstItem = (List<Integer>) paddingNoCast.get(0);\r\n                isNested = true;\r\n                paddingList = new ArrayList(2 * dimension);\r\n            } catch (Exception e) {\r\n                int firstItem = (int) paddingNoCast.get(0);\r\n                isNested = false;\r\n                paddingList = new ArrayList(dimension);\r\n            }\r\n            if ((paddingNoCast.size() == dimension) && !isNested) {\r\n                for (int i = 0; i < dimension; i++) paddingList.add((int) paddingNoCast.get(i));\r\n                padding = ArrayUtil.toArray(paddingList);\r\n            } else if ((paddingNoCast.size() == dimension) && isNested) {\r\n                for (int j = 0; j < dimension; j++) {\r\n                    @SuppressWarnings(\"unchecked\")\r\n                    List<Integer> item = (List<Integer>) paddingNoCast.get(0);\r\n                    paddingList.add((item.get(0)));\r\n                    paddingList.add((item.get(1)));\r\n                }\r\n                padding = ArrayUtil.toArray(paddingList);\r\n            } else {\r\n                throw new InvalidKerasConfigurationException(\"Found Keras ZeroPadding\" + dimension + \"D layer with invalid \" + paddingList.size() + \"D padding.\");\r\n            }\r\n        } catch (Exception e) {\r\n            int paddingInt = (int) innerConfig.get(layerField);\r\n            if (dimension == 2) {\r\n                padding = new int[] { paddingInt, paddingInt, paddingInt, paddingInt };\r\n            } else {\r\n                padding = new int[] { paddingInt, paddingInt, paddingInt, paddingInt, paddingInt, paddingInt };\r\n            }\r\n        }\r\n    } else if (dimension == 1) {\r\n        int paddingInt = (int) innerConfig.get(layerField);\r\n        padding = new int[] { paddingInt, paddingInt };\r\n    } else {\r\n        throw new UnsupportedKerasConfigurationException(\"Keras padding layer not supported\");\r\n    }\r\n    return padding;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.blas.impl.SparseBaseLevel1.rot",
	"Comment": "applies givens rotation to sparse vectors one of which is in compressed form.",
	"Method": "void rot(long N,INDArray X,INDArray Y,double c,double s){\r\n    if (X instanceof BaseSparseNDArray) {\r\n        BaseSparseNDArray sparseX = (BaseSparseNDArray) X;\r\n        switch(X.data().dataType()) {\r\n            case DOUBLE:\r\n                droti(N, X, sparseX.getVectorCoordinates(), Y, c, s);\r\n                break;\r\n            case FLOAT:\r\n                sroti(N, X, sparseX.getVectorCoordinates(), Y, c, s);\r\n                break;\r\n            case HALF:\r\n                hroti(N, X, sparseX.getVectorCoordinates(), Y, c, s);\r\n                break;\r\n            default:\r\n                throw new UnsupportedOperationException();\r\n        }\r\n    } else {\r\n        throw new UnsupportedOperationException();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLPServer.livenessServer",
	"Comment": "if we have a separate liveness port, start a server on a separate thread pool whose onlyjob is to watch for when the corenlp server becomes ready.this will also immediately signal liveness.",
	"Method": "void livenessServer(AtomicBoolean live){\r\n    if (this.serverPort != this.statusPort) {\r\n        try {\r\n            if (this.ssl) {\r\n                server = addSSLContext(HttpsServer.create(new InetSocketAddress(statusPort), 0));\r\n            } else {\r\n                server = HttpServer.create(new InetSocketAddress(statusPort), 0);\r\n            }\r\n            withAuth(server.createContext(\"/live\", new LiveHandler()), Optional.empty());\r\n            withAuth(server.createContext(\"/ready\", new ReadyHandler(live)), Optional.empty());\r\n            server.start();\r\n            log(\"Liveness server started at \" + server.getAddress());\r\n        } catch (IOException e) {\r\n            err(\"Could not start liveness server. This will probably result in very bad things happening soon.\", e);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.spanish.SpanishXMLTreeReader.process",
	"Comment": "read trees from the given file and output their processed forms tostandard output.",
	"Method": "void process(File file,TreeReader tr,Pattern posPattern,Pattern wordPattern,boolean plainPrint){\r\n    Tree t;\r\n    int numTrees = 0, numTreesRetained = 0;\r\n    String canonicalFileName = file.getName().substring(0, file.getName().lastIndexOf('.'));\r\n    while ((t = tr.readTree()) != null) {\r\n        numTrees++;\r\n        if (!shouldPrintTree(t, posPattern, wordPattern))\r\n            continue;\r\n        numTreesRetained++;\r\n        String ftbID = ((CoreLabel) t.label()).get(CoreAnnotations.SentenceIDAnnotation.class);\r\n        String output = toString(t, plainPrint);\r\n        System.out.printf(\"%s-%s\\t%s%n\", canonicalFileName, ftbID, output);\r\n    }\r\n    System.err.printf(\"%s: %d trees, %d matched and printed%n\", file.getName(), numTrees, numTreesRetained);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.LossLayer.labelProbabilities",
	"Comment": "returns the probabilities for each labelfor each example row wise",
	"Method": "INDArray labelProbabilities(INDArray examples){\r\n    return activate(examples, false, LayerWorkspaceMgr.noWorkspacesImmutable());\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.ensurePrerequisiteAnnotators",
	"Comment": "take a collection of requested annotators, and produce a list of annotators such that all of theprerequisites for each of the annotators in the input is met.for example, if the user requests lemma, ensure that pos is also run because lemma depends onpos. as a side effect, this function orders the annotators in the proper order.note that this is not guaranteed to return a valid set of annotators,as properties passed to the annotators can change their requirements.",
	"Method": "String ensurePrerequisiteAnnotators(String[] annotators,Properties props){\r\n    Set<String> unorderedAnnotators = new LinkedHashSet();\r\n    Collections.addAll(unorderedAnnotators, annotators);\r\n    for (String annotator : annotators) {\r\n        if (!getNamedAnnotators().containsKey(annotator.toLowerCase())) {\r\n            throw new IllegalArgumentException(\"Unknown annotator: \" + annotator);\r\n        }\r\n        unorderedAnnotators.add(annotator.toLowerCase());\r\n        if (!Annotator.DEFAULT_REQUIREMENTS.containsKey(annotator.toLowerCase())) {\r\n            throw new IllegalArgumentException(\"Cannot infer requirements for annotator: \" + annotator);\r\n        }\r\n        Queue<String> fringe = new LinkedList(Annotator.DEFAULT_REQUIREMENTS.get(annotator.toLowerCase()));\r\n        int ticks = 0;\r\n        while (!fringe.isEmpty()) {\r\n            ticks += 1;\r\n            if (ticks == 1000000) {\r\n                throw new IllegalStateException(\"[INTERNAL ERROR] Annotators have a circular dependency.\");\r\n            }\r\n            String prereq = fringe.poll();\r\n            unorderedAnnotators.add(prereq);\r\n            fringe.addAll(Annotator.DEFAULT_REQUIREMENTS.get(prereq.toLowerCase()));\r\n        }\r\n    }\r\n    List<String> orderedAnnotators = new ArrayList();\r\n    while (!unorderedAnnotators.isEmpty()) {\r\n        boolean somethingAdded = false;\r\n        Iterator<String> iter = unorderedAnnotators.iterator();\r\n        while (iter.hasNext()) {\r\n            String candidate = iter.next();\r\n            boolean canAdd = true;\r\n            for (String prereq : Annotator.DEFAULT_REQUIREMENTS.get(candidate.toLowerCase())) {\r\n                if (!orderedAnnotators.contains(prereq)) {\r\n                    canAdd = false;\r\n                    break;\r\n                }\r\n            }\r\n            if (canAdd) {\r\n                orderedAnnotators.add(candidate);\r\n                iter.remove();\r\n                somethingAdded = true;\r\n            }\r\n        }\r\n        if (!somethingAdded) {\r\n            throw new IllegalArgumentException(\"Unsatisfiable annotator list: \" + StringUtils.join(annotators, \",\"));\r\n        }\r\n    }\r\n    if (orderedAnnotators.contains(STANFORD_PARSE) && !ArrayUtils.contains(annotators, STANFORD_DEPENDENCIES)) {\r\n        orderedAnnotators.remove(STANFORD_DEPENDENCIES);\r\n    }\r\n    if ((orderedAnnotators.contains(Annotator.STANFORD_COREF_MENTION) || orderedAnnotators.contains(Annotator.STANFORD_COREF)) && !orderedAnnotators.contains(Annotator.STANFORD_PARSE) && !props.containsKey(\"coref.md.type\")) {\r\n        props.setProperty(\"coref.md.type\", \"dep\");\r\n    }\r\n    if (orderedAnnotators.contains(Annotator.STANFORD_NER) && orderedAnnotators.contains(STANFORD_REGEXNER)) {\r\n        orderedAnnotators.remove(STANFORD_REGEXNER);\r\n        int nerIndex = orderedAnnotators.indexOf(Annotator.STANFORD_NER);\r\n        orderedAnnotators.add(nerIndex + 1, STANFORD_REGEXNER);\r\n    }\r\n    if (orderedAnnotators.contains(Annotator.STANFORD_COREF) && orderedAnnotators.contains(STANFORD_OPENIE)) {\r\n        int maxIndex = Math.max(orderedAnnotators.indexOf(STANFORD_OPENIE), orderedAnnotators.indexOf(STANFORD_COREF));\r\n        if (Objects.equals(orderedAnnotators.get(maxIndex), STANFORD_OPENIE)) {\r\n            orderedAnnotators.add(maxIndex, STANFORD_COREF);\r\n            orderedAnnotators.remove(STANFORD_COREF);\r\n        } else {\r\n            orderedAnnotators.add(maxIndex + 1, STANFORD_OPENIE);\r\n            orderedAnnotators.remove(STANFORD_OPENIE);\r\n        }\r\n    }\r\n    return StringUtils.join(orderedAnnotators, \",\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.Ssurgeon.getEltText",
	"Comment": "for a given element, treats the first child as a text elementand returns its value.",
	"Method": "String getEltText(Element element){\r\n    try {\r\n        NodeList childNodeList = element.getChildNodes();\r\n        if (childNodeList.getLength() == 0)\r\n            return \"\";\r\n        return childNodeList.item(0).getNodeValue();\r\n    } catch (Exception e) {\r\n        log.warning(\"Exception e=\" + e.getMessage() + \" thrown calling getEltText on element=\" + element);\r\n    }\r\n    return \"\";\r\n}"
}, {
	"Path": "org.deeplearning4j.util.ModelSerializer.restoreNormalizerFromInputStream",
	"Comment": "this method restores the normalizer form a persisted model file.",
	"Method": "T restoreNormalizerFromInputStream(InputStream is){\r\n    checkInputStream(is);\r\n    File tmpFile = null;\r\n    try {\r\n        tmpFile = tempFileFromStream(is);\r\n        return restoreNormalizerFromFile(tmpFile);\r\n    } finally {\r\n        if (tmpFile != null) {\r\n            tmpFile.delete();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.normalization.KerasBatchNormalization.getBetaRegularizerFromConfig",
	"Comment": "get batchnormalization beta regularizer from keras layer configuration. currently unsupported.",
	"Method": "void getBetaRegularizerFromConfig(Map<String, Object> layerConfig,boolean enforceTrainingConfig){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (innerConfig.get(LAYER_FIELD_BETA_REGULARIZER) != null) {\r\n        if (enforceTrainingConfig)\r\n            throw new UnsupportedKerasConfigurationException(\"Regularization for BatchNormalization beta parameter not supported\");\r\n        else\r\n            log.warn(\"Regularization for BatchNormalization beta parameter not supported...ignoring.\");\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.assign",
	"Comment": "assign all of the elements in the givenndarray to this ndarray",
	"Method": "INDArray assign(INDArray arr,INDArray assign,Number value){\r\n    Nd4j.getExecutioner().exec(new ScalarSet(this, value));\r\n    return this;\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getRaw",
	"Comment": "get the value of the name property, without doingvariable expansion.",
	"Method": "String getRaw(String name){\r\n    return getProps().getProperty(name);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.BaseLayer.params",
	"Comment": "returns the parameters of the neural network as a flattened row vector",
	"Method": "INDArray params(){\r\n    return paramsFlattened;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.StringUtils.pennPOSToWordnetPOS",
	"Comment": "computes the wordnet 2.0 pos tag corresponding to the ptb pos tag s.",
	"Method": "String pennPOSToWordnetPOS(String s){\r\n    if (s.matches(\"NN|NNP|NNS|NNPS\")) {\r\n        return \"noun\";\r\n    }\r\n    if (s.matches(\"VB|VBD|VBG|VBN|VBZ|VBP|MD\")) {\r\n        return \"verb\";\r\n    }\r\n    if (s.matches(\"JJ|JJR|JJS|CD\")) {\r\n        return \"adjective\";\r\n    }\r\n    if (s.matches(\"RB|RBR|RBS|RP|WRB\")) {\r\n        return \"adverb\";\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.datavec.api.transform.sequence.ReduceSequenceTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return columnNames();\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.descendants",
	"Comment": "returns the set of descendants governed by this node in the graph.",
	"Method": "Set<IndexedWord> descendants(IndexedWord vertex){\r\n    if (!containsVertex(vertex)) {\r\n        throw new IllegalArgumentException();\r\n    }\r\n    Set<IndexedWord> descendantSet = wordMapFactory.newSet();\r\n    descendantsHelper(vertex, descendantSet);\r\n    return descendantSet;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Document.asAnnotation",
	"Comment": "return this document as an annotation object.note that, importantly, only the fields which have already been called will be populated inthe annotation!therefore, this method is generally not recommended.",
	"Method": "Annotation asAnnotation(Annotation asAnnotation,boolean cache){\r\n    Annotation ann = cachedAnnotation == null ? null : cachedAnnotation.get();\r\n    if (!cache || ann == null) {\r\n        ann = serializer.fromProto(serialize());\r\n    }\r\n    cachedAnnotation = new SoftReference(ann);\r\n    return ann;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.learning.impl.elements.GloVe.pretrain",
	"Comment": "pretrain is used to build cooccurrence matrix for glove algorithm",
	"Method": "void pretrain(SequenceIterator<T> iterator){\r\n    coOccurrences = // TODO: symmetric should be handled via VectorsConfiguration\r\n    new AbstractCoOccurrences.Builder<T>().symmetric(this.symmetric).windowSize(configuration.getWindow()).iterate(iterator).workers(workers).vocabCache(vocabCache).maxMemory(maxmemory).build();\r\n    coOccurrences.fit();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArrayCoreMapTest.testEqualsReversedInsertOrder",
	"Comment": "tests equals in the case of different annotations added indifferent orders",
	"Method": "void testEqualsReversedInsertOrder(){\r\n    ArrayCoreMap foo = new ArrayCoreMap();\r\n    List<CoreMap> paragraphs = new ArrayList<CoreMap>();\r\n    ArrayCoreMap f1 = new ArrayCoreMap();\r\n    f1.set(CoreAnnotations.TextAnnotation.class, \"f\");\r\n    paragraphs.add(f1);\r\n    ArrayCoreMap f2 = new ArrayCoreMap();\r\n    f2.set(CoreAnnotations.TextAnnotation.class, \"o\");\r\n    paragraphs.add(f2);\r\n    foo.set(CoreAnnotations.ParagraphsAnnotation.class, paragraphs);\r\n    foo.set(CoreAnnotations.TextAnnotation.class, \"A\");\r\n    foo.set(CoreAnnotations.PartOfSpeechAnnotation.class, \"B\");\r\n    ArrayCoreMap bar = new ArrayCoreMap();\r\n    List<CoreMap> paragraphs2 = new ArrayList<CoreMap>(paragraphs);\r\n    bar.set(CoreAnnotations.TextAnnotation.class, \"A\");\r\n    bar.set(CoreAnnotations.PartOfSpeechAnnotation.class, \"B\");\r\n    bar.set(CoreAnnotations.ParagraphsAnnotation.class, paragraphs2);\r\n    assertEquals(foo, bar);\r\n    assertEquals(bar, foo);\r\n    assertFalse(foo.equals(f1));\r\n    assertFalse(foo.equals(f2));\r\n    assertEquals(3, foo.size());\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.vectorsAlongDimension",
	"Comment": "returns the number of possible vectors for a given dimension",
	"Method": "long vectorsAlongDimension(int dimension){\r\n    if (dimension == 0 && isVector() || isRowVectorOrScalar())\r\n        return 1;\r\n    if (size(dimension) == 1 && !isVector()) {\r\n        for (int i = dimension; i < rank(); i++) {\r\n            if (size(i) != 1)\r\n                return vectorsAlongDimension(i);\r\n        }\r\n        return length();\r\n    } else if (size(0) == 1 && !isVectorOrScalar()) {\r\n        int realDimension = rank() - getLeadingOnes();\r\n        long length = length();\r\n        if (length / size(realDimension) >= Integer.MAX_VALUE)\r\n            throw new IllegalArgumentException(\"Vectors along dimension can not be >= Integer.MAX_VALUE\");\r\n        return length / size(realDimension);\r\n    }\r\n    long length = length();\r\n    if (dimension >= jvmShapeInfo.rank) {\r\n        if (length / size(jvmShapeInfo.rank - 1) >= Integer.MAX_VALUE)\r\n            throw new IllegalArgumentException(\"Vectors along dimension can not be >= Integer.MAX_VALUE\");\r\n        return (int) (length / size(jvmShapeInfo.rank - 1));\r\n    }\r\n    if (length / size(dimension) >= Integer.MAX_VALUE)\r\n        throw new IllegalArgumentException(\"Vectors along dimension can not be >= Integer.MAX_VALUE\");\r\n    return length / size(dimension);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.hasPlaceHolderVariables",
	"Comment": "returns true if the given vertex id has any placeholder variables",
	"Method": "boolean hasPlaceHolderVariables(String vertexId){\r\n    return placeHolderMap.containsKey(vertexId);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.layers.BaseLayer.resetLayerDefaultConfig",
	"Comment": "reset the learning related configs of the layer to default. when instantiated with a global neural network configurationthe parameters specified in the neural network configuration will be used.for internal use with the transfer learning api. users should not have to call this method directly.",
	"Method": "void resetLayerDefaultConfig(){\r\n    this.setIUpdater(null);\r\n    this.setWeightInit(null);\r\n    this.setBiasInit(Double.NaN);\r\n    this.setDist(null);\r\n    this.setL1(Double.NaN);\r\n    this.setL2(Double.NaN);\r\n    this.setGradientNormalization(GradientNormalization.None);\r\n    this.setGradientNormalizationThreshold(1.0);\r\n    this.iUpdater = null;\r\n    this.biasUpdater = null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.StringUtils.columnStringToObject",
	"Comment": "converts a tab delimited string into an object with given fieldsrequires the object has public access for the specified fields",
	"Method": "T columnStringToObject(Class objClass,String str,String delimiterRegex,String[] fieldNames,T columnStringToObject,Class<?> objClass,String str,Pattern delimiterPattern,String[] fieldNames){\r\n    String[] fields = delimiterPattern.split(str);\r\n    T item = ErasureUtils.uncheckedCast(objClass.newInstance());\r\n    for (int i = 0; i < fields.length; i++) {\r\n        try {\r\n            Field field = objClass.getDeclaredField(fieldNames[i]);\r\n            field.set(item, fields[i]);\r\n        } catch (IllegalAccessException ex) {\r\n            Method method = objClass.getDeclaredMethod(\"set\" + StringUtils.capitalize(fieldNames[i]), String.class);\r\n            method.invoke(item, fields[i]);\r\n        }\r\n    }\r\n    return item;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.DeltaCollectionValuedMap.addAll",
	"Comment": "adds all of the mappings in m to this collectionvaluedmap.if m is a collectionvaluedmap, it will behave strangely. use the constructor instead.",
	"Method": "void addAll(Map<K, V> m){\r\n    for (Map.Entry<K, V> e : m.entrySet()) {\r\n        add(e.getKey(), e.getValue());\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.wordstore.VocabConstructor.buildMergedVocabulary",
	"Comment": "this method transfers existing wordvectors model into current one",
	"Method": "VocabCache<T> buildMergedVocabulary(WordVectors wordVectors,boolean fetchLabels,VocabCache<T> buildMergedVocabulary,VocabCache<T> vocabCache,boolean fetchLabels){\r\n    if (cache == null)\r\n        cache = new AbstractCache.Builder<T>().build();\r\n    for (int t = 0; t < vocabCache.numWords(); t++) {\r\n        String label = vocabCache.wordAtIndex(t);\r\n        if (label == null)\r\n            continue;\r\n        T element = vocabCache.wordFor(label);\r\n        if (!fetchLabels && element.isLabel())\r\n            continue;\r\n        cache.addToken(element);\r\n        cache.addWordToIndex(element.getIndex(), element.getLabel());\r\n        cache.putVocabWord(element.getLabel());\r\n    }\r\n    if (cache.numWords() == 0)\r\n        throw new IllegalStateException(\"Source VocabCache has no indexes available, transfer is impossible\");\r\n    log.info(\"Vocab size before labels: \" + cache.numWords());\r\n    if (fetchLabels) {\r\n        for (VocabSource<T> source : sources) {\r\n            SequenceIterator<T> iterator = source.getIterator();\r\n            iterator.reset();\r\n            while (iterator.hasMoreSequences()) {\r\n                Sequence<T> sequence = iterator.nextSequence();\r\n                seqCount.incrementAndGet();\r\n                if (sequence.getSequenceLabels() != null)\r\n                    for (T label : sequence.getSequenceLabels()) {\r\n                        if (!cache.containsWord(label.getLabel())) {\r\n                            label.markAsLabel(true);\r\n                            label.setSpecial(true);\r\n                            label.setIndex(cache.numWords());\r\n                            cache.addToken(label);\r\n                            cache.addWordToIndex(label.getIndex(), label.getLabel());\r\n                            cache.putVocabWord(label.getLabel());\r\n                        }\r\n                    }\r\n            }\r\n        }\r\n    }\r\n    log.info(\"Vocab size after labels: \" + cache.numWords());\r\n    return cache;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.asFlatBuffers",
	"Comment": "this method exports the current samediff instance into flatbuffers format, returning the array ops andall arrays as a bytebuffer containing the flatbuffers format data",
	"Method": "ByteBuffer asFlatBuffers(ExecutorConfiguration configuration,ByteBuffer asFlatBuffers,long graphId,ExecutorConfiguration configuration,ByteBuffer asFlatBuffers){\r\n    val configuration = ExecutorConfiguration.builder().outputMode(OutputMode.VARIABLE_SPACE).executionMode(org.nd4j.autodiff.execution.conf.ExecutionMode.SEQUENTIAL).profilingMode(OpExecutioner.ProfilingMode.DISABLED).gatherTimings(true).build();\r\n    return asFlatBuffers(configuration);\r\n}"
}, {
	"Path": "org.datavec.api.transform.ui.HtmlAnalysis.createHtmlAnalysisFile",
	"Comment": "render a data analysis object as a html file. this will produce a summary table, along charts fornumerical columns",
	"Method": "void createHtmlAnalysisFile(DataAnalysis dataAnalysis,File output){\r\n    String str = createHtmlAnalysisString(dataAnalysis);\r\n    FileUtils.writeStringToFile(output, str);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraLexer.yypushback",
	"Comment": "pushes the specified amount of characters back into the input stream.they will be read again by then next call of the scanning method",
	"Method": "void yypushback(int number){\r\n    if (number > yylength())\r\n        zzScanError(ZZ_PUSHBACK_2BIG);\r\n    zzMarkedPos -= number;\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.toProtoSection",
	"Comment": "create a section coremap protocol buffer from the given section coremap",
	"Method": "CoreNLPProtos.Section toProtoSection(CoreMap section){\r\n    CoreNLPProtos.Section.Builder builder = CoreNLPProtos.Section.newBuilder();\r\n    builder.setCharBegin(section.get(CharacterOffsetBeginAnnotation.class));\r\n    builder.setCharEnd(section.get(CharacterOffsetEndAnnotation.class));\r\n    if (section.get(AuthorAnnotation.class) != null)\r\n        builder.setAuthor(section.get(AuthorAnnotation.class));\r\n    if (section.get(SectionDateAnnotation.class) != null)\r\n        builder.setDatetime(section.get(SectionDateAnnotation.class));\r\n    for (CoreMap sentence : section.get(SentencesAnnotation.class)) {\r\n        int sentenceIndex = sentence.get(SentenceIndexAnnotation.class);\r\n        builder.addSentenceIndexes(sentenceIndex);\r\n    }\r\n    for (CoreMap quote : section.get(QuotesAnnotation.class)) {\r\n        builder.addQuotes(toProtoQuote(quote));\r\n    }\r\n    if (section.get(SectionAuthorCharacterOffsetBeginAnnotation.class) != null) {\r\n        builder.setAuthorCharBegin(section.get(SectionAuthorCharacterOffsetBeginAnnotation.class));\r\n    }\r\n    if (section.get(SectionAuthorCharacterOffsetEndAnnotation.class) != null) {\r\n        builder.setAuthorCharEnd(section.get(SectionAuthorCharacterOffsetEndAnnotation.class));\r\n    }\r\n    builder.setXmlTag(toProto(section.get(SectionTagAnnotation.class)));\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.normalization.KerasBatchNormalization.getMomentumFromConfig",
	"Comment": "get batchnormalization momentum parameter from keras layer configuration.",
	"Method": "double getMomentumFromConfig(Map<String, Object> layerConfig){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (!innerConfig.containsKey(LAYER_FIELD_MOMENTUM))\r\n        throw new InvalidKerasConfigurationException(\"Keras BatchNorm layer config missing \" + LAYER_FIELD_MOMENTUM + \" field\");\r\n    return (double) innerConfig.get(LAYER_FIELD_MOMENTUM);\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.parse.ParseDoubleTransform.transform",
	"Comment": "get the output schema for this transformation, given an input schema",
	"Method": "Schema transform(Schema inputSchema){\r\n    Schema.Builder newSchema = new Schema.Builder();\r\n    for (int i = 0; i < inputSchema.numColumns(); i++) {\r\n        if (inputSchema.getType(i) == ColumnType.String) {\r\n            newSchema.addColumnDouble(inputSchema.getMetaData(i).getName());\r\n        } else\r\n            newSchema.addColumn(inputSchema.getMetaData(i));\r\n    }\r\n    return newSchema.build();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerNetwork",
	"Comment": "build a multilayernetwork from this keras sequential model configuration and import weights.",
	"Method": "MultiLayerNetwork getMultiLayerNetwork(MultiLayerNetwork getMultiLayerNetwork,boolean importWeights){\r\n    MultiLayerNetwork model = new MultiLayerNetwork(getMultiLayerConfiguration());\r\n    model.init();\r\n    if (importWeights)\r\n        model = (MultiLayerNetwork) KerasModelUtils.copyWeightsToModel(model, this.layers);\r\n    return model;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.findMatchingNode",
	"Comment": "finds the vertex in the given semanticgraph that corresponds to the given node.returns null if cannot find. uses first match on index, sentindex, and word values.",
	"Method": "IndexedWord findMatchingNode(IndexedWord node,SemanticGraph sg){\r\n    for (IndexedWord tgt : sg.vertexSet()) {\r\n        if ((tgt.index() == node.index()) && (tgt.sentIndex() == node.sentIndex()) && (tgt.word().equals(node.word())))\r\n            return tgt;\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.datavec.api.util.ndarray.RecordConverter.toArray",
	"Comment": "convert a record to an indarray. may contain a mix of writables and row vector ndarraywritables.",
	"Method": "INDArray toArray(Collection<Writable> record,int size,INDArray toArray,Collection<? extends Writable> record){\r\n    List<Writable> l;\r\n    if (record instanceof List) {\r\n        l = (List<Writable>) record;\r\n    } else {\r\n        l = new ArrayList(record);\r\n    }\r\n    if (l.size() == 1 && l.get(0) instanceof NDArrayWritable) {\r\n        return ((NDArrayWritable) l.get(0)).get();\r\n    }\r\n    int length = 0;\r\n    for (Writable w : record) {\r\n        if (w instanceof NDArrayWritable) {\r\n            INDArray a = ((NDArrayWritable) w).get();\r\n            if (!a.isRowVector()) {\r\n                throw new UnsupportedOperationException(\"Multiple writables present but NDArrayWritable is \" + \"not a row vector. Can only concat row vectors with other writables. Shape: \" + Arrays.toString(a.shape()));\r\n            }\r\n            length += a.length();\r\n        } else {\r\n            length++;\r\n        }\r\n    }\r\n    INDArray arr = Nd4j.create(1, length);\r\n    int k = 0;\r\n    for (Writable w : record) {\r\n        if (w instanceof NDArrayWritable) {\r\n            INDArray toPut = ((NDArrayWritable) w).get();\r\n            arr.put(new INDArrayIndex[] { NDArrayIndex.point(0), NDArrayIndex.interval(k, k + toPut.length()) }, toPut);\r\n            k += toPut.length();\r\n        } else {\r\n            arr.putScalar(0, k, w.toDouble());\r\n            k++;\r\n        }\r\n    }\r\n    return arr;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.gather",
	"Comment": "gather slices from the input variable where the indices are specified as dynamic sdvariable values.output shape is same as input shape, except for axis dimension, which has size equal to indices.length.",
	"Method": "SDVariable gather(SDVariable df,int[] indices,int axis,SDVariable gather,String name,SDVariable df,int[] indices,int axis,SDVariable gather,SDVariable df,SDVariable indices,int axis,SDVariable gather,String name,SDVariable df,SDVariable indices,int axis){\r\n    SDVariable ret = f().gather(df, indices, axis);\r\n    return updateVariableNameAndReference(ret, name);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.setLastEtlTime",
	"Comment": "this method allows to set etl field time, useful for performance tracking",
	"Method": "void setLastEtlTime(long time){\r\n    lastEtlTime.set(time);\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.generateUniform",
	"Comment": "this will generate a series of uniformally distributednumbers between l times",
	"Method": "double[] generateUniform(int l){\r\n    double[] ret = new double[l];\r\n    Random rgen = new Random();\r\n    for (int i = 0; i < l; i++) {\r\n        ret[i] = rgen.nextDouble();\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.SequenceGibbsSampler.bestSequence",
	"Comment": "finds the best sequence by collecting numsamples samples, scoring them, and then choosingthe highest scoring sample.",
	"Method": "int[] bestSequence(SequenceModel model){\r\n    int[] initialSequence = getRandomSequence(model);\r\n    return findBestUsingSampling(model, numSamples, sampleInterval, initialSequence);\r\n}"
}, {
	"Path": "dagger.internal.codegen.GraphAnalysisLoader.resolveType",
	"Comment": "recursively explores the space of possible canonical names for a given binary class name.",
	"Method": "TypeElement resolveType(Elements elements,String className,TypeElement resolveType,Elements elements,String className,StringBuilder sb,int index){\r\n    sb.setCharAt(index, '.');\r\n    int nextIndex = nextDollar(className, sb, index + 1);\r\n    TypeElement type = nextIndex == -1 ? getTypeElement(elements, sb) : resolveType(elements, className, sb, nextIndex);\r\n    if (type != null) {\r\n        return type;\r\n    }\r\n    sb.setCharAt(index, '$');\r\n    nextIndex = nextDollar(className, sb, index + 1);\r\n    return nextIndex == -1 ? getTypeElement(elements, sb) : resolveType(elements, className, sb, nextIndex);\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.randomprojection.RPUtils.sortCandidates",
	"Comment": "get the sorted distances given thequery vector, input data, given the list of possible search candidates",
	"Method": "List<Pair<Double, Integer>> sortCandidates(INDArray x,INDArray X,List<Integer> candidates,String similarityFunction){\r\n    int prevIdx = -1;\r\n    List<Pair<Double, Integer>> ret = new ArrayList();\r\n    for (int i = 0; i < candidates.size(); i++) {\r\n        if (candidates.get(i) != prevIdx) {\r\n            ret.add(Pair.of(computeDistance(similarityFunction, X.slice(candidates.get(i)), x), candidates.get(i)));\r\n        }\r\n        prevIdx = i;\r\n    }\r\n    Collections.sort(ret, new Comparator<Pair<Double, Integer>>() {\r\n        @Override\r\n        public int compare(Pair<Double, Integer> doubleIntegerPair, Pair<Double, Integer> t1) {\r\n            return Doubles.compare(doubleIntegerPair.getFirst(), t1.getFirst());\r\n        }\r\n    });\r\n    return ret;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.randomprojection.RPUtils.sortCandidates",
	"Comment": "get the sorted distances given thequery vector, input data, given the list of possible search candidates",
	"Method": "List<Pair<Double, Integer>> sortCandidates(INDArray x,INDArray X,List<Integer> candidates,String similarityFunction){\r\n    return Doubles.compare(doubleIntegerPair.getFirst(), t1.getFirst());\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CollectionValuedMap.removeAll",
	"Comment": "removes the mappings associated with the keys from this map.",
	"Method": "void removeAll(Collection<K> keys){\r\n    for (K k : keys) {\r\n        remove(k);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.GrammaticalFunctionTreeNormalizer.cleanUpLabel",
	"Comment": "remove things like hyphened functional tags and equals from theend of a node label.",
	"Method": "String cleanUpLabel(String label){\r\n    if (label == null) {\r\n        return root;\r\n    } else if (nodeCleanup == 1) {\r\n        return tlp.categoryAndFunction(label);\r\n    } else if (nodeCleanup == 2) {\r\n        return tlp.basicCategory(label);\r\n    } else {\r\n        return label;\r\n    }\r\n}"
}, {
	"Path": "org.datavec.arrow.ArrowConverter.toArrowColumnsStringTimeSeries",
	"Comment": "convert a set of input strings to arrow columnsfor a time series.",
	"Method": "List<FieldVector> toArrowColumnsStringTimeSeries(BufferAllocator bufferAllocator,Schema schema,List<List<List<String>>> dataVecRecord){\r\n    return toArrowColumnsTimeSeriesHelper(bufferAllocator, schema, dataVecRecord);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.feedForward",
	"Comment": "conduct forward pass using a single input array. note that this method can only be used with computationgraphswith a single input array.",
	"Method": "Map<String, INDArray> feedForward(INDArray input,int layerTillIndex,boolean train,Map<String, INDArray> feedForward,INDArray[] input,int layerTillIndex,boolean train,boolean clearInputs,Map<String, INDArray> feedForward,INDArray[] input,int layerTillIndex,boolean train,Map<String, INDArray> feedForward,boolean train,int layerTillIndex,Map<String, INDArray> feedForward,INDArray input,boolean train,Map<String, INDArray> feedForward,INDArray[] input,boolean train,Map<String, INDArray> feedForward,INDArray[] input,boolean train,boolean clearInputs,Map<String, INDArray> feedForward,Map<String, INDArray> feedForward,boolean train,Map<String, INDArray> feedForward,boolean train,boolean excludeOutputLayers,boolean includeNonLayerVertexActivations){\r\n    int[] exclude = null;\r\n    if (excludeOutputLayers) {\r\n        exclude = getOutputLayerIndices();\r\n    }\r\n    Map<String, INDArray> m = ffToLayerActivationsDetached(train, FwdPassType.STANDARD, false, vertices.length - 1, exclude, inputs, inputMaskArrays, labelMaskArrays, true);\r\n    if (includeNonLayerVertexActivations) {\r\n        return m;\r\n    } else {\r\n        Map<String, INDArray> out = new HashMap();\r\n        for (Map.Entry<String, INDArray> e : m.entrySet()) {\r\n            GraphVertex v = verticesMap.get(e.getKey());\r\n            if (v instanceof LayerVertex || v instanceof InputVertex) {\r\n                out.put(e.getKey(), e.getValue());\r\n            }\r\n        }\r\n        return out;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.sumOfMeanDifferences",
	"Comment": "used for calculating top part of simple regression forbeta 1",
	"Method": "double sumOfMeanDifferences(double[] vector,double[] vector2){\r\n    double mean = sum(vector) / vector.length;\r\n    double mean2 = sum(vector2) / vector2.length;\r\n    double ret = 0;\r\n    for (int i = 0; i < vector.length; i++) {\r\n        double vec1Diff = vector[i] - mean;\r\n        double vec2Diff = vector2[i] - mean2;\r\n        ret += vec1Diff * vec2Diff;\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.StringUtils.padOrTrim",
	"Comment": "pad or trim so as to produce a string of exactly a certain length.",
	"Method": "String padOrTrim(String str,int num,String padOrTrim,Object obj,int totalChars){\r\n    return padOrTrim(obj.toString(), totalChars);\r\n}"
}, {
	"Path": "org.datavec.api.records.Buffer.truncate",
	"Comment": "change the capacity of the backing store to be the same as the currentcount of buffer.",
	"Method": "void truncate(){\r\n    setCapacity(count);\r\n}"
}, {
	"Path": "org.datavec.api.util.ReflectionUtils.newInstance",
	"Comment": "create an object for the given class and initialize it from conf",
	"Method": "T newInstance(Class<T> theClass,Configuration conf){\r\n    T result;\r\n    try {\r\n        Constructor<T> meth = (Constructor<T>) CONSTRUCTOR_CACHE.get(theClass);\r\n        if (meth == null) {\r\n            meth = theClass.getDeclaredConstructor(EMPTY_ARRAY);\r\n            meth.setAccessible(true);\r\n            CONSTRUCTOR_CACHE.put(theClass, meth);\r\n        }\r\n        result = meth.newInstance();\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    setConf(result, conf);\r\n    return result;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.FilteringTreebank.loadPath",
	"Comment": "load trees from given path specification.passes the path andfilter to the underlying treebank.",
	"Method": "void loadPath(File path,FileFilter filt){\r\n    treebank.loadPath(path, filt);\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.addAll",
	"Comment": "adds the counts in the given counter to the counts in this counter.to copy the values from another counter rather than adding them, use",
	"Method": "void addAll(IntCounter<E> counter){\r\n    for (E key : counter.keySet()) {\r\n        int count = counter.getIntCount(key);\r\n        incrementCount(key, count);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.inmemory.InMemoryLookupTable.getLr",
	"Comment": "this method is deprecated, since all logic was pulled out from this class and is not used anymore.however this method will be around for a while, due to backward compatibility issues.",
	"Method": "AtomicDouble getLr(){\r\n    return lr;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.reader.impl.FlatModelUtils.wordsNearest",
	"Comment": "this method does full scan against whole vocabulary, building descending list of similar words",
	"Method": "Collection<String> wordsNearest(String label,int n,Collection<String> wordsNearest,INDArray words,int top){\r\n    Counter<String> distances = new Counter();\r\n    for (String s : vocabCache.words()) {\r\n        INDArray otherVec = lookupTable.vector(s);\r\n        double sim = Transforms.cosineSim(Transforms.unitVec(words.dup()), Transforms.unitVec(otherVec.dup()));\r\n        distances.incrementCount(s, (float) sim);\r\n    }\r\n    distances.keepTopNElements(top);\r\n    return distances.keySetSorted();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.NPTmpRetainingTreeNormalizer.cleanUpLabel",
	"Comment": "remove things like hyphened functional tags and equals from theend of a node label.",
	"Method": "String cleanUpLabel(String label){\r\n    if (label == null) {\r\n        return \"ROOT\";\r\n    } else if (leaveItAll == 1) {\r\n        return tlp.categoryAndFunction(label);\r\n    } else if (leaveItAll == 2) {\r\n        return label;\r\n    } else {\r\n        boolean nptemp = NPTmpPattern.matcher(label).matches();\r\n        boolean pptemp = PPTmpPattern.matcher(label).matches();\r\n        boolean advptemp = ADVPTmpPattern.matcher(label).matches();\r\n        boolean anytemp = TmpPattern.matcher(label).matches();\r\n        boolean subj = NPSbjPattern.matcher(label).matches();\r\n        boolean npadv = NPAdvPattern.matcher(label).matches();\r\n        label = tlp.basicCategory(label);\r\n        if (anytemp && temporalAnnotation == TEMPORAL_ANY_TMP_PERCOLATED) {\r\n            label += \"-TMP\";\r\n        } else if (pptemp && (temporalAnnotation == TEMPORAL_ALL_NP_AND_PP || temporalAnnotation == TEMPORAL_NP_AND_PP_WITH_NP_HEAD || temporalAnnotation == TEMPORAL_ALL_NP_EVEN_UNDER_PP || temporalAnnotation == TEMPORAL_ALL_NP_PP_ADVP || temporalAnnotation == TEMPORAL_9)) {\r\n            label = label + \"-TMP\";\r\n        } else if (advptemp && (temporalAnnotation == TEMPORAL_ALL_NP_PP_ADVP || temporalAnnotation == TEMPORAL_9)) {\r\n            label = label + \"-TMP\";\r\n        } else if (temporalAnnotation > 0 && nptemp) {\r\n            label = label + \"-TMP\";\r\n        }\r\n        if (doAdverbialNP && npadv) {\r\n            label = label + \"-ADV\";\r\n        }\r\n        if (doSGappedStuff && subj) {\r\n            label = label + \"-SBJ\";\r\n        }\r\n        return label;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.SystemUtils.getTimestampString",
	"Comment": "returns a string representing the current date and time in the givenformat.",
	"Method": "String getTimestampString(String fmt,String getTimestampString){\r\n    return getTimestampString(\"yyyyMMdd-HHmmss\");\r\n}"
}, {
	"Path": "org.datavec.image.transform.ResizeImageTransform.doTransform",
	"Comment": "takes an image and returns a transformed image.uses the random object in the case of random transformations.",
	"Method": "ImageWritable doTransform(ImageWritable image,Random random){\r\n    if (image == null) {\r\n        return null;\r\n    }\r\n    Mat mat = converter.convert(image.getFrame());\r\n    Mat result = new Mat();\r\n    srch = mat.rows();\r\n    srcw = mat.cols();\r\n    resize(mat, result, new Size(newWidth, newHeight));\r\n    return new ImageWritable(converter.convert(result));\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.KerasLayer.getInputPreprocessor",
	"Comment": "gets appropriate dl4j inputpreprocessor for given inputtypes.",
	"Method": "InputPreProcessor getInputPreprocessor(InputType inputType){\r\n    InputPreProcessor preprocessor = null;\r\n    if (this.layer != null) {\r\n        if (inputType.length > 1)\r\n            throw new InvalidKerasConfigurationException(\"Keras layer of type \\\"\" + this.className + \"\\\" accepts only one input\");\r\n        preprocessor = this.layer.getPreProcessorForInputType(inputType[0]);\r\n    }\r\n    return preprocessor;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.paragraphvectors.ParagraphVectors.predict",
	"Comment": "this method predicts label of the document.computes a similarity wrt the mean of therepresentation of words in the document",
	"Method": "String predict(String rawText,String predict,LabelledDocument document,String predict,List<VocabWord> document){\r\n    if (document.isEmpty())\r\n        throw new IllegalStateException(\"Document has no words inside\");\r\n    INDArray docMean = inferVector(document);\r\n    Counter<String> distances = new Counter();\r\n    for (String s : labelsSource.getLabels()) {\r\n        INDArray otherVec = getWordVectorMatrix(s);\r\n        double sim = Transforms.cosineSim(docMean, otherVec);\r\n        distances.incrementCount(s, (float) sim);\r\n    }\r\n    return distances.argMax();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.ud.CoNLLUDocumentReaderWriterTest.testSingleReadAndWrite",
	"Comment": "tests whether reading a semantic graph and printing itis equal to the original input.",
	"Method": "void testSingleReadAndWrite(String input){\r\n    String clean = input.replaceAll(\"[\\\\t ]+\", \"\\t\");\r\n    CoNLLUDocumentReader reader = new CoNLLUDocumentReader();\r\n    CoNLLUDocumentWriter writer = new CoNLLUDocumentWriter();\r\n    Reader stringReader = new StringReader(clean);\r\n    Iterator<SemanticGraph> it = reader.getIterator(stringReader);\r\n    SemanticGraph sg = it.next();\r\n    String output = writer.printSemanticGraph(sg);\r\n    assertEquals(clean, output);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.Tdiff.markDiff",
	"Comment": "marks bracketings in t2 not in t1 using the doannotation field.returns a list of brackets in t1 not in t2.",
	"Method": "Set<Constituent> markDiff(Tree t1,Tree t2){\r\n    Set<Constituent> t1Labels = (t1 == null) ? Generics.<Constituent>newHashSet() : t1.constituents(cf);\r\n    if (t2 != null) {\r\n        t2.setSpans();\r\n        for (Tree subTree : t2) {\r\n            if (subTree.isPhrasal()) {\r\n                IntPair span = subTree.getSpan();\r\n                Constituent c = cf.newConstituent(span.getSource(), span.getTarget(), subTree.label(), 0.0);\r\n                if (t1Labels.contains(c)) {\r\n                    t1Labels.remove(c);\r\n                    ((CoreLabel) subTree.label()).set(CoreAnnotations.DoAnnotation.class, false);\r\n                } else {\r\n                    ((CoreLabel) subTree.label()).set(CoreAnnotations.DoAnnotation.class, true);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return t1Labels;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.PTBLexer.yytext",
	"Comment": "returns the text matched by the current regular expression.",
	"Method": "String yytext(){\r\n    return new String(zzBuffer, zzStartRead, zzMarkedPos - zzStartRead);\r\n}"
}, {
	"Path": "org.deeplearning4j.iterator.CnnSentenceDataSetIterator.loadSingleSentence",
	"Comment": "generally used post training time to load a single sentence for predictions",
	"Method": "INDArray loadSingleSentence(String sentence){\r\n    List<String> tokens = tokenizeSentence(sentence);\r\n    if (format == Format.CNN1D || format == Format.RNN) {\r\n        int[] featuresShape = new int[] { 1, wordVectorSize, Math.min(maxSentenceLength, tokens.size()) };\r\n        INDArray features = Nd4j.create(featuresShape, (format == Format.CNN1D ? 'c' : 'f'));\r\n        INDArrayIndex[] indices = new INDArrayIndex[3];\r\n        indices[0] = NDArrayIndex.point(0);\r\n        for (int i = 0; i < featuresShape[2]; i++) {\r\n            INDArray vector = getVector(tokens.get(i));\r\n            indices[1] = NDArrayIndex.all();\r\n            indices[2] = NDArrayIndex.point(i);\r\n            features.put(indices, vector);\r\n        }\r\n        return features;\r\n    } else {\r\n        int[] featuresShape = new int[] { 1, 1, 0, 0 };\r\n        if (sentencesAlongHeight) {\r\n            featuresShape[2] = Math.min(maxSentenceLength, tokens.size());\r\n            featuresShape[3] = wordVectorSize;\r\n        } else {\r\n            featuresShape[2] = wordVectorSize;\r\n            featuresShape[3] = Math.min(maxSentenceLength, tokens.size());\r\n        }\r\n        INDArray features = Nd4j.create(featuresShape);\r\n        int length = (sentencesAlongHeight ? featuresShape[2] : featuresShape[3]);\r\n        INDArrayIndex[] indices = new INDArrayIndex[4];\r\n        indices[0] = NDArrayIndex.point(0);\r\n        indices[1] = NDArrayIndex.point(0);\r\n        for (int i = 0; i < length; i++) {\r\n            INDArray vector = getVector(tokens.get(i));\r\n            if (sentencesAlongHeight) {\r\n                indices[2] = NDArrayIndex.point(i);\r\n                indices[3] = NDArrayIndex.all();\r\n            } else {\r\n                indices[2] = NDArrayIndex.all();\r\n                indices[3] = NDArrayIndex.point(i);\r\n            }\r\n            features.put(indices, vector);\r\n        }\r\n        return features;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.Ssurgeon.parseArgs",
	"Comment": "this is a specialized args parser, as we want to split onwhitespace, but retain everything inside quotes, so we can passin hashmaps in string form.",
	"Method": "String[] parseArgs(String argsString){\r\n    List<String> retList = new ArrayList();\r\n    String patternString = \"(?:[^\\\\s\\\\\\\"]++|\\\\\\\"[^\\\\\\\"]*+\\\\\\\"|(\\\\\\\"))++\";\r\n    Pattern pattern = Pattern.compile(patternString);\r\n    Matcher matcher = pattern.matcher(argsString);\r\n    while (matcher.find()) {\r\n        if (matcher.group(1) == null) {\r\n            String matched = matcher.group();\r\n            if (matched.charAt(0) == '\"' && matched.charAt(matched.length() - 1) == '\"')\r\n                retList.add(matched.substring(1, matched.length() - 1));\r\n            else\r\n                retList.add(matched);\r\n        } else\r\n            throw new IllegalArgumentException(\"Unmatched quote in string to parse\");\r\n    }\r\n    return retList.toArray(StringUtils.EMPTY_STRING_ARRAY);\r\n}"
}, {
	"Path": "org.datavec.api.transform.sequence.comparator.BaseColumnComparator.transform",
	"Comment": "get the output schema for this transformation, given an input schema",
	"Method": "Schema transform(Schema inputSchema){\r\n    return inputSchema;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.PrecisionRecallStats.addCounts",
	"Comment": "adds the counts from the given stats to the counts of this stats.",
	"Method": "void addCounts(PrecisionRecallStats prs){\r\n    addTP(prs.getTP());\r\n    addFP(prs.getFP());\r\n    addFN(prs.getFN());\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.flow.FlowPath.getNumberOfCycles",
	"Comment": "this method returns number of iterations of specified node.",
	"Method": "long getNumberOfCycles(String frameName){\r\n    return states.get(frameName).getNumberOfCycles();\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.tokens",
	"Comment": "the tokens in this sentence. each token class is just a helper for the methods in this class.",
	"Method": "List<Token> tokens(){\r\n    ArrayList<Token> tokens = new ArrayList(this.length());\r\n    for (int i = 0; i < length(); ++i) {\r\n        tokens.add(new Token(this, i));\r\n    }\r\n    return tokens;\r\n}"
}, {
	"Path": "org.datavec.arrow.ArrowConverter.floatField",
	"Comment": "shortcut method for creating a double fieldwith 32 bit floating point",
	"Method": "Field floatField(String name){\r\n    return getFieldForColumn(name, ColumnType.Float);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractTreebankLanguagePack.isSentenceFinalPunctuationTag",
	"Comment": "accepts a string that is a sentence endpunctuation tag, and rejects everything else.",
	"Method": "boolean isSentenceFinalPunctuationTag(String str){\r\n    return sFPunctTagStringAcceptFilter.test(str);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.CollocationFinder.wordNetContains",
	"Comment": "checks to see if wordnet contains the given word in its lexicon.",
	"Method": "boolean wordNetContains(String s){\r\n    return wnConnect.wordNetContains(s);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.Ssurgeon.writeToFile",
	"Comment": "given a target filepath and a list of ssurgeon patterns, writes them out as xml forms.",
	"Method": "void writeToFile(File tgtFile,List<SsurgeonPattern> patterns){\r\n    try {\r\n        Document domDoc = createPatternXMLDoc(patterns);\r\n        if (domDoc != null) {\r\n            Transformer tformer = TransformerFactory.newInstance().newTransformer();\r\n            tformer.setOutputProperty(OutputKeys.INDENT, \"yes\");\r\n            tformer.transform(new DOMSource(domDoc), new StreamResult(tgtFile));\r\n        } else {\r\n            log.warning(\"Was not able to create XML document for pattern list, file not written.\");\r\n        }\r\n    } catch (Exception e) {\r\n        log.error(Ssurgeon.class.getName(), \"writeToFile\");\r\n        log.error(e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.RedwoodConfiguration.empty",
	"Comment": "an empty redwood configuration.note that without a console handler, redwood will not print anything",
	"Method": "RedwoodConfiguration empty(){\r\n    return new RedwoodConfiguration().clear();\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.Dictionary.setAmbClasses",
	"Comment": "this makes ambiguity classes from all words in the dictionary and rememberstheir classes in the tagcounts",
	"Method": "void setAmbClasses(AmbiguityClasses ambClasses,int veryCommonWordThresh,TTags ttags){\r\n    for (Map.Entry<String, TagCount> entry : dict.entrySet()) {\r\n        String w = entry.getKey();\r\n        TagCount count = entry.getValue();\r\n        int ambClassId = ambClasses.getClass(w, this, veryCommonWordThresh, ttags);\r\n        count.setAmbClassId(ambClassId);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FixedPrioritiesPriorityQueue.toString",
	"Comment": "returns a representation of the queue in decreasing priority order,displaying at most maxkeystoprint elements.",
	"Method": "String toString(String toString,int maxKeysToPrint,String toString,int maxKeysToPrint,String dblFmt){\r\n    if (maxKeysToPrint <= 0)\r\n        maxKeysToPrint = Integer.MAX_VALUE;\r\n    FixedPrioritiesPriorityQueue<E> pq = clone();\r\n    StringBuilder sb = new StringBuilder(\"[\");\r\n    int numKeysPrinted = 0;\r\n    while (numKeysPrinted < maxKeysToPrint && pq.hasNext()) {\r\n        double priority = pq.getPriority();\r\n        E element = pq.next();\r\n        sb.append(element);\r\n        sb.append('=');\r\n        if (dblFmt == null) {\r\n            sb.append(priority);\r\n        } else {\r\n            sb.append(String.format(dblFmt, priority));\r\n        }\r\n        if (numKeysPrinted < size() - 1)\r\n            sb.append(\", \");\r\n        numKeysPrinted++;\r\n    }\r\n    if (numKeysPrinted < size()) {\r\n        sb.append(\"...\");\r\n    }\r\n    sb.append(']');\r\n    return sb.toString();\r\n}"
}, {
	"Path": "org.datavec.arrow.ArrowConverter.intField",
	"Comment": "shortcut method for creating a double fieldwith 32 bit integer field",
	"Method": "Field intField(String name){\r\n    return getFieldForColumn(name, ColumnType.Integer);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Trees.getPreTerminal",
	"Comment": "gets the nth preterminal in tree.the first terminal is number zero.",
	"Method": "Tree getPreTerminal(Tree tree,int n,Tree getPreTerminal,Tree tree,MutableInteger i,int n){\r\n    if (i.intValue() == n) {\r\n        if (tree.isPreTerminal()) {\r\n            return tree;\r\n        } else {\r\n            return getPreTerminal(tree.children()[0], i, n);\r\n        }\r\n    } else {\r\n        if (tree.isPreTerminal()) {\r\n            i.set(i.intValue() + tree.yield().size());\r\n            return null;\r\n        } else {\r\n            for (Tree kid : tree.children()) {\r\n                Tree result = getPreTerminal(kid, i, n);\r\n                if (result != null) {\r\n                    return result;\r\n                }\r\n            }\r\n            return null;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.filter",
	"Comment": "filtered view of the given iterable.returns only those elementsfrom the iterable for which the given function returns true.",
	"Method": "Iterable<T> filter(Iterable<T> iterable,Predicate<T> accept){\r\n    return new Iterable<T>() {\r\n        public Iterator<T> iterator() {\r\n            return new Iterator<T>() {\r\n                Iterator<T> inner = iterable.iterator();\r\n                boolean queued = false;\r\n                T next = null;\r\n                public boolean hasNext() {\r\n                    prepare();\r\n                    return queued;\r\n                }\r\n                public T next() {\r\n                    prepare();\r\n                    if (!queued) {\r\n                        throw new RuntimeException(\"Filter .next() called with no next\");\r\n                    }\r\n                    T rv = next;\r\n                    next = null;\r\n                    queued = false;\r\n                    return rv;\r\n                }\r\n                public void prepare() {\r\n                    if (queued) {\r\n                        return;\r\n                    }\r\n                    while (inner.hasNext()) {\r\n                        T next = inner.next();\r\n                        if (accept.test(next)) {\r\n                            this.next = next;\r\n                            this.queued = true;\r\n                            return;\r\n                        }\r\n                    }\r\n                }\r\n                public void remove() {\r\n                    throw new UnsupportedOperationException();\r\n                }\r\n            };\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.filter",
	"Comment": "filtered view of the given iterable.returns only those elementsfrom the iterable for which the given function returns true.",
	"Method": "Iterable<T> filter(Iterable<T> iterable,Predicate<T> accept){\r\n    return new Iterator<T>() {\r\n        Iterator<T> inner = iterable.iterator();\r\n        boolean queued = false;\r\n        T next = null;\r\n        public boolean hasNext() {\r\n            prepare();\r\n            return queued;\r\n        }\r\n        public T next() {\r\n            prepare();\r\n            if (!queued) {\r\n                throw new RuntimeException(\"Filter .next() called with no next\");\r\n            }\r\n            T rv = next;\r\n            next = null;\r\n            queued = false;\r\n            return rv;\r\n        }\r\n        public void prepare() {\r\n            if (queued) {\r\n                return;\r\n            }\r\n            while (inner.hasNext()) {\r\n                T next = inner.next();\r\n                if (accept.test(next)) {\r\n                    this.next = next;\r\n                    this.queued = true;\r\n                    return;\r\n                }\r\n            }\r\n        }\r\n        public void remove() {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.filter",
	"Comment": "filtered view of the given iterable.returns only those elementsfrom the iterable for which the given function returns true.",
	"Method": "Iterable<T> filter(Iterable<T> iterable,Predicate<T> accept){\r\n    prepare();\r\n    return queued;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.filter",
	"Comment": "filtered view of the given iterable.returns only those elementsfrom the iterable for which the given function returns true.",
	"Method": "Iterable<T> filter(Iterable<T> iterable,Predicate<T> accept){\r\n    prepare();\r\n    if (!queued) {\r\n        throw new RuntimeException(\"Filter .next() called with no next\");\r\n    }\r\n    T rv = next;\r\n    next = null;\r\n    queued = false;\r\n    return rv;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.filter",
	"Comment": "filtered view of the given iterable.returns only those elementsfrom the iterable for which the given function returns true.",
	"Method": "Iterable<T> filter(Iterable<T> iterable,Predicate<T> accept){\r\n    if (queued) {\r\n        return;\r\n    }\r\n    while (inner.hasNext()) {\r\n        T next = inner.next();\r\n        if (accept.test(next)) {\r\n            this.next = next;\r\n            this.queued = true;\r\n            return;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.filter",
	"Comment": "filtered view of the given iterable.returns only those elementsfrom the iterable for which the given function returns true.",
	"Method": "Iterable<T> filter(Iterable<T> iterable,Predicate<T> accept){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.condition.ConditionalCopyValueTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return new String[] { columnToReplace };\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.InputPanel.getMatchTreeVisitor",
	"Comment": "check all active treebanks to find the trees that match the given pattern when interpretedas a tregex pattern.",
	"Method": "TRegexGUITreeVisitor getMatchTreeVisitor(String patternString,Thread t){\r\n    TRegexGUITreeVisitor vis = null;\r\n    try {\r\n        TregexPattern pattern = compiler.compile(patternString);\r\n        vis = new TRegexGUITreeVisitor(pattern);\r\n        List<FileTreeNode> treebanks = FilePanel.getInstance().getActiveTreebanks();\r\n        double multiplier = 100.0 / treebanks.size();\r\n        int treebankNum = 1;\r\n        for (FileTreeNode treebank : treebanks) {\r\n            if (t.isInterrupted()) {\r\n                t.interrupt();\r\n                SwingUtilities.invokeLater(() -> {\r\n                    setTregexState(false);\r\n                    InputPanel.this.searchThread = null;\r\n                });\r\n                return vis;\r\n            }\r\n            vis.setFilename(treebank.getFilename().intern());\r\n            treebank.getTreebank().apply(vis);\r\n            updateProgressBar(multiplier * treebankNum++);\r\n        }\r\n    } catch (OutOfMemoryError oome) {\r\n        vis = null;\r\n        doError(\"Sorry, search aborted as out of memory.\\nTry either running Tregex with more memory or sticking to searches that don't produce thousands of matches.\", oome);\r\n    } catch (Exception e) {\r\n        doError(\"Sorry, there was an error compiling or running the Tregex pattern.  Please press Help if you need assistance.\", e);\r\n    }\r\n    return vis;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.learning.impl.elements.GloVe.isEarlyTerminationHit",
	"Comment": "since glove is learning representations using elements cooccurences, all training is done in glove class internally, so only first thread will execute learning process, and the rest of parent threads will just exit learning process",
	"Method": "boolean isEarlyTerminationHit(){\r\n    return isTerminate.get();\r\n}"
}, {
	"Path": "org.datavec.camel.component.DataVecConsumer.inputFromExchange",
	"Comment": "stub, still need to fill out more of the end point yet..endpoint will likely be initialized with a split",
	"Method": "InputSplit inputFromExchange(Exchange exchange){\r\n    return marshaller.getSplit(exchange);\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.wordstore.VocabularyHolderTest.testSpecial1",
	"Comment": "in this test we make sure special words are not affected by truncation in extending vocab",
	"Method": "void testSpecial1(){\r\n    VocabularyHolder holder = new VocabularyHolder.Builder().minWordFrequency(1).build();\r\n    holder.addWord(\"test\");\r\n    holder.addWord(\"tests\");\r\n    holder.truncateVocabulary();\r\n    assertEquals(2, holder.numWords());\r\n    VocabCache cache = new InMemoryLookupCache();\r\n    holder.transferBackToVocabCache(cache);\r\n    VocabularyHolder holder2 = new VocabularyHolder.Builder().externalCache(cache).minWordFrequency(10).build();\r\n    holder2.addWord(\"testz\");\r\n    assertEquals(3, holder2.numWords());\r\n    holder2.truncateVocabulary();\r\n    assertEquals(2, holder2.numWords());\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.f1Score",
	"Comment": "sets the input and labels and returns a score for the predictionwrt true labels",
	"Method": "double f1Score(org.nd4j.linalg.dataset.api.DataSet data,double f1Score,INDArray input,INDArray labels){\r\n    feedForward(input);\r\n    setLabels(labels);\r\n    Evaluation eval = new Evaluation();\r\n    eval.eval(labels, labelProbabilities(input));\r\n    return eval.f1();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.TreeShapedStack.pop",
	"Comment": "returns the previous state.if the size of the stack is 0, anexception is thrown.if the size is 1, an empty node isreturned.",
	"Method": "TreeShapedStack<T> pop(){\r\n    if (size == 0) {\r\n        throw new EmptyStackException();\r\n    }\r\n    return previous;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.information",
	"Comment": "this returns the entropy for a given vector of probabilities.",
	"Method": "double information(double[] probabilities){\r\n    double total = 0.0;\r\n    for (double d : probabilities) {\r\n        total += (-1.0 * log2(d) * d);\r\n    }\r\n    return total;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.getConfiguration",
	"Comment": "this method returns configuration of this computationgraph",
	"Method": "ComputationGraphConfiguration getConfiguration(){\r\n    return configuration;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.pennchinese.CHTBLexer.yypushback",
	"Comment": "pushes the specified amount of characters back into the input stream.they will be read again by then next call of the scanning method",
	"Method": "void yypushback(int number){\r\n    if (number > yylength())\r\n        zzScanError(ZZ_PUSHBACK_2BIG);\r\n    zzMarkedPos -= number;\r\n}"
}, {
	"Path": "org.datavec.api.transform.metadata.BinaryMetaData.isValid",
	"Comment": "is the given object valid for this column,given the column type and anyrestrictions given by thecolumnmetadata object?",
	"Method": "boolean isValid(Writable writable,boolean isValid,Object input){\r\n    boolean value;\r\n    try {\r\n        value = Boolean.parseBoolean(input.toString());\r\n    } catch (NumberFormatException e) {\r\n        return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.algorithm.BaseClusteringAlgorithm.iterations",
	"Comment": "run clustering iterations until atermination condition is hit.this is done by first classifying all points,and then updating cluster centers based onthose classified points",
	"Method": "void iterations(){\r\n    int iterationCount = 0;\r\n    while ((clusteringStrategy.getTerminationCondition() != null && !clusteringStrategy.getTerminationCondition().isSatisfied(iterationHistory)) || iterationHistory.getMostRecentIterationInfo().isStrategyApplied()) {\r\n        currentIteration++;\r\n        removePoints();\r\n        classifyPoints();\r\n        applyClusteringStrategy();\r\n        log.info(\"Completed clustering iteration {}\", ++iterationCount);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.isEmpty",
	"Comment": "gets whether the cache is empty, including elements on disk.note that this returns true if the cache is empty.",
	"Method": "boolean isEmpty(){\r\n    return size() == 0;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.pred.SsurgTestManager.getNodeTest",
	"Comment": "given the id of the test, and the match name argument, returns a new instanceof the given nodetest, otherwise throws an exception if not available.",
	"Method": "NodeTest getNodeTest(String id,String matchName){\r\n    NodeTest test = (NodeTest) nodeTests.get(id).getConstructor(String.class).newInstance(matchName);\r\n    return test;\r\n}"
}, {
	"Path": "org.datavec.audio.fingerprint.FingerprintSimilarity.getScore",
	"Comment": "get the similarity score of the fingerprintsnumber of features found in the fingerprints per frame",
	"Method": "float getScore(){\r\n    return score;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.PTBLexer.handleHyphenatedNumber",
	"Comment": "if an apparent negative number is generated from a hyphenated word, tokenize the hyphen.",
	"Method": "void handleHyphenatedNumber(String in){\r\n    if (prevWord != null && in.length() >= 2 && in.charAt(0) == '-' && in.charAt(1) != '-') {\r\n        String lastWord = prevWord.originalText();\r\n        switch(lastWord) {\r\n            case \"mid\":\r\n            case \"late\":\r\n            case \"early\":\r\n                yypushback(in.length() - 1);\r\n            default:\r\n                if (lastWord.length() > 0 && lastWord.charAt(0) <= 57 && lastWord.charAt(0) >= 48 && prevWordAfter != null && prevWordAfter.length() == 0) {\r\n                    yypushback(in.length() - 1);\r\n                }\r\n                break;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.exec",
	"Comment": "executes the list of operations.this exec method is for only invoking operations rather than creating them",
	"Method": "List<DifferentialFunction> exec(List<DifferentialFunction> ops,Pair<Map<SDVariable, DifferentialFunction>, List<DifferentialFunction>> exec,String functionName,List<DifferentialFunction> exec,String functionName,List<DifferentialFunction> cachedOps,Pair<Map<SDVariable, DifferentialFunction>, List<DifferentialFunction>> exec){\r\n    if (log.isTraceEnabled()) {\r\n        log.trace(\"Starting execution: {} functions\", functionInstancesById.size());\r\n    }\r\n    if (!resolvedVariables)\r\n        resolveVariablesWith(new LinkedHashMap<String, INDArray>());\r\n    List<DifferentialFunction> ops = new ArrayList();\r\n    localFlowPath.set(new FlowPath());\r\n    val flowPath = localFlowPath.get();\r\n    Map<SDVariable, DifferentialFunction> opMap = new HashMap();\r\n    val funcs = new ArrayList<DifferentialFunction>(functionInstancesById.values());\r\n    List<String> funcNames = new ArrayList(functionInstancesById.keySet());\r\n    boolean onBackward = false;\r\n    val frames = new ArrayDeque<String>();\r\n    boolean inFrame = false;\r\n    boolean frameLeft = false;\r\n    boolean isExecBackwards = functionInstancesById.containsKey(GradientBackwardsMarker.OP_NAME);\r\n    associateSameDiffWithOpsAndVariables();\r\n    int i = 0;\r\n    int exec_counter = 0;\r\n    for (; i < funcs.size(); i++) {\r\n        ++exec_counter;\r\n        if (log.isTraceEnabled()) {\r\n            val f = funcs.get(i);\r\n            String[] argNames = f.argNames();\r\n            String[] outNames = f.outputVariablesNames();\r\n            log.trace(\"Starting execution of step {} of {}: Function {} (ownName={}) - {}\", exec_counter, funcs.size(), f.opName(), f.getOwnName(), f.getClass().getName());\r\n            log.trace(\"Function inputs: {} - Function outputs: {}\", (argNames == null ? \"(none)\" : Arrays.toString(argNames)), (outNames == null ? \"(none)\" : Arrays.toString(outNames)));\r\n            SDVariable[] args = f.args();\r\n            for (int arg = 0; arg < args.length; arg++) {\r\n                if (args[arg] == null) {\r\n                    log.trace(\"--> arg {} - {}: argument is null!\", arg, argNames[arg]);\r\n                } else {\r\n                    INDArray arr = args[arg].getArr();\r\n                    String arrShape = (arr == null ? \"<array not present>\" : Arrays.toString(arr.shape()));\r\n                    log.trace(\"--> arg {} - {}: array shape: {}\", arg, argNames[arg], arrShape);\r\n                }\r\n            }\r\n        }\r\n        val opName = funcs.get(i).opName();\r\n        if (!onBackward && GradientBackwardsMarker.OP_NAME.equals(opName)) {\r\n            onBackward = true;\r\n        }\r\n        if (GradientBackwardsMarker.OP_NAME.equals(opName))\r\n            continue;\r\n        DifferentialFunction differentialFunction = funcs.get(i);\r\n        if ((differentialFunction instanceof ExternalErrorsFunction)) {\r\n            if (isExecBackwards)\r\n                ((ExternalErrorsFunction) differentialFunction).updateBeforeExecution();\r\n            continue;\r\n        }\r\n        val ownName = differentialFunction.getOwnName();\r\n        flowPath.ensureNodeStateExists(differentialFunction.getOwnName());\r\n        if (differentialFunction instanceof SDVariable) {\r\n            if (log.isTraceEnabled()) {\r\n                log.trace(\"Skipping differentialFunction that is instanceof SDVariable: {}\", opName);\r\n            }\r\n            continue;\r\n        }\r\n        val args = getInputsForFunction(differentialFunction);\r\n        log.trace(\"Step: {}; Executing op [{}] for node [{}]\", exec_counter, opName, ownName);\r\n        boolean shouldSkip = false;\r\n        if (differentialFunction instanceof Merge) {\r\n            val arg0 = args[0];\r\n            val arg1 = args[1];\r\n            if (!flowPath.isActive(arg0) && !flowPath.isActive(arg1))\r\n                shouldSkip = true;\r\n        } else {\r\n            if (!(differentialFunction instanceof Exit)) {\r\n                if (frameLeft) {\r\n                    frameLeft = false;\r\n                    val frame_name = frames.removeLast();\r\n                    flowPath.activateFrame(frame_name, false);\r\n                    flowPath.forgetFrame(frame_name);\r\n                }\r\n                for (val input : args) {\r\n                    if (!flowPath.isActive(input)) {\r\n                        flowPath.markActive(differentialFunction.getOwnName(), false);\r\n                        shouldSkip = true;\r\n                        break;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        if (shouldSkip) {\r\n            if (log.isTraceEnabled()) {\r\n                log.trace(\"Skipping function {}: shouldSkip = true\", opName);\r\n            }\r\n            continue;\r\n        }\r\n        differentialFunction.resolvePropertiesFromSameDiffBeforeExecution();\r\n        flowPath.markActive(differentialFunction.getOwnName(), true);\r\n        if (differentialFunction instanceof LoopCond) {\r\n            if (log.isTraceEnabled())\r\n                log.trace(\"Starting execution of LoopCond op\");\r\n            val inputs = getInputVariablesForFunction(differentialFunction);\r\n            val array = inputs[0].getArr();\r\n            variableNameToArr.put(differentialFunction.getOwnName(), array.dup(array.ordering()));\r\n            flowPath.markExecuted(differentialFunction.getOwnName(), true);\r\n            if ((int) array.getDouble(0) == 1) {\r\n                val frameName = frames.getLast();\r\n                flowPath.incrementNumberOfCycles(frameName);\r\n            }\r\n        } else if (differentialFunction instanceof Enter) {\r\n            if (log.isTraceEnabled())\r\n                log.trace(\"Starting execution of Enter op\");\r\n            val inputs = getInputVariablesForFunction(differentialFunction);\r\n            val array = inputs[0].getArr();\r\n            val name = inputs[0].getVarName();\r\n            if (array != null)\r\n                variableNameToArr.put(differentialFunction.getOwnName(), array.dup(array.ordering()));\r\n            else {\r\n                val cleansed = name.replaceAll(\":.*\", \"\");\r\n                val list = lists.get(cleansed);\r\n                if (list != null)\r\n                    lists.put(ownName, list);\r\n            }\r\n            flowPath.markExecuted(differentialFunction.getOwnName(), true);\r\n            val frame_name = ((Enter) differentialFunction).getFrameName();\r\n            if (!flowPath.isRegisteredFrame(frame_name)) {\r\n                flowPath.registerFrame(frame_name);\r\n                frames.addLast(frame_name);\r\n                inFrame = true;\r\n            }\r\n        } else if (differentialFunction instanceof Exit) {\r\n            if (log.isTraceEnabled())\r\n                log.trace(\"Starting execution of Exit op\");\r\n            val frame_name = frames.getLast();\r\n            ((Exit) differentialFunction).setFrameName(frame_name);\r\n            if (!flowPath.isFrameActive(frame_name)) {\r\n                flowPath.markActive(differentialFunction.getOwnName(), false);\r\n                frameLeft = true;\r\n                continue;\r\n            }\r\n            if (flowPath.isRewindPlanned(frame_name)) {\r\n                flowPath.planRewind(frame_name, false);\r\n                val currentPosition = i;\r\n                i = flowPath.getRewindPosition(frame_name);\r\n                val startPosition = i + 1;\r\n                flowPath.setRewindPosition(frame_name, -1);\r\n                continue;\r\n            }\r\n            val inputs = getInputVariablesForFunction(differentialFunction);\r\n            val array = inputs[0].getArr();\r\n            val name = inputs[0].getVarName();\r\n            if (array != null)\r\n                variableNameToArr.put(differentialFunction.getOwnName(), array.dup(array.ordering()));\r\n            else {\r\n                val cleansed = name.replaceAll(\":.*\", \"\");\r\n                val list = lists.get(cleansed);\r\n                if (list != null)\r\n                    lists.put(ownName, list);\r\n            }\r\n            flowPath.markExecuted(differentialFunction.getOwnName(), true);\r\n            frameLeft = true;\r\n        } else if (differentialFunction instanceof NextIteration) {\r\n            if (log.isTraceEnabled())\r\n                log.trace(\"Starting execution of NextIteration op\");\r\n            val inputs = getInputVariablesForFunction(differentialFunction);\r\n            val frame_name = frames.getLast();\r\n            val array = inputs[0].getArr();\r\n            val name = inputs[0].getVarName();\r\n            if (array != null)\r\n                variableNameToArr.put(differentialFunction.getOwnName(), array.dup(array.ordering()));\r\n            else {\r\n                val cleansed = name.replaceAll(\":.*\", \"\");\r\n                val list = lists.get(cleansed);\r\n                if (list != null)\r\n                    lists.put(ownName, list);\r\n            }\r\n            flowPath.markExecuted(differentialFunction.getOwnName(), true);\r\n            if (!flowPath.isRewindPlanned(frame_name)) {\r\n                flowPath.planRewind(frame_name, true);\r\n                continue;\r\n            }\r\n        } else if (differentialFunction instanceof Merge) {\r\n            if (log.isTraceEnabled())\r\n                log.trace(\"Starting execution of Merge op\");\r\n            val inputs = getInputVariablesForFunction(differentialFunction);\r\n            val frame_name = frames.size() > 0 ? frames.getLast() : null;\r\n            if (frame_name != null)\r\n                flowPath.activateFrame(frame_name, true);\r\n            if (frame_name != null)\r\n                flowPath.setRewindPositionOnce(frame_name, i - 1);\r\n            if (inputs.length == 2) {\r\n                val secondArg = functionInstancesById.get(inputs[1].getVarName());\r\n                if (secondArg != null && secondArg instanceof NextIteration) {\r\n                    ((NextIteration) secondArg).setFrameName(frame_name);\r\n                }\r\n            }\r\n            if (flowPath.wasExecuted(inputs[1].getVarName())) {\r\n                val array = inputs[1].getArr();\r\n                val name = inputs[1].getVarName();\r\n                if (array != null)\r\n                    variableNameToArr.put(differentialFunction.getOwnName(), array.dup(array.ordering()));\r\n                else {\r\n                    val cleansed = name.replaceAll(\":.*\", \"\");\r\n                    val list = lists.get(cleansed);\r\n                    if (list != null)\r\n                        lists.put(ownName, list);\r\n                }\r\n                flowPath.markExecuted(inputs[1].getVarName(), false);\r\n            } else {\r\n                val array = inputs[0].getArr();\r\n                val name = inputs[0].getVarName();\r\n                if (array != null)\r\n                    variableNameToArr.put(differentialFunction.getOwnName(), array.dup(array.ordering()));\r\n                else {\r\n                    val cleansed = name.replaceAll(\":.*\", \"\");\r\n                    val list = lists.get(cleansed);\r\n                    if (list != null)\r\n                        lists.put(ownName, list);\r\n                }\r\n            }\r\n            flowPath.markExecuted(differentialFunction.getOwnName(), true);\r\n        } else if (differentialFunction instanceof Switch) {\r\n            if (log.isTraceEnabled())\r\n                log.trace(\"Starting execution of Switch op\");\r\n            ((CustomOp) differentialFunction).populateInputsAndOutputsFromSameDiff();\r\n            val inputs = getInputVariablesForFunction(differentialFunction);\r\n            val input = inputs[0].getArr();\r\n            val bool = inputs[1].getArr();\r\n            val name = inputs[0].getVarName();\r\n            if ((int) bool.getDouble(0) == 0) {\r\n                flowPath.setActiveBranch(differentialFunction.getOwnName(), 0);\r\n                flowPath.markActive(differentialFunction.getOwnName(), true);\r\n                flowPath.markActive(differentialFunction.getOwnName() + \":1\", false);\r\n                if (input != null)\r\n                    variableNameToArr.put(differentialFunction.getOwnName(), input.dup(input.ordering()));\r\n                else {\r\n                    val cleansed = name.replaceAll(\":.*\", \"\");\r\n                    val list = lists.get(cleansed);\r\n                    if (list != null)\r\n                        lists.put(ownName, list);\r\n                }\r\n            } else {\r\n                flowPath.setActiveBranch(differentialFunction.getOwnName(), 1);\r\n                if (input != null)\r\n                    variableNameToArr.put(differentialFunction.getOwnName() + \":1\", input.dup(input.ordering()));\r\n                else {\r\n                    val cleansed = name.replaceAll(\":.*\", \"\");\r\n                    val list = lists.get(cleansed);\r\n                    if (list != null)\r\n                        lists.put(ownName, list);\r\n                }\r\n                flowPath.markActive(differentialFunction.getOwnName(), false);\r\n                flowPath.markActive(differentialFunction.getOwnName() + \":1\", true);\r\n            }\r\n            flowPath.markExecuted(differentialFunction.getOwnName(), true);\r\n        } else if (differentialFunction instanceof BaseTensorOp) {\r\n            log.info(\"Starting execution of Tensor op [{}]\", opName);\r\n            val list = ((BaseTensorOp) differentialFunction).execute(this);\r\n            if (!lists.containsKey(list.getName()))\r\n                lists.put(list.getName(), list);\r\n            ops.add(differentialFunction);\r\n        } else if (differentialFunction instanceof If) {\r\n            if (log.isTraceEnabled())\r\n                log.trace(\"Starting execution of If op\");\r\n            If ifOp = (If) differentialFunction;\r\n            if (!onBackward) {\r\n                ifOp.getPredicateExecution().exec();\r\n                if (ifOp.getTargetBoolean().getArr().sumNumber().doubleValue() > 0) {\r\n                    ifOp.getLoopBodyExecution().exec();\r\n                    ifOp.exectedTrueOrFalse(true);\r\n                } else {\r\n                    ifOp.getFalseBodyExecution().exec();\r\n                    ifOp.exectedTrueOrFalse(false);\r\n                }\r\n            } else {\r\n                if (ifOp.getTrueBodyExecuted() != null) {\r\n                    Pair<Map<SDVariable, DifferentialFunction>, List<DifferentialFunction>> execBackwards = null;\r\n                    List<SDVariable> variablesForFunctions = null;\r\n                    if (ifOp.getTrueBodyExecuted()) {\r\n                        execBackwards = ifOp.getLoopBodyExecution().execBackwards();\r\n                        variablesForFunctions = ifOp.getLoopBodyExecution().getVariablesAssociatedWithFunctions(execBackwards.getRight());\r\n                    } else {\r\n                        execBackwards = ifOp.getFalseBodyExecution().execBackwards();\r\n                        variablesForFunctions = ifOp.getFalseBodyExecution().getVariablesAssociatedWithFunctions(execBackwards.getRight());\r\n                    }\r\n                    for (SDVariable variable : variablesForFunctions) {\r\n                        SDVariable proxyVar = var(variable);\r\n                    }\r\n                } else\r\n                    throw new ND4JIllegalStateException(\"No body was run.\");\r\n            }\r\n            flowPath.markExecuted(differentialFunction.getOwnName(), true);\r\n            ops.add(differentialFunction);\r\n        } else if (differentialFunction instanceof While) {\r\n            if (log.isTraceEnabled())\r\n                log.trace(\"Starting execution of While op\");\r\n            While whileOp = (While) differentialFunction;\r\n            if (!onBackward) {\r\n                SameDiff execBody = whileOp.getLoopBodyExecution();\r\n                whileOp.getPredicateExecution().exec();\r\n                if (execBody.outputs == null) {\r\n                    while (whileOp.getTargetBoolean().getArr().sumNumber().doubleValue() > 0) {\r\n                        execBody.exec();\r\n                        whileOp.getPredicateExecution().exec();\r\n                        whileOp.incrementLoopCounter();\r\n                    }\r\n                } else {\r\n                    if (whileOp.getTargetBoolean().getSameDiff().inputs == null) {\r\n                        whileOp.getTargetBoolean().getSameDiff().inputs = new SDVariable[whileOp.getInputVars().length];\r\n                        for (int e = 0; e < whileOp.getInputVars().length; e++) {\r\n                            whileOp.getTargetBoolean().getSameDiff().inputs[i] = whileOp.getTargetBoolean().getSameDiff().variables().get(i);\r\n                        }\r\n                    }\r\n                    while (whileOp.getTargetBoolean().getArr().sumNumber().doubleValue() > 0) {\r\n                        execBody.exec();\r\n                        val outputs = execBody.outputs;\r\n                        int cnt = 0;\r\n                        for (val out : execBody.outputs) {\r\n                            execBody.associateArrayWithVariable(out.getArr(), execBody.inputs[cnt]);\r\n                            whileOp.getTargetBoolean().getSameDiff().associateArrayWithVariable(out.getArr(), whileOp.getTargetBoolean().getSameDiff().inputs[cnt++]);\r\n                        }\r\n                        whileOp.getPredicateExecution().exec();\r\n                        whileOp.incrementLoopCounter();\r\n                    }\r\n                }\r\n                List<SDVariable> outputs = new ArrayList();\r\n                val outputFuncArgs = new ArrayList(execBody.functionInstancesById.values()).get(execBody.functionInstancesById.values().size() - 1).outputVariables();\r\n                outputs.addAll(Arrays.asList(outputFuncArgs));\r\n                whileOp.setOutputVars(outputs.toArray(new SDVariable[outputs.size()]));\r\n                ops.add(differentialFunction);\r\n            } else {\r\n                Pair<Map<SDVariable, DifferentialFunction>, List<DifferentialFunction>> mapListPair = whileOp.getLoopBodyExecution().execBackwards();\r\n                for (SDVariable variable : mapListPair.getFirst().keySet()) {\r\n                    variable.getArr().muli(whileOp.getNumLooped());\r\n                }\r\n            }\r\n            flowPath.markExecuted(differentialFunction.getOwnName(), true);\r\n        } else if (differentialFunction instanceof CustomOp) {\r\n            if (log.isTraceEnabled())\r\n                log.trace(\"Starting execution of CustomOp op\");\r\n            DynamicCustomOp customOp = (DynamicCustomOp) differentialFunction;\r\n            if (customOp.opName().equalsIgnoreCase(\"identity\")) {\r\n                val cleansed = args[0].replaceAll(\":.*\", \"\");\r\n                val list = lists.get(cleansed);\r\n                if (list != null) {\r\n                    lists.put(ownName, list);\r\n                    flowPath.markExecuted(differentialFunction.getOwnName(), true);\r\n                    ops.add(customOp);\r\n                    continue;\r\n                }\r\n            }\r\n            try {\r\n                customOp.populateInputsAndOutputsFromSameDiff();\r\n            } catch (Throwable t) {\r\n                throw new RuntimeException(\"Error populating inputs and outputs for function \\\"\" + differentialFunction.getOwnName() + \"\\\" of type \" + differentialFunction.getClass().getName(), t);\r\n            }\r\n            customOp.assertValidForExecution();\r\n            Nd4j.getExecutioner().exec(customOp);\r\n            flowPath.markExecuted(differentialFunction.getOwnName(), true);\r\n            ops.add(customOp);\r\n        } else if (differentialFunction instanceof Op) {\r\n            if (log.isTraceEnabled())\r\n                log.trace(\"Starting execution of Op op\");\r\n            val inputs = getInputVariablesForFunction(differentialFunction);\r\n            Op op = (Op) differentialFunction;\r\n            String outVarName = ((BaseOp) op).outputVariable().getVarName();\r\n            if (inputs != null && inputs.length > 0) {\r\n                op.setX(inputs[0].getArr());\r\n                if (inputs.length == 2)\r\n                    op.setY(inputs[1].getArr());\r\n            }\r\n            List<long[]> outputShape = ((BaseOp) op).calculateOutputShape();\r\n            Preconditions.checkState(outputShape != null && outputShape.size() == 1, \"Could not calculate output shape for op: %s\", op.getClass());\r\n            putOrUpdateShapeForVarName(outVarName, outputShape.get(0), true);\r\n            INDArray z = op.z();\r\n            Preconditions.checkNotNull(z, \"Could not get output array for op: %s\", op.getClass());\r\n            if (!Arrays.equals(outputShape.get(0), z.shape())) {\r\n                if (log.isTraceEnabled()) {\r\n                    log.trace(\"Existing op result (z) array shape for op {} was {}, allocating new array of shape {}\", op.getClass().getSimpleName(), Arrays.toString(z.shape()), Arrays.toString(outputShape.get(0)));\r\n                }\r\n                String fnName = funcNames.get(i);\r\n                String outputName = outgoingArgsReverse.get(fnName)[0];\r\n                SDVariable outputVar = getVariable(outputName);\r\n                putOrUpdateShapeForVarName(outputName, outputShape.get(0), true);\r\n                z = outputVar.storeAndAllocateNewArray();\r\n                op.setZ(z);\r\n            }\r\n            if (getArrForVarName(outVarName) != z) {\r\n                putOrUpdateArrayForVarName(outVarName, z);\r\n            }\r\n            if (differentialFunction.getDimensions() == null)\r\n                Nd4j.getExecutioner().exec(op);\r\n            else if (op.isExecSpecial()) {\r\n                op.exec();\r\n            } else {\r\n                int[] axes = differentialFunction.getDimensions();\r\n                if (differentialFunction instanceof Accumulation) {\r\n                    Accumulation accumulation = (Accumulation) differentialFunction;\r\n                    Nd4j.getExecutioner().exec(accumulation, axes);\r\n                    if (differentialFunction.outputVariable().getArr() == null) {\r\n                        val var = differentialFunction.outputVariables()[0];\r\n                        updateVariable(var.getVarName(), accumulation.z());\r\n                        updateShapeForVarName(var.getVarName(), accumulation.z().shape());\r\n                    }\r\n                } else if (differentialFunction instanceof BroadcastOp) {\r\n                    BroadcastOp broadcastOp = (BroadcastOp) differentialFunction;\r\n                    Nd4j.getExecutioner().exec(broadcastOp, axes);\r\n                } else if (differentialFunction instanceof GradientOp) {\r\n                    Nd4j.getExecutioner().exec(op);\r\n                } else if (differentialFunction instanceof IndexAccumulation) {\r\n                    IndexAccumulation indexAccumulation = (IndexAccumulation) differentialFunction;\r\n                    Nd4j.getExecutioner().exec(indexAccumulation, axes);\r\n                } else if (differentialFunction instanceof TransformOp) {\r\n                    TransformOp t = (TransformOp) differentialFunction;\r\n                    Nd4j.getExecutioner().exec(t, axes);\r\n                }\r\n            }\r\n            flowPath.markExecuted(differentialFunction.getOwnName(), true);\r\n            ops.add(differentialFunction);\r\n        } else {\r\n            throw new IllegalStateException(\"Unknown function type: \" + differentialFunction.getClass().getName());\r\n        }\r\n        if (log.isTraceEnabled()) {\r\n            log.trace(\"Execution completed for DifferentialFunction {} - {}\", opName, differentialFunction.getOwnName());\r\n            SDVariable[] outputVars = differentialFunction.outputVariables();\r\n            for (int x = 0; x < outputVars.length; x++) {\r\n                INDArray arr = outputVars[x].getArr();\r\n                String arrShape = (arr == null ? \"<no array>\" : Arrays.toString(arr.shape()));\r\n                log.trace(\"--> output {} - {}: array shape {}\", x, outputVars[x].getVarName(), arrShape);\r\n            }\r\n        }\r\n    }\r\n    if (log.isTraceEnabled()) {\r\n        log.trace(\"Execution complete\");\r\n    }\r\n    val ret = new Pair(opMap, ops);\r\n    exec_cache = ret;\r\n    if (parent != null) {\r\n        parent.exec_cache = exec_cache;\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getLong",
	"Comment": "get the value of the name property as a long.if no such property is specified, or if the specified value is not a validlong, then defaultvalue is returned.",
	"Method": "long getLong(String name,long defaultValue){\r\n    String valueString = get(name);\r\n    if (valueString == null)\r\n        return defaultValue;\r\n    try {\r\n        String hexString = getHexDigits(valueString);\r\n        if (hexString != null) {\r\n            return Long.parseLong(hexString, 16);\r\n        }\r\n        return Long.parseLong(valueString);\r\n    } catch (NumberFormatException e) {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.wordstore.inmemory.AbstractCache.vocabWords",
	"Comment": "returns collection of sequenceelements stored in this vocabulary",
	"Method": "Collection<T> vocabWords(){\r\n    return vocabulary.values();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CollectionUtils.compareLists",
	"Comment": "provides a consistent ordering over lists. first compares by the firstelement. if that element is equal, the next element is considered, and so",
	"Method": "int compareLists(List<T> list1,List<T> list2){\r\n    if (list1 == null && list2 == null)\r\n        return 0;\r\n    if (list1 == null || list2 == null) {\r\n        throw new IllegalArgumentException();\r\n    }\r\n    int size1 = list1.size();\r\n    int size2 = list2.size();\r\n    int size = Math.min(size1, size2);\r\n    for (int i = 0; i < size; i++) {\r\n        int c = list1.get(i).compareTo(list2.get(i));\r\n        if (c != 0)\r\n            return c;\r\n    }\r\n    if (size1 < size2)\r\n        return -1;\r\n    if (size1 > size2)\r\n        return 1;\r\n    return 0;\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.MultipleEpochsIterator.next",
	"Comment": "like the standard next method but allows acustomizable number of examples returned",
	"Method": "DataSet next(int num,DataSet next){\r\n    return next(-1);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.constituents",
	"Comment": "returns the constituents generated by the parse tree.the constituents of a sentence include the preterminal categoriesbut not the leaves.",
	"Method": "Set<Constituent> constituents(Set<Constituent> constituents,ConstituentFactory cf,Set<Constituent> constituents,ConstituentFactory cf,int maxDepth,Set<Constituent> constituents,ConstituentFactory cf,boolean charLevel,Set<Constituent> constituents,ConstituentFactory cf,boolean charLevel,Predicate<Tree> filter,int constituents,Set<Constituent> constituentsSet,int left,ConstituentFactory cf,boolean charLevel,Predicate<Tree> filter,int maxDepth,int depth){\r\n    if (isPreTerminal())\r\n        return left + ((charLevel) ? firstChild().value().length() : 1);\r\n    int position = left;\r\n    Tree[] kids = children();\r\n    for (Tree kid : kids) {\r\n        position = kid.constituents(constituentsSet, position, cf, charLevel, filter, maxDepth, depth + 1);\r\n    }\r\n    if ((filter == null || filter.test(this)) && (maxDepth < 0 || depth <= maxDepth)) {\r\n        constituentsSet.add(cf.newConstituent(left, position - 1, label(), score()));\r\n    }\r\n    return position;\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.ReconstructionDataSetIterator.next",
	"Comment": "like the standard next method but allows acustomizable number of examples returned",
	"Method": "DataSet next(int num,DataSet next){\r\n    DataSet next = iter.next();\r\n    next.setLabels(next.getFeatures());\r\n    return next;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.DisplayMatchesPanel.addMatch",
	"Comment": "adds the given tree to the display without removing alreadydisplayed trees",
	"Method": "void addMatch(TreeFromFile match,List<Tree> matchedParts){\r\n    JPanel treeDisplay = new JPanel(new BorderLayout());\r\n    JTextField filename = new JTextField(\"From file: \" + match.getFilename());\r\n    filename.setEditable(false);\r\n    MouseInputAdapter listener = new FilenameMouseInputAdapter(filename);\r\n    filename.addMouseListener(listener);\r\n    filename.addMouseMotionListener(listener);\r\n    treeDisplay.add(filename, BorderLayout.NORTH);\r\n    if (TregexGUI.getInstance().isTdiffEnabled()) {\r\n        tjp = getTreeJPanel(match.getDiffDecoratedTree(), matchedParts);\r\n        tjp.setDiffConstituents(match.getDiffConstituents());\r\n    } else {\r\n        tjp = getTreeJPanel(match.getTree(), matchedParts);\r\n    }\r\n    matchedPartCoordinates = tjp.getMatchedPartCoordinates();\r\n    matchedPartCoordinateIdx = -1;\r\n    treeDisplay.add(tjp, BorderLayout.CENTER);\r\n    filename.setOpaque(true);\r\n    filename.setBackground(tjp.getBackground());\r\n    filename.setBorder(BorderFactory.createEmptyBorder(0, 5, 0, 0));\r\n    scroller.setViewportView(treeDisplay);\r\n    this.revalidate();\r\n    this.repaint();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.XMLUtils.getTagElementTriplesFromFile",
	"Comment": "returns the elements in the given file with the given tag associated withthe text content of the two previous siblings and two next siblings.",
	"Method": "List<Triple<String, Element, String>> getTagElementTriplesFromFile(File f,String tag){\r\n    List<Triple<String, Element, String>> sents = Generics.newArrayList();\r\n    try {\r\n        sents = getTagElementTriplesFromFileSAXException(f, tag);\r\n    } catch (SAXException e) {\r\n        log.warn(e);\r\n    }\r\n    return sents;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.Ssurgeon.parseEditLine",
	"Comment": "given a string entry, converts it into a ssurgeonedit object.",
	"Method": "SsurgeonEdit parseEditLine(String editLine){\r\n    String[] tuples1 = editLine.split(\"\\\\s+\", 2);\r\n    if (tuples1.length < 2) {\r\n        throw new IllegalArgumentException(\"Error in SsurgeonEdit.parseEditLine: invalid number of arguments\");\r\n    }\r\n    String command = tuples1[0];\r\n    String[] argsArray = parseArgs(tuples1[1]);\r\n    SsurgeonArgs argsBox = new SsurgeonArgs();\r\n    for (int argIndex = 0; argIndex < argsArray.length; ++argIndex) {\r\n        switch(argsArray[argIndex]) {\r\n            case GOV_NODENAME_ARG:\r\n                argsBox.govNodeName = argsArray[argIndex + 1];\r\n                argIndex += 2;\r\n                break;\r\n            case DEP_NODENAME_ARG:\r\n                argsBox.dep = argsArray[argIndex + 1];\r\n                argIndex += 2;\r\n                break;\r\n            case EDGE_NAME_ARG:\r\n                argsBox.edge = argsArray[argIndex + 1];\r\n                argIndex += 2;\r\n                break;\r\n            case RELN_ARG:\r\n                argsBox.reln = argsArray[argIndex + 1];\r\n                argIndex += 2;\r\n                break;\r\n            case NODENAME_ARG:\r\n                argsBox.node = argsArray[argIndex + 1];\r\n                argIndex += 2;\r\n                break;\r\n            case NODE_PROTO_ARG:\r\n                argsBox.nodeString = argsArray[argIndex + 1];\r\n                argIndex += 2;\r\n                break;\r\n            case WEIGHT_ARG:\r\n                argsBox.weight = Double.valueOf(argsArray[argIndex + 1]);\r\n                argIndex += 2;\r\n                break;\r\n            case NAME_ARG:\r\n                argsBox.name = argsArray[argIndex + 1];\r\n                argIndex += 2;\r\n                break;\r\n            default:\r\n                throw new IllegalArgumentException(\"Parsing Ssurgeon args: unknown flag \" + argsArray[argIndex]);\r\n        }\r\n    }\r\n    SsurgeonEdit retEdit;\r\n    if (command.equalsIgnoreCase(AddDep.LABEL)) {\r\n        retEdit = AddDep.createEngAddDep(argsBox.govNodeName, argsBox.reln, argsBox.nodeString);\r\n    } else if (command.equalsIgnoreCase(AddNode.LABEL)) {\r\n        retEdit = AddNode.createAddNode(argsBox.nodeString, argsBox.name);\r\n    } else if (command.equalsIgnoreCase(AddEdge.LABEL)) {\r\n        retEdit = AddEdge.createEngAddEdge(argsBox.govNodeName, argsBox.dep, argsBox.reln);\r\n    } else if (command.equalsIgnoreCase(DeleteGraphFromNode.LABEL)) {\r\n        retEdit = new DeleteGraphFromNode(argsBox.node);\r\n    } else if (command.equalsIgnoreCase(RemoveEdge.LABEL)) {\r\n        retEdit = new RemoveEdge(GrammaticalRelation.valueOf(argsBox.reln), argsBox.govNodeName, argsBox.dep);\r\n    } else if (command.equalsIgnoreCase(RemoveNamedEdge.LABEL)) {\r\n        retEdit = new RemoveNamedEdge(argsBox.edge, argsBox.govNodeName, argsBox.dep);\r\n    } else if (command.equalsIgnoreCase(SetRoots.LABEL)) {\r\n        String[] names = tuples1[1].split(\"\\\\s+\");\r\n        List<String> newRoots = Arrays.asList(names);\r\n        retEdit = new SetRoots(newRoots);\r\n    } else if (command.equalsIgnoreCase(KillNonRootedNodes.LABEL)) {\r\n        retEdit = new KillNonRootedNodes();\r\n    } else if (command.equalsIgnoreCase(KillAllIncomingEdges.LABEL)) {\r\n        retEdit = new KillAllIncomingEdges(argsBox.node);\r\n    } else {\r\n        throw new IllegalArgumentException(\"Error in SsurgeonEdit.parseEditLine: command '\" + command + \"' is not supported\");\r\n    }\r\n    return retEdit;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.TregexGUI.doSaveSentencesFile",
	"Comment": "method for saving the sentences with trees that match the current tregex expression",
	"Method": "void doSaveSentencesFile(){\r\n    if (chooser == null)\r\n        chooser = createFileChooser();\r\n    int status = chooser.showSaveDialog(this);\r\n    if (status == JFileChooser.APPROVE_OPTION) {\r\n        Thread t = new Thread() {\r\n            @Override\r\n            public void run() {\r\n                try {\r\n                    BufferedWriter out = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(chooser.getSelectedFile()), FileTreeModel.getCurEncoding()));\r\n                    String str = MatchesPanel.getInstance().getMatchedSentences();\r\n                    out.write(str);\r\n                    out.flush();\r\n                    out.close();\r\n                } catch (Exception e) {\r\n                    log.info(\"Exception in save\");\r\n                    e.printStackTrace();\r\n                }\r\n            }\r\n        };\r\n        t.start();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.TregexGUI.doSaveSentencesFile",
	"Comment": "method for saving the sentences with trees that match the current tregex expression",
	"Method": "void doSaveSentencesFile(){\r\n    try {\r\n        BufferedWriter out = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(chooser.getSelectedFile()), FileTreeModel.getCurEncoding()));\r\n        String str = MatchesPanel.getInstance().getMatchedSentences();\r\n        out.write(str);\r\n        out.flush();\r\n        out.close();\r\n    } catch (Exception e) {\r\n        log.info(\"Exception in save\");\r\n        e.printStackTrace();\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ops.DynamicCustomOp.builder",
	"Comment": "this method takes custom opname, and return op dynamiccustomopsbuilder instance",
	"Method": "DynamicCustomOpsBuilder builder(String opName){\r\n    val map = Nd4j.getExecutioner().getCustomOperations();\r\n    val lcName = map.containsKey(opName) ? opName : opName.toLowerCase();\r\n    val desc = map.get(lcName);\r\n    if (desc == null)\r\n        throw new ND4JIllegalStateException(\"Unknown operations requested: [\" + opName + \"]\");\r\n    return new DynamicCustomOpsBuilder(lcName, desc.getHash(), desc.getNumInputs(), desc.getNumOutputs(), desc.isAllowsInplace(), desc.getNumTArgs(), desc.getNumIArgs());\r\n}"
}, {
	"Path": "org.datavec.api.io.WritableUtils.readCompressedStringArray",
	"Comment": "write a string array as a nework int n, followed by int n byte array strings.could be generalised using introspection. handles null arrays and null values.",
	"Method": "String[] readCompressedStringArray(DataInput in){\r\n    int len = in.readInt();\r\n    if (len == -1)\r\n        return null;\r\n    String[] s = new String[len];\r\n    for (int i = 0; i < len; i++) {\r\n        s[i] = readCompressedString(in);\r\n    }\r\n    return s;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ops.aggregates.Batch.isFull",
	"Comment": "this method checks, if number of batched aggregates equals to maximum possible value",
	"Method": "boolean isFull(){\r\n    return batchLimit == numAggregates;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.IntCounter.keysAt",
	"Comment": "returns the set of keys that have exactly the given count.this set may have 0 elements but will not be null.",
	"Method": "Set<E> keysAt(int count){\r\n    Set<E> keys = Generics.newHashSet();\r\n    for (E key : map.keySet()) {\r\n        if (getIntCount(key) == count) {\r\n            keys.add(key);\r\n        }\r\n    }\r\n    return keys;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CoreMaps.dumpCoreMap",
	"Comment": "utility function for dumping all the keys and values of a coremap to a string.",
	"Method": "String dumpCoreMap(CoreMap cm){\r\n    StringBuilder sb = new StringBuilder();\r\n    dumpCoreMapToStringBuilder(cm, sb);\r\n    return sb.toString();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ConfusionMatrix.printTable",
	"Comment": "prints the current confusion in table form to a string, with contingency",
	"Method": "String printTable(){\r\n    List<U> sortedLabels = sortKeys();\r\n    if (confTable.size() == 0) {\r\n        return \"Empty table!\";\r\n    }\r\n    StringWriter ret = new StringWriter();\r\n    ret.write(StringUtils.padLeft(\"Guess/Gold\", leftPadSize));\r\n    for (int i = 0; i < sortedLabels.size(); i++) {\r\n        String placeHolder = getPlaceHolder(i, sortedLabels.get(i));\r\n        ret.write(StringUtils.padLeft(placeHolder, delimPadSize));\r\n    }\r\n    ret.write(\"    Marg. (Guess)\");\r\n    ret.write(\"\\n\");\r\n    for (int guessI = 0; guessI < sortedLabels.size(); guessI++) {\r\n        String placeHolder = getPlaceHolder(guessI, sortedLabels.get(guessI));\r\n        ret.write(StringUtils.padLeft(placeHolder, leftPadSize));\r\n        U guess = sortedLabels.get(guessI);\r\n        for (U gold : sortedLabels) {\r\n            Integer value = get(guess, gold);\r\n            ret.write(StringUtils.padLeft(value.toString(), delimPadSize));\r\n        }\r\n        ret.write(StringUtils.padLeft(guessMarginal(guess).toString(), delimPadSize));\r\n        ret.write(\"\\n\");\r\n    }\r\n    ret.write(StringUtils.padLeft(\"Marg. (Gold)\", leftPadSize));\r\n    for (U gold : sortedLabels) {\r\n        ret.write(StringUtils.padLeft(goldMarginal(gold).toString(), delimPadSize));\r\n    }\r\n    ret.write(\"\\n\\n\");\r\n    for (int labelI = 0; labelI < sortedLabels.size(); labelI++) {\r\n        U classLabel = sortedLabels.get(labelI);\r\n        String placeHolder = getPlaceHolder(labelI, classLabel);\r\n        ret.write(StringUtils.padLeft(placeHolder, leftPadSize));\r\n        if (!useRealLabels) {\r\n            ret.write(\" = \");\r\n            ret.write(classLabel.toString());\r\n        }\r\n        ret.write(StringUtils.padLeft(\"\", delimPadSize));\r\n        Contingency contingency = getContingency(classLabel);\r\n        ret.write(contingency.toString());\r\n        ret.write(\"\\n\");\r\n    }\r\n    return ret.toString();\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.column.RenameColumnsTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return newNames.toArray(new String[newNames.size()]);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraPennTreeNormalizer.normalizeTerminal",
	"Comment": "normalizes a leaf contents.this implementation interns the leaf.",
	"Method": "String normalizeTerminal(String leaf){\r\n    return leaf.intern();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArrayCoreMap.compact",
	"Comment": "reduces memory consumption to the minimum for representing the valuescurrently stored stored in this object.",
	"Method": "void compact(){\r\n    if (keys.length > size) {\r\n        Class[] newKeys = new Class[size];\r\n        Object[] newValues = new Object[size];\r\n        System.arraycopy(keys, 0, newKeys, 0, size);\r\n        System.arraycopy(values, 0, newValues, 0, size);\r\n        keys = ErasureUtils.uncheckedCast(newKeys);\r\n        values = newValues;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.pennchinese.UniversalChineseSemanticHeadFinder.ruleChanges",
	"Comment": "makes modifications of head finder rules to better fit with semantic notions of heads.",
	"Method": "void ruleChanges(){\r\n    nonTerminalInfo.put(\"VP\", new String[][] { { \"left\", \"VP\", \"VCD\", \"VSB\", \"VPT\", \"VV\", \"VCP\", \"VA\", \"VE\", \"IP\", \"VRD\", \"VNV\", \"NP\" }, leftExceptPunct });\r\n    nonTerminalInfo.put(\"CP\", new String[][] { { \"rightexcept\", \"DEC\", \"WHNP\", \"WHPP\", \"SP\" }, rightExceptPunct });\r\n    nonTerminalInfo.put(\"DVP\", new String[][] { { \"leftdis\", \"VP\", \"ADVP\" } });\r\n    nonTerminalInfo.put(\"LST\", new String[][] { { \"right\", \"CD\", \"NP\", \"QP\", \"PU\" } });\r\n    nonTerminalInfo.put(\"QP\", new String[][] { { \"right\", \"QP\", \"CD\", \"OD\", \"NP\", \"NT\", \"M\", \"CLP\" } });\r\n    nonTerminalInfo.put(\"PP\", new String[][] { { \"leftexcept\", \"P\" } });\r\n    nonTerminalInfo.put(\"LCP\", new String[][] { { \"leftexcept\", \"LC\" } });\r\n    nonTerminalInfo.put(\"DNP\", new String[][] { { \"rightexcept\", \"DEG\", \"DEC\" } });\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasConstraintUtils.getConstraintsFromConfig",
	"Comment": "get constraint initialization from keras layer configuration.",
	"Method": "LayerConstraint getConstraintsFromConfig(Map<String, Object> layerConfig,String constraintField,KerasLayerConfiguration conf,int kerasMajorVersion){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (!innerConfig.containsKey(constraintField)) {\r\n        return null;\r\n    }\r\n    HashMap constraintMap = (HashMap) innerConfig.get(constraintField);\r\n    if (constraintMap == null)\r\n        return null;\r\n    String kerasConstraint;\r\n    if (constraintMap.containsKey(conf.getLAYER_FIELD_CONSTRAINT_NAME())) {\r\n        kerasConstraint = (String) constraintMap.get(conf.getLAYER_FIELD_CONSTRAINT_NAME());\r\n    } else {\r\n        throw new InvalidKerasConfigurationException(\"Keras layer is missing \" + conf.getLAYER_FIELD_CONSTRAINT_NAME() + \" field\");\r\n    }\r\n    Map<String, Object> constraintConfig;\r\n    if (kerasMajorVersion == 2) {\r\n        constraintConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(constraintMap, conf);\r\n    } else {\r\n        constraintConfig = constraintMap;\r\n    }\r\n    LayerConstraint layerConstraint = mapConstraint(kerasConstraint, conf, constraintConfig);\r\n    return layerConstraint;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.getParentsWithReln",
	"Comment": "returns a set of all parents bearing a certain grammatical relation, or anempty set if none.",
	"Method": "Set<IndexedWord> getParentsWithReln(IndexedWord vertex,GrammaticalRelation reln){\r\n    if (vertex.equals(IndexedWord.NO_WORD))\r\n        return Collections.emptySet();\r\n    if (!containsVertex(vertex))\r\n        throw new IllegalArgumentException();\r\n    Set<IndexedWord> parentList = wordMapFactory.newSet();\r\n    for (SemanticGraphEdge edge : incomingEdgeIterable(vertex)) {\r\n        if (edge.getRelation().equals(reln)) {\r\n            parentList.add(edge.getSource());\r\n        }\r\n    }\r\n    return parentList;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.TreeGraphNode.label",
	"Comment": "returns the label associated with the current node, or nullif there is no label.",
	"Method": "CoreLabel label(){\r\n    return label;\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getFloat",
	"Comment": "get the value of the name property as a float.if no such property is specified, or if the specified value is not a validfloat, then defaultvalue is returned.",
	"Method": "float getFloat(String name,float defaultValue){\r\n    String valueString = get(name);\r\n    if (valueString == null)\r\n        return defaultValue;\r\n    try {\r\n        return Float.parseFloat(valueString);\r\n    } catch (NumberFormatException e) {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.cluster.ClusterUtils.classifyPoints",
	"Comment": "classify the set of points base on cluster centers. this also adds each point to the clusterset",
	"Method": "ClusterSetInfo classifyPoints(ClusterSet clusterSet,List<Point> points,ExecutorService executorService){\r\n    final ClusterSetInfo clusterSetInfo = ClusterSetInfo.initialize(clusterSet, true);\r\n    List<Runnable> tasks = new ArrayList();\r\n    for (final Point point : points) {\r\n        tasks.add(new Runnable() {\r\n            public void run() {\r\n                try {\r\n                    PointClassification result = classifyPoint(clusterSet, point);\r\n                    if (result.isNewLocation())\r\n                        clusterSetInfo.getPointLocationChange().incrementAndGet();\r\n                    clusterSetInfo.getClusterInfo(result.getCluster().getId()).getPointDistancesFromCenter().put(point.getId(), result.getDistanceFromCenter());\r\n                } catch (Throwable t) {\r\n                    log.warn(\"Error classifying point\", t);\r\n                }\r\n            }\r\n        });\r\n    }\r\n    MultiThreadUtils.parallelTasks(tasks, executorService);\r\n    return clusterSetInfo;\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.cluster.ClusterUtils.classifyPoints",
	"Comment": "classify the set of points base on cluster centers. this also adds each point to the clusterset",
	"Method": "ClusterSetInfo classifyPoints(ClusterSet clusterSet,List<Point> points,ExecutorService executorService){\r\n    try {\r\n        PointClassification result = classifyPoint(clusterSet, point);\r\n        if (result.isNewLocation())\r\n            clusterSetInfo.getPointLocationChange().incrementAndGet();\r\n        clusterSetInfo.getClusterInfo(result.getCluster().getId()).getPointDistancesFromCenter().put(point.getId(), result.getDistanceFromCenter());\r\n    } catch (Throwable t) {\r\n        log.warn(\"Error classifying point\", t);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.Word2Vec.setTokenizerFactory",
	"Comment": "this method defines tokenizerfactory instance to be using during model building",
	"Method": "void setTokenizerFactory(TokenizerFactory tokenizerFactory){\r\n    this.tokenizerFactory = tokenizerFactory;\r\n    if (sentenceIter != null) {\r\n        SentenceTransformer transformer = new SentenceTransformer.Builder().iterator(sentenceIter).tokenizerFactory(this.tokenizerFactory).build();\r\n        this.iterator = new AbstractSequenceIterator.Builder(transformer).build();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.Ssurgeon.getResource",
	"Comment": "returns the given resource with the id.if does not exist, will throw exception.",
	"Method": "SsurgeonWordlist getResource(String id){\r\n    return wordListResources.get(id);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasModelUtils.parseModelConfig",
	"Comment": "parse keras model configuration from json or yaml string representation",
	"Method": "Map<String, Object> parseModelConfig(String modelJson,String modelYaml){\r\n    Map<String, Object> modelConfig;\r\n    if (modelJson != null)\r\n        modelConfig = parseJsonString(modelJson);\r\n    else if (modelYaml != null)\r\n        modelConfig = parseYamlString(modelYaml);\r\n    else\r\n        throw new InvalidKerasConfigurationException(\"Requires model configuration as either JSON or YAML string.\");\r\n    return modelConfig;\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.iterator.impl.EmnistDataSetIterator.numExamplesTrain",
	"Comment": "get the number of training examples for the specified subset",
	"Method": "int numExamplesTrain(Set dataSet){\r\n    switch(dataSet) {\r\n        case COMPLETE:\r\n            return NUM_COMPLETE_TRAIN;\r\n        case MERGE:\r\n            return NUM_MERGE_TRAIN;\r\n        case BALANCED:\r\n            return NUM_BALANCED_TRAIN;\r\n        case LETTERS:\r\n            return NUM_LETTERS_TRAIN;\r\n        case DIGITS:\r\n            return NUM_DIGITS_TRAIN;\r\n        case MNIST:\r\n            return NUM_MNIST_TRAIN;\r\n        default:\r\n            throw new UnsupportedOperationException(\"Unknown Set: \" + dataSet);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.sequencevectors.graph.walkers.impl.RandomWalker.hasNext",
	"Comment": "this method checks, if walker has any more sequences left in queue",
	"Method": "boolean hasNext(){\r\n    return position.get() < sourceGraph.numVertices();\r\n}"
}, {
	"Path": "org.deeplearning4j.arbiter.ComputationGraphSpace.fromJson",
	"Comment": "instantiate a computation graph space froma raw json string",
	"Method": "ComputationGraphSpace fromJson(String json){\r\n    try {\r\n        return JsonMapper.getMapper().readValue(json, ComputationGraphSpace.class);\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.TregexGUI.setSaveEnabled",
	"Comment": "used to change the status of the save file menu item to reflectwhether any trees are available to save.",
	"Method": "void setSaveEnabled(boolean enabled){\r\n    if (saveMatches.isEnabled() != enabled) {\r\n        saveMatches.setEnabled(enabled);\r\n        saveSentences.setEnabled(enabled);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.GrammaticalStructure.getGrammaticalRelation",
	"Comment": "get grammaticalrelation between gov and dep, and null if gov is not thegovernor of dep.",
	"Method": "GrammaticalRelation getGrammaticalRelation(int govIndex,int depIndex,GrammaticalRelation getGrammaticalRelation,IndexedWord gov,IndexedWord dep){\r\n    List<GrammaticalRelation> labels = Generics.newArrayList();\r\n    for (TypedDependency dependency : typedDependencies(Extras.MAXIMAL)) {\r\n        if (dependency.gov().equals(gov) && dependency.dep().equals(dep)) {\r\n            labels.add(dependency.reln());\r\n        }\r\n    }\r\n    return getGrammaticalRelationCommonAncestor(gov, gov, dep, dep, labels);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.drop",
	"Comment": "returns a view of the given data, ignoring the first todrop elements.",
	"Method": "Iterable<T> drop(T[] array,int toDrop,Iterable<T> drop,Iterable<T> iterable,int toDrop){\r\n    return new Iterable<T>() {\r\n        final Iterator<T> iterator = iterable.iterator();\r\n        public Iterator<T> iterator() {\r\n            return new Iterator<T>() {\r\n                int skipped = 0;\r\n                public boolean hasNext() {\r\n                    while (skipped < toDrop && iterator.hasNext()) {\r\n                        iterator.next();\r\n                        skipped += 1;\r\n                    }\r\n                    return iterator.hasNext();\r\n                }\r\n                public T next() {\r\n                    while (skipped < toDrop && iterator.hasNext()) {\r\n                        iterator.next();\r\n                        skipped += 1;\r\n                    }\r\n                    return iterator.next();\r\n                }\r\n                public void remove() {\r\n                    iterator.remove();\r\n                }\r\n            };\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.drop",
	"Comment": "returns a view of the given data, ignoring the first todrop elements.",
	"Method": "Iterable<T> drop(T[] array,int toDrop,Iterable<T> drop,Iterable<T> iterable,int toDrop){\r\n    return new Iterator<T>() {\r\n        int skipped = 0;\r\n        public boolean hasNext() {\r\n            while (skipped < toDrop && iterator.hasNext()) {\r\n                iterator.next();\r\n                skipped += 1;\r\n            }\r\n            return iterator.hasNext();\r\n        }\r\n        public T next() {\r\n            while (skipped < toDrop && iterator.hasNext()) {\r\n                iterator.next();\r\n                skipped += 1;\r\n            }\r\n            return iterator.next();\r\n        }\r\n        public void remove() {\r\n            iterator.remove();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.drop",
	"Comment": "returns a view of the given data, ignoring the first todrop elements.",
	"Method": "Iterable<T> drop(T[] array,int toDrop,Iterable<T> drop,Iterable<T> iterable,int toDrop){\r\n    while (skipped < toDrop && iterator.hasNext()) {\r\n        iterator.next();\r\n        skipped += 1;\r\n    }\r\n    return iterator.hasNext();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.drop",
	"Comment": "returns a view of the given data, ignoring the first todrop elements.",
	"Method": "Iterable<T> drop(T[] array,int toDrop,Iterable<T> drop,Iterable<T> iterable,int toDrop){\r\n    while (skipped < toDrop && iterator.hasNext()) {\r\n        iterator.next();\r\n        skipped += 1;\r\n    }\r\n    return iterator.next();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.drop",
	"Comment": "returns a view of the given data, ignoring the first todrop elements.",
	"Method": "Iterable<T> drop(T[] array,int toDrop,Iterable<T> drop,Iterable<T> iterable,int toDrop){\r\n    iterator.remove();\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.ROC.merge",
	"Comment": "merge this roc instance with another.this roc instance is modified, by adding the stats from the other instance.",
	"Method": "void merge(ROC other){\r\n    if (this.thresholdSteps != other.thresholdSteps) {\r\n        throw new UnsupportedOperationException(\"Cannot merge ROC instances with different numbers of threshold steps (\" + this.thresholdSteps + \" vs. \" + other.thresholdSteps + \")\");\r\n    }\r\n    this.countActualPositive += other.countActualPositive;\r\n    this.countActualNegative += other.countActualNegative;\r\n    this.auc = null;\r\n    this.auprc = null;\r\n    this.rocCurve = null;\r\n    this.prCurve = null;\r\n    if (isExact) {\r\n        if (other.exampleCount == 0) {\r\n            return;\r\n        }\r\n        if (this.exampleCount == 0) {\r\n            this.exampleCount = other.exampleCount;\r\n            this.probAndLabel = other.probAndLabel;\r\n            return;\r\n        }\r\n        if (this.exampleCount + other.exampleCount > this.probAndLabel.size(0)) {\r\n            val newSize = this.probAndLabel.size(0) + Math.max(other.probAndLabel.size(0), exactAllocBlockSize);\r\n            INDArray newProbAndLabel = Nd4j.create(newSize, 2);\r\n            newProbAndLabel.put(new INDArrayIndex[] { interval(0, exampleCount), all() }, probAndLabel.get(interval(0, exampleCount), all()));\r\n            probAndLabel = newProbAndLabel;\r\n        }\r\n        INDArray toPut = other.probAndLabel.get(interval(0, other.exampleCount), all());\r\n        probAndLabel.put(new INDArrayIndex[] { interval(exampleCount, exampleCount + other.exampleCount), all() }, toPut);\r\n    } else {\r\n        for (Double d : this.counts.keySet()) {\r\n            CountsForThreshold cft = this.counts.get(d);\r\n            CountsForThreshold otherCft = other.counts.get(d);\r\n            cft.countTruePositive += otherCft.countTruePositive;\r\n            cft.countFalsePositive += otherCft.countFalsePositive;\r\n        }\r\n    }\r\n    this.exampleCount += other.exampleCount;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.retainNonZeros",
	"Comment": "removes all entries with 0 count in the counter, returning the set ofremoved entries.",
	"Method": "Set<E> retainNonZeros(Counter<E> counter){\r\n    Set<E> removed = Generics.newHashSet();\r\n    for (E key : counter.keySet()) {\r\n        if (counter.getCount(key) == 0.0) {\r\n            removed.add(key);\r\n        }\r\n    }\r\n    for (E key : removed) {\r\n        counter.remove(key);\r\n    }\r\n    return removed;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.Americanize.main",
	"Comment": "americanize and print the command line arguments.this main method is just for debugging.",
	"Method": "void main(String[] args){\r\n    log.info(new Americanize());\r\n    if (args.length == 0) {\r\n        try (BufferedReader buf = new BufferedReader(new InputStreamReader(System.in))) {\r\n            for (String line; (line = buf.readLine()) != null; ) {\r\n                for (String w : line.split(\"\\\\s+\")) {\r\n                    System.out.print(Americanize.americanize(w));\r\n                    System.out.print(' ');\r\n                }\r\n                System.out.println();\r\n            }\r\n        }\r\n    }\r\n    for (String arg : args) {\r\n        System.out.print(arg);\r\n        System.out.print(\" --> \");\r\n        System.out.println(americanize(arg));\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.minInPlace",
	"Comment": "places the minimum of first and second keys values in the first counter.",
	"Method": "void minInPlace(Counter<E> target,Counter<E> other){\r\n    for (E e : CollectionUtils.union(other.keySet(), target.keySet())) {\r\n        target.setCount(e, Math.min(target.getCount(e), other.getCount(e)));\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.group",
	"Comment": "groups consecutive elements from the given iterable based on the valuein the given comparator.each inner iterable will iterate over consecutiveitems from the input until the comparator says that the next item is notequal to the previous.",
	"Method": "Iterable<Iterable<V>> group(Iterable<V> iterable,Comparator<V> comparator){\r\n    return new Iterable<Iterable<V>>() {\r\n        public Iterator<Iterable<V>> iterator() {\r\n            return new Iterator<Iterable<V>>() {\r\n                Iterator<V> it = iterable.iterator();\r\n                V next;\r\n                public boolean hasNext() {\r\n                    return next != null || it.hasNext();\r\n                }\r\n                public Iterable<V> next() {\r\n                    return () -> new Iterator<V>() {\r\n                        V last = null;\r\n                        public boolean hasNext() {\r\n                            if (next == null && it.hasNext()) {\r\n                                next = it.next();\r\n                            }\r\n                            if (last != null && next != null) {\r\n                                return comparator.compare(last, next) == 0;\r\n                            }\r\n                            return next != null;\r\n                        }\r\n                        public V next() {\r\n                            if (!hasNext()) {\r\n                                throw new IllegalStateException(\"Didn't have next\");\r\n                            }\r\n                            V rv = next;\r\n                            last = next;\r\n                            next = null;\r\n                            return rv;\r\n                        }\r\n                        public void remove() {\r\n                            throw new UnsupportedOperationException();\r\n                        }\r\n                    };\r\n                }\r\n                public void remove() {\r\n                    throw new UnsupportedOperationException();\r\n                }\r\n            };\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.group",
	"Comment": "groups consecutive elements from the given iterable based on the valuein the given comparator.each inner iterable will iterate over consecutiveitems from the input until the comparator says that the next item is notequal to the previous.",
	"Method": "Iterable<Iterable<V>> group(Iterable<V> iterable,Comparator<V> comparator){\r\n    return new Iterator<Iterable<V>>() {\r\n        Iterator<V> it = iterable.iterator();\r\n        V next;\r\n        public boolean hasNext() {\r\n            return next != null || it.hasNext();\r\n        }\r\n        public Iterable<V> next() {\r\n            return () -> new Iterator<V>() {\r\n                V last = null;\r\n                public boolean hasNext() {\r\n                    if (next == null && it.hasNext()) {\r\n                        next = it.next();\r\n                    }\r\n                    if (last != null && next != null) {\r\n                        return comparator.compare(last, next) == 0;\r\n                    }\r\n                    return next != null;\r\n                }\r\n                public V next() {\r\n                    if (!hasNext()) {\r\n                        throw new IllegalStateException(\"Didn't have next\");\r\n                    }\r\n                    V rv = next;\r\n                    last = next;\r\n                    next = null;\r\n                    return rv;\r\n                }\r\n                public void remove() {\r\n                    throw new UnsupportedOperationException();\r\n                }\r\n            };\r\n        }\r\n        public void remove() {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.group",
	"Comment": "groups consecutive elements from the given iterable based on the valuein the given comparator.each inner iterable will iterate over consecutiveitems from the input until the comparator says that the next item is notequal to the previous.",
	"Method": "Iterable<Iterable<V>> group(Iterable<V> iterable,Comparator<V> comparator){\r\n    return next != null || it.hasNext();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.group",
	"Comment": "groups consecutive elements from the given iterable based on the valuein the given comparator.each inner iterable will iterate over consecutiveitems from the input until the comparator says that the next item is notequal to the previous.",
	"Method": "Iterable<Iterable<V>> group(Iterable<V> iterable,Comparator<V> comparator){\r\n    return () -> new Iterator<V>() {\r\n        V last = null;\r\n        public boolean hasNext() {\r\n            if (next == null && it.hasNext()) {\r\n                next = it.next();\r\n            }\r\n            if (last != null && next != null) {\r\n                return comparator.compare(last, next) == 0;\r\n            }\r\n            return next != null;\r\n        }\r\n        public V next() {\r\n            if (!hasNext()) {\r\n                throw new IllegalStateException(\"Didn't have next\");\r\n            }\r\n            V rv = next;\r\n            last = next;\r\n            next = null;\r\n            return rv;\r\n        }\r\n        public void remove() {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.group",
	"Comment": "groups consecutive elements from the given iterable based on the valuein the given comparator.each inner iterable will iterate over consecutiveitems from the input until the comparator says that the next item is notequal to the previous.",
	"Method": "Iterable<Iterable<V>> group(Iterable<V> iterable,Comparator<V> comparator){\r\n    if (next == null && it.hasNext()) {\r\n        next = it.next();\r\n    }\r\n    if (last != null && next != null) {\r\n        return comparator.compare(last, next) == 0;\r\n    }\r\n    return next != null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.group",
	"Comment": "groups consecutive elements from the given iterable based on the valuein the given comparator.each inner iterable will iterate over consecutiveitems from the input until the comparator says that the next item is notequal to the previous.",
	"Method": "Iterable<Iterable<V>> group(Iterable<V> iterable,Comparator<V> comparator){\r\n    if (!hasNext()) {\r\n        throw new IllegalStateException(\"Didn't have next\");\r\n    }\r\n    V rv = next;\r\n    last = next;\r\n    next = null;\r\n    return rv;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.group",
	"Comment": "groups consecutive elements from the given iterable based on the valuein the given comparator.each inner iterable will iterate over consecutiveitems from the input until the comparator says that the next item is notequal to the previous.",
	"Method": "Iterable<Iterable<V>> group(Iterable<V> iterable,Comparator<V> comparator){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.group",
	"Comment": "groups consecutive elements from the given iterable based on the valuein the given comparator.each inner iterable will iterate over consecutiveitems from the input until the comparator says that the next item is notequal to the previous.",
	"Method": "Iterable<Iterable<V>> group(Iterable<V> iterable,Comparator<V> comparator){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "org.datavec.spark.transform.Normalization.minMaxColumns",
	"Comment": "returns the min and max of the given columns.the list returned is a list of size 2 where each row",
	"Method": "List<Row> minMaxColumns(DataRowsFacade data,List<String> columns,List<Row> minMaxColumns,DataRowsFacade data,String columns){\r\n    return aggregate(data, columns, new String[] { \"min\", \"max\" });\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLPServer.respondUnauthorized",
	"Comment": "a helper function to respond to a request with an error stating that the user is not authorizedto make this request.",
	"Method": "void respondUnauthorized(HttpExchange httpExchange){\r\n    log(\"Responding unauthorized to \" + httpExchange.getRemoteAddress());\r\n    httpExchange.getResponseHeaders().add(\"Content-type\", \"application/javascript\");\r\n    byte[] content = \"{\\\"message\\\": \\\"Unauthorized API request\\\"}\".getBytes(\"utf-8\");\r\n    httpExchange.sendResponseHeaders(HTTP_UNAUTHORIZED, content.length);\r\n    httpExchange.getResponseBody().write(content);\r\n    httpExchange.close();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Interval.checkFlagExclusiveSet",
	"Comment": "utility function to check if a particular flag is set exclusivelygiven a particular set of flags and a mask.",
	"Method": "boolean checkFlagExclusiveSet(int flags,int flag,int mask){\r\n    int f = flags & flag;\r\n    if (f != 0) {\r\n        return (flags & mask & ~flag) == 0;\r\n    } else {\r\n        return false;\r\n    }\r\n}"
}, {
	"Path": "org.datavec.nlp.uima.UimaResource.process",
	"Comment": "use the given analysis engine and process the given textyou must release the return cas yourself",
	"Method": "CAS process(String text){\r\n    CAS cas = retrieve();\r\n    cas.setDocumentText(text);\r\n    try {\r\n        analysisEngine.process(cas);\r\n    } catch (AnalysisEngineProcessException e) {\r\n        if (text != null && !text.isEmpty())\r\n            return process(text);\r\n        throw new RuntimeException(e);\r\n    }\r\n    return cas;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.SLF4JHandler.handle",
	"Comment": "override the raw handle method, as potentially we are dropping log levels in slf4jand we do not want to render the resulting message.",
	"Method": "List<Redwood.Record> handle(Redwood.Record record){\r\n    Pair<Logger, Redwood.Flag> loggerAndLevel = getLoggerAndLevel(record.channels());\r\n    switch(loggerAndLevel.second) {\r\n        case FORCE:\r\n            break;\r\n        case ERROR:\r\n            if (!loggerAndLevel.first.isErrorEnabled()) {\r\n                return Collections.emptyList();\r\n            }\r\n            break;\r\n        case WARN:\r\n            if (!loggerAndLevel.first.isWarnEnabled()) {\r\n                return Collections.emptyList();\r\n            }\r\n            break;\r\n        case DEBUG:\r\n            if (!loggerAndLevel.first.isDebugEnabled()) {\r\n                return Collections.emptyList();\r\n            }\r\n            break;\r\n        default:\r\n            if (!loggerAndLevel.first.isInfoEnabled()) {\r\n                return Collections.emptyList();\r\n            }\r\n            break;\r\n    }\r\n    return super.handle(record);\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Document.fromProto",
	"Comment": "read a corefchain from its serialized representation.this is private due to the need for an additional partial document. also, why on earth are you trying to usethis on its own anyways?",
	"Method": "CorefChain fromProto(CoreNLPProtos.CorefChain proto){\r\n    int cid = proto.getChainID();\r\n    Map<IntPair, Set<CorefChain.CorefMention>> mentions = new HashMap();\r\n    CorefChain.CorefMention representative = null;\r\n    for (int i = 0; i < proto.getMentionCount(); ++i) {\r\n        CoreNLPProtos.CorefChain.CorefMention mentionProto = proto.getMention(i);\r\n        StringBuilder mentionSpan = new StringBuilder();\r\n        Sentence sentence = sentence(mentionProto.getSentenceIndex());\r\n        for (int k = mentionProto.getBeginIndex(); k < mentionProto.getEndIndex(); ++k) {\r\n            mentionSpan.append(' ').append(sentence.word(k));\r\n        }\r\n        CorefChain.CorefMention mention = new CorefChain.CorefMention(Dictionaries.MentionType.valueOf(mentionProto.getMentionType()), Dictionaries.Number.valueOf(mentionProto.getNumber()), Dictionaries.Gender.valueOf(mentionProto.getGender()), Dictionaries.Animacy.valueOf(mentionProto.getAnimacy()), mentionProto.getBeginIndex() + 1, mentionProto.getEndIndex() + 1, mentionProto.getHeadIndex() + 1, cid, mentionProto.getMentionID(), mentionProto.getSentenceIndex() + 1, new IntTuple(new int[] { mentionProto.getSentenceIndex() + 1, mentionProto.getPosition() }), mentionSpan.substring(mentionSpan.length() > 0 ? 1 : 0));\r\n        IntPair key = new IntPair(mentionProto.getSentenceIndex() - 1, mentionProto.getHeadIndex() - 1);\r\n        if (!mentions.containsKey(key)) {\r\n            mentions.put(key, new HashSet());\r\n        }\r\n        mentions.get(key).add(mention);\r\n        if (proto.hasRepresentative() && i == proto.getRepresentative()) {\r\n            representative = mention;\r\n        }\r\n    }\r\n    return new CorefChain(cid, mentions, representative);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasModelUtils.copyWeightsToModel",
	"Comment": "helper function to import weights from nested map into existing model. depends criticallyon matched layer and parameter names. in general this seems to be straightforward for mostkeras models and layersordered, but there may be edge cases.",
	"Method": "Model copyWeightsToModel(Model model,Map<String, KerasLayer> kerasLayers){\r\n    Layer[] layersFromModel;\r\n    if (model instanceof MultiLayerNetwork)\r\n        layersFromModel = ((MultiLayerNetwork) model).getLayers();\r\n    else\r\n        layersFromModel = ((ComputationGraph) model).getLayers();\r\n    Set<String> layerNames = new HashSet(kerasLayers.keySet());\r\n    for (org.deeplearning4j.nn.api.Layer layer : layersFromModel) {\r\n        String layerName = layer.conf().getLayer().getLayerName();\r\n        if (!kerasLayers.containsKey(layerName))\r\n            throw new InvalidKerasConfigurationException(\"No weights found for layer in model (named \" + layerName + \")\");\r\n        kerasLayers.get(layerName).copyWeightsToLayer(layer);\r\n        layerNames.remove(layerName);\r\n    }\r\n    for (String layerName : layerNames) {\r\n        if (kerasLayers.get(layerName).getNumParams() > 0)\r\n            throw new InvalidKerasConfigurationException(\"Attemping to copy weights for layer not in model (named \" + layerName + \")\");\r\n    }\r\n    return model;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.prod",
	"Comment": "product array reduction operation, optionally along specified dimensions",
	"Method": "SDVariable prod(SDVariable x,int dimensions,SDVariable prod,String name,SDVariable x,int dimensions,SDVariable prod,String name,SDVariable x,boolean keepDims,int dimensions){\r\n    SDVariable result = functionFactory.prod(x, keepDims, dimensions);\r\n    return updateVariableNameAndReference(result, name);\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.fetchers.CacheableExtractableDataSetFetcher.isCached",
	"Comment": "returns a boolean indicating if the dataset is already cached locally.",
	"Method": "boolean isCached(){\r\n    return getLocalCacheDir().exists();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return new AbstractCounter<T>() {\r\n        public void clear() {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n        public boolean containsKey(T key) {\r\n            return counter.containsKey(key);\r\n        }\r\n        public double getCount(Object key) {\r\n            return counter.getCount(key);\r\n        }\r\n        public Factory<Counter<T>> getFactory() {\r\n            return counter.getFactory();\r\n        }\r\n        public double remove(T key) {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n        public void setCount(T key, double value) {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n        @Override\r\n        public double incrementCount(T key, double value) {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n        @Override\r\n        public double incrementCount(T key) {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n        @Override\r\n        public double logIncrementCount(T key, double value) {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n        public int size() {\r\n            return counter.size();\r\n        }\r\n        public double totalCount() {\r\n            return counter.totalCount();\r\n        }\r\n        public Collection<Double> values() {\r\n            return counter.values();\r\n        }\r\n        public Set<T> keySet() {\r\n            return Collections.unmodifiableSet(counter.keySet());\r\n        }\r\n        public Set<Entry<T, Double>> entrySet() {\r\n            return Collections.unmodifiableSet(new AbstractSet<Map.Entry<T, Double>>() {\r\n                @Override\r\n                public Iterator<Entry<T, Double>> iterator() {\r\n                    return new Iterator<Entry<T, Double>>() {\r\n                        final Iterator<Entry<T, Double>> inner = counter.entrySet().iterator();\r\n                        public boolean hasNext() {\r\n                            return inner.hasNext();\r\n                        }\r\n                        public Entry<T, Double> next() {\r\n                            return new Map.Entry<T, Double>() {\r\n                                final Entry<T, Double> e = inner.next();\r\n                                @Override\r\n                                public T getKey() {\r\n                                    return e.getKey();\r\n                                }\r\n                                @Override\r\n                                @SuppressWarnings({ \"UnnecessaryBoxing\", \"UnnecessaryUnboxing\" })\r\n                                public Double getValue() {\r\n                                    return Double.valueOf(e.getValue().doubleValue());\r\n                                }\r\n                                @Override\r\n                                public Double setValue(Double value) {\r\n                                    throw new UnsupportedOperationException();\r\n                                }\r\n                            };\r\n                        }\r\n                        @Override\r\n                        public void remove() {\r\n                            throw new UnsupportedOperationException();\r\n                        }\r\n                    };\r\n                }\r\n                @Override\r\n                public int size() {\r\n                    return counter.size();\r\n                }\r\n            });\r\n        }\r\n        @Override\r\n        public void setDefaultReturnValue(double rv) {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n        @Override\r\n        public double defaultReturnValue() {\r\n            return counter.defaultReturnValue();\r\n        }\r\n        public void prettyLog(RedwoodChannels channels, String description) {\r\n            PrettyLogger.log(channels, description, asMap(this));\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return counter.containsKey(key);\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return counter.getCount(key);\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return counter.getFactory();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return counter.size();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return counter.totalCount();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return counter.values();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return Collections.unmodifiableSet(counter.keySet());\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return Collections.unmodifiableSet(new AbstractSet<Map.Entry<T, Double>>() {\r\n        @Override\r\n        public Iterator<Entry<T, Double>> iterator() {\r\n            return new Iterator<Entry<T, Double>>() {\r\n                final Iterator<Entry<T, Double>> inner = counter.entrySet().iterator();\r\n                public boolean hasNext() {\r\n                    return inner.hasNext();\r\n                }\r\n                public Entry<T, Double> next() {\r\n                    return new Map.Entry<T, Double>() {\r\n                        final Entry<T, Double> e = inner.next();\r\n                        @Override\r\n                        public T getKey() {\r\n                            return e.getKey();\r\n                        }\r\n                        @Override\r\n                        @SuppressWarnings({ \"UnnecessaryBoxing\", \"UnnecessaryUnboxing\" })\r\n                        public Double getValue() {\r\n                            return Double.valueOf(e.getValue().doubleValue());\r\n                        }\r\n                        @Override\r\n                        public Double setValue(Double value) {\r\n                            throw new UnsupportedOperationException();\r\n                        }\r\n                    };\r\n                }\r\n                @Override\r\n                public void remove() {\r\n                    throw new UnsupportedOperationException();\r\n                }\r\n            };\r\n        }\r\n        @Override\r\n        public int size() {\r\n            return counter.size();\r\n        }\r\n    });\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return new Iterator<Entry<T, Double>>() {\r\n        final Iterator<Entry<T, Double>> inner = counter.entrySet().iterator();\r\n        public boolean hasNext() {\r\n            return inner.hasNext();\r\n        }\r\n        public Entry<T, Double> next() {\r\n            return new Map.Entry<T, Double>() {\r\n                final Entry<T, Double> e = inner.next();\r\n                @Override\r\n                public T getKey() {\r\n                    return e.getKey();\r\n                }\r\n                @Override\r\n                @SuppressWarnings({ \"UnnecessaryBoxing\", \"UnnecessaryUnboxing\" })\r\n                public Double getValue() {\r\n                    return Double.valueOf(e.getValue().doubleValue());\r\n                }\r\n                @Override\r\n                public Double setValue(Double value) {\r\n                    throw new UnsupportedOperationException();\r\n                }\r\n            };\r\n        }\r\n        @Override\r\n        public void remove() {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return inner.hasNext();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return new Map.Entry<T, Double>() {\r\n        final Entry<T, Double> e = inner.next();\r\n        @Override\r\n        public T getKey() {\r\n            return e.getKey();\r\n        }\r\n        @Override\r\n        @SuppressWarnings({ \"UnnecessaryBoxing\", \"UnnecessaryUnboxing\" })\r\n        public Double getValue() {\r\n            return Double.valueOf(e.getValue().doubleValue());\r\n        }\r\n        @Override\r\n        public Double setValue(Double value) {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return e.getKey();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return Double.valueOf(e.getValue().doubleValue());\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return counter.size();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    return counter.defaultReturnValue();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.unmodifiableCounter",
	"Comment": "returns unmodifiable view of the counter. changes to the underlying counterare written through to this counter.",
	"Method": "Counter<T> unmodifiableCounter(Counter<T> counter){\r\n    PrettyLogger.log(channels, description, asMap(this));\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.categorical.CategoricalToOneHotTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return stateNames.toArray(new String[stateNames.size()]);\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.getAnnotatorImplementations",
	"Comment": "get the implementation of each relevant annotator in the pipeline.the primary use of this method is to be overwritten by subclasses of stanfordcorenlpto call different annotators that obey the exact same contract as the defaultannotator.the canonical use case for this is as an implementation of the curator server,where the annotators make server calls rather than calling each annotator locally.",
	"Method": "AnnotatorImplementations getAnnotatorImplementations(){\r\n    return new AnnotatorImplementations();\r\n}"
}, {
	"Path": "org.deeplearning4j.text.tokenization.tokenizer.ChineseTokenizerTest.testFindNamesFromText",
	"Comment": "train model by some data of the chinese names,then find out the names from the dataset",
	"Method": "void testFindNamesFromText(){\r\n    SentenceIterator iter = new BasicLineIterator(\"src/test/resources/chineseName.txt\");\r\n    log.info(\"load is right!\");\r\n    TokenizerFactory tokenizerFactory = new ChineseTokenizerFactory();\r\n    Word2Vec vec = new Word2Vec.Builder().minWordFrequency(2).iterations(5).layerSize(100).seed(42).learningRate(0.1).windowSize(20).iterate(iter).tokenizerFactory(tokenizerFactory).build();\r\n    vec.fit();\r\n    WordVectorSerializer.writeWordVectors(vec, new File(\"src/test/resources/chineseNameWordVector.txt\"));\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.addPropertyToResolve",
	"Comment": "adds a property that needs to be resolve for later.these variables are typically values that are arraysthat are named but have an unknown value till execution time.this is very common for model import.",
	"Method": "void addPropertyToResolve(DifferentialFunction forFunction,String arrayName){\r\n    if (!propertiesToResolve.containsKey(forFunction.getOwnName())) {\r\n        List<String> newVal = new ArrayList();\r\n        newVal.add(arrayName);\r\n        propertiesToResolve.put(forFunction.getOwnName(), newVal);\r\n    } else {\r\n        List<String> newVal = propertiesToResolve.get(forFunction.getOwnName());\r\n        newVal.add(arrayName);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.SemgrexMatcher.getNodeNames",
	"Comment": "returns the set of names for named nodes in this pattern.this is used as a convenience routine, when there are numerous patternswith named nodes to track.",
	"Method": "Set<String> getNodeNames(){\r\n    return namesToNodes.keySet();\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.MaxentTagger.saveExtractors",
	"Comment": "serialize the extractorframes and extractorframesrare to os.",
	"Method": "void saveExtractors(OutputStream os){\r\n    ObjectOutputStream out = new ObjectOutputStream(os);\r\n    out.writeObject(extractors);\r\n    out.writeObject(extractorsRare);\r\n    out.flush();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.ParseException.add_escapes",
	"Comment": "used to convert raw characters to their escaped versionwhen these raw version cannot be used as part of an asciistring literal.",
	"Method": "String add_escapes(String str){\r\n    StringBuffer retval = new StringBuffer();\r\n    char ch;\r\n    for (int i = 0; i < str.length(); i++) {\r\n        switch(str.charAt(i)) {\r\n            case 0:\r\n                continue;\r\n            case '\\b':\r\n                retval.append(\"\\\\b\");\r\n                continue;\r\n            case '\\t':\r\n                retval.append(\"\\\\t\");\r\n                continue;\r\n            case '\\n':\r\n                retval.append(\"\\\\n\");\r\n                continue;\r\n            case '\\f':\r\n                retval.append(\"\\\\f\");\r\n                continue;\r\n            case '\\r':\r\n                retval.append(\"\\\\r\");\r\n                continue;\r\n            case '\\\"':\r\n                retval.append(\"\\\\\\\"\");\r\n                continue;\r\n            case '\\'':\r\n                retval.append(\"\\\\\\'\");\r\n                continue;\r\n            case '\\\\':\r\n                retval.append(\"\\\\\\\\\");\r\n                continue;\r\n            default:\r\n                if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {\r\n                    String s = \"0000\" + Integer.toString(ch, 16);\r\n                    retval.append(\"\\\%u\" + s.substring(s.length() - 4, s.length()));\r\n                } else {\r\n                    retval.append(ch);\r\n                }\r\n                continue;\r\n        }\r\n    }\r\n    return retval.toString();\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.MaxentTagger.runTest",
	"Comment": "tests a tagger on data with gold tags available.this is test mode.",
	"Method": "void runTest(TaggerConfig config){\r\n    if (config.getVerbose()) {\r\n        log.info(\"Tagger testing invoked at \" + new Date() + \" with arguments:\");\r\n        config.dump();\r\n    }\r\n    try {\r\n        MaxentTagger tagger = new MaxentTagger(config.getModel(), config);\r\n        Timing t = new Timing();\r\n        TestClassifier testClassifier = new TestClassifier(tagger);\r\n        long millis = t.stop();\r\n        printErrWordsPerSec(millis, testClassifier.getNumWords());\r\n        testClassifier.printModelAndAccuracy(tagger);\r\n    } catch (Exception e) {\r\n        log.warn(\"An error occurred while testing the tagger.\", e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.SsurgeonTest.simpleTest",
	"Comment": "simple test of an ssurgeon edit script.this instances a simple semantic graph,\ta semgrex pattern, and then the resulting actions over the named nodes in the\tsemgrex match.",
	"Method": "void simpleTest(){\r\n    SemanticGraph sg = SemanticGraph.valueOf(\"[mixed/VBN nsubj>[Joe/NNP appos>[bartender/NN det>the/DT]]  dobj>[drink/NN det>a/DT]]\");\r\n    SemgrexPattern semgrexPattern = SemgrexPattern.compile(\"{}=a1 >appos=e1 {}=a2 <nsubj=e2 {}=a3\");\r\n    SsurgeonPattern pattern = new SsurgeonPattern(semgrexPattern);\r\n    System.out.println(\"Start = \" + sg.toCompactString());\r\n    SsurgeonEdit apposSnip = new RemoveNamedEdge(\"e1\", \"a1\", \"a2\");\r\n    pattern.addEdit(apposSnip);\r\n    SsurgeonEdit nsubjSnip = new RemoveNamedEdge(\"e2\", \"a3\", \"a1\");\r\n    pattern.addEdit(nsubjSnip);\r\n    SsurgeonEdit reattachSubj = new AddEdge(\"a2\", \"a1\", EnglishGrammaticalRelations.NOMINAL_SUBJECT);\r\n    pattern.addEdit(reattachSubj);\r\n    IndexedWord isNode = new IndexedWord();\r\n    isNode.set(CoreAnnotations.TextAnnotation.class, \"is\");\r\n    isNode.set(CoreAnnotations.LemmaAnnotation.class, \"is\");\r\n    isNode.set(CoreAnnotations.OriginalTextAnnotation.class, \"is\");\r\n    isNode.set(CoreAnnotations.PartOfSpeechAnnotation.class, \"VBN\");\r\n    SsurgeonEdit addCopula = new AddDep(\"a2\", EnglishGrammaticalRelations.COPULA, isNode);\r\n    pattern.addEdit(addCopula);\r\n    SsurgeonEdit destroySubgraph = new DeleteGraphFromNode(\"a3\");\r\n    pattern.addEdit(destroySubgraph);\r\n    Collection<SemanticGraph> newSgs = pattern.execute(sg);\r\n    for (SemanticGraph newSg : newSgs) System.out.println(\"Modified = \" + newSg.toCompactString());\r\n    String firstGraphString = newSgs.iterator().next().toCompactString().trim();\r\n    assertEquals(firstGraphString, \"[bartender cop>is nsubj>Joe det>the]\");\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getConfResourceAsInputStream",
	"Comment": "get an input stream attached to the configuration resource with thegiven name.",
	"Method": "InputStream getConfResourceAsInputStream(String name){\r\n    try {\r\n        URL url = getResource(name);\r\n        if (url == null) {\r\n            LOG.info(name + \" not found\");\r\n            return null;\r\n        } else {\r\n            LOG.info(\"found resource \" + name + \" at \" + url);\r\n        }\r\n        return url.openStream();\r\n    } catch (Exception e) {\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Filters.filter",
	"Comment": "applies the given filter to each of the given elements, and returns thearray of elements that were accepted. the runtime type of the returnedarray is the same as the passed in array.",
	"Method": "E[] filter(E[] elems,Predicate<E> filter){\r\n    List<E> filtered = new ArrayList();\r\n    for (E elem : elems) {\r\n        if (filter.test(elem)) {\r\n            filtered.add(elem);\r\n        }\r\n    }\r\n    return (filtered.toArray((E[]) Array.newInstance(elems.getClass().getComponentType(), filtered.size())));\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.MutableInteger.incValue",
	"Comment": "add the argument to the value of this integer.a convenience method.",
	"Method": "void incValue(int val){\r\n    i += val;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.wordstore.VocabularyHolder.arrayToList",
	"Comment": "this method is used only for vocabcache compatibility purposes",
	"Method": "List<Byte> arrayToList(byte[] array,int codeLen,List<Integer> arrayToList,int[] array,int codeLen){\r\n    List<Integer> result = new ArrayList();\r\n    for (int x = 0; x < codeLen; x++) {\r\n        result.add(array[x]);\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Interval.intersect",
	"Comment": "returns interval that is the intersection of this and the other intervalreturns null if intersect is null",
	"Method": "Interval intersect(Interval<E> other){\r\n    if (other == null)\r\n        return null;\r\n    E a = max(this.first, other.first);\r\n    E b = min(this.second, other.second);\r\n    return toInterval(a, b);\r\n}"
}, {
	"Path": "org.datavec.image.loader.AndroidNativeImageLoader.asBitmap",
	"Comment": "converts an indarray to a bitmap. only intended for images with rank 3.",
	"Method": "Bitmap asBitmap(INDArray array,Bitmap asBitmap,INDArray array,int dataType){\r\n    return converter2.convert(asFrame(array, dataType));\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.TregexGUI.setTsurgeonEnabled",
	"Comment": "used to change the status of the tsurgeon file menu item to reflectwhether tsurgeon is enabled",
	"Method": "void setTsurgeonEnabled(boolean enabled){\r\n    if (loadTsurgeon.isEnabled() != enabled)\r\n        loadTsurgeon.setEnabled(enabled);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CollectionUtils.transformAsList",
	"Comment": "transforms the keyset of collection according to the given function and returns a list.",
	"Method": "List<T2> transformAsList(Collection<? extends T1> original,Function<T1, ? extends T2> f){\r\n    List<T2> transformed = new ArrayList();\r\n    for (T1 t : original) {\r\n        transformed.add(f.apply(t));\r\n    }\r\n    return transformed;\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.util.MLLibUtil.toMatrix",
	"Comment": "convert an ndarray to a matrix.note that the matrix will be con",
	"Method": "INDArray toMatrix(Matrix arr,Matrix toMatrix,INDArray arr){\r\n    if (!arr.isMatrix()) {\r\n        throw new IllegalArgumentException(\"passed in array must be a matrix\");\r\n    }\r\n    if (arr.isView()) {\r\n        return Matrices.dense(arr.rows(), arr.columns(), arr.dup('f').data().asDouble());\r\n    } else\r\n        return Matrices.dense(arr.rows(), arr.columns(), arr.ordering() == 'f' ? arr.data().asDouble() : arr.dup('f').data().asDouble());\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.tuebadz.TueBaDZHeadFinder.isLabelAnnotationIntroducingCharacter",
	"Comment": "say whether this character is an annotation introducing character.",
	"Method": "boolean isLabelAnnotationIntroducingCharacter(char ch){\r\n    if (tlp.isLabelAnnotationIntroducingCharacter(ch)) {\r\n        return true;\r\n    }\r\n    if (ch == '-') {\r\n        return true;\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.datavec.arrow.ArrowConverter.readFromFile",
	"Comment": "read a datavec schema and record setfrom the given arrow file.",
	"Method": "Pair<Schema, ArrowWritableRecordBatch> readFromFile(FileInputStream input,Pair<Schema, ArrowWritableRecordBatch> readFromFile,File input){\r\n    return readFromFile(new FileInputStream(input));\r\n}"
}, {
	"Path": "org.datavec.audio.fingerprint.FingerprintSimilarity.getSimilarity",
	"Comment": "get the similarity of the fingerprintssimilarity from 0~1, which 0 means no similar feature is found and 1 means in average there is at least one match in every frame",
	"Method": "float getSimilarity(){\r\n    return similarity;\r\n}"
}, {
	"Path": "org.datavec.image.loader.NativeImageLoader.streamToMat",
	"Comment": "read the stream to the buffer, and return the number of bytes read",
	"Method": "Mat streamToMat(InputStream is){\r\n    if (buffer == null) {\r\n        buffer = IOUtils.toByteArray(is);\r\n        bufferMat = new Mat(buffer);\r\n        return bufferMat;\r\n    } else {\r\n        int numReadTotal = is.read(buffer);\r\n        if (numReadTotal < buffer.length) {\r\n            bufferMat.data().put(buffer, 0, numReadTotal);\r\n            bufferMat.cols(numReadTotal);\r\n            return bufferMat;\r\n        }\r\n        int numReadCurrent = numReadTotal;\r\n        while (numReadCurrent != -1) {\r\n            byte[] oldBuffer = buffer;\r\n            if (oldBuffer.length == Integer.MAX_VALUE) {\r\n                throw new IllegalStateException(\"Cannot read more than Integer.MAX_VALUE bytes\");\r\n            }\r\n            long increase = Math.max(buffer.length, MIN_BUFFER_STEP_SIZE);\r\n            int newBufferLength = (int) Math.min(Integer.MAX_VALUE, buffer.length + increase);\r\n            buffer = new byte[newBufferLength];\r\n            System.arraycopy(oldBuffer, 0, buffer, 0, oldBuffer.length);\r\n            numReadCurrent = is.read(buffer, oldBuffer.length, buffer.length - oldBuffer.length);\r\n            if (numReadCurrent > 0) {\r\n                numReadTotal += numReadCurrent;\r\n            }\r\n        }\r\n        bufferMat = new Mat(buffer);\r\n        return bufferMat;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraLexer.main",
	"Comment": "runs the scanner on input files.this is a standalone scanner, it will print any unmatchedtext to system.out unchanged.",
	"Method": "void main(String argv){\r\n    if (argv.length == 0) {\r\n        System.out.println(\"Usage : java NegraLexer [ --encoding <name> ] <inputfile(s)>\");\r\n    } else {\r\n        int firstFilePos = 0;\r\n        String encodingName = \"UTF-8\";\r\n        if (argv[0].equals(\"--encoding\")) {\r\n            firstFilePos = 2;\r\n            encodingName = argv[1];\r\n            try {\r\n                java.nio.charset.Charset.forName(encodingName);\r\n            } catch (Exception e) {\r\n                System.out.println(\"Invalid encoding '\" + encodingName + \"'\");\r\n                return;\r\n            }\r\n        }\r\n        for (int i = firstFilePos; i < argv.length; i++) {\r\n            NegraLexer scanner = null;\r\n            try {\r\n                java.io.FileInputStream stream = new java.io.FileInputStream(argv[i]);\r\n                java.io.Reader reader = new java.io.InputStreamReader(stream, encodingName);\r\n                scanner = new NegraLexer(reader);\r\n                while (!scanner.zzAtEOF) scanner.yylex();\r\n            } catch (java.io.FileNotFoundException e) {\r\n                System.out.println(\"File not found : \\\"\" + argv[i] + \"\\\"\");\r\n            } catch (java.io.IOException e) {\r\n                System.out.println(\"IO error scanning file \\\"\" + argv[i] + \"\\\"\");\r\n                System.out.println(e);\r\n            } catch (Exception e) {\r\n                System.out.println(\"Unexpected exception:\");\r\n                e.printStackTrace();\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.plot.BarnesHutTsne.saveAsFile",
	"Comment": "save the model as a file with a csv format, adding the label as the last column.",
	"Method": "void saveAsFile(List<String> labels,String path){\r\n    BufferedWriter write = null;\r\n    try {\r\n        write = new BufferedWriter(new FileWriter(new File(path)));\r\n        for (int i = 0; i < Y.rows(); i++) {\r\n            if (i >= labels.size())\r\n                break;\r\n            String word = labels.get(i);\r\n            if (word == null)\r\n                continue;\r\n            StringBuilder sb = new StringBuilder();\r\n            INDArray wordVector = Y.getRow(i);\r\n            for (int j = 0; j < wordVector.length(); j++) {\r\n                sb.append(wordVector.getDouble(j));\r\n                if (j < wordVector.length() - 1)\r\n                    sb.append(\",\");\r\n            }\r\n            sb.append(\",\");\r\n            sb.append(word);\r\n            sb.append(\"\\n\");\r\n            write.write(sb.toString());\r\n        }\r\n        write.flush();\r\n        write.close();\r\n    } finally {\r\n        if (write != null)\r\n            write.close();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.chain",
	"Comment": "chains together all iterables of type t as given in an array orvarargs parameter.",
	"Method": "Iterable<T> chain(Iterable<? extends Iterable<T>> iterables,Iterable<T> chain,Iterable<T> iterables,Iterable<T> chain,T[] arrays){\r\n    LinkedList<Iterable<T>> iterables = new LinkedList();\r\n    for (T[] array : arrays) {\r\n        iterables.add(Arrays.asList(array));\r\n    }\r\n    return chain(iterables);\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.sumOfMeanDifferencesOnePoint",
	"Comment": "used for calculating top part of simple regression forbeta 1",
	"Method": "double sumOfMeanDifferencesOnePoint(double[] vector){\r\n    double mean = sum(vector) / vector.length;\r\n    double ret = 0;\r\n    for (int i = 0; i < vector.length; i++) {\r\n        double vec1Diff = Math.pow(vector[i] - mean, 2);\r\n        ret += vec1Diff;\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.standardDeviation",
	"Comment": "stardard deviation array reduction operation, optionally along specified dimensions",
	"Method": "SDVariable standardDeviation(SDVariable x,boolean biasCorrected,int dimensions,SDVariable standardDeviation,String name,SDVariable x,boolean biasCorrected,int dimensions,SDVariable standardDeviation,String name,SDVariable x,boolean biasCorrected,boolean keepDims,int dimensions){\r\n    SDVariable result = functionFactory.std(x, biasCorrected, keepDims, dimensions);\r\n    return updateVariableNameAndReference(result, name);\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.MultiClassPrecisionRecallExtendedStats.getAccuracyDescription",
	"Comment": "returns a string summarizing overall accuracy that will print nicely.",
	"Method": "String getAccuracyDescription(int numDigits){\r\n    NumberFormat nf = NumberFormat.getNumberInstance();\r\n    nf.setMaximumFractionDigits(numDigits);\r\n    Triple<Double, Integer, Integer> accu = getAccuracyInfo();\r\n    return nf.format(accu.first()) + \"  (\" + accu.second() + \"/\" + (accu.second() + accu.third()) + \")\";\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.maxent.Extractors.rightContext",
	"Comment": "find maximum right context of extractors. used in taginference to decide windows for dynamic programming.",
	"Method": "int rightContext(){\r\n    int max = 0;\r\n    for (Extractor extractor : v) {\r\n        int lf = extractor.rightContext();\r\n        if (lf > max) {\r\n            max = lf;\r\n        }\r\n    }\r\n    return max;\r\n}"
}, {
	"Path": "org.datavec.api.records.reader.impl.misc.SVMLightRecordReader.getNextRecord",
	"Comment": "helper function to help detect lines that arecommented out. may read ahead and cache a line.",
	"Method": "Writable getNextRecord(){\r\n    Writable w = null;\r\n    if (recordLookahead != null) {\r\n        w = recordLookahead;\r\n        recordLookahead = null;\r\n    }\r\n    while (w == null && super.hasNext()) {\r\n        w = super.next().iterator().next();\r\n        if (!w.toString().startsWith(COMMENT_CHAR))\r\n            break;\r\n        w = null;\r\n    }\r\n    return w;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.argmax",
	"Comment": "finds and returns the key in the counter with the largest count. returningnull if count is empty.",
	"Method": "E argmax(Counter<E> c,E argmax,Counter<E> c,Comparator<E> tieBreaker,E argmax,Counter<E> c,Comparator<E> tieBreaker,E defaultIfEmpty){\r\n    if (Thread.interrupted()) {\r\n        throw new RuntimeInterruptedException();\r\n    }\r\n    if (c.size() == 0) {\r\n        return defaultIfEmpty;\r\n    }\r\n    double max = Double.NEGATIVE_INFINITY;\r\n    E argmax = null;\r\n    for (E key : c.keySet()) {\r\n        double count = c.getCount(key);\r\n        if (argmax == null || count > max || (count == max && tieBreaker.compare(key, argmax) < 0)) {\r\n            max = count;\r\n            argmax = key;\r\n        }\r\n    }\r\n    return argmax;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractCollinsHeadFinder.determineHead",
	"Comment": "determine which daughter of the current parse tree is the head.",
	"Method": "Tree determineHead(Tree t,Tree determineHead,Tree t,Tree parent){\r\n    if (nonTerminalInfo == null) {\r\n        throw new IllegalStateException(\"Classes derived from AbstractCollinsHeadFinder must create and fill HashMap nonTerminalInfo.\");\r\n    }\r\n    if (t == null || t.isLeaf()) {\r\n        throw new IllegalArgumentException(\"Can't return head of null or leaf Tree.\");\r\n    }\r\n    if (DEBUG) {\r\n        log.info(\"determineHead for \" + t.value());\r\n    }\r\n    Tree[] kids = t.children();\r\n    Tree theHead;\r\n    if ((theHead = findMarkedHead(t)) != null) {\r\n        if (DEBUG) {\r\n            log.info(\"Find marked head method returned \" + theHead.label() + \" as head of \" + t.label());\r\n        }\r\n        return theHead;\r\n    }\r\n    if (kids.length == 1) {\r\n        if (DEBUG) {\r\n            log.info(\"Only one child determines \" + kids[0].label() + \" as head of \" + t.label());\r\n        }\r\n        return kids[0];\r\n    }\r\n    return determineNonTrivialHead(t, parent);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.recurrent.RnnOutputLayer.computeScoreForExamples",
	"Comment": "compute the score for each example individually, after labels and input have been set.",
	"Method": "INDArray computeScoreForExamples(double fullNetworkL1,double fullNetworkL2,LayerWorkspaceMgr workspaceMgr){\r\n    if (input == null || labels == null)\r\n        throw new IllegalStateException(\"Cannot calculate score without input and labels \" + layerId());\r\n    INDArray preOut = preOutput2d(false, workspaceMgr);\r\n    ILossFunction lossFunction = layerConf().getLossFn();\r\n    INDArray scoreArray = lossFunction.computeScoreArray(getLabels2d(workspaceMgr, ArrayType.FF_WORKING_MEM), preOut, layerConf().getActivationFn(), maskArray);\r\n    INDArray scoreArrayTs = TimeSeriesUtils.reshapeVectorToTimeSeriesMask(scoreArray, (int) input.size(0));\r\n    INDArray summedScores = scoreArrayTs.sum(1);\r\n    double l1l2 = fullNetworkL1 + fullNetworkL2;\r\n    if (l1l2 != 0.0) {\r\n        summedScores.addi(l1l2);\r\n    }\r\n    return summedScores;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.readNextObjectOrNull",
	"Comment": "return the next object in the given stream, or null if there is no such object.this method may be overwritten, but should match the implementation of newinputstream",
	"Method": "Pair<KEY, T> readNextObjectOrNull(InputStream input){\r\n    try {\r\n        return (Pair<KEY, T>) ((ObjectInputStream) input).readObject();\r\n    } catch (EOFException e) {\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.loadPropertiesFromClasspath",
	"Comment": "finds the properties file in the classpath and loads the properties from there.",
	"Method": "Properties loadPropertiesFromClasspath(){\r\n    List<String> validNames = Arrays.asList(\"StanfordCoreNLP\", \"edu.stanford.nlp.pipeline.StanfordCoreNLP\");\r\n    for (String name : validNames) {\r\n        Properties props = loadProperties(name);\r\n        if (props != null)\r\n            return props;\r\n    }\r\n    throw new RuntimeException(\"ERROR: Could not find properties file in the classpath!\");\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.feedforward.autoencoder.recursive.Tree.errorSum",
	"Comment": "returns the total prediction error for thistree and its children",
	"Method": "double errorSum(){\r\n    if (isLeaf()) {\r\n        return 0.0;\r\n    } else if (isPreTerminal()) {\r\n        return error();\r\n    } else {\r\n        double error = 0.0;\r\n        for (Tree child : children()) {\r\n            error += child.errorSum();\r\n        }\r\n        return error() + error;\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ops.aggregates.Batch.append",
	"Comment": "this method tries to append aggregate to the current batch, if it has free room",
	"Method": "boolean append(T aggregate){\r\n    if (!isFull()) {\r\n        aggregates.add(aggregate);\r\n        return true;\r\n    } else\r\n        return false;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.KerasModel.prepareLayers",
	"Comment": "helper method called from constructor. converts layer configurationjson into keraslayer objects.",
	"Method": "Pair<Map<String, KerasLayer>, List<KerasLayer>> prepareLayers(List<Object> layerConfigs){\r\n    Map<String, KerasLayer> layers = new HashMap();\r\n    List<KerasLayer> layersOrdered = new ArrayList();\r\n    for (Object layerConfig : layerConfigs) {\r\n        Map<String, Object> layerConfigMap = (Map<String, Object>) layerConfig;\r\n        layerConfigMap.put(config.getFieldKerasVersion(), this.kerasMajorVersion);\r\n        if (kerasMajorVersion == 2 && this.kerasBackend != null)\r\n            layerConfigMap.put(config.getFieldBackend(), this.kerasBackend);\r\n        KerasLayerConfiguration kerasLayerConf = new KerasLayer(this.kerasMajorVersion).conf;\r\n        if (dimOrder != null) {\r\n            String dimOrderString;\r\n            if (dimOrder == KerasLayer.DimOrder.TENSORFLOW)\r\n                dimOrderString = kerasLayerConf.getDIM_ORDERING_TENSORFLOW();\r\n            else if (dimOrder == KerasLayer.DimOrder.THEANO)\r\n                dimOrderString = kerasLayerConf.getDIM_ORDERING_THEANO();\r\n            else\r\n                throw new InvalidKerasConfigurationException(\"Invalid data format / dim ordering\");\r\n            layerConfigMap.put(kerasLayerConf.getLAYER_FIELD_DIM_ORDERING(), dimOrderString);\r\n        }\r\n        KerasLayer layer = KerasLayerUtils.getKerasLayerFromConfig(layerConfigMap, this.enforceTrainingConfig, kerasLayerConf, customLayers, lambdaLayers, layers);\r\n        layersOrdered.add(layer);\r\n        layers.put(layer.getLayerName(), layer);\r\n        if (layer instanceof KerasLSTM)\r\n            this.useTruncatedBPTT = this.useTruncatedBPTT || ((KerasLSTM) layer).getUnroll();\r\n        if (layer instanceof KerasSimpleRnn)\r\n            this.useTruncatedBPTT = this.useTruncatedBPTT || ((KerasSimpleRnn) layer).getUnroll();\r\n    }\r\n    return new Pair(layers, layersOrdered);\r\n}"
}, {
	"Path": "org.datavec.api.formats.input.impl.ListStringInputFormat.toLong",
	"Comment": "convert writable to long. whether this is supported depends on the specific writable.",
	"Method": "long toLong(){\r\n    return 0;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.AcronymMatcher.isAcronym",
	"Comment": "returns true if either chunk1 or chunk2 is acronym of the other.",
	"Method": "boolean isAcronym(String str,String[] tokens,boolean isAcronym,String str,List<?> tokens,boolean isAcronym,CoreMap chunk1,CoreMap chunk2,boolean isAcronym,String[] chunk1,String[] chunk2){\r\n    String text1 = StringUtils.join(chunk1);\r\n    String text2 = StringUtils.join(chunk2);\r\n    if (text1.length() <= 1 || text2.length() <= 1) {\r\n        return false;\r\n    }\r\n    List<String> tokenStrs1 = Arrays.asList(chunk1);\r\n    List<String> tokenStrs2 = Arrays.asList(chunk2);\r\n    boolean isAcro = isAcronymImpl(text1, tokenStrs2) || isAcronymImpl(text2, tokenStrs1);\r\n    if (!isAcro) {\r\n        tokenStrs1 = getMainTokenStrs(chunk1);\r\n        tokenStrs2 = getMainTokenStrs(chunk2);\r\n        isAcro = isAcronymImpl(text1, tokenStrs2) || isAcronymImpl(text2, tokenStrs1);\r\n    }\r\n    return isAcro;\r\n}"
}, {
	"Path": "dagger.internal.Modules.loadModules",
	"Comment": "returns a full set of module adapters, including module adapters for includedmodules.",
	"Method": "Map<ModuleAdapter<?>, Object> loadModules(Loader loader,Object[] seedModulesOrClasses){\r\n    Map<ModuleAdapter<?>, Object> seedAdapters = new LinkedHashMap<ModuleAdapter<?>, Object>(seedModulesOrClasses.length);\r\n    for (int i = 0; i < seedModulesOrClasses.length; i++) {\r\n        if (seedModulesOrClasses[i] instanceof Class<?>) {\r\n            ModuleAdapter<?> adapter = loader.getModuleAdapter((Class<?>) seedModulesOrClasses[i]);\r\n            seedAdapters.put(adapter, adapter.newModule());\r\n        } else {\r\n            ModuleAdapter<?> adapter = loader.getModuleAdapter(seedModulesOrClasses[i].getClass());\r\n            seedAdapters.put(adapter, seedModulesOrClasses[i]);\r\n        }\r\n    }\r\n    Map<ModuleAdapter<?>, Object> result = new LinkedHashMap<ModuleAdapter<?>, Object>(seedAdapters);\r\n    Map<Class<?>, ModuleAdapter<?>> transitiveInclusions = new LinkedHashMap<Class<?>, ModuleAdapter<?>>();\r\n    for (ModuleAdapter<?> adapter : seedAdapters.keySet()) {\r\n        collectIncludedModulesRecursively(loader, adapter, transitiveInclusions);\r\n    }\r\n    for (ModuleAdapter<?> dependency : transitiveInclusions.values()) {\r\n        if (!result.containsKey(dependency)) {\r\n            result.put(dependency, dependency.newModule());\r\n        }\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.datavec.arrow.ArrowConverter.doubleField",
	"Comment": "shortcut method for creating a double fieldwith 64 bit floating point",
	"Method": "Field doubleField(String name){\r\n    return getFieldForColumn(name, ColumnType.Double);\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.swapAxes",
	"Comment": "mainly here for people coming from numpy.this is equivalent to a call to permute",
	"Method": "INDArray swapAxes(int dimension,int with){\r\n    int[] shape = ArrayUtil.range(0, shape().length);\r\n    shape[dimension] = with;\r\n    shape[with] = dimension;\r\n    return permute(shape);\r\n}"
}, {
	"Path": "org.nd4j.autodiff.execution.input.Operands.getById",
	"Comment": "this method returns array identified its numeric id and index",
	"Method": "INDArray getById(String name,INDArray getById,int id,INDArray getById,int id,int index){\r\n    return map.get(NodeDescriptor.builder().id(id).index(index).build());\r\n}"
}, {
	"Path": "com.atilika.kuromoji.TokenizerBase.createTokenList",
	"Comment": "tokenizes the provided text and returns a list of tokens with various feature informationthis method is thread safe",
	"Method": "List<T> createTokenList(String text,List<T> createTokenList,int offset,String text){\r\n    ArrayList<T> result = new ArrayList();\r\n    ViterbiLattice lattice = viterbiBuilder.build(text);\r\n    List<ViterbiNode> bestPath = viterbiSearcher.search(lattice);\r\n    for (ViterbiNode node : bestPath) {\r\n        int wordId = node.getWordId();\r\n        if (node.getType() == ViterbiNode.Type.KNOWN && wordId == -1) {\r\n            continue;\r\n        }\r\n        @SuppressWarnings(\"unchecked\")\r\n        T token = (T) tokenFactory.createToken(wordId, node.getSurface(), node.getType(), offset + node.getStartIndex(), dictionaryMap.get(node.getType()));\r\n        result.add(token);\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.getFunction",
	"Comment": "get a samediff function instance given the name of the function",
	"Method": "SameDiff getFunction(String functionName){\r\n    return sameDiffFunctionInstances.get(functionName);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.TreeGraphNode.addChild",
	"Comment": "adds a child in the ith location.does so without overwritingthe parent pointers of the rest of the children, which might berelevant in case there are add and remove operations mixedtogether.",
	"Method": "void addChild(int i,Tree t){\r\n    if (!(t instanceof TreeGraphNode)) {\r\n        throw new IllegalArgumentException(\"Horrible error\");\r\n    }\r\n    ((TreeGraphNode) t).setParent(this);\r\n    TreeGraphNode[] kids = this.children;\r\n    TreeGraphNode[] newKids = new TreeGraphNode[kids.length + 1];\r\n    if (i != 0) {\r\n        System.arraycopy(kids, 0, newKids, 0, i);\r\n    }\r\n    newKids[i] = (TreeGraphNode) t;\r\n    if (i != kids.length) {\r\n        System.arraycopy(kids, i, newKids, i + 1, kids.length - i);\r\n    }\r\n    this.children = newKids;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.TransformingTreebank.loadPath",
	"Comment": "load trees from given path specification.not supported for thistype of treebank.",
	"Method": "void loadPath(File path,FileFilter filt){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.GeneralizedCounter.totalCount",
	"Comment": "returns the total count of objects in the generalizedcounter.",
	"Method": "double totalCount(double totalCount,double totalCount){\r\n    if (depth() == 1) {\r\n        return total;\r\n    } else {\r\n        double result = 0.0;\r\n        for (K o : topLevelKeySet()) {\r\n            result += conditionalizeOnce(o).totalCount();\r\n        }\r\n        return result;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Document.deserialize",
	"Comment": "read a document from an input stream.this does not close the input stream.",
	"Method": "Document deserialize(InputStream in){\r\n    return new Document(CoreNLPProtos.Document.parseDelimitedFrom(in));\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Interval.checkFlagSet",
	"Comment": "utility function to check if a particular flag is setgiven a particular set of flags.",
	"Method": "boolean checkFlagSet(int flags,int flag){\r\n    return ((flags & flag) != 0);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.memory.MemoryReport.getTotalMemoryBytes",
	"Comment": "get the total memory use in bytes for the given configuration",
	"Method": "long getTotalMemoryBytes(int minibatchSize,MemoryUseMode memoryUseMode,CacheMode cacheMode,long getTotalMemoryBytes,int minibatchSize,MemoryUseMode memoryUseMode,CacheMode cacheMode,DataBuffer.Type dataType){\r\n    return getTotalMemoryBytes(minibatchSize, memoryUseMode, cacheMode, DataTypeUtil.getDtypeFromContext());\r\n}"
}, {
	"Path": "org.deeplearning4j.plot.BarnesHutTsne.gradient",
	"Comment": "compute the gradient given the current solution, the probabilities and the constant",
	"Method": "Pair<Double, INDArray> gradient(INDArray p,Gradient gradient){\r\n    MemoryWorkspace workspace = workspaceMode == WorkspaceMode.NONE ? new DummyWorkspace() : Nd4j.getWorkspaceManager().getWorkspaceForCurrentThread(workspaceConfigurationExternal, workspaceExternal);\r\n    try (MemoryWorkspace ws = workspace.notifyScopeEntered()) {\r\n        if (yIncs == null)\r\n            yIncs = zeros(Y.shape());\r\n        if (gains == null)\r\n            gains = ones(Y.shape());\r\n        AtomicDouble sumQ = new AtomicDouble(0);\r\n        INDArray posF = Nd4j.create(Y.shape());\r\n        INDArray negF = Nd4j.create(Y.shape());\r\n        if (tree == null) {\r\n            tree = new SpTree(Y);\r\n            tree.setWorkspaceMode(workspaceMode);\r\n        }\r\n        tree.computeEdgeForces(rows, cols, vals, N, posF);\r\n        for (int n = 0; n < N; n++) tree.computeNonEdgeForces(n, theta, negF.slice(n), sumQ);\r\n        INDArray dC = posF.subi(negF.divi(sumQ));\r\n        Gradient ret = new DefaultGradient();\r\n        ret.gradientForVariable().put(Y_GRAD, dC);\r\n        return ret;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Treebank.toString",
	"Comment": "return the whole treebank as a series of big bracketed lists.calling this is a really bad idea if your treebank is large.",
	"Method": "String toString(){\r\n    final StringBuilder sb = new StringBuilder();\r\n    apply(t -> {\r\n        sb.append(t);\r\n        sb.append('\\n');\r\n    });\r\n    return sb.toString();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.convolutional.KerasConvolution.getConvParameterValues",
	"Comment": "return processed parameter values obtained from keras convolutional layers.",
	"Method": "INDArray getConvParameterValues(INDArray kerasParamValue){\r\n    INDArray paramValue;\r\n    switch(this.getDimOrder()) {\r\n        case TENSORFLOW:\r\n            paramValue = kerasParamValue.permute(3, 2, 0, 1);\r\n            break;\r\n        case THEANO:\r\n            paramValue = kerasParamValue.dup();\r\n            for (int i = 0; i < paramValue.tensorssAlongDimension(2, 3); i++) {\r\n                INDArray copyFilter = paramValue.tensorAlongDimension(i, 2, 3).dup();\r\n                double[] flattenedFilter = copyFilter.ravel().data().asDouble();\r\n                ArrayUtils.reverse(flattenedFilter);\r\n                INDArray newFilter = Nd4j.create(flattenedFilter, copyFilter.shape());\r\n                INDArray inPlaceFilter = paramValue.tensorAlongDimension(i, 2, 3);\r\n                inPlaceFilter.muli(0).addi(newFilter);\r\n            }\r\n            break;\r\n        default:\r\n            throw new InvalidKerasConfigurationException(\"Unknown keras backend \" + this.getDimOrder());\r\n    }\r\n    return paramValue;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.KerasModel.importTrainingConfiguration",
	"Comment": "helper method called from constructor. incorporate training configuration details into model.includes loss function, optimization details, etc.",
	"Method": "void importTrainingConfiguration(String trainingConfigJson){\r\n    Map<String, Object> trainingConfig = KerasModelUtils.parseJsonString(trainingConfigJson);\r\n    Map<String, Object> optimizerConfig = getOptimizerConfig(trainingConfig);\r\n    this.optimizer = KerasOptimizerUtils.mapOptimizer(optimizerConfig);\r\n    List<KerasLayer> lossLayers = new ArrayList();\r\n    if (!trainingConfig.containsKey(config.getTrainingLoss()))\r\n        throw new InvalidKerasConfigurationException(\"Could not determine training loss function (no \" + config.getTrainingLoss() + \" field found in training config)\");\r\n    Object kerasLossObj = trainingConfig.get(config.getTrainingLoss());\r\n    if (kerasLossObj instanceof String) {\r\n        String kerasLoss = (String) kerasLossObj;\r\n        for (String outputLayerName : this.outputLayerNames) lossLayers.add(new KerasLoss(outputLayerName + \"_loss\", outputLayerName, kerasLoss));\r\n    } else if (kerasLossObj instanceof Map) {\r\n        Map<String, Object> kerasLossMap = (Map<String, Object>) kerasLossObj;\r\n        for (String outputLayerName : kerasLossMap.keySet()) {\r\n            Object kerasLoss = kerasLossMap.get(outputLayerName);\r\n            if (kerasLoss instanceof String)\r\n                lossLayers.add(new KerasLoss(outputLayerName + \"_loss\", outputLayerName, (String) kerasLoss));\r\n            else\r\n                throw new InvalidKerasConfigurationException(\"Unknown Keras loss \" + kerasLoss.toString());\r\n        }\r\n    }\r\n    this.outputLayerNames.clear();\r\n    for (KerasLayer lossLayer : lossLayers) {\r\n        this.layersOrdered.add(lossLayer);\r\n        this.layers.put(lossLayer.getLayerName(), lossLayer);\r\n        this.outputLayerNames.add(lossLayer.getLayerName());\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.relns",
	"Comment": "returns a set of relations which this node has with its parents.",
	"Method": "Set<GrammaticalRelation> relns(IndexedWord vertex){\r\n    if (!containsVertex(vertex)) {\r\n        throw new IllegalArgumentException();\r\n    }\r\n    Set<GrammaticalRelation> relns = Generics.newHashSet();\r\n    List<Pair<GrammaticalRelation, IndexedWord>> pairs = parentPairs(vertex);\r\n    for (Pair<GrammaticalRelation, IndexedWord> p : pairs) {\r\n        relns.add(p.first());\r\n    }\r\n    return relns;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.setCacheMode",
	"Comment": "this method sets specified cachemode for all layers within network",
	"Method": "void setCacheMode(CacheMode mode){\r\n    if (mode == null)\r\n        mode = CacheMode.NONE;\r\n    for (Layer layer : layers) {\r\n        layer.setCacheMode(mode);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraPennTreeNormalizer.normalizeNonterminal",
	"Comment": "normalizes a nonterminal contents.this implementation strips functional tags, etc. and interns thenonterminal.",
	"Method": "String normalizeNonterminal(String category){\r\n    if (junkCPP.equals(category))\r\n        category = cpp;\r\n    category = cleanUpLabel(category);\r\n    return (category == null) ? null : category.intern();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.zip",
	"Comment": "zips up two iterators into one iterator over pairs of correspondingelements.ends when the shorter iterator ends.",
	"Method": "Iterable<Pair<T1, T2>> zip(Iterable<T1> iter1,Iterable<T2> iter2,Iterable<Pair<T1, T2>> zip,Iterable<T1> iter,T2 array,Iterable<Pair<T1, T2>> zip,T1 array,Iterable<T2> iter,Iterable<Pair<T1, T2>> zip,T1 array1,T2 array2,Iterator<Pair<T1, T2>> zip,Iterator<T1> iter1,Iterator<T2> iter2){\r\n    return new Iterator<Pair<T1, T2>>() {\r\n        public boolean hasNext() {\r\n            return iter1.hasNext() && iter2.hasNext();\r\n        }\r\n        public Pair<T1, T2> next() {\r\n            return new Pair(iter1.next(), iter2.next());\r\n        }\r\n        public void remove() {\r\n            iter1.remove();\r\n            iter2.remove();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.zip",
	"Comment": "zips up two iterators into one iterator over pairs of correspondingelements.ends when the shorter iterator ends.",
	"Method": "Iterable<Pair<T1, T2>> zip(Iterable<T1> iter1,Iterable<T2> iter2,Iterable<Pair<T1, T2>> zip,Iterable<T1> iter,T2 array,Iterable<Pair<T1, T2>> zip,T1 array,Iterable<T2> iter,Iterable<Pair<T1, T2>> zip,T1 array1,T2 array2,Iterator<Pair<T1, T2>> zip,Iterator<T1> iter1,Iterator<T2> iter2){\r\n    return iter1.hasNext() && iter2.hasNext();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.zip",
	"Comment": "zips up two iterators into one iterator over pairs of correspondingelements.ends when the shorter iterator ends.",
	"Method": "Iterable<Pair<T1, T2>> zip(Iterable<T1> iter1,Iterable<T2> iter2,Iterable<Pair<T1, T2>> zip,Iterable<T1> iter,T2 array,Iterable<Pair<T1, T2>> zip,T1 array,Iterable<T2> iter,Iterable<Pair<T1, T2>> zip,T1 array1,T2 array2,Iterator<Pair<T1, T2>> zip,Iterator<T1> iter1,Iterator<T2> iter2){\r\n    return new Pair(iter1.next(), iter2.next());\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Iterables.zip",
	"Comment": "zips up two iterators into one iterator over pairs of correspondingelements.ends when the shorter iterator ends.",
	"Method": "Iterable<Pair<T1, T2>> zip(Iterable<T1> iter1,Iterable<T2> iter2,Iterable<Pair<T1, T2>> zip,Iterable<T1> iter,T2 array,Iterable<Pair<T1, T2>> zip,T1 array,Iterable<T2> iter,Iterable<Pair<T1, T2>> zip,T1 array1,T2 array2,Iterator<Pair<T1, T2>> zip,Iterator<T1> iter1,Iterator<T2> iter2){\r\n    iter1.remove();\r\n    iter2.remove();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.KerasLayer.getNInFromConfig",
	"Comment": "some dl4j layers need explicit specification of number of inputs, which keras does infer.this method searches through previous layers until a feedforwardlayer is found. these layershave nout values that subsequently correspond to the nin value of this layer.",
	"Method": "long getNInFromConfig(Map<String, ? extends KerasLayer> previousLayers){\r\n    int size = previousLayers.size();\r\n    int count = 0;\r\n    long nIn;\r\n    String inboundLayerName = inboundLayerNames.get(0);\r\n    while (count <= size) {\r\n        if (previousLayers.containsKey(inboundLayerName)) {\r\n            KerasLayer inbound = previousLayers.get(inboundLayerName);\r\n            try {\r\n                FeedForwardLayer ffLayer = (FeedForwardLayer) inbound.getLayer();\r\n                nIn = ffLayer.getNOut();\r\n                if (nIn > 0)\r\n                    return nIn;\r\n                count++;\r\n                inboundLayerName = inbound.getInboundLayerNames().get(0);\r\n            } catch (Exception e) {\r\n                inboundLayerName = inbound.getInboundLayerNames().get(0);\r\n            }\r\n        }\r\n    }\r\n    throw new UnsupportedKerasConfigurationException(\"Could not determine number of input channels for\" + \"depthwise convolution.\");\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.util.TestDataSetConsumer.consumeOnce",
	"Comment": "this method consumes single dataset, and spends delay time simulating execution of this dataset",
	"Method": "long consumeOnce(DataSet dataSet,boolean consumeWithSleep){\r\n    long timeMs = System.currentTimeMillis() + delay;\r\n    while (System.currentTimeMillis() < timeMs) {\r\n        if (consumeWithSleep)\r\n            try {\r\n                Thread.sleep(delay);\r\n            } catch (Exception e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n    }\r\n    count.incrementAndGet();\r\n    if (count.get() % 100 == 0)\r\n        logger.info(\"Passed {} datasets...\", count.get());\r\n    return count.get();\r\n}"
}, {
	"Path": "org.deeplearning4j.zoo.util.imagenet.ImageNetLabels.decodePredictions",
	"Comment": "given predictions from the trained model this method will return a stringlisting the top five matches and the respective probabilities",
	"Method": "String decodePredictions(INDArray predictions){\r\n    Preconditions.checkState(predictions.size(1) == predictionLabels.size(), \"Invalid input array:\" + \" expected array with size(1) equal to numLabels (%s), got array with shape %s\", predictionLabels.size(), predictions.shape());\r\n    String predictionDescription = \"\";\r\n    int[] top5 = new int[5];\r\n    float[] top5Prob = new float[5];\r\n    int i = 0;\r\n    for (int batch = 0; batch < predictions.size(0); batch++) {\r\n        predictionDescription += \"Predictions for batch \";\r\n        if (predictions.size(0) > 1) {\r\n            predictionDescription += String.valueOf(batch);\r\n        }\r\n        predictionDescription += \" :\";\r\n        INDArray currentBatch = predictions.getRow(batch).dup();\r\n        while (i < 5) {\r\n            top5[i] = Nd4j.argMax(currentBatch, 1).getInt(0, 0);\r\n            top5Prob[i] = currentBatch.getFloat(batch, top5[i]);\r\n            currentBatch.putScalar(0, top5[i], 0);\r\n            predictionDescription += \"\\n\\t\" + String.format(\"?\", top5Prob[i] * 100) + \"%, \" + predictionLabels.get(top5[i]);\r\n            i++;\r\n        }\r\n    }\r\n    return predictionDescription;\r\n}"
}, {
	"Path": "edu.stanford.nlp.time.JodaTimeUtils.timexDurationValue",
	"Comment": "return the timex string for the difference between two datestodo not really sure if this works...",
	"Method": "String timexDurationValue(ReadablePeriod duration,ConversionOptions opts,String timexDurationValue,ReadablePeriod duration,String timexDurationValue,ReadableDateTime begin,ReadableDateTime end){\r\n    return timexDurationValue(new Period(end.getMillis() - begin.getMillis()));\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.logs2probs",
	"Comment": "converts an array containing the natural logarithms ofprobabilities stored in a vector back into probabilities.the probabilities are assumed to sum to one.",
	"Method": "double[] logs2probs(double[] a){\r\n    double max = a[maxIndex(a)];\r\n    double sum = 0.0;\r\n    double[] result = new double[a.length];\r\n    for (int i = 0; i < a.length; i++) {\r\n        result[i] = Math.exp(a[i] - max);\r\n        sum += result[i];\r\n    }\r\n    normalize(result, sum);\r\n    return result;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.PrecisionRecallStats.toString",
	"Comment": "returns a string representation of this precisionrecallstats, indicating the number of tp, fp, fn counts.",
	"Method": "String toString(String toString,int numDigits){\r\n    return \"PrecisionRecallStats[tp=\" + getTP() + \",fp=\" + getFP() + \",fn=\" + getFN() + \",p=\" + getPrecisionDescription(numDigits) + \",r=\" + getRecallDescription(numDigits) + \",f1=\" + getF1Description(numDigits) + \"]\";\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.HashIndex.addToIndexUnsafe",
	"Comment": "add the given item to the index, but without taking any locks.use this method with care!but, this offers a noticable performance improvement if it is safe to use.",
	"Method": "int addToIndexUnsafe(E o){\r\n    if (indexes.isEmpty()) {\r\n        objects.add(o);\r\n        indexes.put(o, 0);\r\n        return 0;\r\n    } else {\r\n        Integer index = indexes.get(o);\r\n        if (index == null) {\r\n            if (locked) {\r\n                index = -1;\r\n            } else {\r\n                index = objects.size();\r\n                objects.add(o);\r\n                indexes.put(o, index);\r\n            }\r\n        }\r\n        return index;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.GrammaticalStructure.getRoots",
	"Comment": "return a list of typeddependencies which are not dependent on any node from the list.",
	"Method": "Collection<TypedDependency> getRoots(Collection<TypedDependency> list){\r\n    Collection<TypedDependency> roots = new ArrayList();\r\n    Collection<IndexedWord> deps = Generics.newHashSet();\r\n    for (TypedDependency typedDep : list) {\r\n        deps.add(typedDep.dep());\r\n    }\r\n    Collection<IndexedWord> govs = Generics.newHashSet();\r\n    for (TypedDependency typedDep : list) {\r\n        IndexedWord gov = typedDep.gov();\r\n        if (!deps.contains(gov) && !govs.contains(gov)) {\r\n            roots.add(typedDep);\r\n        }\r\n        govs.add(gov);\r\n    }\r\n    return roots;\r\n}"
}, {
	"Path": "org.datavec.api.transform.TransformProcess.getFinalSchema",
	"Comment": "get the schema of the output data, after executing the process",
	"Method": "Schema getFinalSchema(){\r\n    return getSchemaAfterStep(actionList.size());\r\n}"
}, {
	"Path": "edu.stanford.nlp.time.JodaTimeUtils.minimumValue",
	"Comment": "return the minimum value of a field, closest to the reference time",
	"Method": "int minimumValue(DateTimeFieldType type,ReadableDateTime reference){\r\n    return reference.toDateTime().property(type).getMinimumValue();\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.max",
	"Comment": "max array reduction operation, optionally along specified dimensions",
	"Method": "SDVariable max(SDVariable x,int dimensions,SDVariable max,String name,SDVariable x,int dimensions,SDVariable max,String name,SDVariable x,boolean keepDims,int dimensions,SDVariable max,SDVariable first,SDVariable second,SDVariable max,String name,SDVariable first,SDVariable second){\r\n    SDVariable result = f().max(first, second);\r\n    return updateVariableNameAndReference(result, name);\r\n}"
}, {
	"Path": "dagger.internal.codegen.AdapterJavadocs.bindingTypeDocs",
	"Comment": "creates an appropriate javadoc depending on aspects of the type in question.",
	"Method": "CodeBlock bindingTypeDocs(TypeName type,boolean abstrakt,boolean members,boolean dependent){\r\n    CodeBlock.Builder result = CodeBlock.builder().add(\"A {@code Binding<$T>} implementation which satisfies\\n\", type).add(\"Dagger's infrastructure requirements including:\\n\");\r\n    if (dependent) {\r\n        result.add(\"\\n\").add(\"Owning the dependency links between {@code $T} and its\\n\", type).add(\"dependencies.\\n\");\r\n    }\r\n    if (!abstrakt) {\r\n        result.add(\"\\n\").add(\"Being a {@code Provider<$T>} and handling creation and\\n\", type).add(\"preparation of object instances.\\n\");\r\n    }\r\n    if (members) {\r\n        result.add(\"\\n\").add(\"Being a {@code MembersInjector<$T>} and handling injection\\n\", type).add(\"of annotated fields.\\n\");\r\n    }\r\n    return result.build();\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.SequenceGibbsSampler.sampleSequenceRepeatedly",
	"Comment": "samples the sequence repeatedly, making numsamples passes over the entire sequence.destructively modifies the sequence in place.",
	"Method": "double sampleSequenceRepeatedly(SequenceModel model,int[] sequence,int numSamples,double sampleSequenceRepeatedly,SequenceModel model,int numSamples){\r\n    int[] sequence = getRandomSequence(model);\r\n    return sampleSequenceRepeatedly(model, sequence, numSamples);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.KerasSequentialModel.getMultiLayerConfiguration",
	"Comment": "configure a multilayerconfiguration from this keras sequential model configuration.",
	"Method": "MultiLayerConfiguration getMultiLayerConfiguration(){\r\n    if (!this.className.equals(config.getFieldClassNameSequential()))\r\n        throw new InvalidKerasConfigurationException(\"Keras model class name \" + this.className + \" incompatible with MultiLayerNetwork\");\r\n    if (this.inputLayerNames.size() != 1)\r\n        throw new InvalidKerasConfigurationException(\"MultiLayerNetwork expects only 1 input (found \" + this.inputLayerNames.size() + \")\");\r\n    if (this.outputLayerNames.size() != 1)\r\n        throw new InvalidKerasConfigurationException(\"MultiLayerNetwork expects only 1 output (found \" + this.outputLayerNames.size() + \")\");\r\n    NeuralNetConfiguration.Builder modelBuilder = new NeuralNetConfiguration.Builder();\r\n    if (optimizer != null) {\r\n        modelBuilder.updater(optimizer);\r\n    }\r\n    NeuralNetConfiguration.ListBuilder listBuilder = modelBuilder.list();\r\n    KerasLayer prevLayer = null;\r\n    int layerIndex = 0;\r\n    for (KerasLayer layer : this.layersOrdered) {\r\n        if (layer.isLayer()) {\r\n            int nbInbound = layer.getInboundLayerNames().size();\r\n            if (nbInbound != 1)\r\n                throw new InvalidKerasConfigurationException(\"Layers in MultiLayerConfiguration must have exactly one inbound layer (found \" + nbInbound + \" for layer \" + layer.getLayerName() + \")\");\r\n            if (prevLayer != null) {\r\n                InputType[] inputTypes = new InputType[1];\r\n                InputPreProcessor preprocessor;\r\n                if (prevLayer.isInputPreProcessor()) {\r\n                    inputTypes[0] = this.outputTypes.get(prevLayer.getInboundLayerNames().get(0));\r\n                    preprocessor = prevLayer.getInputPreprocessor(inputTypes);\r\n                } else {\r\n                    inputTypes[0] = this.outputTypes.get(prevLayer.getLayerName());\r\n                    preprocessor = layer.getInputPreprocessor(inputTypes);\r\n                }\r\n                if (preprocessor != null)\r\n                    listBuilder.inputPreProcessor(layerIndex, preprocessor);\r\n            }\r\n            listBuilder.layer(layerIndex++, layer.getLayer());\r\n        } else if (layer.getVertex() != null)\r\n            throw new InvalidKerasConfigurationException(\"Cannot add vertex to MultiLayerConfiguration (class name \" + layer.getClassName() + \", layer name \" + layer.getLayerName() + \")\");\r\n        prevLayer = layer;\r\n    }\r\n    InputType inputType = this.layersOrdered.get(0).getOutputType();\r\n    if (inputType != null)\r\n        listBuilder.setInputType(inputType);\r\n    if (this.useTruncatedBPTT && this.truncatedBPTT > 0)\r\n        listBuilder.backpropType(BackpropType.TruncatedBPTT).tBPTTForwardLength(truncatedBPTT).tBPTTBackwardLength(truncatedBPTT);\r\n    else\r\n        listBuilder.backpropType(BackpropType.Standard);\r\n    return listBuilder.build();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.StringUtils.truncate",
	"Comment": "this returns a string from decimal digit smallestdigit to decimal digitbiggest digit. smallest digit is labeled 1, and the limits areinclusive.",
	"Method": "String truncate(int n,int smallestDigit,int biggestDigit){\r\n    int numDigits = biggestDigit - smallestDigit + 1;\r\n    char[] result = new char[numDigits];\r\n    for (int j = 1; j < smallestDigit; j++) {\r\n        n = n / 10;\r\n    }\r\n    for (int j = numDigits - 1; j >= 0; j--) {\r\n        result[j] = Character.forDigit(n % 10, 10);\r\n        n = n / 10;\r\n    }\r\n    return new String(result);\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.util.MLLibUtil.fromContinuousLabeledPoint",
	"Comment": "converts a continuous javardd labeledpoint to a javardd dataset.",
	"Method": "JavaRDD<DataSet> fromContinuousLabeledPoint(JavaSparkContext sc,JavaRDD<LabeledPoint> data,JavaRDD<DataSet> fromContinuousLabeledPoint,JavaRDD<LabeledPoint> data,JavaRDD<DataSet> fromContinuousLabeledPoint,JavaRDD<LabeledPoint> data,boolean preCache){\r\n    if (preCache && !data.getStorageLevel().useMemory()) {\r\n        data.cache();\r\n    }\r\n    return data.map(new Function<LabeledPoint, DataSet>() {\r\n        @Override\r\n        public DataSet call(LabeledPoint lp) {\r\n            return convertToDataset(lp);\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.util.MLLibUtil.fromContinuousLabeledPoint",
	"Comment": "converts a continuous javardd labeledpoint to a javardd dataset.",
	"Method": "JavaRDD<DataSet> fromContinuousLabeledPoint(JavaSparkContext sc,JavaRDD<LabeledPoint> data,JavaRDD<DataSet> fromContinuousLabeledPoint,JavaRDD<LabeledPoint> data,JavaRDD<DataSet> fromContinuousLabeledPoint,JavaRDD<LabeledPoint> data,boolean preCache){\r\n    return convertToDataset(lp);\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.ConfusionMatrix.getClasses",
	"Comment": "gives the applytransformtodestination of all classes in the confusion matrix.",
	"Method": "List<T> getClasses(){\r\n    if (classes == null)\r\n        classes = new ArrayList();\r\n    return classes;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.CollinsDependency.extractFromTree",
	"Comment": "this method assumes that a start symbol node has been added to the tree.",
	"Method": "Set<CollinsDependency> extractFromTree(Tree t,String startSymbol,HeadFinder hf,Set<CollinsDependency> extractFromTree,Tree t,String startSymbol,HeadFinder hf,boolean normPOS){\r\n    if (t == null || startSymbol.equals(\"\") || hf == null)\r\n        return null;\r\n    final Set<CollinsDependency> deps = Generics.newHashSet();\r\n    if (t.value().equals(startSymbol))\r\n        t = t.firstChild();\r\n    boolean mustProcessRoot = true;\r\n    for (final Tree node : t) {\r\n        if (node.isLeaf() || node.numChildren() < 2)\r\n            continue;\r\n        final Tree headDaughter = hf.determineHead(node);\r\n        final Tree head = node.headTerminal(hf);\r\n        if (headDaughter == null || head == null) {\r\n            log.info(\"WARNING: CollinsDependency.extractFromTree() could not find root for:\\n\" + node.pennString());\r\n        } else {\r\n            if (mustProcessRoot) {\r\n                mustProcessRoot = false;\r\n                final CoreLabel startLabel = makeStartLabel(startSymbol);\r\n                deps.add(new CollinsDependency(new CoreLabel(head.label()), startLabel, new CollinsRelation(startSymbol, startSymbol, node.value(), Direction.Right)));\r\n            }\r\n            Direction dir = Direction.Left;\r\n            for (final Tree daughter : node.children()) {\r\n                if (daughter.equals(headDaughter)) {\r\n                    dir = Direction.Right;\r\n                } else {\r\n                    final Tree headOfDaughter = daughter.headTerminal(hf);\r\n                    final String relParent = (normPOS && node.isPreTerminal()) ? normPOSLabel : node.value();\r\n                    final String relHead = (normPOS && headDaughter.isPreTerminal()) ? normPOSLabel : headDaughter.value();\r\n                    final String relModifier = (normPOS && daughter.isPreTerminal()) ? normPOSLabel : daughter.value();\r\n                    final CollinsDependency newDep = new CollinsDependency(new CoreLabel(headOfDaughter.label()), new CoreLabel(head.label()), new CollinsRelation(relParent, relHead, relModifier, dir));\r\n                    deps.add(newDep);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if (t.yield().size() != deps.size()) {\r\n        System.err.printf(\"WARNING: Number of extracted dependencies (%d) does not match yield (%d):\\n\", deps.size(), t.yield().size());\r\n        log.info(t.pennString());\r\n        log.info();\r\n        int num = 0;\r\n        for (CollinsDependency dep : deps) log.info(num++ + \": \" + dep.toString());\r\n    }\r\n    return deps;\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.ConfusionMatrix.getPredictedTotal",
	"Comment": "computes the total number of times the class was predicted by the classifier.",
	"Method": "int getPredictedTotal(T predicted){\r\n    int total = 0;\r\n    for (T actual : classes) {\r\n        total += getCount(actual, predicted);\r\n    }\r\n    return total;\r\n}"
}, {
	"Path": "edu.stanford.nlp.patterns.ScorePhrasesLearnFeatWt.chooseUnknownAsNegatives",
	"Comment": "this chooses the ones that are not close to the positive phrases!",
	"Method": "Set<CandidatePhrase> chooseUnknownAsNegatives(Set<CandidatePhrase> candidatePhrases,String label,Collection<CandidatePhrase> positivePhrases,Map<String, Collection<CandidatePhrase>> knownNegativePhrases,BufferedWriter logFile){\r\n    List<List<CandidatePhrase>> threadedCandidates = GetPatternsFromDataMultiClass.getThreadBatches(CollectionUtils.toList(candidatePhrases), constVars.numThreads);\r\n    Counter<CandidatePhrase> sims = new ClassicCounter();\r\n    AtomicDouble allMaxSim = new AtomicDouble(Double.MIN_VALUE);\r\n    ExecutorService executor = Executors.newFixedThreadPool(constVars.numThreads);\r\n    List<Future<Pair<Counter<CandidatePhrase>, Counter<CandidatePhrase>>>> list = new ArrayList();\r\n    for (List<CandidatePhrase> keys : threadedCandidates) {\r\n        Callable<Pair<Counter<CandidatePhrase>, Counter<CandidatePhrase>>> task = new ComputeSim(label, keys, allMaxSim, positivePhrases, knownNegativePhrases);\r\n        Future<Pair<Counter<CandidatePhrase>, Counter<CandidatePhrase>>> submit = executor.submit(task);\r\n        list.add(submit);\r\n    }\r\n    for (Future<Pair<Counter<CandidatePhrase>, Counter<CandidatePhrase>>> future : list) {\r\n        try {\r\n            sims.addAll(future.get().first());\r\n        } catch (Exception e) {\r\n            executor.shutdownNow();\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n    executor.shutdown();\r\n    if (allMaxSim.get() == Double.MIN_VALUE) {\r\n        Redwood.log(Redwood.DBG, \"No similarity recorded between the positives and the unknown!\");\r\n    }\r\n    CandidatePhrase k = Counters.argmax(sims);\r\n    System.out.println(\"Maximum similarity was \" + sims.getCount(k) + \" for word \" + k);\r\n    Counter<CandidatePhrase> removed = Counters.retainBelow(sims, constVars.positiveSimilarityThresholdLowPrecision);\r\n    System.out.println(\"removing phrases as negative phrases that were higher that positive similarity threshold of \" + constVars.positiveSimilarityThresholdLowPrecision + removed);\r\n    if (logFile != null && wordVectors != null) {\r\n        for (Entry<CandidatePhrase, Double> en : removed.entrySet()) if (wordVectors.containsKey(en.getKey().getPhrase()))\r\n            logFile.write(en.getKey() + \"-PN \" + ArrayUtils.toString(wordVectors.get(en.getKey().getPhrase()), \" \") + \"\\n\");\r\n    }\r\n    return sims.keySet();\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.util.ComputationGraphUtil.toMultiDataSetIterator",
	"Comment": "convert a datasetiterator to a multidatasetiterator, via an adaptor class",
	"Method": "MultiDataSetIterator toMultiDataSetIterator(DataSetIterator iterator){\r\n    return new MultiDataSetIteratorAdapter(iterator);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.isPrePreTerminal",
	"Comment": "return whether all the children of this node are preterminals or not.a preterminal isdefined to be a node with one child which is itself a leaf.considered false if the node has no children",
	"Method": "boolean isPrePreTerminal(){\r\n    Tree[] kids = children();\r\n    if (kids.length == 0) {\r\n        return false;\r\n    }\r\n    for (Tree kid : kids) {\r\n        if (!kid.isPreTerminal()) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.CoreDocument.wrapAnnotations",
	"Comment": "complete the wrapping process post annotation by a pipeline",
	"Method": "void wrapAnnotations(){\r\n    if (this.annotationDocument.get(CoreAnnotations.SentencesAnnotation.class) != null) {\r\n        wrapSentences();\r\n        if (!sentences.isEmpty() && sentences.get(0).entityMentions() != null) {\r\n            buildDocumentEntityMentionsList();\r\n        }\r\n        if (QuoteAnnotator.gatherQuotes(this.annotationDocument) != null)\r\n            buildDocumentQuotesList();\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.layers.samediff.SameDiffLambdaLayer.defineLayer",
	"Comment": "the definelayer method is used to define the foward pass for the layer",
	"Method": "SDVariable defineLayer(SameDiff sameDiff,SDVariable layerInput,SDVariable defineLayer,SameDiff sameDiff,SDVariable layerInput,Map<String, SDVariable> paramTable){\r\n    return defineLayer(sameDiff, layerInput);\r\n}"
}, {
	"Path": "org.datavec.arrow.ArrowConverter.longField",
	"Comment": "shortcut method for creating a long fieldwith 64 bit long field",
	"Method": "Field longField(String name){\r\n    return getFieldForColumn(name, ColumnType.Long);\r\n}"
}, {
	"Path": "edu.stanford.nlp.patterns.surface.SurfacePattern.subsumesArray",
	"Comment": "true if array1 contains array2. also true if both array1 and array2 arenull",
	"Method": "boolean subsumesArray(Object[] array1,Object[] array2){\r\n    if ((array1 == null && array2 == null)) {\r\n        return true;\r\n    }\r\n    if (array1 == null || array2 == null) {\r\n        return false;\r\n    }\r\n    if (array2.length > array1.length) {\r\n        return false;\r\n    }\r\n    for (int i = 0; i < array1.length; i++) {\r\n        if (array1[i].equals(array2[0])) {\r\n            boolean found = true;\r\n            for (int j = 0; j < array2.length; j++) {\r\n                if (array1.length <= i + j || !array2[j].equals(array1[i + j])) {\r\n                    found = false;\r\n                    break;\r\n                }\r\n            }\r\n            if (found) {\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.util.MLLibUtil.toClassifierPrediction",
	"Comment": "this is for the edge case whereyou have a single output layerand need to convert the output layer toan index",
	"Method": "double toClassifierPrediction(Vector vector){\r\n    double max = Double.NEGATIVE_INFINITY;\r\n    int maxIndex = 0;\r\n    for (int i = 0; i < vector.size(); i++) {\r\n        double curr = vector.apply(i);\r\n        if (curr > max) {\r\n            maxIndex = i;\r\n            max = curr;\r\n        }\r\n    }\r\n    return maxIndex;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.deserialize",
	"Comment": "read a sentence from an input stream.this does not close the input stream.",
	"Method": "Sentence deserialize(InputStream in){\r\n    return new Sentence(CoreNLPProtos.Sentence.parseDelimitedFrom(in));\r\n}"
}, {
	"Path": "org.datavec.api.io.WritableUtils.writeCompressedStringArray",
	"Comment": "write a string array as a nework int n, followed by int n byte array ofcompressed strings. handles also null arrays and null values.could be generalised using introspection.",
	"Method": "void writeCompressedStringArray(DataOutput out,String[] s){\r\n    if (s == null) {\r\n        out.writeInt(-1);\r\n        return;\r\n    }\r\n    out.writeInt(s.length);\r\n    for (int i = 0; i < s.length; i++) {\r\n        writeCompressedString(out, s[i]);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.batchToSpace",
	"Comment": "convolution 2d layer batch to space operation on 4d input.reduces input batch dimension by rearranging data into a larger spatial dimensions",
	"Method": "SDVariable batchToSpace(SDVariable x,int[] blocks,int[][] crops,SDVariable batchToSpace,String name,SDVariable x,int[] blocks,int[][] crops){\r\n    SDVariable ret = f().batchToSpace(x, blocks, crops);\r\n    return updateVariableNameAndReference(ret, name);\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.bernoullis",
	"Comment": "this will return the bernoulli trial for the given event.a bernoulli trial is a mechanism for detecting the probabilityof a given event occurring k times in n independent trials",
	"Method": "double bernoullis(double n,double k,double successProb){\r\n    double combo = MathUtils.combination(n, k);\r\n    double q = 1 - successProb;\r\n    return combo * Math.pow(successProb, k) * Math.pow(q, n - k);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.rnnUpdateStateWithTBPTTState",
	"Comment": "update the internal state of rnn layers after a truncated bptt fit call",
	"Method": "void rnnUpdateStateWithTBPTTState(){\r\n    for (int i = 0; i < layers.length; i++) {\r\n        if (layers[i] instanceof RecurrentLayer) {\r\n            RecurrentLayer l = ((RecurrentLayer) layers[i]);\r\n            l.rnnSetPreviousState(l.rnnGetTBPTTState());\r\n        } else if (layers[i] instanceof MultiLayerNetwork) {\r\n            ((MultiLayerNetwork) layers[i]).updateRnnStateWithTBPTTState();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.conf.Configuration.getStrings",
	"Comment": "get the comma delimited values of the name property asan array of strings.if no such property is specified then default value is returned.",
	"Method": "String[] getStrings(String name,String[] getStrings,String name,String defaultValue){\r\n    String valueString = get(name);\r\n    if (valueString == null) {\r\n        return defaultValue;\r\n    } else {\r\n        return StringUtils.getStrings(valueString);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.Tree.score",
	"Comment": "returns the score associated with the current node, or nanif there is no score.the default implementation returns nan.",
	"Method": "double score(){\r\n    return Double.NaN;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.PTBLexer.yypushback",
	"Comment": "pushes the specified amount of characters back into the input stream.they will be read again by then next call of the scanning method",
	"Method": "void yypushback(int number){\r\n    if (number > yylength())\r\n        zzScanError(ZZ_PUSHBACK_2BIG);\r\n    zzMarkedPos -= number;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.word2vec.wordstore.VocabularyHolder.incrementWordCounter",
	"Comment": "increments by one number of occurrences of the word in corpus",
	"Method": "void incrementWordCounter(String word){\r\n    if (vocabulary.containsKey(word)) {\r\n        vocabulary.get(word).incrementCount();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.SequenceGibbsSampler.sampleSequenceBackward",
	"Comment": "samples the complete sequence once in the backward directiondestructively modifies the sequence in place.",
	"Method": "double sampleSequenceBackward(SequenceModel model,int[] sequence,double sampleSequenceBackward,SequenceModel model,int[] sequence,double temperature){\r\n    double returnScore = Double.NEGATIVE_INFINITY;\r\n    for (int pos = sequence.length - 1; pos >= 0; pos--) {\r\n        returnScore = samplePosition(model, sequence, pos, temperature);\r\n    }\r\n    return returnScore;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.TregexTest.testReuse",
	"Comment": "a tregex pattern should be able to go more than once.just like me.",
	"Method": "void testReuse(){\r\n    final TregexPattern pMWE = TregexPattern.compile(\"/^MW/\");\r\n    Tree tree = treeFromString(\"(ROOT (MWE (N 1) (N 2) (N 3)) (MWV (A B)))\");\r\n    TregexMatcher matcher = pMWE.matcher(tree);\r\n    assertTrue(matcher.find());\r\n    assertTrue(matcher.find());\r\n    assertFalse(matcher.find());\r\n    tree = treeFromString(\"(ROOT (MWE (N 1) (N 2) (N 3)))\");\r\n    matcher = pMWE.matcher(tree);\r\n    assertTrue(matcher.find());\r\n    assertFalse(matcher.find());\r\n    tree = treeFromString(\"(Foo)\");\r\n    matcher = pMWE.matcher(tree);\r\n    assertFalse(matcher.find());\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.transferlearning.TransferLearningHelper.unfrozenGraph",
	"Comment": "returns the unfrozen subset of the original computation graph as a computation graphnote that with each call to featurizedfit the parameters to the original computation graph are also updated",
	"Method": "ComputationGraph unfrozenGraph(){\r\n    if (!isGraph)\r\n        errorIfGraphIfMLN();\r\n    return unFrozenSubsetGraph;\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.BaseColumnTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return new String[] { columnName };\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.impl.graph.SparkComputationGraph.fitPaths",
	"Comment": "fit the network using a list of paths for serialized dataset objects.",
	"Method": "ComputationGraph fitPaths(JavaRDD<String> paths,ComputationGraph fitPaths,JavaRDD<String> paths,DataSetLoader loader,ComputationGraph fitPaths,JavaRDD<String> paths,MultiDataSetLoader loader){\r\n    trainingMaster.executeTrainingPaths(null, this, paths, null, loader);\r\n    network.incrementEpochCount();\r\n    return network;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.ensureKeys",
	"Comment": "ensures that counter t has all keys in keys. if the counter does not have the keys, then add the key with count value.note that it does not change counts that exist in the counter",
	"Method": "void ensureKeys(Counter<E> t,Collection<E> keys,double value){\r\n    for (E k : keys) {\r\n        if (!t.containsKey(k))\r\n            t.setCount(k, value);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseShapeInfoProvider.createShapeInformation",
	"Comment": "this method creates shapeinformation buffer, based on shape & order being passed in",
	"Method": "Pair<DataBuffer, long[]> createShapeInformation(int[] shape,Pair<DataBuffer, long[]> createShapeInformation,int[] shape,char order,Pair<DataBuffer, long[]> createShapeInformation,int[] shape,int[] stride,long offset,int elementWiseStride,char order,Pair<DataBuffer, long[]> createShapeInformation,int[] shape,int[] stride,long offset,int elementWiseStride,char order,long extras,Pair<DataBuffer, long[]> createShapeInformation,long[] shape,long[] stride,long offset,long elementWiseStride,char order,long extras,Pair<DataBuffer, long[]> createShapeInformation,long[] shape,Pair<DataBuffer, long[]> createShapeInformation,long[] shape,char order,Pair<DataBuffer, long[]> createShapeInformation,long[] shape,long[] stride,long offset,long elementWiseStride,char order){\r\n    DataBuffer buffer = Shape.createShapeInformation(shape, stride, offset, elementWiseStride, order);\r\n    buffer.setConstant(true);\r\n    return Pair.create(buffer, buffer.asLong());\r\n}"
}, {
	"Path": "org.datavec.api.formats.input.impl.ListStringInputFormat.toInt",
	"Comment": "convert writable to int. whether this is supported depends on the specific writable.",
	"Method": "int toInt(){\r\n    return 0;\r\n}"
}, {
	"Path": "org.deeplearning4j.text.tokenization.tokenizer.DefaultStreamTokenizer.hasMoreTokens",
	"Comment": "checks, if any prebuffered tokens left, otherswise checks underlying stream",
	"Method": "boolean hasMoreTokens(){\r\n    log.info(\"Tokens size: [\" + tokens.size() + \"], position: [\" + position.get() + \"]\");\r\n    if (!tokens.isEmpty())\r\n        return position.get() < tokens.size();\r\n    else\r\n        return streamHasMoreTokens();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.GrammaticalStructureConversionUtils.printDependencies",
	"Comment": "print typed dependencies in either the stanford dependency representationor in the conllx format.",
	"Method": "void printDependencies(GrammaticalStructure gs,Collection<TypedDependency> deps,Tree tree,boolean conllx,boolean extraSep,boolean convertToUPOS){\r\n    System.out.println(dependenciesToString(gs, deps, tree, conllx, extraSep, convertToUPOS));\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.layers.normalization.KerasBatchNormalization.getBatchNormAxis",
	"Comment": "get batchnormalization axis from keras layer configuration. currently unused.",
	"Method": "int getBatchNormAxis(Map<String, Object> layerConfig){\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    return (int) innerConfig.get(LAYER_FIELD_AXIS);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractCollinsHeadFinder.traverseLocate",
	"Comment": "attempt to locate head daughter tree from among daughters.go through daughtertrees looking for things from or not in a set given bythe contents of the array how, and ifyou do not find one, take leftmost or rightmost perhaps matching thing ifflastresort is true, otherwise return null.",
	"Method": "Tree traverseLocate(Tree[] daughterTrees,String[] how,boolean lastResort){\r\n    int headIdx;\r\n    switch(how[0]) {\r\n        case \"left\":\r\n            headIdx = findLeftHead(daughterTrees, how);\r\n            break;\r\n        case \"leftdis\":\r\n            headIdx = findLeftDisHead(daughterTrees, how);\r\n            break;\r\n        case \"leftexcept\":\r\n            headIdx = findLeftExceptHead(daughterTrees, how);\r\n            break;\r\n        case \"right\":\r\n            headIdx = findRightHead(daughterTrees, how);\r\n            break;\r\n        case \"rightdis\":\r\n            headIdx = findRightDisHead(daughterTrees, how);\r\n            break;\r\n        case \"rightexcept\":\r\n            headIdx = findRightExceptHead(daughterTrees, how);\r\n            break;\r\n        default:\r\n            throw new IllegalStateException(\"ERROR: invalid direction type \" + how[0] + \" to nonTerminalInfo map in AbstractCollinsHeadFinder.\");\r\n    }\r\n    if (headIdx < 0) {\r\n        if (lastResort) {\r\n            String[] rule;\r\n            if (how[0].startsWith(\"left\")) {\r\n                headIdx = 0;\r\n                rule = defaultLeftRule;\r\n            } else {\r\n                headIdx = daughterTrees.length - 1;\r\n                rule = defaultRightRule;\r\n            }\r\n            Tree child = traverseLocate(daughterTrees, rule, false);\r\n            if (child != null) {\r\n                return child;\r\n            } else {\r\n                return daughterTrees[headIdx];\r\n            }\r\n        } else {\r\n            return null;\r\n        }\r\n    }\r\n    headIdx = postOperationFix(headIdx, daughterTrees);\r\n    return daughterTrees[headIdx];\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.AbstractTreebankLanguagePack.startSymbolAcceptFilter",
	"Comment": "return a filter that accepts a string that is a start symbolof the treebank, and rejects everything else.",
	"Method": "Predicate<String> startSymbolAcceptFilter(){\r\n    return startSymbolAcceptFilter;\r\n}"
}, {
	"Path": "org.datavec.api.transform.metadata.DoubleMetaData.isValid",
	"Comment": "is the given object valid for this column,given the column type and anyrestrictions given by thecolumnmetadata object?",
	"Method": "boolean isValid(Writable writable,boolean isValid,Object input){\r\n    double d;\r\n    try {\r\n        d = Double.valueOf(input.toString());\r\n    } catch (Exception e) {\r\n        return false;\r\n    }\r\n    if (allowNaN && Double.isNaN(d))\r\n        return true;\r\n    if (allowInfinite && Double.isInfinite(d))\r\n        return true;\r\n    if (minAllowedValue != null && d < minAllowedValue)\r\n        return false;\r\n    if (maxAllowedValue != null && d > maxAllowedValue)\r\n        return false;\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.Ssurgeon.getFirstTag",
	"Comment": "for the given element, finds the first child element with the given tag.",
	"Method": "Element getFirstTag(Element element,String tag){\r\n    try {\r\n        NodeList nodeList = element.getElementsByTagName(tag);\r\n        if (nodeList.getLength() == 0)\r\n            return null;\r\n        for (int i = 0; i < nodeList.getLength(); i++) {\r\n            Node node = nodeList.item(i);\r\n            if (node.getNodeType() == Node.ELEMENT_NODE)\r\n                return (Element) node;\r\n        }\r\n    } catch (Exception e) {\r\n        log.warning(\"Error getting first tag \" + tag + \" under element=\" + element);\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.DiskTreebank.loadPath",
	"Comment": "load trees from given directory.this version just recordsthe paths to be processed, and actually processes them at apply time.",
	"Method": "void loadPath(File path,FileFilter filt){\r\n    if (path.exists()) {\r\n        filePaths.add(path);\r\n        fileFilters.add(filt);\r\n    } else {\r\n        System.err.printf(\"%s: File/path %s does not exist. Skipping.%n\", this.getClass().getName(), path.getPath());\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ssurgeon.SsurgeonPattern.addNamedNode",
	"Comment": "adds the node to the set of named nodes registered, using the given name.",
	"Method": "void addNamedNode(IndexedWord node,String name){\r\n    nodeMap.put(name, node);\r\n}"
}, {
	"Path": "org.deeplearning4j.text.documentiterator.FileDocumentIteratorTest.testNextDocument",
	"Comment": "checks actual number of documents retrieved by documentiterator",
	"Method": "void testNextDocument(){\r\n    ClassPathResource reuters5250 = new ClassPathResource(\"/reuters/5250\");\r\n    File f = reuters5250.getFile();\r\n    DocumentIterator iter = new FileDocumentIterator(f.getAbsolutePath());\r\n    log.info(f.getAbsolutePath());\r\n    int cnt = 0;\r\n    while (iter.hasNext()) {\r\n        InputStream stream = iter.nextDocument();\r\n        stream.close();\r\n        cnt++;\r\n    }\r\n    assertEquals(24, cnt);\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.SentenceAlgorithms.headOfSpan",
	"Comment": "get the index of the head word for a given span, based off of the dependency parse.",
	"Method": "int headOfSpan(Span tokenSpan){\r\n    if (tokenSpan.size() == 0) {\r\n        throw new IllegalArgumentException(\"Cannot find head word of empty span!\");\r\n    }\r\n    List<Optional<Integer>> governors = sentence.governors();\r\n    if (tokenSpan.start() >= governors.size()) {\r\n        throw new IllegalArgumentException(\"Span is out of range: \" + tokenSpan + \"; sentence: \" + sentence);\r\n    }\r\n    if (tokenSpan.end() > governors.size()) {\r\n        throw new IllegalArgumentException(\"Span is out of range: \" + tokenSpan + \"; sentence: \" + sentence);\r\n    }\r\n    int candidateStart = tokenSpan.end() - 1;\r\n    Optional<Integer> parent;\r\n    while (!(parent = governors.get(candidateStart)).isPresent()) {\r\n        candidateStart -= 1;\r\n        if (candidateStart < tokenSpan.start()) {\r\n            return tokenSpan.end() - 1;\r\n        }\r\n    }\r\n    int candidate = candidateStart;\r\n    Set<Integer> seen = new HashSet();\r\n    while (parent.isPresent() && parent.get() >= tokenSpan.start() && parent.get() < tokenSpan.end()) {\r\n        candidate = parent.get();\r\n        if (seen.contains(candidate)) {\r\n            return candidate;\r\n        }\r\n        seen.add(candidate);\r\n        parent = governors.get(candidate);\r\n    }\r\n    return candidate;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.BaseOutputLayer.labelProbabilities",
	"Comment": "returns the probabilities for each labelfor each example row wise",
	"Method": "INDArray labelProbabilities(INDArray examples){\r\n    return activate(examples, false, LayerWorkspaceMgr.noWorkspacesImmutable());\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseSparseNDArrayCOO.checkBufferCoherence",
	"Comment": "check that the length of indices and values are coherent and matches the rank of the matrix.",
	"Method": "void checkBufferCoherence(){\r\n    if (values.length() < length) {\r\n        throw new IllegalStateException(\"nnz is larger than capacity of buffers\");\r\n    }\r\n    if (values.length() * rank() != indices.length()) {\r\n        throw new IllegalArgumentException(\"Sizes of values, indices and shape are incoherent.\");\r\n    }\r\n}"
}, {
	"Path": "org.datavec.image.transform.CropImageTransform.doTransform",
	"Comment": "takes an image and returns a transformed image.uses the random object in the case of random transformations.",
	"Method": "ImageWritable doTransform(ImageWritable image,Random random){\r\n    if (image == null) {\r\n        return null;\r\n    }\r\n    Mat mat = converter.convert(image.getFrame());\r\n    int top = random != null ? random.nextInt(cropTop + 1) : cropTop;\r\n    int left = random != null ? random.nextInt(cropLeft + 1) : cropLeft;\r\n    int bottom = random != null ? random.nextInt(cropBottom + 1) : cropBottom;\r\n    int right = random != null ? random.nextInt(cropRight + 1) : cropRight;\r\n    y = Math.min(top, mat.rows() - 1);\r\n    x = Math.min(left, mat.cols() - 1);\r\n    int h = Math.max(1, mat.rows() - bottom - y);\r\n    int w = Math.max(1, mat.cols() - right - x);\r\n    Mat result = mat.apply(new Rect(x, y, w, h));\r\n    return new ImageWritable(converter.convert(result));\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.XMLOutputter.addCorefGraphInfo",
	"Comment": "generates the xml content for the coreference chain object.",
	"Method": "boolean addCorefGraphInfo(Options options,Element corefInfo,List<CoreMap> sentences,Map<Integer, CorefChain> corefChains,String curNS){\r\n    boolean foundCoref = false;\r\n    for (CorefChain chain : corefChains.values()) {\r\n        if (!options.printSingletons && chain.getMentionsInTextualOrder().size() <= 1)\r\n            continue;\r\n        foundCoref = true;\r\n        Element chainElem = new Element(\"coreference\", curNS);\r\n        CorefChain.CorefMention source = chain.getRepresentativeMention();\r\n        addCorefMention(options, chainElem, curNS, sentences, source, true);\r\n        for (CorefChain.CorefMention mention : chain.getMentionsInTextualOrder()) {\r\n            if (mention == source)\r\n                continue;\r\n            addCorefMention(options, chainElem, curNS, sentences, mention, false);\r\n        }\r\n        corefInfo.appendChild(chainElem);\r\n    }\r\n    return foundCoref;\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.Clique.maxLeft",
	"Comment": "convenience method for finding the most far leftrelative index.",
	"Method": "int maxLeft(){\r\n    return relativeIndices[0];\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main",
	"Comment": "the main method.read the command line arguments and run the server.",
	"Method": "void main(String[] args){\r\n    log(\"--- \" + StanfordCoreNLPServer.class.getSimpleName() + \"#main() called ---\");\r\n    String build = System.getenv(\"BUILD\");\r\n    if (build != null) {\r\n        log(\"    Build: \" + build);\r\n    }\r\n    Runtime.getRuntime().addShutdownHook(new Thread(() -> log(\"CoreNLP Server is shutting down.\")));\r\n    ArgumentParser.fillOptions(StanfordCoreNLPServer.class, args);\r\n    Properties serverProperties = StringUtils.argsToProperties(args);\r\n    StanfordCoreNLPServer server = new StanfordCoreNLPServer(serverProperties);\r\n    ArgumentParser.fillOptions(server, args);\r\n    if (serverProperties != null && !serverProperties.containsKey(\"status_port\") && serverProperties.containsKey(\"port\")) {\r\n        server.statusPort = Integer.parseInt(serverProperties.getProperty(\"port\"));\r\n    }\r\n    log(\"    Threads: \" + ArgumentParser.threads);\r\n    AtomicBoolean live = new AtomicBoolean(false);\r\n    server.livenessServer(live);\r\n    FileHandler homepage;\r\n    try {\r\n        homepage = new FileHandler(\"edu/stanford/nlp/pipeline/demo/corenlp-brat.html\");\r\n    } catch (IOException e) {\r\n        throw new RuntimeIOException(e);\r\n    }\r\n    if (StanfordCoreNLPServer.preloadedAnnotators != null && !StanfordCoreNLPServer.preloadedAnnotators.trim().isEmpty()) {\r\n        Properties props = new Properties();\r\n        server.defaultProps.forEach((key1, value) -> props.setProperty(key1.toString(), value.toString()));\r\n        props.setProperty(\"annotators\", StanfordCoreNLPServer.preloadedAnnotators);\r\n        try {\r\n            new StanfordCoreNLP(props);\r\n        } catch (Throwable ignored) {\r\n            err(\"Could not pre-load annotators in server; encountered exception:\");\r\n            ignored.printStackTrace();\r\n        }\r\n    }\r\n    Optional<Pair<String, String>> credentials = Optional.empty();\r\n    if (server.username != null && server.password != null) {\r\n        credentials = Optional.of(Pair.makePair(server.username, server.password));\r\n    }\r\n    log(\"Starting server...\");\r\n    if (server.ssl) {\r\n        server.run(credentials, req -> true, res -> {\r\n        }, homepage, true, live);\r\n    } else {\r\n        server.run(credentials, req -> true, res -> {\r\n        }, homepage, false, live);\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.optimize.solvers.accumulation.EncodingHandler.getAverageThresholdAlgorithm",
	"Comment": "this should only be called once all training threads have completed",
	"Method": "ThresholdAlgorithm getAverageThresholdAlgorithm(){\r\n    Collection<ThresholdAlgorithm> c = this.allThreadThresholdAlgorithms.values();\r\n    if (c.isEmpty()) {\r\n        return null;\r\n    }\r\n    if (c.size() == 1) {\r\n        return c.iterator().next();\r\n    }\r\n    Iterator<ThresholdAlgorithm> iter = c.iterator();\r\n    ThresholdAlgorithmReducer r = null;\r\n    while (iter.hasNext()) {\r\n        ThresholdAlgorithm ta = iter.next();\r\n        if (r == null) {\r\n            r = ta.newReducer();\r\n        }\r\n        r.add(ta);\r\n    }\r\n    ThresholdAlgorithm ta = r.getFinalResult();\r\n    thresholdAlgorithm = new ThreadLocal();\r\n    allThreadThresholdAlgorithms.clear();\r\n    return ta;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.StringUtils.pad",
	"Comment": "return a string of length a minimum of totalchars characters bypadding the input string str at the right end with spaces.if str is already longerthan totalchars, it is returned unchanged.",
	"Method": "String pad(String str,int totalChars,String pad,String str,int totalChars,char pad,String pad,Object obj,int totalChars){\r\n    return pad(obj.toString(), totalChars);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.samediff.SameDiffLayer.params",
	"Comment": "returns the parameters of the neural network as a flattened row vector",
	"Method": "INDArray params(){\r\n    return params;\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.condition.ConditionalReplaceValueTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    return columnNames();\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.IntervalTree.rightRotate",
	"Comment": "moves this node to the right and the left child up and returns the new root",
	"Method": "TreeNode<E, T> rightRotate(TreeNode<E, T> oldRoot){\r\n    if (oldRoot == null || oldRoot.isEmpty() || oldRoot.left == null)\r\n        return oldRoot;\r\n    TreeNode<E, T> oldLeftRight = oldRoot.left.right;\r\n    TreeNode<E, T> newRoot = oldRoot.left;\r\n    newRoot.right = oldRoot;\r\n    oldRoot.left = oldLeftRight;\r\n    newRoot.parent = oldRoot.parent;\r\n    newRoot.maxEnd = oldRoot.maxEnd;\r\n    newRoot.size = oldRoot.size;\r\n    if (newRoot.parent != null) {\r\n        if (newRoot.parent.left == oldRoot) {\r\n            newRoot.parent.left = newRoot;\r\n        } else if (newRoot.parent.right == oldRoot) {\r\n            newRoot.parent.right = newRoot;\r\n        } else {\r\n            throw new IllegalStateException(\"Old root not a child of it's parent\");\r\n        }\r\n    }\r\n    oldRoot.parent = newRoot;\r\n    if (oldLeftRight != null)\r\n        oldLeftRight.parent = oldRoot;\r\n    adjust(oldRoot);\r\n    return newRoot;\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.impl.multilayer.SparkDl4jMultiLayer.fitPaths",
	"Comment": "fit the network using a list of paths for serialized dataset objects.",
	"Method": "MultiLayerNetwork fitPaths(JavaRDD<String> paths,MultiLayerNetwork fitPaths,JavaRDD<String> paths,DataSetLoader loader){\r\n    trainingMaster.executeTrainingPaths(this, null, paths, loader, null);\r\n    network.incrementEpochCount();\r\n    return network;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.generateOutputVariableForOp",
	"Comment": "generate the variables based on the given input opand return the output variable names.",
	"Method": "SDVariable[] generateOutputVariableForOp(DifferentialFunction function,String baseName,SDVariable[] generateOutputVariableForOp,DifferentialFunction function){\r\n    return generateOutputVariableForOp(function, function.opName());\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseSparseNDArrayCOO.canInsert",
	"Comment": "return if there is enough allocated memory space to add data of a given length in the databuffer",
	"Method": "boolean canInsert(DataBuffer buffer,int length){\r\n    return buffer.capacity() - buffer.length() >= length;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.conf.layers.Layer.getUpdaterByParam",
	"Comment": "get the updater for the given parameter. typically the same updater will be used for all updaters, but thisis not necessarily the case",
	"Method": "IUpdater getUpdaterByParam(String paramName){\r\n    throw new UnsupportedOperationException(\"Not supported: all layers with parameters should override this method\");\r\n}"
}, {
	"Path": "org.deeplearning4j.text.movingwindow.Windows.windows",
	"Comment": "constructs a list of window of size windowsize.note that padding for each window is created as well.",
	"Method": "List<Window> windows(InputStream words,int windowSize,List<Window> windows,InputStream words,TokenizerFactory tokenizerFactory,int windowSize,List<Window> windows,String words,int windowSize,List<Window> windows,String words,TokenizerFactory tokenizerFactory,int windowSize,WordVectors vectors,List<Window> windows,String words,List<Window> windows,String words,TokenizerFactory tokenizerFactory,List<Window> windows,List<String> words,int windowSize){\r\n    List<Window> ret = new ArrayList();\r\n    for (int i = 0; i < words.size(); i++) ret.add(windowForWordInPosition(windowSize, i, words));\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.tuebadz.TueBaDZPennTreeNormalizer.cleanUpLabel",
	"Comment": "remove things like hyphened functional tags and equals from theend of a node label.",
	"Method": "String cleanUpLabel(String label){\r\n    if (label == null) {\r\n        return root;\r\n    } else if (nodeCleanup == 1) {\r\n        return tlp.categoryAndFunction(label);\r\n    } else if (nodeCleanup == 2) {\r\n        return tlp.basicCategory(label);\r\n    } else {\r\n        return label;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.text.documentiterator.FileDocumentIteratorTest.testDocumentReset",
	"Comment": "checks actual number of documents retrieved by documentiterator after being reset",
	"Method": "void testDocumentReset(){\r\n    ClassPathResource reuters5250 = new ClassPathResource(\"/reuters/5250\");\r\n    File f = reuters5250.getFile();\r\n    DocumentIterator iter = new FileDocumentIterator(f.getAbsolutePath());\r\n    int cnt = 0;\r\n    while (iter.hasNext()) {\r\n        InputStream stream = iter.nextDocument();\r\n        stream.close();\r\n        cnt++;\r\n    }\r\n    iter.reset();\r\n    while (iter.hasNext()) {\r\n        InputStream stream = iter.nextDocument();\r\n        stream.close();\r\n        cnt++;\r\n    }\r\n    assertEquals(48, cnt);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.parentPairs",
	"Comment": "returns a list of pairs of a relation name and the parentindexedfeaturelabel to which we bear that relation.",
	"Method": "List<Pair<GrammaticalRelation, IndexedWord>> parentPairs(IndexedWord vertex){\r\n    if (!containsVertex(vertex)) {\r\n        throw new IllegalArgumentException();\r\n    }\r\n    List<Pair<GrammaticalRelation, IndexedWord>> parentPairs = Generics.newArrayList();\r\n    for (SemanticGraphEdge e : incomingEdgeIterable(vertex)) {\r\n        parentPairs.add(new Pair(e.getRelation(), e.getSource()));\r\n    }\r\n    return parentPairs;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.semgrex.ParseException.add_escapes",
	"Comment": "used to convert raw characters to their escaped versionwhen these raw version cannot be used as part of an asciistring literal.",
	"Method": "String add_escapes(String str){\r\n    StringBuffer retval = new StringBuffer();\r\n    char ch;\r\n    for (int i = 0; i < str.length(); i++) {\r\n        switch(str.charAt(i)) {\r\n            case 0:\r\n                continue;\r\n            case '\\b':\r\n                retval.append(\"\\\\b\");\r\n                continue;\r\n            case '\\t':\r\n                retval.append(\"\\\\t\");\r\n                continue;\r\n            case '\\n':\r\n                retval.append(\"\\\\n\");\r\n                continue;\r\n            case '\\f':\r\n                retval.append(\"\\\\f\");\r\n                continue;\r\n            case '\\r':\r\n                retval.append(\"\\\\r\");\r\n                continue;\r\n            case '\\\"':\r\n                retval.append(\"\\\\\\\"\");\r\n                continue;\r\n            case '\\'':\r\n                retval.append(\"\\\\\\'\");\r\n                continue;\r\n            case '\\\\':\r\n                retval.append(\"\\\\\\\\\");\r\n                continue;\r\n            default:\r\n                if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {\r\n                    String s = \"0000\" + Integer.toString(ch, 16);\r\n                    retval.append(\"\\\%u\" + s.substring(s.length() - 4, s.length()));\r\n                } else {\r\n                    retval.append(ch);\r\n                }\r\n                continue;\r\n        }\r\n    }\r\n    return retval.toString();\r\n}"
}, {
	"Path": "nd4j.graph.GraphInferenceServerGrpc.newStub",
	"Comment": "creates a new async stub that supports all call types for the service",
	"Method": "GraphInferenceServerStub newStub(io.grpc.Channel channel){\r\n    return new GraphInferenceServerStub(channel);\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.argmin",
	"Comment": "finds and returns the key in this counter with the smallest count.",
	"Method": "E argmin(Counter<E> c,E argmin,Counter<E> c,Comparator<E> tieBreaker){\r\n    double min = Double.POSITIVE_INFINITY;\r\n    E argmin = null;\r\n    for (E key : c.keySet()) {\r\n        double count = c.getCount(key);\r\n        if (argmin == null || count < min || (count == min && tieBreaker.compare(key, argmin) < 0)) {\r\n            min = count;\r\n            argmin = key;\r\n        }\r\n    }\r\n    return argmin;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.sequencevectors.sequence.SequenceElement.equals",
	"Comment": "equals method override should be properly implemented for any extended class, otherwise it will be based on label equality",
	"Method": "boolean equals(Object object){\r\n    if (this == object)\r\n        return true;\r\n    if (object == null)\r\n        return false;\r\n    if (!(object instanceof SequenceElement))\r\n        return false;\r\n    return this.getLabel().equals(((SequenceElement) object).getLabel());\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.GrammaticalStructure.getNodeByIndex",
	"Comment": "return the node in the this treegraph corresponding to thespecified integer index.",
	"Method": "TreeGraphNode getNodeByIndex(int index){\r\n    return indexMap.get(Integer.valueOf(index));\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.LabeledScoredTreeNode.score",
	"Comment": "returns the score associated with the current node, or nanif there is no score",
	"Method": "double score(){\r\n    return score;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.LabeledScoredTreeNode.label",
	"Comment": "returns the label associated with the current node, or nullif there is no label",
	"Method": "Label label(){\r\n    return label;\r\n}"
}, {
	"Path": "org.datavec.audio.fingerprint.MapRankDouble.locate",
	"Comment": "sort the partitions by quick sort, and locate the target index",
	"Method": "void locate(double[] array,int left,int right,int index){\r\n    int mid = (left + right) / 2;\r\n    if (right == left) {\r\n        return;\r\n    }\r\n    if (left < right) {\r\n        double s = array[mid];\r\n        int i = left - 1;\r\n        int j = right + 1;\r\n        while (true) {\r\n            while (array[++i] < s) ;\r\n            while (array[--j] > s) ;\r\n            if (i >= j)\r\n                break;\r\n            swap(array, i, j);\r\n        }\r\n        if (i > index) {\r\n            locate(array, left, i - 1, index);\r\n        } else {\r\n            locate(array, j + 1, right, index);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.datasets.datavec.SequenceRecordReaderDataSetIterator.loadFromMetaData",
	"Comment": "load a multiple sequence examples to a dataset, using the provided recordmetadata instances.",
	"Method": "DataSet loadFromMetaData(RecordMetaData recordMetaData,DataSet loadFromMetaData,List<RecordMetaData> list){\r\n    if (underlying == null) {\r\n        SequenceRecord r = recordReader.loadSequenceFromMetaData(list.get(0));\r\n        initializeUnderlying(r);\r\n    }\r\n    List<RecordMetaData> l = new ArrayList(list.size());\r\n    if (singleSequenceReaderMode) {\r\n        for (RecordMetaData m : list) {\r\n            l.add(new RecordMetaDataComposableMap(Collections.singletonMap(READER_KEY, m)));\r\n        }\r\n    } else {\r\n        for (RecordMetaData m : list) {\r\n            RecordMetaDataComposable rmdc = (RecordMetaDataComposable) m;\r\n            Map<String, RecordMetaData> map = new HashMap(2);\r\n            map.put(READER_KEY, rmdc.getMeta()[0]);\r\n            map.put(READER_KEY_LABEL, rmdc.getMeta()[1]);\r\n            l.add(new RecordMetaDataComposableMap(map));\r\n        }\r\n    }\r\n    return mdsToDataSet(underlying.loadFromMetaData(l));\r\n}"
}, {
	"Path": "org.deeplearning4j.text.documentiterator.LabelsSource.getLabels",
	"Comment": "this method returns the list of labels used by this generator instance.if external list os labels was used as source, whole list will be returned.",
	"Method": "List<String> getLabels(){\r\n    if (labels != null && !labels.isEmpty())\r\n        return labels;\r\n    else {\r\n        List<String> result = new ArrayList();\r\n        for (long x = 0; x < counter.get(); x++) result.add(formatLabel(x));\r\n        return result;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ArgumentParser.scrapeFields",
	"Comment": "get all the declared fields of this class and all super classes.",
	"Method": "Field[] scrapeFields(Class<?> clazz){\r\n    List<Field> fields = new ArrayList();\r\n    while (clazz != null && !clazz.equals(Object.class)) {\r\n        fields.addAll(Arrays.asList(clazz.getDeclaredFields()));\r\n        clazz = clazz.getSuperclass();\r\n    }\r\n    return fields.toArray(new Field[fields.size()]);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.getPathToRoot",
	"Comment": "helper function for the public function with the same name.builds up the list backwards.",
	"Method": "List<IndexedWord> getPathToRoot(IndexedWord vertex,List<IndexedWord> used,List<IndexedWord> getPathToRoot,IndexedWord vertex){\r\n    List<IndexedWord> path = getPathToRoot(vertex, Generics.newArrayList());\r\n    if (path != null)\r\n        Collections.reverse(path);\r\n    return path;\r\n}"
}, {
	"Path": "org.datavec.api.writable.Text.find",
	"Comment": "finds any occurence of what in the backingbuffer, starting as position start. the startingposition is measured in bytes and the return value is interms of byte position in the buffer. the backing buffer isnot converted to a string for this operation.",
	"Method": "int find(String what,int find,String what,int start){\r\n    try {\r\n        ByteBuffer src = ByteBuffer.wrap(this.bytes, 0, this.length);\r\n        ByteBuffer tgt = encode(what);\r\n        byte b = tgt.get();\r\n        src.position(start);\r\n        while (src.hasRemaining()) {\r\n            if (b == src.get()) {\r\n                src.mark();\r\n                tgt.mark();\r\n                boolean found = true;\r\n                int pos = src.position() - 1;\r\n                while (tgt.hasRemaining()) {\r\n                    if (!src.hasRemaining()) {\r\n                        tgt.reset();\r\n                        src.reset();\r\n                        found = false;\r\n                        break;\r\n                    }\r\n                    if (!(tgt.get() == src.get())) {\r\n                        tgt.reset();\r\n                        src.reset();\r\n                        found = false;\r\n                        break;\r\n                    }\r\n                }\r\n                if (found)\r\n                    return pos;\r\n            }\r\n        }\r\n        return -1;\r\n    } catch (CharacterCodingException e) {\r\n        e.printStackTrace();\r\n        return -1;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Interval.getRelationFlags",
	"Comment": "return set of flags indicating possible relationships between this interval and another interval.",
	"Method": "int getRelationFlags(Interval<E> other){\r\n    if (other == null)\r\n        return 0;\r\n    int flags = 0;\r\n    int comp11 = this.first.compareTo(other.first());\r\n    flags |= toRelFlags(comp11, REL_FLAGS_SS_SHIFT);\r\n    int comp22 = this.second.compareTo(other.second());\r\n    flags |= toRelFlags(comp22, REL_FLAGS_EE_SHIFT);\r\n    int comp12 = this.first.compareTo(other.second());\r\n    flags |= toRelFlags(comp12, REL_FLAGS_SE_SHIFT);\r\n    int comp21 = this.second.compareTo(other.first());\r\n    flags |= toRelFlags(comp21, REL_FLAGS_ES_SHIFT);\r\n    flags = addIntervalRelationFlags(flags, false);\r\n    return flags;\r\n}"
}, {
	"Path": "dagger.testing.it.BuildLogValidator.assertHasText",
	"Comment": "processes a log file, ensuring it has all the provided strings within it.",
	"Method": "void assertHasText(File buildLogfile,String expectedStrings){\r\n    String buildOutput = getBuildOutput(buildLogfile);\r\n    StringBuilder sb = new StringBuilder(\"Build output did not contain expected error text:\");\r\n    boolean missing = false;\r\n    for (String expected : expectedStrings) {\r\n        if (!buildOutput.contains(expected)) {\r\n            missing = true;\r\n            sb.append(\"\\n    \\\"\").append(expected).append(\"\\\"\");\r\n        }\r\n    }\r\n    if (missing) {\r\n        appendBuildStatus(sb, buildOutput);\r\n        throw new Exception(sb.toString());\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.wordvectors.WordVectorsImpl.similarity",
	"Comment": "returns similarity of two elements, provided by modelutils",
	"Method": "double similarity(String word,String word2){\r\n    return modelUtils.similarity(word, word2);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.HighlightUtils.isInHighlight",
	"Comment": "returns true if the given mouse event occurred within a highlight h on label.",
	"Method": "boolean isInHighlight(MouseEvent e,JTextField label,Highlighter h){\r\n    Highlight[] hls = h.getHighlights();\r\n    if (hls == null || hls.length == 0)\r\n        return false;\r\n    Highlight hl = hls[0];\r\n    FontMetrics fm = label.getFontMetrics(label.getFont());\r\n    int offset = getCharOffset(fm, label.getText(), e.getX());\r\n    return hl.getStartOffset() <= offset && offset < hl.getEndOffset();\r\n}"
}, {
	"Path": "org.deeplearning4j.plot.BarnesHutTsne.computeGaussianKernel",
	"Comment": "computes a gaussian kernelgiven a vector of squared distance distances",
	"Method": "Pair<INDArray, Double> computeGaussianKernel(INDArray distances,double beta,int k){\r\n    INDArray currP = Nd4j.create(k);\r\n    for (int m = 0; m < k; m++) currP.putScalar(m, FastMath.exp(-beta * distances.getDouble(m + 1)));\r\n    double sum = currP.sum(Integer.MAX_VALUE).getDouble(0);\r\n    double h = 0.0;\r\n    for (int m = 0; m < k; m++) h += beta * (distances.getDouble(m + 1) * currP.getDouble(m));\r\n    h = (h / sum) + FastMath.log(sum);\r\n    return new Pair(currP, h);\r\n}"
}, {
	"Path": "edu.stanford.nlp.wordseg.MaxMatchSegmenter.buildSegmentationLattice",
	"Comment": "builds a lattice of all possible segmentations using only wordspresent in the lexicon. this function must be run prior torunning maxmatchsegmentation.",
	"Method": "void buildSegmentationLattice(String s){\r\n    edgesNb = 0;\r\n    len = s.length();\r\n    states = new ArrayList();\r\n    lattice = new DFSA(\"wordLattice\");\r\n    for (int i = 0; i <= s.length(); ++i) states.add(new DFSAState(i, lattice));\r\n    lattice.setInitialState(states.get(0));\r\n    states.get(len).setAccepting(true);\r\n    for (int start = 0; start < len; ++start) {\r\n        for (int end = len; end > start; --end) {\r\n            String str = s.substring(start, end);\r\n            assert (str.length() > 0);\r\n            boolean isOneChar = (start + 1 == end);\r\n            boolean isInDict = words.contains(str);\r\n            if (isInDict || isOneChar) {\r\n                double cost = isInDict ? 1 : 100;\r\n                DFSATransition<Word, Integer> trans = new DFSATransition(null, states.get(start), states.get(end), new Word(str), null, cost);\r\n                states.get(start).addTransition(trans);\r\n                ++edgesNb;\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.CoreEntityMention.canonicalEntityMention",
	"Comment": "return the canonical entity mention for this entity mention",
	"Method": "Optional<CoreEntityMention> canonicalEntityMention(){\r\n    CoreDocument myDocument = sentence.document();\r\n    Optional<Integer> canonicalEntityMentionIndex = Optional.ofNullable(coreMap().get(CoreAnnotations.CanonicalEntityMentionIndexAnnotation.class));\r\n    return canonicalEntityMentionIndex.isPresent() ? Optional.of(sentence.document().entityMentions().get(canonicalEntityMentionIndex.get())) : Optional.empty();\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLP.logTimingInfo",
	"Comment": "helper method for printing out timing info after an annotation run",
	"Method": "void logTimingInfo(StanfordCoreNLP pipeline,Timing tim){\r\n    logger.info(\"\");\r\n    logger.info(pipeline.timingInformation());\r\n    logger.info(\"Pipeline setup: \" + Timing.toSecondsString(pipeline.pipelineSetupTime) + \" sec.\");\r\n    logger.info(\"Total time for StanfordCoreNLP pipeline: \" + Timing.toSecondsString(pipeline.pipelineSetupTime + tim.report()) + \" sec.\");\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasLayerUtils.checkForUnsupportedConfigurations",
	"Comment": "checks whether layer config contains unsupported options.",
	"Method": "void checkForUnsupportedConfigurations(Map<String, Object> layerConfig,boolean enforceTrainingConfig,KerasLayerConfiguration conf){\r\n    getBiasL1RegularizationFromConfig(layerConfig, enforceTrainingConfig, conf);\r\n    getBiasL2RegularizationFromConfig(layerConfig, enforceTrainingConfig, conf);\r\n    Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\r\n    if (innerConfig.containsKey(conf.getLAYER_FIELD_W_REGULARIZER())) {\r\n        checkForUnknownRegularizer((Map<String, Object>) innerConfig.get(conf.getLAYER_FIELD_W_REGULARIZER()), enforceTrainingConfig, conf);\r\n    }\r\n    if (innerConfig.containsKey(conf.getLAYER_FIELD_B_REGULARIZER())) {\r\n        checkForUnknownRegularizer((Map<String, Object>) innerConfig.get(conf.getLAYER_FIELD_B_REGULARIZER()), enforceTrainingConfig, conf);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer.readUndelimited",
	"Comment": "read a single protocol buffer, which constitutes the entire stream.this is in contrast to the default, where mutliple buffers may come out of the stream,and therefore each one is prepended by the length of the buffer to follow.",
	"Method": "Annotation readUndelimited(File in){\r\n    CoreNLPProtos.Document doc;\r\n    try (FileInputStream delimited = new FileInputStream(in)) {\r\n        doc = CoreNLPProtos.Document.parseFrom(delimited);\r\n    } catch (Exception e) {\r\n        try (FileInputStream undelimited = new FileInputStream(in)) {\r\n            doc = CoreNLPProtos.Document.parseDelimitedFrom(undelimited);\r\n        }\r\n    }\r\n    return fromProto(doc);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.SimpleCharStream.adjustBeginLineColumn",
	"Comment": "method to adjust line and column numbers for the start of a token.",
	"Method": "void adjustBeginLineColumn(int newLine,int newCol){\r\n    int start = tokenBegin;\r\n    int len;\r\n    if (bufpos >= tokenBegin) {\r\n        len = bufpos - tokenBegin + inBuf + 1;\r\n    } else {\r\n        len = bufsize - tokenBegin + bufpos + 1 + inBuf;\r\n    }\r\n    int i = 0, j = 0, k = 0;\r\n    int nextColDiff = 0, columnDiff = 0;\r\n    while (i < len && bufline[j = start % bufsize] == bufline[k = ++start % bufsize]) {\r\n        bufline[j] = newLine;\r\n        nextColDiff = columnDiff + bufcolumn[k] - bufcolumn[j];\r\n        bufcolumn[j] = newCol + columnDiff;\r\n        columnDiff = nextColDiff;\r\n        i++;\r\n    }\r\n    if (i < len) {\r\n        bufline[j] = newLine++;\r\n        bufcolumn[j] = newCol + columnDiff;\r\n        while (i++ < len) {\r\n            if (bufline[j = start % bufsize] != bufline[++start % bufsize])\r\n                bufline[j] = newLine++;\r\n            else\r\n                bufline[j] = newLine;\r\n        }\r\n    }\r\n    line = bufline[j];\r\n    column = bufcolumn[j];\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.getChildrenWithPrepC",
	"Comment": "since graphs can be have preps collapsed, finds all the immediate children of this nodethat are linked by a collapsed preposition edge.",
	"Method": "List<IndexedWord> getChildrenWithPrepC(SemanticGraph sg,IndexedWord vertex){\r\n    List<IndexedWord> ret = new ArrayList();\r\n    for (SemanticGraphEdge edge : sg.outgoingEdgeIterable(vertex)) {\r\n        if (edge.getRelation().toString().startsWith(\"prep\"))\r\n            ret.add(edge.getDependent());\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.quoteattribution.QuoteAttributionUtils.mapBammanToCharacterMap",
	"Comment": "return map of index of characteroffsetbeginannotation to name of character.",
	"Method": "Map<Integer, String> mapBammanToCharacterMap(Map<Integer, List<CoreLabel>> BammanTokens,Map<String, List<Person>> characterMap){\r\n    Map<Integer, String> indexToCharacterName = new HashMap();\r\n    for (Integer characterID : BammanTokens.keySet()) {\r\n        List<CoreLabel> tokens = BammanTokens.get(characterID);\r\n        Counter<String> names = new ClassicCounter();\r\n        int prevEnd = -2;\r\n        String prevName = \"\";\r\n        for (CoreLabel token : tokens) {\r\n            if (token.tag().equals(\"NNP\")) {\r\n                int beginIndex = token.beginPosition();\r\n                if (prevEnd + 1 == beginIndex) {\r\n                    prevName += \" \" + token.word();\r\n                } else {\r\n                    if (!prevName.equals(\"\"))\r\n                        names.incrementCount(prevName, 1);\r\n                    prevName = token.word();\r\n                    prevEnd = token.endPosition();\r\n                }\r\n            } else {\r\n                if (!prevName.equals(\"\")) {\r\n                    names.incrementCount(prevName, 1);\r\n                }\r\n                prevName = \"\";\r\n                prevEnd = -2;\r\n            }\r\n        }\r\n        boolean flag = false;\r\n        for (String name : Counters.toSortedList(names)) {\r\n            if (characterMap.keySet().contains(name)) {\r\n                indexToCharacterName.put(characterID, name);\r\n                flag = true;\r\n                break;\r\n            }\r\n        }\r\n        if (!flag) {\r\n            for (String charName : characterMap.keySet()) {\r\n                for (String name : Counters.toSortedList(names)) {\r\n                    if (charName.contains(name)) {\r\n                        indexToCharacterName.put(characterID, charName);\r\n                        flag = true;\r\n                        System.out.println(\"contingency name found\" + characterID);\r\n                        for (String n : Counters.toSortedList(names)) System.out.print(n + \"|\");\r\n                        System.out.println();\r\n                        break;\r\n                    }\r\n                }\r\n                if (flag) {\r\n                    break;\r\n                }\r\n            }\r\n            System.out.println();\r\n        }\r\n        if (!flag) {\r\n            System.err.println(\"no name found :( \" + characterID);\r\n            for (String name : Counters.toSortedList(names)) System.err.print(name + \"| \");\r\n            System.err.println();\r\n        }\r\n    }\r\n    Map<Integer, String> beginIndexToName = new HashMap();\r\n    for (Integer charId : BammanTokens.keySet()) {\r\n        if (indexToCharacterName.get(charId) == null)\r\n            continue;\r\n        List<CoreLabel> tokens = BammanTokens.get(charId);\r\n        for (CoreLabel btoken : tokens) {\r\n            if (btoken.tag().equals(\"PRP\"))\r\n                beginIndexToName.put(btoken.beginPosition(), indexToCharacterName.get(charId));\r\n        }\r\n    }\r\n    return beginIndexToName;\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.flow.FlowPath.wasExecuted",
	"Comment": "this method returns true if specified node was already executed during current pass, false otherwise",
	"Method": "boolean wasExecuted(String nodeName){\r\n    ensureNodeStateExists(nodeName);\r\n    return states.get(nodeName).isExecuted();\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Counters.equals",
	"Comment": "equality comparison between two counters, allowing for a tolerance fudge factor.",
	"Method": "boolean equals(Counter<E> o1,Counter<E> o2,boolean equals,Counter<E> o1,Counter<E> o2,double tolerance){\r\n    if (o1 == o2) {\r\n        return true;\r\n    }\r\n    if (Math.abs(o1.totalCount() - o2.totalCount()) > tolerance) {\r\n        return false;\r\n    }\r\n    if (!o1.keySet().equals(o2.keySet())) {\r\n        return false;\r\n    }\r\n    for (E key : o1.keySet()) {\r\n        if (Math.abs(o1.getCount(key) - o2.getCount(key)) > tolerance) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.datavec.api.transform.transform.integer.IntegerToOneHotTransform.outputColumnNames",
	"Comment": "the output column namesthis will often be the same as the input",
	"Method": "String[] outputColumnNames(){\r\n    List<String> l = transform(inputSchema).getColumnNames();\r\n    return l.toArray(new String[l.size()]);\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Trilean.isFalse",
	"Comment": "returns true if this trilean is false, and false if it is true or unknown.",
	"Method": "boolean isFalse(){\r\n    return value == 0;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.graph.ComputationGraph.memoryInfo",
	"Comment": "generate information regarding memory use for the network, for the given input types and minibatch size.note that when using workspaces or cudnn, the network should be trained for some iterations so that the memoryworkspaces have time to initialize. without this, the memory requirements during training may be underestimated.note also that this is the same information that is generated during an oom crash when training or performinginference.",
	"Method": "String memoryInfo(int minibatch,InputType inputTypes){\r\n    return CrashReportingUtil.generateMemoryStatus(this, minibatch, inputTypes);\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.isCompressed",
	"Comment": "returns true if this array is compressed, and false otherwise",
	"Method": "boolean isCompressed(){\r\n    return compressed;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ndarray.BaseNDArray.assignIf",
	"Comment": "assign all elements from given ndarray that are matching given condition,ndarray to this ndarray",
	"Method": "INDArray assignIf(INDArray arr,Condition condition){\r\n    BooleanIndexing.assignIf(this, arr, condition);\r\n    return this;\r\n}"
}, {
	"Path": "org.deeplearning4j.zoo.ZooModel.initPretrained",
	"Comment": "returns a pretrained model for the given dataset, if available.",
	"Method": "Model initPretrained(M initPretrained,PretrainedType pretrainedType){\r\n    String remoteUrl = pretrainedUrl(pretrainedType);\r\n    if (remoteUrl == null)\r\n        throw new UnsupportedOperationException(\"Pretrained \" + pretrainedType + \" weights are not available for this model.\");\r\n    String localFilename = new File(remoteUrl).getName();\r\n    File rootCacheDir = DL4JResources.getDirectory(ResourceType.ZOO_MODEL, modelName());\r\n    File cachedFile = new File(rootCacheDir, localFilename);\r\n    if (!cachedFile.exists()) {\r\n        log.info(\"Downloading model to \" + cachedFile.toString());\r\n        FileUtils.copyURLToFile(new URL(remoteUrl), cachedFile);\r\n    } else {\r\n        log.info(\"Using cached model at \" + cachedFile.toString());\r\n    }\r\n    long expectedChecksum = pretrainedChecksum(pretrainedType);\r\n    if (expectedChecksum != 0L) {\r\n        log.info(\"Verifying download...\");\r\n        Checksum adler = new Adler32();\r\n        FileUtils.checksum(cachedFile, adler);\r\n        long localChecksum = adler.getValue();\r\n        log.info(\"Checksum local is \" + localChecksum + \", expecting \" + expectedChecksum);\r\n        if (expectedChecksum != localChecksum) {\r\n            log.error(\"Checksums do not match. Cleaning up files and failing...\");\r\n            cachedFile.delete();\r\n            throw new IllegalStateException(\"Pretrained model file failed checksum. If this error persists, please open an issue at https://github.com/deeplearning4j/deeplearning4j.\");\r\n        }\r\n    }\r\n    if (modelType() == MultiLayerNetwork.class) {\r\n        return (M) ModelSerializer.restoreMultiLayerNetwork(cachedFile);\r\n    } else if (modelType() == ComputationGraph.class) {\r\n        return (M) ModelSerializer.restoreComputationGraph(cachedFile);\r\n    } else {\r\n        throw new UnsupportedOperationException(\"Pretrained models are only supported for MultiLayerNetwork and ComputationGraph.\");\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.transform.TransformProcess.getActionList",
	"Comment": "get the action list that this transform processwill execute",
	"Method": "List<DataAction> getActionList(){\r\n    return actionList;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.incomingDependencyLabel",
	"Comment": "returns the incoming dependency label to a particular index, according to the basic dependencies.",
	"Method": "Optional<String> incomingDependencyLabel(Properties props,int index,SemanticGraphFactory.Mode mode,Optional<String> incomingDependencyLabel,Properties props,int index,Optional<String> incomingDependencyLabel,int index,SemanticGraphFactory.Mode mode,Optional<String> incomingDependencyLabel,int index){\r\n    return incomingDependencyLabel(this.defaultProps, index);\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.QuoteAttributionAnnotator.entityMentionsToCharacterMap",
	"Comment": "if no character list is provided, produce a list of person names from entity mentions annotation",
	"Method": "void entityMentionsToCharacterMap(Annotation annotation){\r\n    characterMap = new HashMap<String, List<Person>>();\r\n    for (CoreMap entityMention : annotation.get(CoreAnnotations.MentionsAnnotation.class)) {\r\n        String entityMentionString = entityMention.toString();\r\n        if (entityMention.get(CoreAnnotations.NamedEntityTagAnnotation.class).equals(\"PERSON\")) {\r\n            Person newPerson = new Person(entityMentionString, \"UNK\", new ArrayList());\r\n            List<Person> newPersonList = new ArrayList<Person>();\r\n            newPersonList.add(newPerson);\r\n            characterMap.put(entityMentionString, newPersonList);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.layers.feedforward.autoencoder.recursive.Tree.yield",
	"Comment": "returns the list of labels for this node andall of its children recursively",
	"Method": "List<String> yield(List<String> yield,List<String> labels){\r\n    labels.add(label);\r\n    for (Tree t : children()) {\r\n        labels.addAll(t.yield());\r\n    }\r\n    return labels;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.Americanize.americanize",
	"Comment": "convert the spelling of a word from british to american english.this is deterministic spelling conversion, and so cannot deal withcertain cases involving complex ambiguities, but it can do most of thesimple cases of english to american conversion.",
	"Method": "String americanize(String str,String americanize,String str,boolean capitalizeTimex){\r\n    int length = str.length();\r\n    if (length < MINIMUM_LENGTH_CHANGED) {\r\n        return str;\r\n    }\r\n    String result;\r\n    if (capitalizeTimex) {\r\n        result = timexMapping.get(str);\r\n        if (result != null) {\r\n            return result;\r\n        }\r\n    }\r\n    result = mapping.get(str);\r\n    if (result != null) {\r\n        return result;\r\n    }\r\n    if (length < MINIMUM_LENGTH_PATTERN_MATCH) {\r\n        return str;\r\n    }\r\n    if (!disjunctivePattern.matcher(str).find()) {\r\n        return str;\r\n    }\r\n    for (int i = 0; i < pats.length; i++) {\r\n        Matcher m = pats[i].matcher(str);\r\n        if (m.find()) {\r\n            Pattern ex = excepts[i];\r\n            if (ex != null) {\r\n                Matcher me = ex.matcher(str);\r\n                if (me.find()) {\r\n                    continue;\r\n                }\r\n            }\r\n            return m.replaceAll(reps[i]);\r\n        }\r\n    }\r\n    return str;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.logging.VisibilityHandler.alsoShow",
	"Comment": "show all the channels currently being printed, in additionto a new one",
	"Method": "boolean alsoShow(Object filter){\r\n    switch(this.defaultState) {\r\n        case HIDE_ALL:\r\n            return this.deltaPool.add(filter);\r\n        case SHOW_ALL:\r\n            return this.deltaPool.remove(filter);\r\n        default:\r\n            throw new IllegalStateException(\"Unknown default state setting: \" + this.defaultState);\r\n    }\r\n}"
}, {
	"Path": "org.datavec.api.transform.schema.Schema.sameTypes",
	"Comment": "returns true if the given schemahas the same types at each index",
	"Method": "boolean sameTypes(Schema schema){\r\n    if (schema.numColumns() != numColumns())\r\n        return false;\r\n    for (int i = 0; i < schema.numColumns(); i++) {\r\n        if (getType(i) != schema.getType(i))\r\n            return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.datavec.api.io.WritableUtils.readEnum",
	"Comment": "read an enum value from datainput, enums are read and writtenusing string values.",
	"Method": "T readEnum(DataInput in,Class<T> enumType){\r\n    return T.valueOf(enumType, Text.readString(in));\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.Distribution.getDistribution",
	"Comment": "creates a distribution from the given counter. it makes an internalcopy of the counter and divides all counts by the total count.",
	"Method": "Distribution<E> getDistribution(Counter<E> counter){\r\n    return getDistributionWithReservedMass(counter, 0.0);\r\n}"
}, {
	"Path": "org.deeplearning4j.text.tokenization.tokenizer.DefaultStreamTokenizer.nextTokenFromStream",
	"Comment": "this method returns next token from underlying inputstream",
	"Method": "String nextTokenFromStream(){\r\n    StringBuilder sb = new StringBuilder();\r\n    if (streamTokenizer.ttype == StreamTokenizer.TT_WORD) {\r\n        sb.append(streamTokenizer.sval);\r\n    } else if (streamTokenizer.ttype == StreamTokenizer.TT_NUMBER) {\r\n        sb.append(streamTokenizer.nval);\r\n    } else if (streamTokenizer.ttype == StreamTokenizer.TT_EOL) {\r\n        try {\r\n            while (streamTokenizer.ttype == StreamTokenizer.TT_EOL) streamTokenizer.nextToken();\r\n        } catch (IOException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    } else if (streamHasMoreTokens())\r\n        return nextTokenFromStream();\r\n    String ret = sb.toString();\r\n    if (tokenPreProcess != null)\r\n        ret = tokenPreProcess.preProcess(ret);\r\n    return ret;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.transferlearning.TransferLearningHelper.unfrozenMLN",
	"Comment": "returns the unfrozen layers of the multilayernetwork as a multilayernetworknote that with each call to featurizedfit the parameters to the original mln are also updated",
	"Method": "MultiLayerNetwork unfrozenMLN(){\r\n    if (isGraph)\r\n        errorIfGraphIfMLN();\r\n    return unFrozenSubsetMLN;\r\n}"
}, {
	"Path": "com.atilika.kuromoji.trie.PatriciaTrie.findFirstDifferingBit",
	"Comment": "returns the leftmost differing bit index when doing a bitwise comparison of key1 and key2",
	"Method": "int findFirstDifferingBit(String key1,String key2){\r\n    int bit = 0;\r\n    while (keyMapper.isSet(bit, key1) == keyMapper.isSet(bit, key2)) {\r\n        bit++;\r\n    }\r\n    return bit;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.tsurgeon.SimpleCharStream.adjustBeginLineColumn",
	"Comment": "method to adjust line and column numbers for the start of a token.",
	"Method": "void adjustBeginLineColumn(int newLine,int newCol){\r\n    int start = tokenBegin;\r\n    int len;\r\n    if (bufpos >= tokenBegin) {\r\n        len = bufpos - tokenBegin + inBuf + 1;\r\n    } else {\r\n        len = bufsize - tokenBegin + bufpos + 1 + inBuf;\r\n    }\r\n    int i = 0, j = 0, k = 0;\r\n    int nextColDiff = 0, columnDiff = 0;\r\n    while (i < len && bufline[j = start % bufsize] == bufline[k = ++start % bufsize]) {\r\n        bufline[j] = newLine;\r\n        nextColDiff = columnDiff + bufcolumn[k] - bufcolumn[j];\r\n        bufcolumn[j] = newCol + columnDiff;\r\n        columnDiff = nextColDiff;\r\n        i++;\r\n    }\r\n    if (i < len) {\r\n        bufline[j] = newLine++;\r\n        bufcolumn[j] = newCol + columnDiff;\r\n        while (i++ < len) {\r\n            if (bufline[j = start % bufsize] != bufline[++start % bufsize])\r\n                bufline[j] = newLine++;\r\n            else\r\n                bufline[j] = newLine;\r\n        }\r\n    }\r\n    line = bufline[j];\r\n    column = bufcolumn[j];\r\n}"
}, {
	"Path": "edu.stanford.nlp.tagger.util.CountClosedTags.countTrainingTags",
	"Comment": "count trainingratio of the sentences for both trainingwords andallwords, and count the rest for just allwords",
	"Method": "void countTrainingTags(TaggedFileRecord file){\r\n    int sentences = countSentences(file);\r\n    int trainSentences = (int) (sentences * trainingRatio);\r\n    TaggedFileReader reader = file.reader();\r\n    List<TaggedWord> line;\r\n    for (int i = 0; i < trainSentences && reader.hasNext(); ++i) {\r\n        line = reader.next();\r\n        addTaggedWords(line, trainingWords);\r\n        addTaggedWords(line, allWords);\r\n    }\r\n    while (reader.hasNext()) {\r\n        line = reader.next();\r\n        addTaggedWords(line, allWords);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.AnnotatorPool.register",
	"Comment": "register an annotator that can be created by the pool.note that factories are used here so that many possible annotators canbe defined within the annotatorpool, but an annotator is only createdwhen one is actually needed.",
	"Method": "boolean register(String name,Properties props,Lazy<Annotator> annotator){\r\n    boolean newAnnotator = false;\r\n    String newSig = PropertiesUtils.getSignature(name, props);\r\n    synchronized (this.cachedAnnotators) {\r\n        CachedAnnotator oldAnnotator = this.cachedAnnotators.get(name);\r\n        if (oldAnnotator == null || !Objects.equals(oldAnnotator.signature, newSig)) {\r\n            if (oldAnnotator != null) {\r\n                log.debug(\"Replacing old annotator \\\"\" + name + \"\\\" with signature [\" + oldAnnotator.signature + \"] with new annotator with signature [\" + newSig + \"]\");\r\n            }\r\n            this.cachedAnnotators.put(name, new CachedAnnotator(newSig, annotator));\r\n            Optional.ofNullable(oldAnnotator).flatMap(ann -> Optional.ofNullable(ann.annotator.getIfDefined())).ifPresent(Annotator::unmount);\r\n            newAnnotator = true;\r\n        }\r\n    }\r\n    return newAnnotator;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.wordvectors.WordVectorsImpl.accuracy",
	"Comment": "accuracy based on questions which are a space separated list of strings where the first word is the query word, the next 2 words are negative, and the last word is the predicted word to be nearest",
	"Method": "Map<String, Double> accuracy(List<String> questions){\r\n    return modelUtils.accuracy(questions);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.TregexGUI.doSaveFile",
	"Comment": "method for saving the trees that match the current tregex expression",
	"Method": "void doSaveFile(){\r\n    if (chooser == null)\r\n        chooser = createFileChooser();\r\n    int status = chooser.showSaveDialog(this);\r\n    if (status == JFileChooser.APPROVE_OPTION) {\r\n        Thread t = new Thread() {\r\n            @Override\r\n            public void run() {\r\n                try {\r\n                    BufferedWriter out = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(chooser.getSelectedFile()), FileTreeModel.getCurEncoding()));\r\n                    String str = MatchesPanel.getInstance().getMatches();\r\n                    out.write(str);\r\n                    out.flush();\r\n                    out.close();\r\n                } catch (Exception e) {\r\n                    log.info(\"Exception in save\");\r\n                    e.printStackTrace();\r\n                }\r\n            }\r\n        };\r\n        t.start();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.TregexGUI.doSaveFile",
	"Comment": "method for saving the trees that match the current tregex expression",
	"Method": "void doSaveFile(){\r\n    try {\r\n        BufferedWriter out = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(chooser.getSelectedFile()), FileTreeModel.getCurEncoding()));\r\n        String str = MatchesPanel.getInstance().getMatches();\r\n        out.write(str);\r\n        out.flush();\r\n        out.close();\r\n    } catch (Exception e) {\r\n        log.info(\"Exception in save\");\r\n        e.printStackTrace();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.PTBTokenizer.newPTBTokenizer",
	"Comment": "constructs a new ptbtokenizer that returns word tokens and which treatscarriage returns as normal whitespace.",
	"Method": "PTBTokenizer<Word> newPTBTokenizer(Reader r,PTBTokenizer<CoreLabel> newPTBTokenizer,Reader r,boolean tokenizeNLs,boolean invertible){\r\n    return new PTBTokenizer(r, tokenizeNLs, invertible, false, new CoreLabelTokenFactory());\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.tsurgeon.JJTTsurgeonParserState.nodeArity",
	"Comment": "returns the number of children on the stack in the current node scope.",
	"Method": "int nodeArity(){\r\n    return sp - mk;\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.modelimport.keras.utils.KerasLayerUtils.checkForUnknownRegularizer",
	"Comment": "check whether keras weight regularization is of unknown type. currently prints a warningsince main use case for model import is inference, not further training. unlikely sincestandard keras weight regularizers are l1 and l2.",
	"Method": "void checkForUnknownRegularizer(Map<String, Object> regularizerConfig,boolean enforceTrainingConfig,KerasLayerConfiguration conf){\r\n    if (regularizerConfig != null) {\r\n        for (String field : regularizerConfig.keySet()) {\r\n            if (!field.equals(conf.getREGULARIZATION_TYPE_L1()) && !field.equals(conf.getREGULARIZATION_TYPE_L2()) && !field.equals(conf.getLAYER_FIELD_NAME()) && !field.equals(conf.getLAYER_FIELD_CLASS_NAME()) && !field.equals(conf.getLAYER_FIELD_CONFIG())) {\r\n                if (enforceTrainingConfig)\r\n                    throw new UnsupportedKerasConfigurationException(\"Unknown regularization field \" + field);\r\n                else\r\n                    log.warn(\"Ignoring unknown regularization field \" + field);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.autodiff.samediff.SameDiff.removeArgFromFunction",
	"Comment": "remove an argument for a function. note that if this function does not contain the argument, it will just be a no op.",
	"Method": "void removeArgFromFunction(String varName,DifferentialFunction function){\r\n    val args = function.args();\r\n    for (int i = 0; i < args.length; i++) {\r\n        if (args[i].getVarName().equals(varName)) {\r\n            val reverseArgs = incomingArgsReverse.get(function.getOwnName());\r\n            incomingArgsReverse.remove(function.getOwnName());\r\n            val newArgs = new ArrayList<String>(args.length - 1);\r\n            for (int arg = 0; arg < args.length; arg++) {\r\n                if (!reverseArgs[arg].equals(varName)) {\r\n                    newArgs.add(reverseArgs[arg]);\r\n                }\r\n            }\r\n            val newArgsArr = newArgs.toArray(new String[newArgs.size()]);\r\n            incomingArgsReverse.put(function.getOwnName(), newArgsArr);\r\n            break;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.datavec.spark.transform.DataFrames.rowToWritables",
	"Comment": "convert a given row to a list of writables, given the specified schema",
	"Method": "List<Writable> rowToWritables(Schema schema,Row row){\r\n    List<Writable> ret = new ArrayList();\r\n    for (int i = 0; i < row.size(); i++) {\r\n        switch(schema.getType(i)) {\r\n            case Double:\r\n                ret.add(new DoubleWritable(row.getDouble(i)));\r\n                break;\r\n            case Float:\r\n                ret.add(new FloatWritable(row.getFloat(i)));\r\n                break;\r\n            case Integer:\r\n                ret.add(new IntWritable(row.getInt(i)));\r\n                break;\r\n            case Long:\r\n                ret.add(new LongWritable(row.getLong(i)));\r\n                break;\r\n            case String:\r\n                ret.add(new Text(row.getString(i)));\r\n                break;\r\n            default:\r\n                throw new IllegalStateException(\"Illegal type\");\r\n        }\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Lazy.isGarbageCollected",
	"Comment": "check if this lazy has been garbage collected, if it is a cached value.useful for, e.g., clearing keys in a map when the values are already gone.",
	"Method": "boolean isGarbageCollected(){\r\n    return this.isCache() && (this.implOrNullCache == null || this.implOrNullCache.get() == null);\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.SentenceAlgorithms.unescapeHTML",
	"Comment": "a funky little helper method to interpret each token of the sentence as an html string, and translate it back to text.note that this is in place.",
	"Method": "void unescapeHTML(){\r\n    for (int i = 0; i < sentence.length(); ++i) {\r\n        CoreNLPProtos.Token.Builder token = sentence.rawToken(i);\r\n        token.setWord(StringUtils.unescapeHtml3(token.getWord()));\r\n        token.setLemma(StringUtils.unescapeHtml3(token.getLemma()));\r\n    }\r\n    CoreMap cm = sentence.document.asAnnotation().get(CoreAnnotations.SentencesAnnotation.class).get(sentence.sentenceIndex());\r\n    for (CoreLabel token : cm.get(CoreAnnotations.TokensAnnotation.class)) {\r\n        token.setWord(StringUtils.unescapeHtml3(token.word()));\r\n        token.setLemma(StringUtils.unescapeHtml3(token.lemma()));\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.Characters.isPunctuation",
	"Comment": "returns true if a character is punctuation, and falseotherwise.",
	"Method": "boolean isPunctuation(char c){\r\n    int cType = Character.getType(c);\r\n    return cType == Character.START_PUNCTUATION || cType == Character.END_PUNCTUATION || cType == Character.OTHER_PUNCTUATION || cType == Character.CONNECTOR_PUNCTUATION || cType == Character.DASH_PUNCTUATION || cType == Character.INITIAL_QUOTE_PUNCTUATION || cType == Character.FINAL_QUOTE_PUNCTUATION;\r\n}"
}, {
	"Path": "org.datavec.api.transform.filter.ConditionFilter.transform",
	"Comment": "get the output schema for this transformation, given an input schema",
	"Method": "Schema transform(Schema inputSchema){\r\n    return inputSchema;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.BinaryHeapPriorityQueue.getFirst",
	"Comment": "finds the e with the highest priority and returns it, withoutmodifying the queue.",
	"Method": "E getFirst(){\r\n    if (isEmpty()) {\r\n        throw new NoSuchElementException();\r\n    }\r\n    return getEntry(0).key;\r\n}"
}, {
	"Path": "org.deeplearning4j.BaseDL4JTest.getDataType",
	"Comment": "override this to set the datatype of the tests defined in the child class",
	"Method": "DataBuffer.Type getDataType(){\r\n    return DataBuffer.Type.DOUBLE;\r\n}"
}, {
	"Path": "dagger.internal.Keys.startOfType",
	"Comment": "returns the start of a key if it is a plain key, and the start of theunderlying key if it is an annotated key",
	"Method": "int startOfType(String key){\r\n    return (key.startsWith(\"@\")) ? key.lastIndexOf('/') + 1 : 0;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.gui.FileTreeModel.addFileFolder",
	"Comment": "forks off a new thread to load your files based on the filters you set in the interface",
	"Method": "void addFileFolder(EnumMap<FilterType, String> filters,File[] files){\r\n    List<FileTreeNode> newFiles = new ArrayList();\r\n    findLoadableFiles(filters, files, newFiles, FileTreeModel.this.getRoot());\r\n    for (FileTreeNode fileNode : newFiles) {\r\n        Treebank treebank = new DiskTreebank(trf, curEncoding);\r\n        treebank.loadPath(fileNode.getFile(), null, true);\r\n        TreeTransformer transformer = TregexGUI.getInstance().transformer;\r\n        if (transformer != null) {\r\n            treebank = new TransformingTreebank(treebank, transformer);\r\n        }\r\n        fileNode.setTreebank(treebank);\r\n    }\r\n    FileTreeModel.this.fireTreeStructureChanged(new TreePath(getRoot()));\r\n}"
}, {
	"Path": "org.deeplearning4j.graph.models.embeddings.GraphVectorsImpl.similarity",
	"Comment": "returns the cosine similarity of the vector representations of two vertices in the graph,given the indices of these verticies",
	"Method": "double similarity(Vertex<V> vertex1,Vertex<V> vertex2,double similarity,int vertexIdx1,int vertexIdx2){\r\n    if (vertexIdx1 == vertexIdx2)\r\n        return 1.0;\r\n    INDArray vector = Transforms.unitVec(getVertexVector(vertexIdx1));\r\n    INDArray vector2 = Transforms.unitVec(getVertexVector(vertexIdx2));\r\n    return Nd4j.getBlasWrapper().dot(vector, vector2);\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraphUtils.replaceNode",
	"Comment": "replaces a node in the given semanticgraph with the new node,replacing its position in the node edges.",
	"Method": "void replaceNode(IndexedWord newNode,IndexedWord oldNode,SemanticGraph sg){\r\n    List<SemanticGraphEdge> govEdges = sg.outgoingEdgeList(oldNode);\r\n    List<SemanticGraphEdge> depEdges = sg.incomingEdgeList(oldNode);\r\n    boolean oldNodeRemoved = sg.removeVertex(oldNode);\r\n    if (oldNodeRemoved) {\r\n        if (!sg.containsVertex(newNode)) {\r\n            sg.addVertex(newNode);\r\n        }\r\n        for (SemanticGraphEdge govEdge : govEdges) {\r\n            sg.removeEdge(govEdge);\r\n            sg.addEdge(newNode, govEdge.getDependent(), govEdge.getRelation(), govEdge.getWeight(), govEdge.isExtra());\r\n        }\r\n        for (SemanticGraphEdge depEdge : depEdges) {\r\n            sg.removeEdge(depEdge);\r\n            sg.addEdge(depEdge.getGovernor(), newNode, depEdge.getRelation(), depEdge.getWeight(), depEdge.isExtra());\r\n        }\r\n    } else {\r\n        log.info(\"SemanticGraphUtils.replaceNode: previous node does not exist\");\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLPClient.process",
	"Comment": "runs the entire pipeline on the content of the given text passed in.",
	"Method": "Annotation process(String text){\r\n    Annotation annotation = new Annotation(text);\r\n    annotate(annotation);\r\n    return annotation;\r\n}"
}, {
	"Path": "edu.stanford.nlp.semgraph.SemanticGraph.getShortestDirectedPathNodes",
	"Comment": "returns the shortest directed path between two edges in the graph.",
	"Method": "List<IndexedWord> getShortestDirectedPathNodes(IndexedWord source,IndexedWord target){\r\n    return graph.getShortestPath(source, target, true);\r\n}"
}, {
	"Path": "org.nd4j.evaluation.EvaluationUtils.falseNegativeRate",
	"Comment": "calculate the false negative rate from the false negative counts and true positive count",
	"Method": "double falseNegativeRate(long fnCount,long tpCount,double edgeCase){\r\n    if (fnCount == 0 && tpCount == 0) {\r\n        return edgeCase;\r\n    }\r\n    return fnCount / (double) (fnCount + tpCount);\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.numParams",
	"Comment": "returns a 1 x m vector where the vector is composed ofa flattened vector of all of the weights for thevarious neuralnets and output layer",
	"Method": "long numParams(long numParams,boolean backwards){\r\n    int length = 0;\r\n    for (int i = 0; i < layers.length; i++) length += layers[i].numParams(backwards);\r\n    return length;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.PennTreebankLanguagePack.main",
	"Comment": "prints a few aspects of the treebanklanguagepack, just for debugging.",
	"Method": "void main(String[] args){\r\n    TreebankLanguagePack tlp = new PennTreebankLanguagePack();\r\n    System.out.println(\"Start symbol: \" + tlp.startSymbol());\r\n    String start = tlp.startSymbol();\r\n    System.out.println(\"Should be true: \" + (tlp.isStartSymbol(start)));\r\n    String[] strs = { \"-\", \"-LLB-\", \"NP-2\", \"NP=3\", \"NP-LGS\", \"NP-TMP=3\" };\r\n    for (String str : strs) {\r\n        System.out.println(\"String: \" + str + \" basic: \" + tlp.basicCategory(str) + \" basicAndFunc: \" + tlp.categoryAndFunction(str));\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.sequences.SequenceGibbsSampler.samplePosition",
	"Comment": "samples a single position in the sequence.destructively modifies the sequence in place.returns the score of the new sequence",
	"Method": "double samplePosition(SequenceModel model,int[] sequence,int pos,double samplePosition,SequenceModel model,int[] sequence,int pos,double temperature){\r\n    int oldTag = sequence[pos];\r\n    Pair<Integer, Double> newPosProb = samplePositionHelper(model, sequence, pos, temperature);\r\n    int newTag = newPosProb.first();\r\n    sequence[pos] = newTag;\r\n    listener.updateSequenceElement(sequence, pos, oldTag);\r\n    return newPosProb.second();\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.treebank.DistributionPackage.make",
	"Comment": "create the distribution and name the file according to the specified parameter.",
	"Method": "boolean make(String distribName){\r\n    boolean createdDir = (new File(distribName)).mkdir();\r\n    if (createdDir) {\r\n        String currentFile = \"\";\r\n        try {\r\n            for (String filename : distFiles) {\r\n                currentFile = filename;\r\n                File destFile = new File(filename);\r\n                String relativePath = distribName + \"/\" + destFile.getName();\r\n                destFile = new File(relativePath);\r\n                FileSystem.copyFile(new File(filename), destFile);\r\n            }\r\n            String tarFileName = String.format(\"%s.tar\", distribName);\r\n            Runtime r = Runtime.getRuntime();\r\n            Process p = r.exec(String.format(\"tar -cf %s %s/\", tarFileName, distribName));\r\n            if (p.waitFor() == 0) {\r\n                File tarFile = new File(tarFileName);\r\n                FileSystem.gzipFile(tarFile, new File(tarFileName + \".gz\"));\r\n                tarFile.delete();\r\n                FileSystem.deleteDir(new File(distribName));\r\n                lastCreatedDistribution = distribName;\r\n                return true;\r\n            } else {\r\n                System.err.printf(\"%s: Unable to create tar file %s\\n\", this.getClass().getName(), tarFileName);\r\n            }\r\n        } catch (IOException e) {\r\n            System.err.printf(\"%s: Unable to add file %s to distribution %s\\n\", this.getClass().getName(), currentFile, distribName);\r\n        } catch (InterruptedException e) {\r\n            System.err.printf(\"%s: tar did not return from building %s.tar\\n\", this.getClass().getName(), distribName);\r\n            throw new RuntimeInterruptedException(e);\r\n        }\r\n    } else {\r\n        System.err.printf(\"%s: Unable to create temp directory %s\\n\", this.getClass().getName(), distribName);\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.CollectionValuedMap.put",
	"Comment": "replaces current collection mapped to key with the specified collection.use carefully!",
	"Method": "Collection<V> put(K key,Collection<V> collection){\r\n    return map.put(key, collection);\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.CleanXmlAnnotator.setTokenBeginTokenEnd",
	"Comment": "helper method to set the tokenbeginannotation and tokenendannotation of every token.",
	"Method": "void setTokenBeginTokenEnd(List<CoreLabel> tokensList){\r\n    int tokenIndex = 0;\r\n    for (CoreLabel token : tokensList) {\r\n        token.set(CoreAnnotations.TokenBeginAnnotation.class, tokenIndex);\r\n        token.set(CoreAnnotations.TokenEndAnnotation.class, tokenIndex + 1);\r\n        tokenIndex++;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.text.documentiterator.BasicLabelAwareIterator.getLabelsSource",
	"Comment": "this method returns labelssource instance, containing all labels derived from this iterator",
	"Method": "LabelsSource getLabelsSource(){\r\n    return generator;\r\n}"
}, {
	"Path": "edu.stanford.nlp.stats.PrecisionRecallStats.getRecallDescription",
	"Comment": "returns a string summarizing recall that will print nicely.",
	"Method": "String getRecallDescription(int numDigits){\r\n    NumberFormat nf = NumberFormat.getNumberInstance();\r\n    nf.setMaximumFractionDigits(numDigits);\r\n    return nf.format(getRecall()) + \"  (\" + tpCount + \"/\" + (tpCount + fnCount) + \")\";\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.ops.impl.controlflow.While.incrementLoopCounter",
	"Comment": "increments the loop counter.this should be called when the loopactually executes.",
	"Method": "void incrementLoopCounter(){\r\n    numLooped++;\r\n}"
}, {
	"Path": "org.deeplearning4j.models.embeddings.reader.impl.BasicModelUtils.accuracy",
	"Comment": "accuracy based on questions which are a space separated list of stringswhere the first word is the query word, the next 2 words are negative,and the last word is the predicted word to be nearest",
	"Method": "Map<String, Double> accuracy(List<String> questions){\r\n    Map<String, Double> accuracy = new HashMap();\r\n    Counter<String> right = new Counter();\r\n    String analogyType = \"\";\r\n    for (String s : questions) {\r\n        if (s.startsWith(\":\")) {\r\n            double correct = right.getCount(CORRECT);\r\n            double wrong = right.getCount(WRONG);\r\n            if (analogyType.isEmpty()) {\r\n                analogyType = s;\r\n                continue;\r\n            }\r\n            double accuracyRet = 100.0 * correct / (correct + wrong);\r\n            accuracy.put(analogyType, accuracyRet);\r\n            analogyType = s;\r\n            right.clear();\r\n        } else {\r\n            String[] split = s.split(\" \");\r\n            List<String> positive = Arrays.asList(split[1], split[2]);\r\n            List<String> negative = Arrays.asList(split[0]);\r\n            String predicted = split[3];\r\n            String w = wordsNearest(positive, negative, 1).iterator().next();\r\n            if (predicted.equals(w))\r\n                right.incrementCount(CORRECT, 1.0f);\r\n            else\r\n                right.incrementCount(WRONG, 1.0f);\r\n        }\r\n    }\r\n    if (!analogyType.isEmpty()) {\r\n        double correct = right.getCount(CORRECT);\r\n        double wrong = right.getCount(WRONG);\r\n        double accuracyRet = 100.0 * correct / (correct + wrong);\r\n        accuracy.put(analogyType, accuracyRet);\r\n    }\r\n    return accuracy;\r\n}"
}, {
	"Path": "edu.stanford.nlp.process.WhitespaceLexer.yytext",
	"Comment": "returns the text matched by the current regular expression.",
	"Method": "String yytext(){\r\n    return new String(zzBuffer, zzStartRead, zzMarkedPos - zzStartRead);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.tregex.tsurgeon.TsurgeonParser.TreeRoot",
	"Comment": "the argument says whether there must be a foot node on the aux tree.",
	"Method": "AuxiliaryTree TreeRoot(boolean requiresFoot){\r\n    SimpleNode jjtn000 = new SimpleNode(JJTTREEROOT);\r\n    boolean jjtc000 = true;\r\n    jjtree.openNodeScope(jjtn000);\r\n    Tree t;\r\n    try {\r\n        t = TreeNode();\r\n        jjtree.closeNodeScope(jjtn000, true);\r\n        jjtc000 = false;\r\n        {\r\n            if (\"\" != null)\r\n                return new AuxiliaryTree(t, requiresFoot);\r\n        }\r\n    } catch (Throwable jjte000) {\r\n        if (jjtc000) {\r\n            jjtree.clearNodeScope(jjtn000);\r\n            jjtc000 = false;\r\n        } else {\r\n            jjtree.popNode();\r\n        }\r\n        if (jjte000 instanceof RuntimeException) {\r\n            {\r\n                if (true)\r\n                    throw (RuntimeException) jjte000;\r\n            }\r\n        }\r\n        if (jjte000 instanceof ParseException) {\r\n            {\r\n                if (true)\r\n                    throw (ParseException) jjte000;\r\n            }\r\n        }\r\n        {\r\n            if (true)\r\n                throw (Error) jjte000;\r\n        }\r\n    } finally {\r\n        if (jjtc000) {\r\n            jjtree.closeNodeScope(jjtn000, true);\r\n        }\r\n    }\r\n    throw new Error(\"Missing return statement in function\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.StanfordCoreNLPClient.shell",
	"Comment": "runs an interactive shell where input text is processed with the given pipeline.",
	"Method": "void shell(StanfordCoreNLPClient pipeline){\r\n    log.info(\"Entering interactive shell. Type q RETURN or EOF to quit.\");\r\n    final StanfordCoreNLP.OutputFormat outputFormat = StanfordCoreNLP.OutputFormat.valueOf(pipeline.properties.getProperty(\"outputFormat\", \"text\").toUpperCase());\r\n    IOUtils.console(\"NLP> \", line -> {\r\n        if (!line.isEmpty()) {\r\n            Annotation anno = pipeline.process(line);\r\n            try {\r\n                switch(outputFormat) {\r\n                    case XML:\r\n                        new XMLOutputter().print(anno, System.out);\r\n                        break;\r\n                    case JSON:\r\n                        new JSONOutputter().print(anno, System.out);\r\n                        System.out.println();\r\n                        break;\r\n                    case CONLL:\r\n                        new CoNLLOutputter().print(anno, System.out);\r\n                        System.out.println();\r\n                        break;\r\n                    case TEXT:\r\n                        new TextOutputter().print(anno, System.out);\r\n                        break;\r\n                    case SERIALIZED:\r\n                        warn(\"You probably cannot read the serialized output, so printing in text instead\");\r\n                        new TextOutputter().print(anno, System.out);\r\n                        break;\r\n                    default:\r\n                        throw new IllegalArgumentException(\"Cannot output in format \" + outputFormat + \" from the interactive shell\");\r\n                }\r\n            } catch (IOException e) {\r\n                throw new RuntimeIOException(e);\r\n            }\r\n        }\r\n    });\r\n}"
}, {
	"Path": "edu.stanford.nlp.pipeline.DefaultPaths.main",
	"Comment": "go through all of the paths via reflection, and print them out in a tsv format.this is useful for command line scripts.",
	"Method": "void main(String[] args){\r\n    for (Field field : DefaultPaths.class.getFields()) {\r\n        System.out.println(field.getName() + \"\\t\" + field.get(null));\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.concurrent.MulticoreWrapper.toString",
	"Comment": "return status information about the underlying threadpool.",
	"Method": "String toString(){\r\n    return String.format(\"active: %d/%d  submitted: %d  completed: %d  input_q: %d  output_q: %d  idle_q: %d\", threadPool.getActiveCount(), threadPool.getPoolSize(), threadPool.getTaskCount(), threadPool.getCompletedTaskCount(), threadPool.getQueue().size(), outputQueue.size(), idleProcessors.size());\r\n}"
}, {
	"Path": "edu.stanford.nlp.sentiment.SentimentUtils.readTreesWithPredictedLabels",
	"Comment": "given a file name, reads in those trees and returns them as list withlabels attached as predictions",
	"Method": "List<Tree> readTreesWithPredictedLabels(String path){\r\n    return readTreesWithLabels(path, RNNCoreAnnotations.PredictedClass.class);\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.negra.NegraPennLanguagePack.getEncoding",
	"Comment": "return the input charset encoding for the treebank.see documentation for the charset class.",
	"Method": "String getEncoding(){\r\n    return NEGRA_ENCODING;\r\n}"
}, {
	"Path": "edu.stanford.nlp.simple.Sentence.sentenceTokenOffsetEnd",
	"Comment": "the token offset of the end of this sentence within the document.",
	"Method": "int sentenceTokenOffsetEnd(){\r\n    synchronized (impl) {\r\n        return impl.getTokenOffsetEnd();\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.blas.impl.BaseLevel2.tpsv",
	"Comment": "tpsv solves a system of linear equations whose coefficients are in a triangular packed matrix.",
	"Method": "void tpsv(char order,char Uplo,char TransA,char Diag,INDArray Ap,INDArray X){\r\n    if (Nd4j.getExecutioner().getProfilingMode() == OpExecutioner.ProfilingMode.ALL)\r\n        OpProfiler.getInstance().processBlasCall(false, Ap, X);\r\n    if (X.data().dataType() == DataBuffer.Type.DOUBLE) {\r\n        DefaultOpExecutioner.validateDataType(DataBuffer.Type.DOUBLE, X, Ap);\r\n        dtpsv(order, Uplo, TransA, Diag, (int) X.length(), Ap, X, X.majorStride());\r\n    } else {\r\n        DefaultOpExecutioner.validateDataType(DataBuffer.Type.FLOAT, Ap, X);\r\n        stpsv(order, Uplo, TransA, Diag, (int) X.length(), Ap, X, X.majorStride());\r\n    }\r\n    OpExecutionerUtil.checkForAny(X);\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.Evaluation.merge",
	"Comment": "merge the other evaluation object into this one. the result is that this evaluation instance contains the countsetc from both",
	"Method": "void merge(Evaluation other){\r\n    if (other == null)\r\n        return;\r\n    truePositives.incrementAll(other.truePositives);\r\n    falsePositives.incrementAll(other.falsePositives);\r\n    trueNegatives.incrementAll(other.trueNegatives);\r\n    falseNegatives.incrementAll(other.falseNegatives);\r\n    if (confusion == null) {\r\n        if (other.confusion != null)\r\n            confusion = new ConfusionMatrix(other.confusion);\r\n    } else {\r\n        if (other.confusion != null)\r\n            confusion().add(other.confusion);\r\n    }\r\n    numRowCounter += other.numRowCounter;\r\n    if (labelsList.isEmpty())\r\n        labelsList.addAll(other.labelsList);\r\n    if (topN != other.topN) {\r\n        log.warn(\"Different topN values ({} vs {}) detected during Evaluation merging. Top N accuracy may not be accurate.\", topN, other.topN);\r\n    }\r\n    this.topNCorrectCount += other.topNCorrectCount;\r\n    this.topNTotalCount += other.topNTotalCount;\r\n}"
}, {
	"Path": "org.deeplearning4j.text.tokenization.tokenizerfactory.DefaultTokenizerFactory.getTokenPreProcessor",
	"Comment": "returns tokenpreprocessor set for this tokenizerfactory instance",
	"Method": "TokenPreProcess getTokenPreProcessor(){\r\n    return tokenPreProcess;\r\n}"
}, {
	"Path": "org.datavec.api.io.WritableUtils.writeStringArray",
	"Comment": "write a string array as a nework int n, followed by int n byte array strings.could be generalised using introspection.",
	"Method": "void writeStringArray(DataOutput out,String[] s){\r\n    out.writeInt(s.length);\r\n    for (int i = 0; i < s.length; i++) {\r\n        writeString(out, s[i]);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.ConfusionMatrix.uniqueLabels",
	"Comment": "returns the set of distinct class labelsentered into this confusion table.",
	"Method": "Set<U> uniqueLabels(){\r\n    HashSet<U> ret = new HashSet();\r\n    for (Pair<U, U> pair : confTable.keySet()) {\r\n        ret.add(pair.first());\r\n        ret.add(pair.second());\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "edu.stanford.nlp.trees.international.tuebadz.TueBaDZLanguagePack.getEncoding",
	"Comment": "return the input charset encoding for the treebank.see documentation for the charset class.",
	"Method": "String getEncoding(){\r\n    return \"iso-8859-15\";\r\n}"
}, {
	"Path": "edu.stanford.nlp.util.FileBackedCache.keySet",
	"Comment": "returns all the keys for this cache that are found on disk.this is an expensive operation.",
	"Method": "Set<KEY> keySet(){\r\n    readCache();\r\n    return mapping.keySet();\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.impl.paramavg.util.ExportSupport.assertExportSupported",
	"Comment": "verify that exporting data is supported, and throw an informative exception if not.",
	"Method": "void assertExportSupported(JavaSparkContext sc){\r\n    if (!exportSupported(sc)) {\r\n        throw new RuntimeException(\"Export training approach is not supported in the current environment. \" + \"This means that the default Hadoop file system is the local file system and Spark is running \" + \"in a non-local mode. You can fix this by either adding hadoop configuration to your environment \" + \"or using the Direct training approach. Configuring Hadoop can be done by adding config files (\" + \"https://spark.apache.org/docs/1.6.3/configuration.html#inheriting-hadoop-cluster-configuration\" + \") or adding a setting to your SparkConf object with \" + \"`sparkConf.set(\\\"spark.hadoop.fs.defaultFS\\\", \\\"hdfs://my-hdfs-host:9000\\\");`. Alternatively, \" + \"you can use some other non-local storage like S3.\");\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.spark.parameterserver.pw.SharedTrainingWrapper.attachMDS",
	"Comment": "this method registers given iterable in virtualmultidatasetiterator",
	"Method": "void attachMDS(Iterator<MultiDataSet> iterator){\r\n    log.debug(\"Attaching thread...\");\r\n    if (iteratorDataSetCount.get() == null)\r\n        iteratorDataSetCount.set(new AtomicInteger(0));\r\n    AtomicInteger count = iteratorDataSetCount.get();\r\n    count.set(0);\r\n    VirtualIterator<MultiDataSet> wrapped = new VirtualIterator(new CountingIterator(iterator, count));\r\n    BlockingObserver obs = new BlockingObserver(exceptionEncountered);\r\n    wrapped.addObserver(obs);\r\n    iteratorsMDS.add(wrapped);\r\n    observer.set(obs);\r\n}"
}, {
	"Path": "org.nd4j.evaluation.classification.ConfusionMatrix.getActualTotal",
	"Comment": "computes the total number of times the class actually appeared in the data.",
	"Method": "int getActualTotal(T actual){\r\n    if (!matrix.containsKey(actual)) {\r\n        return 0;\r\n    } else {\r\n        int total = 0;\r\n        for (T elem : matrix.get(actual).elementSet()) {\r\n            total += matrix.get(actual).count(elem);\r\n        }\r\n        return total;\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.nn.multilayer.MultiLayerNetwork.reconstruct",
	"Comment": "reconstructs the input.this is equivalent functionality to adeep autoencoder.",
	"Method": "INDArray reconstruct(INDArray x,int layerNum){\r\n    List<INDArray> forward = feedForward(x);\r\n    return forward.get(layerNum - 1);\r\n}"
}, {
	"Path": "org.deeplearning4j.clustering.util.MathUtils.xVals",
	"Comment": "this returns the x values of the given vector.these are assumed to be the even values of the vector.",
	"Method": "double[] xVals(double[] vector){\r\n    if (vector == null)\r\n        return null;\r\n    double[] x = new double[vector.length / 2];\r\n    int count = 0;\r\n    for (int i = 0; i < vector.length; i++) {\r\n        if (i % 2 != 0)\r\n            x[count++] = vector[i];\r\n    }\r\n    return x;\r\n}"
}]