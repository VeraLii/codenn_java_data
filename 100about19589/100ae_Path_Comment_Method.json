[{
	"Path": "org.elasticsearch.client.Response.getWarnings",
	"Comment": "returns a list of all warning headers returned in the response.",
	"Method": "List<String> getWarnings(){\r\n    List<String> warnings = new ArrayList();\r\n    for (Header header : response.getHeaders(\"Warning\")) {\r\n        String warning = header.getValue();\r\n        final Matcher matcher = WARNING_HEADER_PATTERN.matcher(warning);\r\n        if (matcher.matches()) {\r\n            warnings.add(matcher.group(1));\r\n        } else {\r\n            warnings.add(warning);\r\n        }\r\n    }\r\n    return warnings;\r\n}"
}, {
	"Path": "org.elasticsearch.script.mustache.SearchTemplateIT.testTemplateQueryAsEscapedStringWithConditionalClauseAtEnd",
	"Comment": "test that template can contain conditional clause. in this case it is atthe end of the string.",
	"Method": "void testTemplateQueryAsEscapedStringWithConditionalClauseAtEnd(){\r\n    SearchRequest searchRequest = new SearchRequest();\r\n    searchRequest.indices(\"_all\");\r\n    String templateString = \"{\" + \"  \\\"source\\\" : \\\"{ \\\\\\\"query\\\\\\\":{\\\\\\\"match_all\\\\\\\":{}} {{#use_size}}, \\\\\\\"size\\\\\\\": \\\\\\\"{{size}}\\\\\\\" {{/use_size}} }\\\",\" + \"  \\\"params\\\":{\" + \"    \\\"size\\\": 1,\" + \"    \\\"use_size\\\": true\" + \"  }\" + \"}\";\r\n    SearchTemplateRequest request = SearchTemplateRequest.fromXContent(createParser(JsonXContent.jsonXContent, templateString));\r\n    request.setRequest(searchRequest);\r\n    SearchTemplateResponse searchResponse = client().execute(SearchTemplateAction.INSTANCE, request).get();\r\n    assertThat(searchResponse.getResponse().getHits().getHits().length, equalTo(1));\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.process.DataCounts.getMissingFieldCount",
	"Comment": "the number of missing fields that had beenconfigured for analysis.",
	"Method": "long getMissingFieldCount(){\r\n    return missingFieldCount;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.generateUniform",
	"Comment": "this will generate a series of uniformally distributednumbers between l times",
	"Method": "double[] generateUniform(int l){\r\n    double[] ret = new double[l];\r\n    Random rgen = new Random();\r\n    for (int i = 0; i < l; i++) {\r\n        ret[i] = rgen.nextDouble();\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.deleteByQueryAsync",
	"Comment": "asynchronously executes a delete by query request.see delete by query api on elastic.co",
	"Method": "void deleteByQueryAsync(DeleteByQueryRequest deleteByQueryRequest,RequestOptions options,ActionListener<BulkByScrollResponse> listener){\r\n    performRequestAsyncAndParseEntity(deleteByQueryRequest, RequestConverters::deleteByQuery, options, BulkByScrollResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.index.rankeval.PrecisionAtK.evaluate",
	"Comment": "compute precisionatn based on provided relevant document ids.",
	"Method": "EvalQueryQuality evaluate(String taskId,SearchHit[] hits,List<RatedDocument> ratedDocs){\r\n    int truePositives = 0;\r\n    int falsePositives = 0;\r\n    List<RatedSearchHit> ratedSearchHits = joinHitsWithRatings(hits, ratedDocs);\r\n    for (RatedSearchHit hit : ratedSearchHits) {\r\n        OptionalInt rating = hit.getRating();\r\n        if (rating.isPresent()) {\r\n            if (rating.getAsInt() >= this.relevantRatingThreshhold) {\r\n                truePositives++;\r\n            } else {\r\n                falsePositives++;\r\n            }\r\n        } else if (ignoreUnlabeled == false) {\r\n            falsePositives++;\r\n        }\r\n    }\r\n    double precision = 0.0;\r\n    if (truePositives + falsePositives > 0) {\r\n        precision = (double) truePositives / (truePositives + falsePositives);\r\n    }\r\n    EvalQueryQuality evalQueryQuality = new EvalQueryQuality(taskId, precision);\r\n    evalQueryQuality.setMetricDetails(new PrecisionAtK.Detail(truePositives, truePositives + falsePositives));\r\n    evalQueryQuality.addHitsAndRatings(ratedSearchHits);\r\n    return evalQueryQuality;\r\n}"
}, {
	"Path": "org.elasticsearch.analysis.common.KeywordMarkerFilterFactoryTests.testKeywordPattern",
	"Comment": "tests using a regular expression pattern for the keyword marker filter.",
	"Method": "void testKeywordPattern(){\r\n    Settings settings = Settings.builder().put(\"index.analysis.filter.my_keyword.type\", \"keyword_marker\").put(\"index.analysis.filter.my_keyword.keywords_pattern\", \"run[a-z]ing\").put(\"index.analysis.analyzer.my_keyword.type\", \"custom\").put(\"index.analysis.analyzer.my_keyword.tokenizer\", \"standard\").put(\"index.analysis.analyzer.my_keyword.filter\", \"my_keyword, porter_stem\").put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build();\r\n    TestAnalysis analysis = AnalysisTestsHelper.createTestAnalysisFromSettings(settings, new CommonAnalysisPlugin());\r\n    TokenFilterFactory tokenFilter = analysis.tokenFilter.get(\"my_keyword\");\r\n    assertThat(tokenFilter, instanceOf(KeywordMarkerTokenFilterFactory.class));\r\n    TokenStream filter = tokenFilter.create(new WhitespaceTokenizer());\r\n    assertThat(filter, instanceOf(PatternKeywordMarkerFilter.class));\r\n    NamedAnalyzer analyzer = analysis.indexAnalyzers.get(\"my_keyword\");\r\n    assertAnalyzesTo(analyzer, \"running sleeping\", new String[] { \"running\", \"sleep\" });\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.mergeCoords",
	"Comment": "this will merge the coordinates of the given coordinate system.",
	"Method": "double[] mergeCoords(double[] x,double[] y,List<Double> mergeCoords,List<Double> x,List<Double> y){\r\n    if (x.size() != y.size())\r\n        throw new IllegalArgumentException(\"Sample sizes must be the same for each data applyTransformToDestination.\");\r\n    List<Double> ret = new ArrayList<Double>();\r\n    for (int i = 0; i < x.size(); i++) {\r\n        ret.add(x.get(i));\r\n        ret.add(y.get(i));\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.tools.PropertyParser.toBoolean",
	"Comment": "get property. the method returns the default value if the property is not parsed.",
	"Method": "boolean toBoolean(String name,boolean toBoolean,String name,boolean defaultValue){\r\n    try {\r\n        return parseBoolean(name);\r\n    } catch (Exception e) {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.graph.grpc.GraphInferenceServerGrpc.newStub",
	"Comment": "creates a new async stub that supports all call types for the service",
	"Method": "GraphInferenceServerStub newStub(io.grpc.Channel channel){\r\n    return new GraphInferenceServerStub(channel);\r\n}"
}, {
	"Path": "org.elasticsearch.client.CcrClient.putFollowAsync",
	"Comment": "asynchronously executes the put follow api, which creates a follower index and then the follower index startsfollowing the leader index.see the docs for more.",
	"Method": "void putFollowAsync(PutFollowRequest request,RequestOptions options,ActionListener<PutFollowResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, CcrRequestConverters::putFollow, options, PutFollowResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.internalPerformRequestAsync",
	"Comment": "provides common functionality for asynchronously performing a request.",
	"Method": "void internalPerformRequestAsync(Req request,CheckedFunction<Req, Request, IOException> requestConverter,RequestOptions options,CheckedFunction<Response, Resp, IOException> responseConverter,ActionListener<Resp> listener,Set<Integer> ignores){\r\n    Request req;\r\n    try {\r\n        req = requestConverter.apply(request);\r\n    } catch (Exception e) {\r\n        listener.onFailure(e);\r\n        return;\r\n    }\r\n    req.setOptions(options);\r\n    ResponseListener responseListener = wrapResponseListener(responseConverter, listener, ignores);\r\n    client.performRequestAsync(req, responseListener);\r\n}"
}, {
	"Path": "org.nd4j.linalg.inverse.InvertMatrix.pinvert",
	"Comment": "calculates pseudo inverse of a matrix using qr decomposition",
	"Method": "INDArray pinvert(INDArray arr,boolean inPlace){\r\n    RealMatrix realMatrix = CheckUtil.convertToApacheMatrix(arr);\r\n    QRDecomposition decomposition = new QRDecomposition(realMatrix, 0);\r\n    DecompositionSolver solver = decomposition.getSolver();\r\n    if (!solver.isNonSingular()) {\r\n        throw new IllegalArgumentException(\"invalid array: must be singular matrix\");\r\n    }\r\n    RealMatrix pinvRM = solver.getInverse();\r\n    INDArray pseudoInverse = CheckUtil.convertFromApacheMatrix(pinvRM);\r\n    if (inPlace)\r\n        arr.assign(pseudoInverse);\r\n    return pseudoInverse;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getDatafeedAsync",
	"Comment": "gets one or more machine learning datafeed configuration info, asynchronously.for additional infosee ml get datafeed documentation",
	"Method": "void getDatafeedAsync(GetDatafeedRequest request,RequestOptions options,ActionListener<GetDatafeedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getDatafeed, options, GetDatafeedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.DataSet.sortByLabel",
	"Comment": "organizes the dataset to minimize sampling errorwhile still allowing efficient batching.",
	"Method": "void sortByLabel(){\r\n    Map<Integer, Queue<DataSet>> map = new HashMap();\r\n    List<DataSet> data = asList();\r\n    int numLabels = numOutcomes();\r\n    int examples = numExamples();\r\n    for (DataSet d : data) {\r\n        int label = d.outcome();\r\n        Queue<DataSet> q = map.get(label);\r\n        if (q == null) {\r\n            q = new ArrayDeque();\r\n            map.put(label, q);\r\n        }\r\n        q.add(d);\r\n    }\r\n    for (Map.Entry<Integer, Queue<DataSet>> label : map.entrySet()) {\r\n        log.info(\"Label \" + label + \" has \" + label.getValue().size() + \" elements\");\r\n    }\r\n    boolean optimal = true;\r\n    for (int i = 0; i < examples; i++) {\r\n        if (optimal) {\r\n            for (int j = 0; j < numLabels; j++) {\r\n                Queue<DataSet> q = map.get(j);\r\n                if (q == null) {\r\n                    optimal = false;\r\n                    break;\r\n                }\r\n                DataSet next = q.poll();\r\n                if (next != null) {\r\n                    addRow(next, i);\r\n                    i++;\r\n                } else {\r\n                    optimal = false;\r\n                    break;\r\n                }\r\n            }\r\n        } else {\r\n            DataSet add = null;\r\n            for (Queue<DataSet> q : map.values()) {\r\n                if (!q.isEmpty()) {\r\n                    add = q.poll();\r\n                    break;\r\n                }\r\n            }\r\n            addRow(add, i);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.gradle.testclusters.TestClustersPlugin.getNodeExtension",
	"Comment": "boilerplate to get testclusters container extensionequivalent to project.testclusters in the dsl",
	"Method": "NamedDomainObjectContainer<ElasticsearchNode> getNodeExtension(Project project){\r\n    return (NamedDomainObjectContainer<ElasticsearchNode>) project.getExtensions().getByName(NODE_EXTENSION_NAME);\r\n}"
}, {
	"Path": "org.elasticsearch.painless.Def.lookupReferenceInternal",
	"Comment": "returns a method handle to an implementation of clazz, given method reference signature.",
	"Method": "MethodHandle lookupReferenceInternal(PainlessLookup painlessLookup,Map<String, LocalMethod> localMethods,MethodHandles.Lookup methodHandlesLookup,Class<?> clazz,String type,String call,int captures){\r\n    final FunctionRef ref = FunctionRef.create(painlessLookup, localMethods, null, clazz, type, call, captures);\r\n    final CallSite callSite = LambdaBootstrap.lambdaBootstrap(methodHandlesLookup, ref.interfaceMethodName, ref.factoryMethodType, ref.interfaceMethodType, ref.delegateClassName, ref.delegateInvokeType, ref.delegateMethodName, ref.delegateMethodType, ref.isDelegateInterface ? 1 : 0);\r\n    return callSite.dynamicInvoker().asType(MethodType.methodType(clazz, ref.factoryMethodType.parameterArray()));\r\n}"
}, {
	"Path": "org.elasticsearch.join.query.JoinQueryBuilders.parentId",
	"Comment": "constructs a new parent id query that returns all child documents of the specified type thatpoint to the specified id.",
	"Method": "ParentIdQueryBuilder parentId(String type,String id){\r\n    return new ParentIdQueryBuilder(type, id);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.get",
	"Comment": "retrieves a document by id using the get api.see get api on elastic.co",
	"Method": "GetResponse get(GetRequest getRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(getRequest, RequestConverters::get, options, GetResponse::fromXContent, singleton(404));\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.ping",
	"Comment": "pings the remote elasticsearch cluster and returns true if the ping succeeded, false otherwise",
	"Method": "boolean ping(RequestOptions options){\r\n    return performRequest(new MainRequest(), (request) -> RequestConverters.ping(), options, RestHighLevelClient::convertExistsResponse, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.authenticate",
	"Comment": "authenticate the current user and return all the information about the authenticated user.see the docs for more.",
	"Method": "AuthenticateResponse authenticate(RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(AuthenticateRequest.INSTANCE, AuthenticateRequest::getRequest, options, AuthenticateResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.attachment.TikaImpl.getRestrictedPermissions",
	"Comment": "the ability to load some resources from jars, and read sysprops",
	"Method": "PermissionCollection getRestrictedPermissions(){\r\n    Permissions perms = new Permissions();\r\n    perms.add(new PropertyPermission(\"*\", \"read\"));\r\n    perms.add(new RuntimePermission(\"getenv.TIKA_CONFIG\"));\r\n    try {\r\n        addReadPermissions(perms, JarHell.parseClassPath());\r\n        if (TikaImpl.class.getClassLoader() instanceof URLClassLoader) {\r\n            URL[] urls = ((URLClassLoader) TikaImpl.class.getClassLoader()).getURLs();\r\n            Set<URL> set = new LinkedHashSet(Arrays.asList(urls));\r\n            if (set.size() != urls.length) {\r\n                throw new AssertionError(\"duplicate jars: \" + Arrays.toString(urls));\r\n            }\r\n            addReadPermissions(perms, set);\r\n        }\r\n        FilePermissionUtils.addDirectoryPath(perms, \"java.io.tmpdir\", PathUtils.get(System.getProperty(\"java.io.tmpdir\")), \"read,readlink,write,delete\");\r\n    } catch (IOException e) {\r\n        throw new UncheckedIOException(e);\r\n    }\r\n    perms.add(new SecurityPermission(\"putProviderProperty.BC\"));\r\n    perms.add(new SecurityPermission(\"insertProvider\"));\r\n    perms.add(new ReflectPermission(\"suppressAccessChecks\"));\r\n    perms.add(new RuntimePermission(\"accessClassInPackage.sun.java2d.cmm.kcms\"));\r\n    perms.add(new RuntimePermission(\"getClassLoader\"));\r\n    if (JavaVersion.current().compareTo(JavaVersion.parse(\"10\")) >= 0) {\r\n        if (JavaVersion.current().compareTo(JavaVersion.parse(\"11\")) < 0) {\r\n            perms.add(new RuntimePermission(\"accessDeclaredMembers\"));\r\n        }\r\n    }\r\n    perms.setReadOnly();\r\n    return perms;\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.provider.BasicWorkspaceManager.printAllocationStatisticsForCurrentThread",
	"Comment": "this method prints out basic statistics for workspaces allocated in current thread",
	"Method": "void printAllocationStatisticsForCurrentThread(){\r\n    ensureThreadExistense();\r\n    Map<String, MemoryWorkspace> map = backingMap.get();\r\n    log.info(\"Workspace statistics: ---------------------------------\");\r\n    log.info(\"Number of workspaces in current thread: {}\", map.size());\r\n    log.info(\"Workspace name: Allocated / external (spilled) / external (pinned)\");\r\n    for (String key : map.keySet()) {\r\n        long current = ((Nd4jWorkspace) map.get(key)).getCurrentSize();\r\n        long spilled = ((Nd4jWorkspace) map.get(key)).getSpilledSize();\r\n        long pinned = ((Nd4jWorkspace) map.get(key)).getPinnedSize();\r\n        log.info(String.format(\"%-26s %8s / %8s / %8s (d / d / d)\", (key + \":\"), StringUtils.TraditionalBinaryPrefix.long2String(current, \"\", 2), StringUtils.TraditionalBinaryPrefix.long2String(spilled, \"\", 2), StringUtils.TraditionalBinaryPrefix.long2String(pinned, \"\", 2), current, spilled, pinned));\r\n    }\r\n}"
}, {
	"Path": "io.dropwizard.testing.junit.DAOTestRule.getSessionFactory",
	"Comment": "returns the current active session factory for injecting to daos.",
	"Method": "SessionFactory getSessionFactory(){\r\n    return daoTest.getSessionFactory();\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.updateDatafeed",
	"Comment": "updates a machine learning datafeedfor additional infosee ml update datafeed documentation",
	"Method": "PutDatafeedResponse updateDatafeed(UpdateDatafeedRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::updateDatafeed, options, PutDatafeedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.security.user.privileges.IndicesPrivileges.getDeniedFields",
	"Comment": "the document fields that cannot be accessed or queried. can be null or empty,in which case no fields are denied.",
	"Method": "Set<String> getDeniedFields(){\r\n    return this.deniedFields;\r\n}"
}, {
	"Path": "io.dropwizard.auth.principal.NoAuthPolymorphicPrincipalEntityResource.principalEntityWithoutAuth",
	"Comment": "principal instance must be injected even when no authentication is required.",
	"Method": "String principalEntityWithoutAuth(JsonPrincipal principal,String principalEntityWithoutAuth,NullPrincipal principal){\r\n    assertThat(principal).isNotNull();\r\n    return principal.getName();\r\n}"
}, {
	"Path": "org.elasticsearch.client.WatcherClient.deactivateWatchAsync",
	"Comment": "asynchronously deactivate an existing watchsee the docs for more.",
	"Method": "void deactivateWatchAsync(DeactivateWatchRequest request,RequestOptions options,ActionListener<DeactivateWatchResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, WatcherRequestConverters::deactivateWatch, options, DeactivateWatchResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.bulkRetryTestCase",
	"Comment": "execute a bulk retry test case. the total number of failures is random and the number of retries attempted is set totestrequest.getmaxretries and controlled by the failwithrejection parameter.",
	"Method": "void bulkRetryTestCase(boolean failWithRejection){\r\n    int totalFailures = randomIntBetween(1, testRequest.getMaxRetries());\r\n    int size = randomIntBetween(1, 100);\r\n    testRequest.setMaxRetries(totalFailures - (failWithRejection ? 1 : 0));\r\n    client.bulksToReject = client.bulksAttempts.get() + totalFailures;\r\n    CountDownLatch successLatch = new CountDownLatch(1);\r\n    DummyAsyncBulkByScrollAction action = new DummyActionWithoutBackoff() {\r\n        @Override\r\n        void startNextScroll(TimeValue lastBatchStartTime, TimeValue now, int lastBatchSize) {\r\n            successLatch.countDown();\r\n        }\r\n    };\r\n    BulkRequest request = new BulkRequest();\r\n    for (int i = 0; i < size + 1; i++) {\r\n        request.add(new IndexRequest(\"index\", \"type\", \"id\" + i));\r\n    }\r\n    action.sendBulkRequest(timeValueNanos(System.nanoTime()), request);\r\n    if (failWithRejection) {\r\n        BulkByScrollResponse response = listener.get();\r\n        assertThat(response.getBulkFailures(), hasSize(1));\r\n        assertEquals(response.getBulkFailures().get(0).getStatus(), RestStatus.TOO_MANY_REQUESTS);\r\n        assertThat(response.getSearchFailures(), empty());\r\n        assertNull(response.getReasonCancelled());\r\n    } else {\r\n        assertTrue(successLatch.await(10, TimeUnit.SECONDS));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.bulkRetryTestCase",
	"Comment": "execute a bulk retry test case. the total number of failures is random and the number of retries attempted is set totestrequest.getmaxretries and controlled by the failwithrejection parameter.",
	"Method": "void bulkRetryTestCase(boolean failWithRejection){\r\n    successLatch.countDown();\r\n}"
}, {
	"Path": "org.nd4j.tools.PropertyParser.toLong",
	"Comment": "get property. the method returns the default value if the property is not parsed.",
	"Method": "long toLong(String name,long toLong,String name,long defaultValue){\r\n    try {\r\n        return parseLong(name);\r\n    } catch (Exception e) {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.NioUtil.copyAtStride",
	"Comment": "copy from the given from bufferto the to buffer at the specifiedoffsets and strides",
	"Method": "void copyAtStride(int n,BufferType bufferType,ByteBuffer from,int fromOffset,int fromStride,ByteBuffer to,int toOffset,int toStride){\r\n    ByteBuffer fromView = from;\r\n    ByteBuffer toView = to;\r\n    fromView.order(ByteOrder.nativeOrder());\r\n    toView.order(ByteOrder.nativeOrder());\r\n    switch(bufferType) {\r\n        case INT:\r\n            IntBuffer fromInt = fromView.asIntBuffer();\r\n            IntBuffer toInt = toView.asIntBuffer();\r\n            for (int i = 0; i < n; i++) {\r\n                int put = fromInt.get(fromOffset + i * fromStride);\r\n                toInt.put(toOffset + i * toStride, put);\r\n            }\r\n            break;\r\n        case FLOAT:\r\n            FloatBuffer fromFloat = fromView.asFloatBuffer();\r\n            FloatBuffer toFloat = toView.asFloatBuffer();\r\n            for (int i = 0; i < n; i++) {\r\n                float put = fromFloat.get(fromOffset + i * fromStride);\r\n                toFloat.put(toOffset + i * toStride, put);\r\n            }\r\n            break;\r\n        case DOUBLE:\r\n            DoubleBuffer fromDouble = fromView.asDoubleBuffer();\r\n            DoubleBuffer toDouble = toView.asDoubleBuffer();\r\n            for (int i = 0; i < n; i++) {\r\n                toDouble.put(toOffset + i * toStride, fromDouble.get(fromOffset + i * fromStride));\r\n            }\r\n            break;\r\n        default:\r\n            throw new IllegalArgumentException(\"Only floats and double supported\");\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.tensorMmul",
	"Comment": "tensor matrix multiplication.both tensors must be the same rank",
	"Method": "INDArray tensorMmul(INDArray a,INDArray b,INDArray result,int[][] axes,INDArray tensorMmul,INDArray a,INDArray b,int[][] axes){\r\n    int validationLength = Math.min(axes[0].length, axes[1].length);\r\n    for (int i = 0; i < validationLength; i++) {\r\n        if (a.size(axes[0][i]) != b.size(axes[1][i]))\r\n            throw new IllegalArgumentException(\"Size of the given axes at each dimension must be the same size.\");\r\n        if (axes[0][i] < 0)\r\n            axes[0][i] += a.rank();\r\n        if (axes[1][i] < 0)\r\n            axes[1][i] += b.rank();\r\n    }\r\n    List<Integer> listA = new ArrayList();\r\n    for (int i = 0; i < a.rank(); i++) {\r\n        if (!Ints.contains(axes[0], i))\r\n            listA.add(i);\r\n    }\r\n    int[] newAxesA = Ints.concat(Ints.toArray(listA), axes[0]);\r\n    List<Integer> listB = new ArrayList();\r\n    for (int i = 0; i < b.rank(); i++) {\r\n        if (!Ints.contains(axes[1], i))\r\n            listB.add(i);\r\n    }\r\n    int[] newAxesB = Ints.concat(axes[1], Ints.toArray(listB));\r\n    int n2 = 1;\r\n    int aLength = Math.min(a.rank(), axes[0].length);\r\n    for (int i = 0; i < aLength; i++) {\r\n        n2 *= a.size(axes[0][i]);\r\n    }\r\n    long[] newShapeA = { -1, n2 };\r\n    long[] oldShapeA;\r\n    if (listA.size() == 0) {\r\n        oldShapeA = new long[] { 1 };\r\n    } else {\r\n        oldShapeA = Longs.toArray(listA);\r\n        for (int i = 0; i < oldShapeA.length; i++) oldShapeA[i] = a.size((int) oldShapeA[i]);\r\n    }\r\n    int n3 = 1;\r\n    int bNax = Math.min(b.rank(), axes[1].length);\r\n    for (int i = 0; i < bNax; i++) {\r\n        n3 *= b.size(axes[1][i]);\r\n    }\r\n    long[] newShapeB = { n3, -1 };\r\n    long[] oldShapeB;\r\n    if (listB.size() == 0) {\r\n        oldShapeB = new long[] { 1 };\r\n    } else {\r\n        oldShapeB = Longs.toArray(listB);\r\n        for (int i = 0; i < oldShapeB.length; i++) oldShapeB[i] = b.size((int) oldShapeB[i]);\r\n    }\r\n    INDArray at = a.permute(newAxesA).reshape(newShapeA);\r\n    INDArray bt = b.permute(newAxesB).reshape(newShapeB);\r\n    INDArray ret = at.mmul(bt);\r\n    long[] aPlusB = Longs.concat(oldShapeA, oldShapeB);\r\n    return ret.reshape(aPlusB);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.deleteLifecyclePolicyAsync",
	"Comment": "asynchronously delete a lifecycle definitionsee the docs for more.",
	"Method": "void deleteLifecyclePolicyAsync(DeleteLifecyclePolicyRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IndexLifecycleRequestConverters::deleteLifecyclePolicy, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.msearchAsync",
	"Comment": "asynchronously executes a multi search using the msearch api.see multi search api onelastic.co",
	"Method": "void msearchAsync(MultiSearchRequest searchRequest,RequestOptions options,ActionListener<MultiSearchResponse> listener){\r\n    performRequestAsyncAndParseEntity(searchRequest, RequestConverters::multiSearch, options, MultiSearchResponse::fromXContext, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.dimensionalityreduction.PCA.convertToComponents",
	"Comment": "takes a set of data on each row, with the same number of features as the constructing dataand returns the data in the coordinates of the basis set about the mean.",
	"Method": "INDArray convertToComponents(INDArray data){\r\n    INDArray dx = data.subRowVector(mean);\r\n    return Nd4j.tensorMmul(eigenvectors.transpose(), dx, new int[][] { { 1 }, { 1 } }).transposei();\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.restoreAsync",
	"Comment": "asynchronously restores a snapshot.seesnapshot and restoreapi on elastic.co",
	"Method": "void restoreAsync(RestoreSnapshotRequest restoreSnapshotRequest,RequestOptions options,ActionListener<RestoreSnapshotResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(restoreSnapshotRequest, SnapshotRequestConverters::restoreSnapshot, options, RestoreSnapshotResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.DefMath.lookupGeneric",
	"Comment": "returns a generic method handle for any operator, that can handle all valid signatures, nulls, corner cases",
	"Method": "MethodHandle lookupGeneric(String name){\r\n    return TYPE_OP_MAPPING.get(Object.class).get(name);\r\n}"
}, {
	"Path": "org.elasticsearch.client.CcrClient.pauseFollowAsync",
	"Comment": "asynchronously instruct a follower index to pause the following of a leader index.see the docs for more.",
	"Method": "void pauseFollowAsync(PauseFollowRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, CcrRequestConverters::pauseFollow, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.closeAsync",
	"Comment": "asynchronously closes an index using the close index api.see close index api on elastic.co",
	"Method": "void closeAsync(CloseIndexRequest closeIndexRequest,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(closeIndexRequest, IndicesRequestConverters::closeIndex, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.mgetAsync",
	"Comment": "asynchronously retrieves multiple documents by id using the multi get api.see multi get api on elastic.co",
	"Method": "void mgetAsync(MultiGetRequest multiGetRequest,RequestOptions options,ActionListener<MultiGetResponse> listener){\r\n    performRequestAsyncAndParseEntity(multiGetRequest, RequestConverters::multiGet, options, MultiGetResponse::fromXContent, listener, singleton(404));\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.simulateScrollResponse",
	"Comment": "simulate a scroll response by setting the scroll id and firing the onscrollresponse method.",
	"Method": "void simulateScrollResponse(DummyAsyncBulkByScrollAction action,TimeValue lastBatchTime,int lastBatchSize,ScrollableHitSource.Response response){\r\n    action.setScroll(scrollId());\r\n    action.onScrollResponse(lastBatchTime, lastBatchSize, response);\r\n}"
}, {
	"Path": "org.nd4j.linalg.collection.MultiDimensionalSet.containsAll",
	"Comment": "returns true if this applytransformtodestination contains all of the elements of thespecified collection.if the specified collection is also a applytransformtodestination, thismethod returns true if it is a subset of this applytransformtodestination.",
	"Method": "boolean containsAll(Collection<?> c){\r\n    return backedSet.containsAll(c);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.security",
	"Comment": "provides methods for accessing the elastic licensed security apis thatare shipped with the elastic stack distribution of elasticsearch. all ofthese apis will 404 if run against the oss distribution of elasticsearch.see the security apis on elastic.co for more information.",
	"Method": "SecurityClient security(){\r\n    return securityClient;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.index",
	"Comment": "index a document using the index api.see index api on elastic.co",
	"Method": "IndexResponse index(IndexRequest indexRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(indexRequest, RequestConverters::index, options, IndexResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.join.mapper.ParentJoinFieldMapper.getParentIdFieldMapper",
	"Comment": "returns the parent id field mapper associated with a parent nameif isparent is true and a child name otherwise.",
	"Method": "ParentIdFieldMapper getParentIdFieldMapper(String name,boolean isParent){\r\n    for (ParentIdFieldMapper mapper : parentIdFields) {\r\n        if (isParent && name.equals(mapper.getParentName())) {\r\n            return mapper;\r\n        } else if (isParent == false && mapper.getChildren().contains(name)) {\r\n            return mapper;\r\n        }\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.join.query.HasChildQueryBuilder.innerHit",
	"Comment": "returns inner hit definition in the scope of this query and reusing the defined type and query.",
	"Method": "InnerHitBuilder innerHit(HasChildQueryBuilder innerHit,InnerHitBuilder innerHit){\r\n    this.innerHitBuilder = innerHit;\r\n    innerHitBuilder.setIgnoreUnmapped(ignoreUnmapped);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.plugin.noop.action.search.NoopSearchRequestBuilder.addSort",
	"Comment": "adds a sort against the given field name and the sort ordering.",
	"Method": "NoopSearchRequestBuilder addSort(String field,SortOrder order,NoopSearchRequestBuilder addSort,SortBuilder<?> sort){\r\n    sourceBuilder().sort(sort);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.util.ArchiveUtils.tarGzListFiles",
	"Comment": "list all of the files and directories in the specified tar.gz file",
	"Method": "List<String> tarGzListFiles(File tarGzFile){\r\n    try (TarArchiveInputStream tin = new TarArchiveInputStream(new GZIPInputStream(new BufferedInputStream(new FileInputStream(tarGzFile))))) {\r\n        ArchiveEntry entry;\r\n        List<String> out = new ArrayList();\r\n        while ((entry = tin.getNextTarEntry()) != null) {\r\n            String name = entry.getName();\r\n            out.add(name);\r\n        }\r\n        return out;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.bulk",
	"Comment": "executes a bulk request using the bulk api.see bulk api on elastic.co",
	"Method": "BulkResponse bulk(BulkRequest bulkRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(bulkRequest, RequestConverters::bulk, options, BulkResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.FlushJobRequest.setAdvanceTime",
	"Comment": "specifies to advance to a particular time value.results are generated and the model is updated for data from the specified time interval.",
	"Method": "void setAdvanceTime(String advanceTime){\r\n    this.advanceTime = advanceTime;\r\n}"
}, {
	"Path": "org.elasticsearch.script.expression.ExpressionScriptEngine.bindFromParams",
	"Comment": "but if we were to reverse it, we could provide a way to supply dynamic defaults for documents missing the field?",
	"Method": "void bindFromParams(Map<String, Object> params,SimpleBindings bindings,String variable){\r\n    Object value = params.get(variable);\r\n    if (value instanceof Number) {\r\n        bindings.add(variable, new DoubleConstValueSource(((Number) value).doubleValue()).asDoubleValuesSource());\r\n    } else {\r\n        throw new ParseException(\"Parameter [\" + variable + \"] must be a numeric type\", 0);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.Response.hasWarnings",
	"Comment": "returns true if there is at least one warning header returned in theresponse.",
	"Method": "boolean hasWarnings(){\r\n    Header[] warnings = response.getHeaders(\"Warning\");\r\n    return warnings != null && warnings.length > 0;\r\n}"
}, {
	"Path": "org.nd4j.serde.binary.BinarySerde.readShapeFromDisk",
	"Comment": "this method returns shape databuffer from saved earlier file",
	"Method": "DataBuffer readShapeFromDisk(File readFrom){\r\n    try (FileInputStream os = new FileInputStream(readFrom)) {\r\n        FileChannel channel = os.getChannel();\r\n        int len = (int) Math.min((32 * 2 + 3) * 8, readFrom.length());\r\n        ByteBuffer buffer = ByteBuffer.allocateDirect(len);\r\n        channel.read(buffer);\r\n        ByteBuffer byteBuffer = buffer == null ? ByteBuffer.allocateDirect(buffer.array().length).put(buffer.array()).order(ByteOrder.nativeOrder()) : buffer.order(ByteOrder.nativeOrder());\r\n        buffer.position(0);\r\n        int rank = byteBuffer.getInt();\r\n        val result = new long[Shape.shapeInfoLength(rank)];\r\n        result[0] = rank;\r\n        byteBuffer.position(16);\r\n        for (int e = 1; e < Shape.shapeInfoLength(rank); e++) {\r\n            result[e] = byteBuffer.getLong();\r\n        }\r\n        DataBuffer dataBuffer = Nd4j.getDataBufferFactory().createLong(result);\r\n        return dataBuffer;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryRequestBuilder.setVerify",
	"Comment": "sets whether or not repository should be verified after creation",
	"Method": "PutRepositoryRequestBuilder setVerify(boolean verify){\r\n    request.verify(verify);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.stats.JobStats.getOpenTime",
	"Comment": "for open jobs only, the elapsed time for which the job has been open",
	"Method": "TimeValue getOpenTime(){\r\n    return openTime;\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.status",
	"Comment": "gets the status of requested snapshots.seesnapshot and restoreapi on elastic.co",
	"Method": "SnapshotsStatusResponse status(SnapshotsStatusRequest snapshotsStatusRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(snapshotsStatusRequest, SnapshotRequestConverters::snapshotsStatus, options, SnapshotsStatusResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.aeron.ipc.NDArrayMessage.chunkedMessages",
	"Comment": "create an array of messages to sendbased on a specified chunk size",
	"Method": "NDArrayMessage[] chunkedMessages(NDArrayMessage arrayMessage,int chunkSize){\r\n    int sizeOfMessage = NDArrayMessage.byteBufferSizeForMessage(arrayMessage) - 4;\r\n    int numMessages = sizeOfMessage / chunkSize;\r\n    ByteBuffer direct = NDArrayMessage.toBuffer(arrayMessage).byteBuffer();\r\n    NDArrayMessage[] ret = new NDArrayMessage[numMessages];\r\n    for (int i = 0; i < numMessages; i++) {\r\n        byte[] chunk = new byte[chunkSize];\r\n        direct.get(chunk, i * chunkSize, chunkSize);\r\n        ret[i] = NDArrayMessage.builder().chunk(chunk).numChunks(numMessages).build();\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.containsAnyNegative",
	"Comment": "returns true if any array elements are negative.if the array is null, it returns false",
	"Method": "boolean containsAnyNegative(int[] arr,boolean containsAnyNegative,long[] arr){\r\n    if (arr == null)\r\n        return false;\r\n    for (int i = 0; i < arr.length; i++) {\r\n        if (arr[i] < 0)\r\n            return true;\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.shrink",
	"Comment": "shrinks an index using the shrink index api.see shrink index api on elastic.co",
	"Method": "ResizeResponse shrink(ResizeRequest resizeRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(resizeRequest, IndicesRequestConverters::shrink, options, ResizeResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.core.CountRequest.routing",
	"Comment": "a comma separated list of routing values to control the shards the count will be executed on.",
	"Method": "CountRequest routing(String routing,CountRequest routing,String routings,String routing){\r\n    return this.routing;\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.ShapeOffsetResolution.exec",
	"Comment": "based on the passed in arraycompute the shape,offsets, and stridesfor the given indexes",
	"Method": "void exec(INDArrayIndex indexes){\r\n    val shape = arr.shape();\r\n    if (arr.isSparse()) {\r\n        resolveFixedDimensionsCOO(indexes);\r\n    }\r\n    for (int i = 0; i < indexes.length; i++) {\r\n        INDArrayIndex idx = indexes[i];\r\n        if (idx instanceof PointIndex && (arr.isVector() && indexes.length == 1 ? idx.current() >= shape[i + 1] : idx.current() >= shape[i])) {\r\n            throw new IllegalArgumentException(\"INDArrayIndex[\" + i + \"] is out of bounds (value: \" + idx.current() + \")\");\r\n        }\r\n    }\r\n    indexes = NDArrayIndex.resolve(arr.shapeInfoDataBuffer(), indexes);\r\n    if (tryShortCircuit(indexes)) {\r\n        return;\r\n    }\r\n    int numIntervals = 0;\r\n    int newAxesPrepend = 0;\r\n    boolean encounteredAll = false;\r\n    int lastPrependIndex = -1;\r\n    List<Integer> oneDimensionWithAllEncountered = new ArrayList();\r\n    List<Long> accumShape = new ArrayList();\r\n    List<Long> accumStrides = new ArrayList();\r\n    List<Long> accumOffsets = new ArrayList();\r\n    List<Long> intervalStrides = new ArrayList();\r\n    List<Long> pointStrides = new ArrayList();\r\n    List<Long> pointOffsets = new ArrayList();\r\n    int numPointIndexes = 0;\r\n    int shapeIndex = 0;\r\n    int strideIndex = 0;\r\n    List<Integer> prependNewAxes = new ArrayList();\r\n    for (int i = 0; i < indexes.length; i++) {\r\n        INDArrayIndex idx = indexes[i];\r\n        if (idx instanceof NDArrayIndexAll) {\r\n            encounteredAll = true;\r\n            if (i < arr.rank() && arr.size(i) == 1)\r\n                oneDimensionWithAllEncountered.add(i);\r\n            if (newAxesPrepend > 0 && lastPrependIndex < 0) {\r\n                lastPrependIndex = i - 1;\r\n            }\r\n        }\r\n        if (idx instanceof PointIndex) {\r\n            pointOffsets.add(idx.offset());\r\n            pointStrides.add((long) arr.stride(strideIndex));\r\n            numPointIndexes++;\r\n            shapeIndex++;\r\n            strideIndex++;\r\n            if (newAxesPrepend > 0 && lastPrependIndex < 0) {\r\n                lastPrependIndex = i - 1;\r\n            }\r\n            continue;\r\n        } else if (idx instanceof NewAxis) {\r\n            accumShape.add(1L);\r\n            accumOffsets.add(0L);\r\n            accumStrides.add(0L);\r\n            prependNewAxes.add(i);\r\n            continue;\r\n        } else if (idx instanceof IntervalIndex && !(idx instanceof NDArrayIndexAll) || idx instanceof SpecifiedIndex) {\r\n            if (idx instanceof IntervalIndex) {\r\n                accumStrides.add(arr.stride(strideIndex) * idx.stride());\r\n                intervalStrides.add(idx.stride());\r\n                numIntervals++;\r\n            } else\r\n                accumStrides.add((long) arr.stride(strideIndex));\r\n            accumShape.add(idx.length());\r\n            if (idx instanceof IntervalIndex) {\r\n                accumOffsets.add(idx.offset());\r\n            } else\r\n                accumOffsets.add(idx.offset());\r\n            shapeIndex++;\r\n            strideIndex++;\r\n            if (newAxesPrepend > 0 && lastPrependIndex < 0) {\r\n                lastPrependIndex = i - 1;\r\n            }\r\n            continue;\r\n        }\r\n        accumShape.add((long) shape[shapeIndex++]);\r\n        accumStrides.add((long) arr.stride(strideIndex++));\r\n        accumOffsets.add(idx.offset());\r\n    }\r\n    while (shapeIndex < shape.length) {\r\n        if (Shape.isVector(shape)) {\r\n            accumShape.add(1L);\r\n            shapeIndex++;\r\n        } else\r\n            accumShape.add((long) shape[shapeIndex++]);\r\n    }\r\n    int delta = (shape.length <= 2 ? shape.length : shape.length - numPointIndexes);\r\n    boolean needsFilledIn = accumShape.size() != accumStrides.size() && accumOffsets.size() != accumShape.size();\r\n    while (accumOffsets.size() < delta && needsFilledIn) accumOffsets.add(0L);\r\n    while (accumShape.size() < 2) {\r\n        if (Shape.isRowVectorShape(arr.shape()))\r\n            accumShape.add(0, 1L);\r\n        else\r\n            accumShape.add(1L);\r\n    }\r\n    while (strideIndex < accumShape.size()) {\r\n        accumStrides.add((long) arr.stride(strideIndex++));\r\n    }\r\n    int trailingZeroRemove = accumOffsets.size() - 1;\r\n    while (accumOffsets.size() > accumShape.size()) {\r\n        if (accumOffsets.get(trailingZeroRemove) == 0)\r\n            accumOffsets.remove(accumOffsets.size() - 1);\r\n        trailingZeroRemove--;\r\n    }\r\n    if (accumStrides.size() < accumOffsets.size())\r\n        accumStrides.addAll(pointStrides);\r\n    while (accumOffsets.size() < accumShape.size()) {\r\n        if (Shape.isRowVectorShape(arr.shape()))\r\n            accumOffsets.add(0, 0L);\r\n        else\r\n            accumOffsets.add(0L);\r\n    }\r\n    if (Shape.isMatrix(shape) && indexes[0] instanceof PointIndex && indexes[1] instanceof NDArrayIndexAll) {\r\n        Collections.reverse(accumShape);\r\n    }\r\n    if (arr.isMatrix() && indexes[0] instanceof PointIndex && indexes[1] instanceof IntervalIndex) {\r\n        this.shapes = new long[2];\r\n        shapes[0] = 1;\r\n        IntervalIndex idx = (IntervalIndex) indexes[1];\r\n        shapes[1] = idx.length();\r\n    } else\r\n        this.shapes = Longs.toArray(accumShape);\r\n    boolean isColumnVector = Shape.isColumnVectorShape(this.shapes);\r\n    while (accumStrides.size() < accumOffsets.size()) {\r\n        if (!isColumnVector)\r\n            accumStrides.add(0, (long) arr.elementStride());\r\n        else\r\n            accumStrides.add((long) arr.elementStride());\r\n    }\r\n    this.strides = Longs.toArray(accumStrides);\r\n    this.offsets = Longs.toArray(accumOffsets);\r\n    if (numPointIndexes > 0 && !pointStrides.isEmpty()) {\r\n        if (newAxesPrepend >= 1) {\r\n            while (pointStrides.size() < accumOffsets.size()) {\r\n                pointStrides.add(1L);\r\n            }\r\n            for (int i = 0; i < accumStrides.size(); i++) {\r\n                if (accumStrides.get(i) == 0 && !(indexes[i] instanceof NewAxis) && lastPrependIndex <= 0)\r\n                    pointStrides.set(i, 0L);\r\n            }\r\n        }\r\n        while (pointOffsets.size() < pointStrides.size()) {\r\n            pointOffsets.add(0L);\r\n        }\r\n        if (arr.isRowVector() && !intervalStrides.isEmpty() && pointOffsets.get(0) == 0 && !(indexes[1] instanceof IntervalIndex))\r\n            this.offset = indexes[1].offset();\r\n        else\r\n            this.offset = ArrayUtil.dotProductLong2(pointOffsets, pointStrides);\r\n    } else {\r\n        this.offset = 0;\r\n    }\r\n    if (numIntervals > 0 && arr.rank() > 2) {\r\n        if (encounteredAll && arr.size(0) != 1 || indexes[0] instanceof PointIndex)\r\n            this.offset += ArrayUtil.dotProductLong2(accumOffsets, accumStrides);\r\n        else\r\n            this.offset += ArrayUtil.dotProductLong2(accumOffsets, accumStrides);\r\n    } else if (numIntervals > 0 && anyHaveStrideOne(indexes))\r\n        this.offset += ArrayUtil.calcOffsetLong2(accumShape, accumOffsets, accumStrides);\r\n    else\r\n        this.offset += ArrayUtil.calcOffsetLong2(accumShape, accumOffsets, accumStrides) / Math.max(1, numIntervals);\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.clearRolesCacheAsync",
	"Comment": "clears the roles cache for a set of roles asynchronously.see the docs for more.",
	"Method": "void clearRolesCacheAsync(ClearRolesCacheRequest request,RequestOptions options,ActionListener<ClearRolesCacheResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, SecurityRequestConverters::clearRolesCache, options, ClearRolesCacheResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.diag",
	"Comment": "creates a new matrix where the values of the given vector are the diagonal values ofthe matrix if a vector is passed in, if a matrix is returns the kth diagonalin the matrix",
	"Method": "INDArray diag(INDArray x,int k,INDArray diag,INDArray x){\r\n    return diag(x, 0);\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.createBuffer",
	"Comment": "create a data bufferbased on a pointerwith the given optype and length",
	"Method": "DataBuffer createBuffer(DataBuffer underlyingBuffer,long offset,long length,DataBuffer createBuffer,int[] shape,DataBuffer.Type type,long offset,DataBuffer createBuffer,ByteBuffer buffer,DataBuffer.Type type,int length,long offset,DataBuffer createBuffer,byte[] data,int length,long offset,DataBuffer createBuffer,int[] data,long offset,DataBuffer createBuffer,int length,long offset,DataBuffer createBuffer,DoublePointer doublePointer,long length,DataBuffer createBuffer,FloatPointer floatPointer,long length,DataBuffer createBuffer,IntPointer intPointer,long length,DataBuffer createBuffer,float[] data,long offset,DataBuffer createBuffer,double[] data,long offset,DataBuffer createBuffer,int[] shape,DataBuffer.Type type,DataBuffer createBuffer,long[] shape,DataBuffer.Type type,DataBuffer createBuffer,ByteBuffer buffer,DataBuffer.Type type,int length,DataBuffer createBuffer,byte[] data,int length,DataBuffer createBuffer,int[] data,DataBuffer createBuffer,long[] data,DataBuffer createBuffer,long length,DataBuffer createBuffer,Pointer pointer,DataBuffer.Type type,long length,Indexer indexer,DataBuffer createBuffer,long length,boolean initialize,DataBuffer createBuffer,float[] data,DataBuffer createBuffer,double[] data){\r\n    DataBuffer ret;\r\n    if (dataType() == DataBuffer.Type.DOUBLE)\r\n        ret = Nd4j.getMemoryManager().getCurrentWorkspace() == null ? DATA_BUFFER_FACTORY_INSTANCE.createDouble(data) : DATA_BUFFER_FACTORY_INSTANCE.createDouble(data, Nd4j.getMemoryManager().getCurrentWorkspace());\r\n    else if (dataType() == DataBuffer.Type.HALF)\r\n        ret = Nd4j.getMemoryManager().getCurrentWorkspace() == null ? DATA_BUFFER_FACTORY_INSTANCE.createHalf(data) : DATA_BUFFER_FACTORY_INSTANCE.createHalf(ArrayUtil.toFloats(data), Nd4j.getMemoryManager().getCurrentWorkspace());\r\n    else\r\n        ret = Nd4j.getMemoryManager().getCurrentWorkspace() == null ? DATA_BUFFER_FACTORY_INSTANCE.createFloat(ArrayUtil.toFloats(data)) : DATA_BUFFER_FACTORY_INSTANCE.createFloat(ArrayUtil.toFloats(data), Nd4j.getMemoryManager().getCurrentWorkspace());\r\n    logCreationIfNecessary(ret);\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.config.AnalysisLimits.getModelMemoryLimit",
	"Comment": "maximum size of the model in mb before the anomaly detectorwill drop new samples to prevent the model using any morememory.",
	"Method": "Long getModelMemoryLimit(){\r\n    return modelMemoryLimit;\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.toggleWorkspaceUse",
	"Comment": "this method allows to temporary disable this workspace, and issue allocations directly.",
	"Method": "void toggleWorkspaceUse(boolean isEnabled){\r\n    isUsed.set(isEnabled);\r\n}"
}, {
	"Path": "com.jakewharton.disklrucache.DiskLruCache.size",
	"Comment": "returns the number of bytes currently being used to store the values inthis cache. this may be greater than the max size if a backgrounddeletion is pending.",
	"Method": "long size(){\r\n    return size;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.lookup.PainlessLookupUtility.buildPainlessMethodKey",
	"Comment": "constructs a painless method key used to lookup painless methods from a painless class.",
	"Method": "String buildPainlessMethodKey(String methodName,int methodArity){\r\n    return methodName + \"/\" + methodArity;\r\n}"
}, {
	"Path": "org.elasticsearch.http.netty4.cors.Netty4CorsHandler.setPreflightHeaders",
	"Comment": "this is a non cors specification feature which enables the setting of preflightresponse headers that might be required by intermediaries.",
	"Method": "void setPreflightHeaders(HttpResponse response){\r\n    response.headers().add(config.preflightResponseHeaders());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.flushJob",
	"Comment": "flushes internally buffered data for the given machine learning job ensuring all data sent to the has been processed.this may cause new results to be calculated depending on the contents of the bufferboth flush and close operations are similar,however the flush is more efficient if you are expecting to send more data for analysis.when flushing, the job remains open and is available to continue analyzing data.a close operation additionally prunes and persists the model state to disk and thejob must be opened again before analyzing further data.for additional infosee flush ml job documentation",
	"Method": "FlushJobResponse flushJob(FlushJobRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::flushJob, options, FlushJobResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getFilter",
	"Comment": "gets machine learning filtersfor additional infosee ml get filter documentation",
	"Method": "GetFiltersResponse getFilter(GetFiltersRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getFilter, options, GetFiltersResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.graph.GraphExploreRequest.createNextHop",
	"Comment": "add a stage in the graph exploration. each hop represents a stage ofquerying elasticsearch to identify terms which can then be connnected toother terms in a subsequent hop.",
	"Method": "Hop createNextHop(QueryBuilder guidingQuery){\r\n    Hop parent = null;\r\n    if (hops.size() > 0) {\r\n        parent = hops.get(hops.size() - 1);\r\n    }\r\n    Hop newHop = new Hop(parent);\r\n    newHop.guidingQuery = guidingQuery;\r\n    hops.add(newHop);\r\n    return newHop;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.getLifecyclePolicyAsync",
	"Comment": "asynchronously retrieve one or more lifecycle policy definition. seethe docs for more.",
	"Method": "void getLifecyclePolicyAsync(GetLifecyclePolicyRequest request,RequestOptions options,ActionListener<GetLifecyclePolicyResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IndexLifecycleRequestConverters::getLifecyclePolicy, options, GetLifecyclePolicyResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.primitives.CounterMap.totalSize",
	"Comment": "this method returns total number of elements in this countermap",
	"Method": "int totalSize(){\r\n    int size = 0;\r\n    for (F first : keySet()) {\r\n        size += getCounter(first).size();\r\n    }\r\n    return size;\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.invalidateTokenAsync",
	"Comment": "asynchronously invalidates an oauth2 token.see the docs for more.",
	"Method": "void invalidateTokenAsync(InvalidateTokenRequest request,RequestOptions options,ActionListener<InvalidateTokenResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, SecurityRequestConverters::invalidateToken, options, InvalidateTokenResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.getFieldMappingAsync",
	"Comment": "asynchronously retrieves the field mappings on an index on indices using the get field mapping api.see get field mapping api on elastic.co",
	"Method": "void getFieldMappingAsync(GetFieldMappingsRequest getFieldMappingsRequest,RequestOptions options,ActionListener<GetFieldMappingsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(getFieldMappingsRequest, IndicesRequestConverters::getFieldMapping, options, GetFieldMappingsResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.jita.conf.Configuration.setMaximumDeviceCache",
	"Comment": "this method allows you to specify maximum memory cache per device",
	"Method": "Configuration setMaximumDeviceCache(long maxCache){\r\n    this.maximumDeviceCache = maxCache;\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.transport.BaseTransport.internalMessageHandler",
	"Comment": "this message handler is responsible for receiving coordination messages on shard side",
	"Method": "void internalMessageHandler(DirectBuffer buffer,int offset,int length,Header header){\r\n    byte[] data = new byte[length];\r\n    buffer.getBytes(offset, data);\r\n    VoidMessage message = VoidMessage.fromBytes(data);\r\n    messages.add(message);\r\n}"
}, {
	"Path": "org.elasticsearch.index.rankeval.ExpectedReciprocalRankTests.testNoResults",
	"Comment": "test that metric returns 0.0 when there are no search results",
	"Method": "void testNoResults(){\r\n    ExpectedReciprocalRank err = new ExpectedReciprocalRank(5, 0, 10);\r\n    assertEquals(0.0, err.evaluate(\"id\", new SearchHit[0], Collections.emptyList()).metricScore(), DELTA);\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.Int16.getCompressionType",
	"Comment": "this method returns compression optype provided by specific ndarraycompressor implementation",
	"Method": "CompressionType getCompressionType(){\r\n    return CompressionType.LOSSY;\r\n}"
}, {
	"Path": "org.elasticsearch.dissect.DissectParserTests.testJsonSpecification",
	"Comment": "shared specification between beats, logstash, and ingest node",
	"Method": "void testJsonSpecification(){\r\n    ObjectMapper mapper = new ObjectMapper();\r\n    JsonNode rootNode = mapper.readTree(this.getClass().getResourceAsStream(\"/specification/tests.json\"));\r\n    Iterator<JsonNode> tests = rootNode.elements();\r\n    while (tests.hasNext()) {\r\n        JsonNode test = tests.next();\r\n        boolean skip = test.path(\"skip\").asBoolean();\r\n        if (!skip) {\r\n            String name = test.path(\"name\").asText();\r\n            logger.debug(\"Running Json specification: \" + name);\r\n            String pattern = test.path(\"tok\").asText();\r\n            String input = test.path(\"msg\").asText();\r\n            String append = test.path(\"append\").asText();\r\n            boolean fail = test.path(\"fail\").asBoolean();\r\n            Iterator<Map.Entry<String, JsonNode>> expected = test.path(\"expected\").fields();\r\n            List<String> expectedKeys = new ArrayList();\r\n            List<String> expectedValues = new ArrayList();\r\n            expected.forEachRemaining(entry -> {\r\n                expectedKeys.add(entry.getKey());\r\n                expectedValues.add(entry.getValue().asText());\r\n            });\r\n            if (fail) {\r\n                assertFail(pattern, input);\r\n            } else {\r\n                assertMatch(pattern, input, expectedKeys, expectedValues, append);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.support.ArrayValuesSourceAggregationBuilder.serializeTargetValueType",
	"Comment": "should this builder serialize its targetvaluetype? defaults to false. all subclasses that override this to trueshould use the three argument read constructor rather than the four argument version.",
	"Method": "boolean serializeTargetValueType(){\r\n    return false;\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.DummyWorkspace.notifyScopeLeft",
	"Comment": "this method notifies given workspace that use cycle just ended",
	"Method": "MemoryWorkspace notifyScopeLeft(){\r\n    close();\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.time.impl.BinaryTimer.getNumberOfEvents",
	"Comment": "this method returns total number of events happened withing predefined timeframe",
	"Method": "long getNumberOfEvents(){\r\n    if (isAlive()) {\r\n        return 1;\r\n    } else {\r\n        return 0;\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.aeron.ipc.response.NDArrayResponseFragmentHandler.onFragment",
	"Comment": "callback for handling fragments of data being read from a log.",
	"Method": "void onFragment(DirectBuffer buffer,int offset,int length,Header header){\r\n    if (buffer != null && length > 0) {\r\n        ByteBuffer byteBuffer = buffer.byteBuffer().order(ByteOrder.nativeOrder());\r\n        byteBuffer.position(offset);\r\n        byte[] b = new byte[length];\r\n        byteBuffer.get(b);\r\n        String hostPort = new String(b);\r\n        System.out.println(\"Host port \" + hostPort + \" offset \" + offset + \" length \" + length);\r\n        String[] split = hostPort.split(\":\");\r\n        if (split == null || split.length != 3) {\r\n            System.err.println(\"no host port stream found\");\r\n            return;\r\n        }\r\n        int port = Integer.parseInt(split[1]);\r\n        int streamToPublish = Integer.parseInt(split[2]);\r\n        String channel = AeronUtil.aeronChannel(split[0], port);\r\n        INDArray arrGet = holder.get();\r\n        AeronNDArrayPublisher publisher = AeronNDArrayPublisher.builder().streamId(streamToPublish).aeron(aeron).channel(channel).build();\r\n        try {\r\n            publisher.publish(arrGet);\r\n        } catch (Exception e) {\r\n            e.printStackTrace();\r\n        }\r\n        try {\r\n            publisher.close();\r\n        } catch (Exception e) {\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getCategories",
	"Comment": "gets the categories for a machine learning job.for additional infosee ml get categories documentation",
	"Method": "GetCategoriesResponse getCategories(GetCategoriesRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getCategories, options, GetCategoriesResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.config.JobUpdateTests.createRandom",
	"Comment": "creates a completely random update when the job is nullor a random update that is is valid for the given job",
	"Method": "JobUpdate createRandom(String jobId){\r\n    JobUpdate.Builder update = new JobUpdate.Builder(jobId);\r\n    if (randomBoolean()) {\r\n        int groupsNum = randomIntBetween(0, 10);\r\n        List<String> groups = new ArrayList(groupsNum);\r\n        for (int i = 0; i < groupsNum; i++) {\r\n            groups.add(JobTests.randomValidJobId());\r\n        }\r\n        update.setGroups(groups);\r\n    }\r\n    if (randomBoolean()) {\r\n        update.setDescription(randomAlphaOfLength(20));\r\n    }\r\n    if (randomBoolean()) {\r\n        update.setDetectorUpdates(createRandomDetectorUpdates());\r\n    }\r\n    if (randomBoolean()) {\r\n        update.setModelPlotConfig(new ModelPlotConfig(randomBoolean(), randomAlphaOfLength(10)));\r\n    }\r\n    if (randomBoolean()) {\r\n        update.setAnalysisLimits(AnalysisLimitsTests.createRandomized());\r\n    }\r\n    if (randomBoolean()) {\r\n        update.setRenormalizationWindowDays(randomNonNegativeLong());\r\n    }\r\n    if (randomBoolean()) {\r\n        update.setBackgroundPersistInterval(TimeValue.timeValueHours(randomIntBetween(1, 24)));\r\n    }\r\n    if (randomBoolean()) {\r\n        update.setModelSnapshotRetentionDays(randomNonNegativeLong());\r\n    }\r\n    if (randomBoolean()) {\r\n        update.setResultsRetentionDays(randomNonNegativeLong());\r\n    }\r\n    if (randomBoolean()) {\r\n        update.setCategorizationFilters(Arrays.asList(generateRandomStringArray(10, 10, false)));\r\n    }\r\n    if (randomBoolean()) {\r\n        update.setCustomSettings(Collections.singletonMap(randomAlphaOfLength(10), randomAlphaOfLength(10)));\r\n    }\r\n    return update.build();\r\n}"
}, {
	"Path": "org.nd4j.linalg.dimensionalityreduction.PCA.pca2",
	"Comment": "this method performs a dimensionality reduction, including principal componentsthat cover a fraction of the total variance of the system.it does all calculationsabout the mean.",
	"Method": "INDArray pca2(INDArray in,double variance){\r\n    INDArray[] covmean = covarianceMatrix(in);\r\n    INDArray[] pce = principalComponents(covmean[0]);\r\n    INDArray vars = Transforms.pow(pce[1], -0.5, true);\r\n    double res = vars.sumNumber().doubleValue();\r\n    double total = 0.0;\r\n    int ndims = 0;\r\n    for (int i = 0; i < vars.columns(); i++) {\r\n        ndims++;\r\n        total += vars.getDouble(i);\r\n        if (total / res > variance)\r\n            break;\r\n    }\r\n    INDArray result = Nd4j.create(in.columns(), ndims);\r\n    for (int i = 0; i < ndims; i++) result.putColumn(i, pce[0].getColumn(i));\r\n    return result;\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.iterator.fetcher.BaseDataFetcher.initializeCurrFromList",
	"Comment": "initializes this data transform fetcher from the passed in datasets",
	"Method": "void initializeCurrFromList(List<DataSet> examples){\r\n    if (examples.isEmpty())\r\n        log.warn(\"Warning: empty dataset from the fetcher\");\r\n    INDArray inputs = createInputMatrix(examples.size());\r\n    INDArray labels = createOutputMatrix(examples.size());\r\n    for (int i = 0; i < examples.size(); i++) {\r\n        inputs.putRow(i, examples.get(i).getFeatures());\r\n        labels.putRow(i, examples.get(i).getLabels());\r\n    }\r\n    curr = new DataSet(inputs, labels);\r\n}"
}, {
	"Path": "org.elasticsearch.client.GraphClient.explore",
	"Comment": "executes an exploration request using the graph api.see graph apion elastic.co.",
	"Method": "GraphExploreResponse explore(GraphExploreRequest graphExploreRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(graphExploreRequest, GraphRequestConverters::explore, options, GraphExploreResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.primitives.CounterMap.incrementAll",
	"Comment": "this method will increment values of this counter, by counts of other counter",
	"Method": "void incrementAll(CounterMap<F, S> other){\r\n    for (Map.Entry<F, Counter<S>> entry : other.maps.entrySet()) {\r\n        F key = entry.getKey();\r\n        Counter<S> innerCounter = entry.getValue();\r\n        for (Map.Entry<S, AtomicDouble> innerEntry : innerCounter.entrySet()) {\r\n            S value = innerEntry.getKey();\r\n            incrementCount(key, value, innerEntry.getValue().get());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.painless.CompilerSettings.setMaxLoopCounter",
	"Comment": "set the cumulative total number of statements that can be made in all loops.",
	"Method": "void setMaxLoopCounter(int max){\r\n    this.maxLoopCounter = max;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.CompilerSettings.getMaxLoopCounter",
	"Comment": "returns the value for the cumulative total number of statements that can be made in all loopsin a script before an exception is thrown.this attempts to prevent infinite loops.note ifthe counter is set to 0, no loop counter will be written.",
	"Method": "int getMaxLoopCounter(){\r\n    return maxLoopCounter;\r\n}"
}, {
	"Path": "org.nd4j.linalg.shape.TADTests.compareShapes",
	"Comment": "this method compares rank, shape and stride for two given shapebuffers",
	"Method": "boolean compareShapes(DataBuffer shapeA,DataBuffer shapeB){\r\n    if (shapeA.dataType() != DataBuffer.Type.INT)\r\n        throw new IllegalStateException(\"ShapeBuffer should have dataType of INT\");\r\n    if (shapeA.dataType() != shapeB.dataType())\r\n        return false;\r\n    int rank = shapeA.getInt(0);\r\n    if (rank != shapeB.getInt(0))\r\n        return false;\r\n    for (int e = 1; e <= rank * 2; e++) {\r\n        if (shapeA.getInt(e) != shapeB.getInt(e))\r\n            return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.logic.completion.Clipboard.pin",
	"Comment": "this method places incoming voidaggregation into clipboard, for further tracking",
	"Method": "boolean pin(VoidAggregation aggregation){\r\n    RequestDescriptor descriptor = RequestDescriptor.createDescriptor(aggregation.getOriginatorId(), aggregation.getTaskId());\r\n    VoidAggregation existing = clipboard.get(descriptor);\r\n    if (existing == null) {\r\n        existing = aggregation;\r\n        trackingCounter.incrementAndGet();\r\n        clipboard.put(descriptor, aggregation);\r\n    }\r\n    existing.accumulateAggregation(aggregation);\r\n    int missing = existing.getMissingChunks();\r\n    if (missing == 0) {\r\n        completedCounter.incrementAndGet();\r\n        return true;\r\n    } else\r\n        return false;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.CompilerSettings.areRegexesEnabled",
	"Comment": "are regexes enabled? they are currently disabled by default because they break out of the loop counter and even fairly simplelooking regexes can cause stack overflows.",
	"Method": "boolean areRegexesEnabled(){\r\n    return regexesEnabled;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.openAsync",
	"Comment": "asynchronously opens an index using the open index api.see open index api on elastic.co",
	"Method": "void openAsync(OpenIndexRequest openIndexRequest,RequestOptions options,ActionListener<OpenIndexResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(openIndexRequest, IndicesRequestConverters::openIndex, options, OpenIndexResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.XContentFactory.jsonBuilder",
	"Comment": "constructs a new json builder that will output the result into the provided output stream.",
	"Method": "XContentBuilder jsonBuilder(XContentBuilder jsonBuilder,OutputStream os){\r\n    return new XContentBuilder(JsonXContent.jsonXContent, os);\r\n}"
}, {
	"Path": "org.elasticsearch.plugin.noop.action.search.NoopSearchRequestBuilder.setTypes",
	"Comment": "the document types to execute the search against. defaults to be executed againstall types.",
	"Method": "NoopSearchRequestBuilder setTypes(String types){\r\n    request.types(types);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.provider.BasicWorkspaceManager.checkIfWorkspaceExists",
	"Comment": "this method checks, if workspace with a given id was created before this call",
	"Method": "boolean checkIfWorkspaceExists(String id){\r\n    ensureThreadExistense();\r\n    return backingMap.get().containsKey(id);\r\n}"
}, {
	"Path": "org.elasticsearch.nio.SelectionKeyUtils.setWriteInterested",
	"Comment": "adds an interest in writes for this selection key while maintaining other interests.",
	"Method": "void setWriteInterested(SelectionKey selectionKey){\r\n    selectionKey.interestOps(selectionKey.interestOps() | SelectionKey.OP_WRITE);\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.datafeed.DelayedDataCheckConfig.disabledDelayedDataCheckConfig",
	"Comment": "this creates a new delayeddatacheckconfig that disables the data check.",
	"Method": "DelayedDataCheckConfig disabledDelayedDataCheckConfig(){\r\n    return new DelayedDataCheckConfig(false, null);\r\n}"
}, {
	"Path": "io.dropwizard.client.HttpClientBuilder.name",
	"Comment": "use the given environment name. this is used in the user agent.",
	"Method": "HttpClientBuilder name(String environmentName){\r\n    this.environmentName = environmentName;\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.serde.base64.Nd4jBase64.isMultiple",
	"Comment": "returns true if the base64contains multiple arraysthis is delimited by tab",
	"Method": "boolean isMultiple(String base64){\r\n    return base64.contains(\"\\t\");\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.acceptChannel",
	"Comment": "this method is called when a server channel signals it is ready to accept a connection. all of theaccept logic should occur in this call.",
	"Method": "void acceptChannel(ServerChannelContext context){\r\n    context.acceptChannels(selectorSupplier);\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.AbstractMultiDataSetNormalizer.isFitLabel",
	"Comment": "whether normalization for the labels is also enabled. most commonly used for regression, not classification.",
	"Method": "boolean isFitLabel(){\r\n    return this.fitLabels;\r\n}"
}, {
	"Path": "org.elasticsearch.nio.SelectionKeyUtils.removeConnectInterested",
	"Comment": "removes an interest in connects for this selection key while maintaining other interests.",
	"Method": "void removeConnectInterested(SelectionKey selectionKey){\r\n    selectionKey.interestOps(selectionKey.interestOps() & ~SelectionKey.OP_CONNECT);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IngestClient.simulateAsync",
	"Comment": "asynchronously simulate a pipeline on a set of documents provided in the requestseesimulate pipeline api on elastic.co",
	"Method": "void simulateAsync(SimulatePipelineRequest request,RequestOptions options,ActionListener<SimulatePipelineResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IngestRequestConverters::simulatePipeline, options, SimulatePipelineResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.NoOp.getDescriptor",
	"Comment": "this method returns compression descriptor. it should be unique for any compressor implementation",
	"Method": "String getDescriptor(){\r\n    return \"NOOP\";\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.deleteRepository",
	"Comment": "deletes a snapshot repository.seesnapshot and restoreapi on elastic.co",
	"Method": "AcknowledgedResponse deleteRepository(DeleteRepositoryRequest deleteRepositoryRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(deleteRepositoryRequest, SnapshotRequestConverters::deleteRepository, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.ChannelContext.closeFromSelector",
	"Comment": "this method cleans up any context resources that need to be released when a channel is closed. itshould only be called by the selector thread.",
	"Method": "void closeFromSelector(){\r\n    if (isOpen()) {\r\n        try {\r\n            rawChannel.close();\r\n            closeContext.complete(null);\r\n        } catch (Exception e) {\r\n            closeContext.completeExceptionally(e);\r\n            throw e;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.primitives.CounterMap.isEmpty",
	"Comment": "this method checks if this countermap has any values stored for a given first element",
	"Method": "boolean isEmpty(boolean isEmpty,F element){\r\n    if (isEmpty())\r\n        return true;\r\n    Counter<S> m = maps.get(element);\r\n    if (m == null)\r\n        return true;\r\n    else\r\n        return m.isEmpty();\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteModelSnapshotAsync",
	"Comment": "deletes machine learning model snapshots asynchronously and notifies the listener on completionfor additional infosee ml delete model snapshot documentation",
	"Method": "void deleteModelSnapshotAsync(DeleteModelSnapshotRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::deleteModelSnapshot, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.workspace.WorkspaceUtils.assertOpenActiveAndCurrent",
	"Comment": "assert that the specified workspace is open, active, and is the current workspace",
	"Method": "void assertOpenActiveAndCurrent(String ws,String errorMsg){\r\n    if (!Nd4j.getWorkspaceManager().checkIfWorkspaceExistsAndActive(ws)) {\r\n        throw new ND4JWorkspaceException(errorMsg + \" - workspace is not open and active\");\r\n    }\r\n    MemoryWorkspace currWs = Nd4j.getMemoryManager().getCurrentWorkspace();\r\n    if (currWs == null || !ws.equals(currWs.getId())) {\r\n        throw new ND4JWorkspaceException(errorMsg + \" - not the current workspace (current workspace: \" + (currWs == null ? null : currWs.getId()));\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.jdbc.hsql.HSqlLoaderTest.initDatabase",
	"Comment": "database initialization for testing i.e.creating tableinserting record",
	"Method": "void initDatabase(){\r\n    try (Connection connection = getConnection();\r\n        Statement statement = connection.createStatement()) {\r\n        statement.execute(String.format(\"CREATE TABLE %s (%s INT NOT NULL,\" + \" %s BLOB NOT NULL, PRIMARY KEY (id))\", TABLE_NAME, ID_COLUMN_NAME, COLUMN_NAME));\r\n        connection.commit();\r\n        hsqlLoader.save(Nd4j.linspace(1, 4, 4), \"1\");\r\n        connection.commit();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.getAsync",
	"Comment": "asynchronously get snapshots.seesnapshot and restoreapi on elastic.co",
	"Method": "void getAsync(GetSnapshotsRequest getSnapshotsRequest,RequestOptions options,ActionListener<GetSnapshotsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(getSnapshotsRequest, SnapshotRequestConverters::getSnapshots, options, GetSnapshotsResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "io.dropwizard.util.Generics.getTypeParameter",
	"Comment": "finds the type parameter for the given class which is assignable to the bound class.",
	"Method": "Class<?> getTypeParameter(Class<?> klass,Class<T> getTypeParameter,Class<?> klass,Class<? super T> bound){\r\n    Type t = requireNonNull(klass);\r\n    while (t instanceof Class<?>) {\r\n        t = ((Class<?>) t).getGenericSuperclass();\r\n    }\r\n    if (t instanceof ParameterizedType) {\r\n        for (Type param : ((ParameterizedType) t).getActualTypeArguments()) {\r\n            if (param instanceof Class<?>) {\r\n                final Class<T> cls = determineClass(bound, param);\r\n                if (cls != null) {\r\n                    return cls;\r\n                }\r\n            } else if (param instanceof TypeVariable) {\r\n                for (Type paramBound : ((TypeVariable<?>) param).getBounds()) {\r\n                    if (paramBound instanceof Class<?>) {\r\n                        final Class<T> cls = determineClass(bound, paramBound);\r\n                        if (cls != null) {\r\n                            return cls;\r\n                        }\r\n                    }\r\n                }\r\n            } else if (param instanceof ParameterizedType) {\r\n                final Type rawType = ((ParameterizedType) param).getRawType();\r\n                if (rawType instanceof Class<?>) {\r\n                    final Class<T> cls = determineClass(bound, rawType);\r\n                    if (cls != null) {\r\n                        return cls;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    throw new IllegalStateException(\"Cannot figure out type parameterization for \" + klass.getName());\r\n}"
}, {
	"Path": "io.dropwizard.jetty.NetUtil.getTcpBacklog",
	"Comment": "the somaxconn value of the current machine.if failed to get the value, defaultbacklog argument isused",
	"Method": "int getTcpBacklog(int getTcpBacklog,int tcpBacklog){\r\n    return AccessController.doPrivileged((PrivilegedAction<Integer>) () -> {\r\n        final File file = new File(TCP_BACKLOG_SETTING_LOCATION);\r\n        try (BufferedReader in = new BufferedReader(new FileReader(file))) {\r\n            return Integer.parseInt(in.readLine().trim());\r\n        } catch (SecurityException | IOException | NumberFormatException | NullPointerException e) {\r\n            return tcpBacklog;\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.choice",
	"Comment": "this method returns new indarray instance, sampled from source array with probabilities given in probs",
	"Method": "INDArray choice(INDArray source,INDArray probs,INDArray target,org.nd4j.linalg.api.rng.Random rng,INDArray choice,INDArray source,INDArray probs,INDArray target,INDArray choice,INDArray source,INDArray probs,int numSamples,org.nd4j.linalg.api.rng.Random rng,INDArray choice,INDArray source,INDArray probs,int numSamples){\r\n    return choice(source, probs, numSamples, Nd4j.getRandom());\r\n}"
}, {
	"Path": "org.elasticsearch.percolator.PercolatorFieldMapperTests.testMultiplePercolatorFields",
	"Comment": "multiple percolator fields are allowed in the mapping, but only one field can be used at index time.",
	"Method": "void testMultiplePercolatorFields(){\r\n    String typeName = \"doc\";\r\n    String percolatorMapper = Strings.toString(// makes testing easier\r\n    XContentFactory.jsonBuilder().startObject().startObject(typeName).startObject(\"_field_names\").field(\"enabled\", false).endObject().startObject(\"properties\").startObject(\"query_field1\").field(\"type\", \"percolator\").endObject().startObject(\"query_field2\").field(\"type\", \"percolator\").endObject().endObject().endObject().endObject());\r\n    mapperService.merge(typeName, new CompressedXContent(percolatorMapper), MapperService.MergeReason.MAPPING_UPDATE);\r\n    QueryBuilder queryBuilder = matchQuery(\"field\", \"value\");\r\n    ParsedDocument doc = mapperService.documentMapper(typeName).parse(SourceToParse.source(\"test\", typeName, \"1\", BytesReference.bytes(jsonBuilder().startObject().field(\"query_field1\", queryBuilder).field(\"query_field2\", queryBuilder).endObject()), XContentType.JSON));\r\n    assertThat(doc.rootDoc().getFields().size(), equalTo(14));\r\n    BytesRef queryBuilderAsBytes = doc.rootDoc().getField(\"query_field1.query_builder_field\").binaryValue();\r\n    assertQueryBuilder(queryBuilderAsBytes, queryBuilder);\r\n    queryBuilderAsBytes = doc.rootDoc().getField(\"query_field2.query_builder_field\").binaryValue();\r\n    assertQueryBuilder(queryBuilderAsBytes, queryBuilder);\r\n}"
}, {
	"Path": "org.nd4j.jita.handler.impl.CudaZeroHandler.getAllocatedDeviceObjects",
	"Comment": "this method returns total number of object allocated on specified device",
	"Method": "long getAllocatedDeviceObjects(Integer deviceId){\r\n    return deviceAllocations.get(deviceId).size();\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.compression.CudaFlexibleThreshold.getDescriptor",
	"Comment": "this method returns compression descriptor. it should be unique for any compressor implementation",
	"Method": "String getDescriptor(){\r\n    return \"FTHRESHOLD\";\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.NDArrayMath.mapIndexOntoTensor",
	"Comment": "this maps an index of a vectoron to a vector in the matrix that can be usedfor indexing in to a tensor",
	"Method": "int mapIndexOntoTensor(int index,INDArray arr,int rank){\r\n    int ret = index * ArrayUtil.prod(ArrayUtil.removeIndex(arr.shape(), rank));\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.revertModelSnapshot",
	"Comment": "reverts to a particular machine learning model snapshotfor additional infosee ml revert model snapshot documentation",
	"Method": "RevertModelSnapshotResponse revertModelSnapshot(RevertModelSnapshotRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::revertModelSnapshot, options, RevertModelSnapshotResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.StartDatafeedRequest.setEnd",
	"Comment": "the time that the datafeed should end. this value is exclusive.if you do not specify an end time, the datafeed runs continuously.",
	"Method": "void setEnd(String end){\r\n    this.end = end;\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.XContentFactory.cborBuilder",
	"Comment": "constructs a new cbor builder that will output the result into the provided output stream.",
	"Method": "XContentBuilder cborBuilder(XContentBuilder cborBuilder,OutputStream os){\r\n    return new XContentBuilder(CborXContent.cborXContent, os);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.getScriptAsync",
	"Comment": "asynchronously get stored script by id.see how to use scripts on elastic.co",
	"Method": "void getScriptAsync(GetStoredScriptRequest request,RequestOptions options,ActionListener<GetStoredScriptResponse> listener){\r\n    performRequestAsyncAndParseEntity(request, RequestConverters::getScript, options, GetStoredScriptResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.putFilter",
	"Comment": "creates a new machine learning filterfor additional infosee ml put filter documentation",
	"Method": "PutFilterResponse putFilter(PutFilterRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::putFilter, options, PutFilterResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.messages.VoidMessage.asUnsafeBuffer",
	"Comment": "this method serializes this voidmessage into unsafebuffer",
	"Method": "UnsafeBuffer asUnsafeBuffer(){\r\n    return new UnsafeBuffer(org.nd4j.linalg.util.SerializationUtils.toByteArray(this));\r\n}"
}, {
	"Path": "org.nd4j.linalg.lossfunctions.TestLossFunctionsSizeChecks.testLossFunctionScoreSizeMismatchCase",
	"Comment": "this method checks that the given loss function will give an assertionif the labels and output vectors are of different sizes.",
	"Method": "void testLossFunctionScoreSizeMismatchCase(ILossFunction loss){\r\n    try {\r\n        INDArray labels = Nd4j.create(100, 32);\r\n        INDArray preOutput = Nd4j.create(100, 44);\r\n        double score = loss.computeScore(labels, preOutput, Activation.IDENTITY.getActivationFunction(), null, true);\r\n        Assert.assertFalse(\"Loss function \" + loss.toString() + \"did not check for size mismatch.  This should fail to compute an activation function because the sizes of the vectors are not equal\", true);\r\n    } catch (IllegalArgumentException ex) {\r\n        String exceptionMessage = ex.getMessage();\r\n        Assert.assertTrue(\"Loss function exception \" + loss.toString() + \" did not indicate size mismatch when vectors of incorrect size were used.\", exceptionMessage.contains(\"shapes\"));\r\n    }\r\n    try {\r\n        INDArray labels = Nd4j.create(100, 32);\r\n        INDArray preOutput = Nd4j.create(100, 44);\r\n        INDArray gradient = loss.computeGradient(labels, preOutput, Activation.IDENTITY.getActivationFunction(), null);\r\n        Assert.assertFalse(\"Loss function \" + loss.toString() + \"did not check for size mismatch.  This should fail to compute an activation function because the sizes of the vectors are not equal\", true);\r\n    } catch (IllegalArgumentException ex) {\r\n        String exceptionMessage = ex.getMessage();\r\n        Assert.assertTrue(\"Loss function exception \" + loss.toString() + \" did not indicate size mismatch when vectors of incorrect size were used.\", exceptionMessage.contains(\"shapes\"));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.graph",
	"Comment": "provides methods for accessing the elastic licensed graph explore api thatis shipped with the default distribution of elasticsearch. all ofthese apis will 404 if run against the oss distribution of elasticsearch.see the graph api on elastic.co for more information.",
	"Method": "GraphClient graph(){\r\n    return graphClient;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.termvectors",
	"Comment": "calls the term vectors apisee term vectors api onelastic.co",
	"Method": "TermVectorsResponse termvectors(TermVectorsRequest request,RequestOptions options){\r\n    return performRequestAndParseEntity(request, RequestConverters::termVectors, options, TermVectorsResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.delete",
	"Comment": "deletes a snapshot.seesnapshot and restoreapi on elastic.co",
	"Method": "AcknowledgedResponse delete(DeleteSnapshotRequest deleteSnapshotRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(deleteSnapshotRequest, SnapshotRequestConverters::deleteSnapshot, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.close",
	"Comment": "closes an index using the close index api.see close index api on elastic.co",
	"Method": "AcknowledgedResponse close(CloseIndexRequest closeIndexRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(closeIndexRequest, IndicesRequestConverters::closeIndex, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "com.example.sslreload.SslReloadAppTest.certBytes",
	"Comment": "issues a post against the reload ssl admin task, asserts that the code and content are as expected, and finally returns the server certificate",
	"Method": "byte[] certBytes(int code,String content){\r\n    final URL url = new URL(\"https://localhost:\" + rule.getAdminPort() + \"/tasks/reload-ssl\");\r\n    final HttpsURLConnection conn = (HttpsURLConnection) url.openConnection();\r\n    try {\r\n        postIt(conn);\r\n        assertThat(conn.getResponseCode()).isEqualTo(code);\r\n        if (code == 200) {\r\n            assertThat(CharStreams.toString(new InputStreamReader(conn.getInputStream(), StandardCharsets.UTF_8))).isEqualTo(content);\r\n        } else {\r\n            assertThat(CharStreams.toString(new InputStreamReader(conn.getErrorStream(), StandardCharsets.UTF_8))).contains(content);\r\n        }\r\n        return conn.getServerCertificates()[0].getEncoded();\r\n    } finally {\r\n        conn.disconnect();\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.buffer.factory.CudaDataBufferFactory.create",
	"Comment": "create a data buffer based on thegiven pointer, data buffer optype,and length of the buffer",
	"Method": "DataBuffer create(DataBuffer underlyingBuffer,long offset,long length,DataBuffer create,Pointer pointer,DataBuffer.Type type,long length,Indexer indexer,DataBuffer create,DoublePointer doublePointer,long length,DataBuffer create,IntPointer intPointer,long length,DataBuffer create,FloatPointer floatPointer,long length){\r\n    return new CudaFloatDataBuffer(floatPointer, FloatIndexer.create(floatPointer), length);\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.buffer.factory.DefaultDataBufferFactory.create",
	"Comment": "create a data buffer based on thegiven pointer, data buffer optype,and length of the buffer",
	"Method": "DataBuffer create(DataBuffer underlyingBuffer,long offset,long length,DataBuffer create,Pointer pointer,DataBuffer.Type type,long length,Indexer indexer,DataBuffer create,DoublePointer doublePointer,long length,DataBuffer create,IntPointer intPointer,long length,DataBuffer create,FloatPointer floatPointer,long length){\r\n    floatPointer.capacity(length);\r\n    floatPointer.limit(length);\r\n    floatPointer.position(0);\r\n    return new FloatBuffer(floatPointer, FloatIndexer.create(floatPointer), length);\r\n}"
}, {
	"Path": "org.elasticsearch.client.xpack.XPackUsageResponse.getUsages",
	"Comment": "return a map from feature name to usage information for that feature.",
	"Method": "Map<String, Map<String, Object>> getUsages(){\r\n    return usages;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.lifecycleManagementStatus",
	"Comment": "get the status of index lifecycle managementsee the docs for more.",
	"Method": "LifecycleManagementStatusResponse lifecycleManagementStatus(LifecycleManagementStatusRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IndexLifecycleRequestConverters::lifecycleManagementStatus, options, LifecycleManagementStatusResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.SynchronousParameterUpdater.shouldReplicate",
	"Comment": "returns true ifthe updater has accumulated enough ndarrays toreplicate to the workers",
	"Method": "boolean shouldReplicate(){\r\n    return numUpdates() == workers;\r\n}"
}, {
	"Path": "org.elasticsearch.client.WatcherClient.stopWatchServiceAsync",
	"Comment": "asynchronously stop the watch servicesee the docs for more.",
	"Method": "void stopWatchServiceAsync(StopWatchServiceRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, WatcherRequestConverters::stopWatchService, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getModelSnapshots",
	"Comment": "gets the snapshots for a machine learning job.for additional infosee ml get model snapshots documentation",
	"Method": "GetModelSnapshotsResponse getModelSnapshots(GetModelSnapshotsRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getModelSnapshots, options, GetModelSnapshotsResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.acceptException",
	"Comment": "this method is called when an attempt to accept a connection throws an exception.",
	"Method": "void acceptException(ServerChannelContext context,Exception exception){\r\n    context.handleException(exception);\r\n}"
}, {
	"Path": "org.elasticsearch.nio.SelectionKeyUtils.setConnectAndReadInterested",
	"Comment": "removes an interest in connects and reads for this selection key while maintaining other interests.",
	"Method": "void setConnectAndReadInterested(SelectionKey selectionKey){\r\n    selectionKey.interestOps(selectionKey.interestOps() | SelectionKey.OP_CONNECT | SelectionKey.OP_READ);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.removeIndexLifecyclePolicyAsync",
	"Comment": "asynchronously remove the index lifecycle policy for an indexsee the docs for more.",
	"Method": "void removeIndexLifecyclePolicyAsync(RemoveIndexLifecyclePolicyRequest request,RequestOptions options,ActionListener<RemoveIndexLifecyclePolicyResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IndexLifecycleRequestConverters::removeIndexLifecyclePolicy, options, RemoveIndexLifecyclePolicyResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ClusterClient.putSettingsAsync",
	"Comment": "asynchronously updates cluster wide specific settings using the cluster update settings api.seecluster update settingsapi on elastic.co",
	"Method": "void putSettingsAsync(ClusterUpdateSettingsRequest clusterUpdateSettingsRequest,RequestOptions options,ActionListener<ClusterUpdateSettingsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(clusterUpdateSettingsRequest, ClusterRequestConverters::clusterPutSettings, options, ClusterUpdateSettingsResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.config.Job.getCreateTime",
	"Comment": "the job creation time. this name is preferred when serialising to therest api.",
	"Method": "Date getCreateTime(){\r\n    return createTime;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.lookup.PainlessCast.unboxTargetType",
	"Comment": "create a cast where the target type will be unboxed, and then the cast will be performed.",
	"Method": "PainlessCast unboxTargetType(Class<?> originalType,Class<?> targetType,boolean explicitCast,Class<?> unboxTargetType){\r\n    Objects.requireNonNull(originalType);\r\n    Objects.requireNonNull(targetType);\r\n    Objects.requireNonNull(unboxTargetType);\r\n    return new PainlessCast(originalType, targetType, explicitCast, null, unboxTargetType, null, null);\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.DummyWorkspace.isScopeActive",
	"Comment": "this method returns true if scope was opened, and not closed yet.",
	"Method": "boolean isScopeActive(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.getMapping",
	"Comment": "retrieves the mappings on an index or indices using the get mapping api.see get mapping api on elastic.co",
	"Method": "GetMappingsResponse getMapping(GetMappingsRequest getMappingsRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(getMappingsRequest, IndicesRequestConverters::getMappings, options, GetMappingsResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.updateByQueryRethrottle",
	"Comment": "executes a update by query rethrottle request.see update by query api on elastic.co",
	"Method": "ListTasksResponse updateByQueryRethrottle(RethrottleRequest rethrottleRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(rethrottleRequest, RequestConverters::rethrottleUpdateByQuery, options, ListTasksResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.InstallPluginCommand.installPluginSupportFiles",
	"Comment": "moves bin and config directories from the plugin if they exist",
	"Method": "void installPluginSupportFiles(PluginInfo info,Path tmpRoot,Path destBinDir,Path destConfigDir,List<Path> deleteOnFailure){\r\n    Path tmpBinDir = tmpRoot.resolve(\"bin\");\r\n    if (Files.exists(tmpBinDir)) {\r\n        deleteOnFailure.add(destBinDir);\r\n        installBin(info, tmpBinDir, destBinDir);\r\n    }\r\n    Path tmpConfigDir = tmpRoot.resolve(\"config\");\r\n    if (Files.exists(tmpConfigDir)) {\r\n        installConfig(info, tmpConfigDir, destConfigDir);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getMlInfo",
	"Comment": "gets machine learning information about default values and limits.for additional infosee machine learning info",
	"Method": "MlInfoResponse getMlInfo(MlInfoRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::mlInfo, options, MlInfoResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getRecordsAsync",
	"Comment": "gets the records for a machine learning job, notifies listener once the requested records are retrieved.for additional infosee ml get records documentation",
	"Method": "void getRecordsAsync(GetRecordsRequest request,RequestOptions options,ActionListener<GetRecordsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getRecords, options, GetRecordsResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteForecastAsync",
	"Comment": "deletes machine learning job forecasts asynchronouslyfor additional infosee delete job forecastdocumentation",
	"Method": "void deleteForecastAsync(DeleteForecastRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::deleteForecast, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.aeron.ipc.AeronNDArraySubscriber.launch",
	"Comment": "launch a background threadthat subscribes tothe aeron context",
	"Method": "void launch(){\r\n    if (init.get())\r\n        return;\r\n    if (!init.get())\r\n        init();\r\n    log.info(\"Subscribing to \" + channel + \" on stream Id \" + streamId);\r\n    log.info(\"Using aeron directory \" + ctx.aeronDirectoryName());\r\n    SigInt.register(() -> running.set(false));\r\n    if (channel == null)\r\n        throw new IllegalStateException(\"No channel for subscriber defined\");\r\n    if (streamId <= 0)\r\n        throw new IllegalStateException(\"No stream for subscriber defined\");\r\n    if (aeron == null)\r\n        throw new IllegalStateException(\"No aeron instance defined\");\r\n    boolean started = false;\r\n    while (!started) {\r\n        try (final Subscription subscription = aeron.addSubscription(channel, streamId)) {\r\n            this.subscription = subscription;\r\n            log.info(\"Beginning subscribe on channel \" + channel + \" and stream \" + streamId);\r\n            AeronUtil.subscriberLoop(new FragmentAssembler(new NDArrayFragmentHandler(ndArrayCallback)), fragmentLimitCount, running, launched).accept(subscription);\r\n            started = true;\r\n        } catch (Exception e) {\r\n            log.warn(\"Unable to connect...trying again on channel \" + channel, e);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.config.AnalysisLimits.getCategorizationExamplesLimit",
	"Comment": "gets the limit to the number of examples that are stored per category",
	"Method": "Long getCategorizationExamplesLimit(){\r\n    return categorizationExamplesLimit;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.updateFilter",
	"Comment": "updates a machine learning filterfor additional infosee ml update filter documentation",
	"Method": "PutFilterResponse updateFilter(UpdateFilterRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::updateFilter, options, PutFilterResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.collection.MultiDimensionalSet.isEmpty",
	"Comment": "returns true if this applytransformtodestination contains no elements.",
	"Method": "boolean isEmpty(){\r\n    return backedSet.isEmpty();\r\n}"
}, {
	"Path": "org.elasticsearch.client.core.TermVectorsRequest.createFromTemplate",
	"Comment": "constructs a new termvectorrequest from a templateusing the provided document id",
	"Method": "TermVectorsRequest createFromTemplate(TermVectorsRequest template,String id){\r\n    TermVectorsRequest request = new TermVectorsRequest(template.getIndex(), template.getType(), id);\r\n    request.realtime = template.getRealtime();\r\n    request.requestPositions = template.requestPositions;\r\n    request.requestPayloads = template.requestPayloads;\r\n    request.requestOffsets = template.requestOffsets;\r\n    request.requestFieldStatistics = template.requestFieldStatistics;\r\n    request.requestTermStatistics = template.requestTermStatistics;\r\n    if (template.routing != null)\r\n        request.setRouting(template.getRouting());\r\n    if (template.preference != null)\r\n        request.setPreference(template.getPreference());\r\n    if (template.fields != null)\r\n        request.setFields(template.getFields());\r\n    if (template.perFieldAnalyzer != null)\r\n        request.setPerFieldAnalyzer(template.perFieldAnalyzer);\r\n    if (template.filterSettings != null)\r\n        request.setFilterSettings(template.filterSettings);\r\n    return request;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.openJob",
	"Comment": "opens a machine learning job.when you open a new job, it starts with an empty model.when you open an existing job, the most recent model state is automatically loaded.the job is ready to resume its analysis from where it left off, once new data is received.for additional infosee ml open job documentation",
	"Method": "OpenJobResponse openJob(OpenJobRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::openJob, options, OpenJobResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AtomicAllocator.seekUnusedDevice",
	"Comment": "this method seeks for unused device memory allocations, for specified thread and device",
	"Method": "long seekUnusedDevice(Long threadId,Integer deviceId,Aggressiveness aggressiveness){\r\n    AtomicLong freeSpace = new AtomicLong(0);\r\n    float shortAverage = deviceShort.getAverage();\r\n    float longAverage = deviceLong.getAverage();\r\n    float shortThreshold = shortAverage / (Aggressiveness.values().length - aggressiveness.ordinal());\r\n    float longThreshold = longAverage / (Aggressiveness.values().length - aggressiveness.ordinal());\r\n    AtomicInteger elementsDropped = new AtomicInteger(0);\r\n    AtomicInteger elementsMoved = new AtomicInteger(0);\r\n    AtomicInteger elementsSurvived = new AtomicInteger(0);\r\n    for (Long object : memoryHandler.getDeviceTrackingPoints(deviceId)) {\r\n        AllocationPoint point = getAllocationPoint(object);\r\n        if (point.getBuffer() == null) {\r\n            if (point.getAllocationStatus() == AllocationStatus.DEVICE) {\r\n                purgeDeviceObject(threadId, deviceId, object, point, false);\r\n                freeSpace.addAndGet(AllocationUtils.getRequiredMemory(point.getShape()));\r\n                purgeZeroObject(point.getBucketId(), object, point, false);\r\n                elementsDropped.incrementAndGet();\r\n                continue;\r\n            }\r\n            ;\r\n        } else {\r\n            elementsSurvived.incrementAndGet();\r\n        }\r\n    }\r\n    log.debug(\"Thread/Device [\" + threadId + \"/\" + deviceId + \"] elements purged: [\" + elementsDropped.get() + \"]; Relocated: [\" + elementsMoved.get() + \"]; Survivors: [\" + elementsSurvived.get() + \"]\");\r\n    return freeSpace.get();\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.BaseParameterUpdater.reset",
	"Comment": "reset internal counterssuch as number of updates accumulated.",
	"Method": "void reset(){\r\n    updateStorage.clear();\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getCategoriesAsync",
	"Comment": "gets the categories for a machine learning job, notifies listener once the requested buckets are retrieved.for additional infosee ml get categories documentation",
	"Method": "void getCategoriesAsync(GetCategoriesRequest request,RequestOptions options,ActionListener<GetCategoriesResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getCategories, options, GetCategoriesResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.XContentFactory.yamlBuilder",
	"Comment": "constructs a new yaml builder that will output the result into the provided output stream.",
	"Method": "XContentBuilder yamlBuilder(XContentBuilder yamlBuilder,OutputStream os){\r\n    return new XContentBuilder(YamlXContent.yamlXContent, os);\r\n}"
}, {
	"Path": "org.elasticsearch.index.rankeval.RatedRequest.getSummaryFields",
	"Comment": "returns a list of fields that should be included in the document summary for matched documents",
	"Method": "List<String> getSummaryFields(){\r\n    return Collections.unmodifiableList(summaryFields);\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.getStepNumber",
	"Comment": "this method returns step number. viable only in circular mode.",
	"Method": "long getStepNumber(){\r\n    return stepsCount.get();\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.DummyWorkspace.notifyScopeEntered",
	"Comment": "this method notifies given workspace that new use cycle is starting now",
	"Method": "MemoryWorkspace notifyScopeEntered(){\r\n    parentWorkspace = Nd4j.getMemoryManager().getCurrentWorkspace();\r\n    Nd4j.getMemoryManager().setCurrentWorkspace(null);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getDatafeedStats",
	"Comment": "gets statistics for one or more machine learning datafeedsfor additional infosee get datafeed stats docs",
	"Method": "GetDatafeedStatsResponse getDatafeedStats(GetDatafeedStatsRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getDatafeedStats, options, GetDatafeedStatsResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.script.mustache.MultiSearchTemplateResponse.getResponses",
	"Comment": "the list of responses, the order is the same as the one provided in the request.",
	"Method": "Item[] getResponses(){\r\n    return this.items;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.maxIndex",
	"Comment": "returns index of maximum element in a givenarray of doubles. first maximum is returned.",
	"Method": "int maxIndex(double[] doubles){\r\n    double maximum = 0;\r\n    int maxIndex = 0;\r\n    for (int i = 0; i < doubles.length; i++) {\r\n        if ((i == 0) || (doubles[i] > maximum)) {\r\n            maxIndex = i;\r\n            maximum = doubles[i];\r\n        }\r\n    }\r\n    return maxIndex;\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.NDArrayIndex.create",
	"Comment": "create from a matrix. the rows are the indicesthe columns are the individual element in each ndarrayindex",
	"Method": "INDArrayIndex[] create(INDArray index){\r\n    if (index.isMatrix()) {\r\n        if (index.rows() > Integer.MAX_VALUE)\r\n            throw new ND4JArraySizeException();\r\n        NDArrayIndex[] ret = new NDArrayIndex[(int) index.rows()];\r\n        for (int i = 0; i < index.rows(); i++) {\r\n            INDArray row = index.getRow(i);\r\n            val nums = new long[(int) index.getRow(i).columns()];\r\n            for (int j = 0; j < row.columns(); j++) {\r\n                nums[j] = (int) row.getFloat(j);\r\n            }\r\n            NDArrayIndex idx = new NDArrayIndex(nums);\r\n            ret[i] = idx;\r\n        }\r\n        return ret;\r\n    } else if (index.isVector()) {\r\n        long[] indices = NDArrayUtil.toLongs(index);\r\n        return new NDArrayIndex[] { new NDArrayIndex(indices) };\r\n    }\r\n    throw new IllegalArgumentException(\"Passed in ndarray must be a matrix or a vector\");\r\n}"
}, {
	"Path": "org.elasticsearch.client.core.TermVectorsRequest.setPreference",
	"Comment": "set a preference of which shard copies to execute the request",
	"Method": "void setPreference(String preference){\r\n    this.preference = preference;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IngestClient.putPipeline",
	"Comment": "add a pipeline or update an existing pipeline.see put pipeline api on elastic.co",
	"Method": "AcknowledgedResponse putPipeline(PutPipelineRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IngestRequestConverters::putPipeline, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.reindexAsync",
	"Comment": "asynchronously executes a reindex request.see reindex api on elastic.co",
	"Method": "void reindexAsync(ReindexRequest reindexRequest,RequestOptions options,ActionListener<BulkByScrollResponse> listener){\r\n    performRequestAsyncAndParseEntity(reindexRequest, RequestConverters::reindex, options, BulkByScrollResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.NioSelector.executeListener",
	"Comment": "executes a success listener with consistent exception handling. this can only be called from currentselector thread.",
	"Method": "void executeListener(BiConsumer<V, Exception> listener,V value){\r\n    assertOnSelectorThread();\r\n    try {\r\n        listener.accept(value, null);\r\n    } catch (Exception e) {\r\n        eventHandler.listenerException(e);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.serializer.MultiMinMaxSerializerStrategy.restore",
	"Comment": "restore a multinormalizerminmaxscaler that was previously serialized by this strategy",
	"Method": "MultiNormalizerMinMaxScaler restore(InputStream stream){\r\n    DataInputStream dis = new DataInputStream(stream);\r\n    boolean fitLabels = dis.readBoolean();\r\n    int numInputs = dis.readInt();\r\n    int numOutputs = dis.readInt();\r\n    double targetMin = dis.readDouble();\r\n    double targetMax = dis.readDouble();\r\n    MultiNormalizerMinMaxScaler result = new MultiNormalizerMinMaxScaler(targetMin, targetMax);\r\n    result.fitLabel(fitLabels);\r\n    List<MinMaxStats> featureStats = new ArrayList();\r\n    for (int i = 0; i < numInputs; i++) {\r\n        featureStats.add(new MinMaxStats(Nd4j.read(dis), Nd4j.read(dis)));\r\n    }\r\n    result.setFeatureStats(featureStats);\r\n    if (fitLabels) {\r\n        List<MinMaxStats> labelStats = new ArrayList();\r\n        for (int i = 0; i < numOutputs; i++) {\r\n            labelStats.add(new MinMaxStats(Nd4j.read(dis), Nd4j.read(dis)));\r\n        }\r\n        result.setLabelStats(labelStats);\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.support.ArrayValuesSourceAggregationBuilder.missingMap",
	"Comment": "gets the value to use when the aggregation finds a missing value in adocument",
	"Method": "AB missingMap(Map<String, Object> missingMap,Map<String, Object> missingMap){\r\n    return missingMap;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.allocation.ClusterAllocationExplainRequestBuilder.setCurrentNode",
	"Comment": "requests the explain api to explain an already assigned replica shard currently allocated tothe given node.",
	"Method": "ClusterAllocationExplainRequestBuilder setCurrentNode(String currentNode){\r\n    request.setCurrentNode(currentNode);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.jita.handler.impl.CudaZeroHandler.free",
	"Comment": "this method frees memory chunk specified by pointer and location",
	"Method": "void free(AllocationPoint point,AllocationStatus target){\r\n    if (point.getAllocationStatus() == AllocationStatus.DEVICE)\r\n        deviceMemoryTracker.subFromAllocation(Thread.currentThread().getId(), point.getDeviceId(), AllocationUtils.getRequiredMemory(point.getShape()));\r\n    memoryProvider.free(point);\r\n}"
}, {
	"Path": "org.elasticsearch.painless.ScriptTestCase.scriptContexts",
	"Comment": "script contexts used to build the script engine. override to customize which script contexts are available.",
	"Method": "Map<ScriptContext<?>, List<Whitelist>> scriptContexts(){\r\n    Map<ScriptContext<?>, List<Whitelist>> contexts = new HashMap();\r\n    contexts.put(PainlessTestScript.CONTEXT, Whitelist.BASE_WHITELISTS);\r\n    return contexts;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestClient.selectNodes",
	"Comment": "select nodes to try and sorts them so that the first one will be tried initially, then the following onesif the previous attempt failed and so on. package private for testing.",
	"Method": "Iterable<Node> selectNodes(NodeTuple<List<Node>> nodeTuple,Map<HttpHost, DeadHostState> blacklist,AtomicInteger lastNodeIndex,NodeSelector nodeSelector){\r\n    List<Node> livingNodes = new ArrayList(nodeTuple.nodes.size() - blacklist.size());\r\n    List<DeadNode> deadNodes = new ArrayList(blacklist.size());\r\n    for (Node node : nodeTuple.nodes) {\r\n        DeadHostState deadness = blacklist.get(node.getHost());\r\n        if (deadness == null) {\r\n            livingNodes.add(node);\r\n            continue;\r\n        }\r\n        if (deadness.shallBeRetried()) {\r\n            livingNodes.add(node);\r\n            continue;\r\n        }\r\n        deadNodes.add(new DeadNode(node, deadness));\r\n    }\r\n    if (false == livingNodes.isEmpty()) {\r\n        List<Node> selectedLivingNodes = new ArrayList(livingNodes);\r\n        nodeSelector.select(selectedLivingNodes);\r\n        if (false == selectedLivingNodes.isEmpty()) {\r\n            Collections.rotate(selectedLivingNodes, lastNodeIndex.getAndIncrement());\r\n            return selectedLivingNodes;\r\n        }\r\n    }\r\n    if (false == deadNodes.isEmpty()) {\r\n        final List<DeadNode> selectedDeadNodes = new ArrayList(deadNodes);\r\n        nodeSelector.select(new Iterable<Node>() {\r\n            @Override\r\n            public Iterator<Node> iterator() {\r\n                return new DeadNodeIteratorAdapter(selectedDeadNodes.iterator());\r\n            }\r\n        });\r\n        if (false == selectedDeadNodes.isEmpty()) {\r\n            return singletonList(Collections.min(selectedDeadNodes).node);\r\n        }\r\n    }\r\n    throw new IOException(\"NodeSelector [\" + nodeSelector + \"] rejected all nodes, \" + \"living \" + livingNodes + \" and dead \" + deadNodes);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestClient.selectNodes",
	"Comment": "select nodes to try and sorts them so that the first one will be tried initially, then the following onesif the previous attempt failed and so on. package private for testing.",
	"Method": "Iterable<Node> selectNodes(NodeTuple<List<Node>> nodeTuple,Map<HttpHost, DeadHostState> blacklist,AtomicInteger lastNodeIndex,NodeSelector nodeSelector){\r\n    return new DeadNodeIteratorAdapter(selectedDeadNodes.iterator());\r\n}"
}, {
	"Path": "io.dropwizard.logging.json.layout.JsonFormatter.toJson",
	"Comment": "converts the provided map as a json object according to the configured json mapper.",
	"Method": "String toJson(Map<String, Object> map){\r\n    if (map == null || map.isEmpty()) {\r\n        return null;\r\n    }\r\n    final StringWriter writer = new StringWriter(bufferSize);\r\n    try {\r\n        objectMapper.writeValue(writer, map);\r\n    } catch (IOException e) {\r\n        throw new IllegalArgumentException(\"Unable to format map as a JSON\", e);\r\n    }\r\n    if (doesAppendLineSeparator) {\r\n        writer.append(CoreConstants.LINE_SEPARATOR);\r\n    }\r\n    return writer.toString();\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.listenerException",
	"Comment": "this method is called when a listener attached to a channel operation throws an exception.",
	"Method": "void listenerException(Exception exception){\r\n    exceptionHandler.accept(exception);\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.time.impl.BinaryTimer.getFrequencyOfEvents",
	"Comment": "this method returns average frequency of events happened within predefined timeframe",
	"Method": "double getFrequencyOfEvents(){\r\n    if (isAlive()) {\r\n        return 1;\r\n    } else {\r\n        return 0;\r\n    }\r\n}"
}, {
	"Path": "io.dropwizard.logging.LoggingUtil.hijackJDKLogging",
	"Comment": "gets the root j.u.l.logger and removes all registered handlersthen redirects all active j.u.l. to slf4jn.b. this should only happen once, hence the flag and locking",
	"Method": "void hijackJDKLogging(){\r\n    JUL_HIJACKING_LOCK.lock();\r\n    try {\r\n        if (!julHijacked) {\r\n            SLF4JBridgeHandler.removeHandlersForRootLogger();\r\n            SLF4JBridgeHandler.install();\r\n            julHijacked = true;\r\n        }\r\n    } finally {\r\n        JUL_HIJACKING_LOCK.unlock();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT.testNormalization",
	"Comment": "test usage of the decomposition option for unicode normalization.",
	"Method": "void testNormalization(){\r\n    String index = \"foo\";\r\n    String type = \"mytype\";\r\n    String[] equivalent = { \"I WI?LL USE TURKİSH CASING\", \"ı will use turkish casıng\" };\r\n    XContentBuilder builder = jsonBuilder().startObject().startObject(\"properties\").startObject(\"collate\").field(\"type\", \"icu_collation_keyword\").field(\"language\", \"tr\").field(\"strength\", \"primary\").field(\"decomposition\", \"canonical\").endObject().endObject().endObject();\r\n    assertAcked(client().admin().indices().prepareCreate(index).addMapping(type, builder));\r\n    indexRandom(true, client().prepareIndex(index, type, \"1\").setSource(\"{\\\"collate\\\":\\\"\" + equivalent[0] + \"\\\"}\", XContentType.JSON), client().prepareIndex(index, type, \"2\").setSource(\"{\\\"collate\\\":\\\"\" + equivalent[1] + \"\\\"}\", XContentType.JSON));\r\n    SearchRequest request = new SearchRequest().indices(index).types(type).source(// secondary sort should kick in because both will collate to same value\r\n    new SearchSourceBuilder().fetchSource(false).query(QueryBuilders.termQuery(\"collate\", randomBoolean() ? equivalent[0] : equivalent[1])).sort(\"collate\").sort(\"_id\", SortOrder.DESC));\r\n    SearchResponse response = client().search(request).actionGet();\r\n    assertNoFailures(response);\r\n    assertHitCount(response, 2L);\r\n    assertOrderedSearchHits(response, \"2\", \"1\");\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.accumulate",
	"Comment": "this method sums given arrays and stores them to a given target array",
	"Method": "INDArray accumulate(Collection<INDArray> arrays,INDArray accumulate,INDArray arrays,INDArray accumulate,INDArray target,Collection<INDArray> arrays,INDArray accumulate,INDArray target,INDArray[] arrays){\r\n    if (arrays == null || arrays.length == 0)\r\n        return target;\r\n    return factory().accumulate(target, arrays);\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.DummyWorkspace.getLastCycleAllocations",
	"Comment": "this method returns amount of memory consumed in last successful cycle, in bytes",
	"Method": "long getLastCycleAllocations(){\r\n    return 0;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.util.MessageSplitter.isTrackedMessage",
	"Comment": "this method checks, if specified message id is being tracked",
	"Method": "boolean isTrackedMessage(String messageId,boolean isTrackedMessage,VoidChunk chunk){\r\n    return isTrackedMessage(chunk.getOriginalId());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.deleteByQuery",
	"Comment": "executes a delete by query request.see delete by query api on elastic.co",
	"Method": "BulkByScrollResponse deleteByQuery(DeleteByQueryRequest deleteByQueryRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(deleteByQueryRequest, RequestConverters::deleteByQuery, options, BulkByScrollResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.BalanceMinibatchesTest.ordering",
	"Comment": "the ordering for this testthis test will only be invoked forthe given testand ignored for others",
	"Method": "char ordering(){\r\n    return 'c';\r\n}"
}, {
	"Path": "org.elasticsearch.painless.lookup.PainlessLookupUtility.buildPainlessConstructorKey",
	"Comment": "constructs a painless constructor key used to lookup painless constructors from a painless class.",
	"Method": "String buildPainlessConstructorKey(int constructorArity){\r\n    return CONSTRUCTOR_NAME + \"/\" + constructorArity;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.binomial",
	"Comment": "generates a binomial distributed number usingthe given rng",
	"Method": "int binomial(RandomGenerator rng,int n,double p){\r\n    if ((p < 0) || (p > 1)) {\r\n        return 0;\r\n    }\r\n    int c = 0;\r\n    for (int i = 0; i < n; i++) {\r\n        if (rng.nextDouble() < p) {\r\n            c++;\r\n        }\r\n    }\r\n    return c;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.fieldCaps",
	"Comment": "executes a request using the field capabilities api.see field capabilities apion elastic.co.",
	"Method": "FieldCapabilitiesResponse fieldCaps(FieldCapabilitiesRequest fieldCapabilitiesRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(fieldCapabilitiesRequest, RequestConverters::fieldCaps, options, FieldCapabilitiesResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.putJob",
	"Comment": "creates a new machine learning jobfor additional infosee ml put job documentation",
	"Method": "PutJobResponse putJob(PutJobRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::putJob, options, PutJobResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.TasksClient.cancelAsync",
	"Comment": "asynchronously cancel one or more cluster tasks using the task management api.see task management api on elastic.co",
	"Method": "void cancelAsync(CancelTasksRequest cancelTasksRequest,RequestOptions options,ActionListener<CancelTasksResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(cancelTasksRequest, TasksRequestConverters::cancelTasks, options, CancelTasksResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.allocation.ClusterAllocationExplanation.getShardAllocationDecision",
	"Comment": "returns the shard allocation decision for attempting to assign or move the shard.",
	"Method": "ShardAllocationDecision getShardAllocationDecision(){\r\n    return shardAllocationDecision;\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.Int16.getDescriptor",
	"Comment": "this method returns compression descriptor. it should be unique for any compressor implementation",
	"Method": "String getDescriptor(){\r\n    return \"INT16\";\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.getAlias",
	"Comment": "gets one or more aliases using the get index aliases api.seeindices aliases api onelastic.co",
	"Method": "GetAliasesResponse getAlias(GetAliasesRequest getAliasesRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(getAliasesRequest, IndicesRequestConverters::getAlias, options, GetAliasesResponse::fromXContent, singleton(RestStatus.NOT_FOUND.getStatus()));\r\n}"
}, {
	"Path": "org.elasticsearch.client.WatcherClient.activateWatchAsync",
	"Comment": "asynchronously activates a watch from the clustersee the docs for more.",
	"Method": "void activateWatchAsync(ActivateWatchRequest request,RequestOptions options,ActionListener<ActivateWatchResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, WatcherRequestConverters::activateWatch, options, ActivateWatchResponse::fromXContent, listener, singleton(404));\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.createAsync",
	"Comment": "asynchronously creates an index using the create index api.see create index api on elastic.co",
	"Method": "void createAsync(CreateIndexRequest createIndexRequest,RequestOptions options,ActionListener<CreateIndexResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(createIndexRequest, IndicesRequestConverters::createIndex, options, CreateIndexResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.ContextExampleTests.testScriptFieldsScript",
	"Comment": "testing only params, as i am not sure how to test script doc values in painless",
	"Method": "void testScriptFieldsScript(){\r\n    Map<String, Object> hit = new HashMap();\r\n    Map<String, Object> fields = new HashMap();\r\n    fields.put(\"number-of-actors\", 4);\r\n    hit.put(\"fields\", fields);\r\n    Map<String, Object> source = new HashMap();\r\n    String[] actors = { \"James Holland\", \"Krissy Smith\", \"Joe Muir\", \"Ryan Earns\" };\r\n    source.put(\"actors\", actors);\r\n    assertEquals(hit, exec(\"Map fields = new HashMap();\" + \"fields[\\\"number-of-actors\\\"] = params['_source']['actors'].length;\" + \"Map rtn = new HashMap();\" + \"rtn[\\\"fields\\\"] = fields;\" + \"return rtn;\", singletonMap(\"_source\", source), true));\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteResponse.getState",
	"Comment": "returns the cluster state resulted from the cluster reroute request execution",
	"Method": "ClusterState getState(){\r\n    return this.state;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.NDArrayMath.tensorsPerSlice",
	"Comment": "computes the tensors per slicegiven a tensor shape and array",
	"Method": "long tensorsPerSlice(INDArray arr,int[] tensorShape){\r\n    return lengthPerSlice(arr) / ArrayUtil.prod(tensorShape);\r\n}"
}, {
	"Path": "org.elasticsearch.painless.CastTests.testArgumentsDef",
	"Comment": "currently these do not adopt the argument value, we issue a separate cast!",
	"Method": "void testArgumentsDef(){\r\n    assertEquals(5, exec(\"def x = 5L; return (+(int)x);\"));\r\n    assertEquals(6, exec(\"def x = 5; def y = 1L; return x + (int)y\"));\r\n    assertEquals('b', exec(\"def x = 'abcdeg'; def y = 1L; x.charAt((int)y)\"));\r\n}"
}, {
	"Path": "org.elasticsearch.upgrades.FullClusterRestartIT.testEmptyShard",
	"Comment": "tests that a single empty shard index is correctly recovered. empty shards are often an edge case.",
	"Method": "void testEmptyShard(){\r\n    final String index = \"test_empty_shard\";\r\n    if (isRunningAgainstOldCluster()) {\r\n        Settings.Builder settings = // before timing out\r\n        Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1).put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 1).put(INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), // fail faster\r\n        \"100ms\").put(SETTING_ALLOCATION_MAX_RETRY.getKey(), \"0\");\r\n        createIndex(index, settings.build());\r\n    }\r\n    ensureGreen(index);\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.training.impl.SkipGramTrainer.aggregationFinished",
	"Comment": "this method is invoked after particular aggregation finished",
	"Method": "void aggregationFinished(VoidAggregation aggregation){\r\n    SkipGramChain chain = chains.get(RequestDescriptor.createDescriptor(aggregation.getOriginatorId(), aggregation.getTaskId()));\r\n    if (chain == null) {\r\n        throw new RuntimeException(\"sI_\" + transport.getShardIndex() + \" Unable to find chain for specified originatorId: [\" + aggregation.getOriginatorId() + \"]; taskId: [\" + aggregation.getTaskId() + \"]\");\r\n    }\r\n    chain.addElement((DotAggregation) aggregation);\r\n    finishTraining(aggregation.getOriginatorId(), aggregation.getTaskId());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.api.Augmentation.asList",
	"Comment": "converts this iterable to a list. returns the original iterable if it is already a list.",
	"Method": "List<T> asList(Iterable<T> receiver){\r\n    if (receiver instanceof List) {\r\n        return (List<T>) receiver;\r\n    }\r\n    List<T> list = new ArrayList();\r\n    for (T t : receiver) {\r\n        list.add(t);\r\n    }\r\n    return list;\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.Gzip.getCompressionType",
	"Comment": "this method returns compression optype provided by specific ndarraycompressor implementation",
	"Method": "CompressionType getCompressionType(){\r\n    return CompressionType.LOSSLESS;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.DeleteJobRequest.setWaitForCompletion",
	"Comment": "set whether this request should wait until the operation has completed before returning",
	"Method": "void setWaitForCompletion(Boolean waitForCompletion){\r\n    this.waitForCompletion = waitForCompletion;\r\n}"
}, {
	"Path": "org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory.average",
	"Comment": "this method averages input arrays, and returns averaged array",
	"Method": "INDArray average(INDArray target,INDArray[] arrays,INDArray average,INDArray target,Collection<INDArray> arrays,INDArray average,INDArray[] arrays,INDArray average,Collection<INDArray> arrays){\r\n    return average(arrays.toArray(new INDArray[0]));\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteForecast",
	"Comment": "deletes machine learning job forecastsfor additional infosee delete job forecastdocumentation",
	"Method": "AcknowledgedResponse deleteForecast(DeleteForecastRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::deleteForecast, options, AcknowledgedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.util.StringUtils.humanReadableInt",
	"Comment": "given an integer, return a string that is in an approximate, but humanreadable format.",
	"Method": "String humanReadableInt(long number){\r\n    return TraditionalBinaryPrefix.long2String(number, \"\", 1);\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.calcStrides",
	"Comment": "computes the standard packed array strides for a given shape.",
	"Method": "int[] calcStrides(int[] shape,int startValue,long[] calcStrides,long[] shape,int startValue,int[] calcStrides,int[] shape,long[] calcStrides,long[] shape){\r\n    return calcStrides(shape, 1);\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getCalendarEvents",
	"Comment": "gets the events for a machine learning calendarfor additional infoseeget calendar events api",
	"Method": "GetCalendarEventsResponse getCalendarEvents(GetCalendarEventsRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getCalendarEvents, options, GetCalendarEventsResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.allocation.ClusterAllocationExplainRequestBuilder.setIncludeDiskInfo",
	"Comment": "whether to include information about the gathered disk information of nodes in the cluster",
	"Method": "ClusterAllocationExplainRequestBuilder setIncludeDiskInfo(boolean includeDiskInfo){\r\n    request.includeDiskInfo(includeDiskInfo);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.http.nio.cors.NioCorsHandler.setPreflightHeaders",
	"Comment": "this is a non cors specification feature which enables the setting of preflightresponse headers that might be required by intermediaries.",
	"Method": "void setPreflightHeaders(HttpResponse response){\r\n    response.headers().add(config.preflightResponseHeaders());\r\n}"
}, {
	"Path": "org.nd4j.graph.GraphInferenceGrpcClient.dropGraph",
	"Comment": "this method allows to remove graph from the graphserver instance",
	"Method": "void dropGraph(long graphId){\r\n    val builder = new FlatBufferBuilder(128);\r\n    val off = FlatDropRequest.createFlatDropRequest(builder, graphId);\r\n    builder.finish(off);\r\n    val req = FlatDropRequest.getRootAsFlatDropRequest(builder.dataBuffer());\r\n    val v = blockingStub.forgetGraph(req);\r\n    if (v.status() != 0)\r\n        throw new ND4JIllegalStateException(\"registerGraph() gRPC call failed\");\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.testBulkRejectionsRetryAndFailAnyway",
	"Comment": "mimicks bulk rejections. these should be retried but we fail anyway because we run out of retries.",
	"Method": "void testBulkRejectionsRetryAndFailAnyway(){\r\n    bulkRetryTestCase(true);\r\n    assertEquals(testRequest.getMaxRetries(), testTask.getStatus().getBulkRetries());\r\n}"
}, {
	"Path": "org.elasticsearch.http.netty4.cors.Netty4CorsConfig.isOriginAllowed",
	"Comment": "returns whether the input origin is allowed by this configuration.",
	"Method": "boolean isOriginAllowed(String origin){\r\n    if (origins.isPresent()) {\r\n        return origins.get().contains(origin);\r\n    } else if (pattern.isPresent()) {\r\n        return pattern.get().matcher(origin).matches();\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests.testSecondaryStrength",
	"Comment": "test secondary strength, for english case is not significant.",
	"Method": "void testSecondaryStrength(){\r\n    Settings settings = Settings.builder().put(\"index.analysis.filter.myCollator.type\", \"icu_collation\").put(\"index.analysis.filter.myCollator.language\", \"en\").put(\"index.analysis.filter.myCollator.strength\", \"secondary\").put(\"index.analysis.filter.myCollator.decomposition\", \"no\").build();\r\n    TestAnalysis analysis = createTestAnalysis(new Index(\"test\", \"_na_\"), settings, new AnalysisICUPlugin());\r\n    TokenFilterFactory filterFactory = analysis.tokenFilter.get(\"myCollator\");\r\n    assertCollatesToSame(filterFactory, \"TESTING\", \"testing\");\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.Float16.getCompressionType",
	"Comment": "this method returns compression optype provided by specific ndarraycompressor implementation",
	"Method": "CompressionType getCompressionType(){\r\n    return CompressionType.LOSSY;\r\n}"
}, {
	"Path": "org.elasticsearch.client.CustomRestHighLevelClientTests.mockPerformRequest",
	"Comment": "mocks the synchronous request execution like if it was executed by elasticsearch.",
	"Method": "Response mockPerformRequest(Request request){\r\n    assertThat(request.getOptions().getHeaders(), hasSize(1));\r\n    Header httpHeader = request.getOptions().getHeaders().get(0);\r\n    final Response mockResponse = mock(Response.class);\r\n    when(mockResponse.getHost()).thenReturn(new HttpHost(\"localhost\", 9200));\r\n    ProtocolVersion protocol = new ProtocolVersion(\"HTTP\", 1, 1);\r\n    when(mockResponse.getStatusLine()).thenReturn(new BasicStatusLine(protocol, 200, \"OK\"));\r\n    MainResponse response = new MainResponse(httpHeader.getValue(), Version.CURRENT, ClusterName.DEFAULT, \"_na\", Build.CURRENT);\r\n    BytesRef bytesRef = XContentHelper.toXContent(response, XContentType.JSON, false).toBytesRef();\r\n    when(mockResponse.getEntity()).thenReturn(new ByteArrayEntity(bytesRef.bytes, ContentType.APPLICATION_JSON));\r\n    RequestLine requestLine = new BasicRequestLine(HttpGet.METHOD_NAME, ENDPOINT, protocol);\r\n    when(mockResponse.getRequestLine()).thenReturn(requestLine);\r\n    return mockResponse;\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.BaseNDArrayFactory.bilinearProducts",
	"Comment": "returns a column vector where each entry is the nth bilinearproduct of the nth slices of the two tensors.",
	"Method": "INDArray bilinearProducts(INDArray curr,INDArray in){\r\n    Preconditions.checkArgument(curr.rank() == 3, \"Argument 'curr' must be rank 3. Got input with rank: %s\", curr.rank());\r\n    if (in.columns() != 1) {\r\n        throw new AssertionError(\"Expected a column vector\");\r\n    }\r\n    if (in.rows() != curr.size(curr.shape().length - 1)) {\r\n        throw new AssertionError(\"Number of rows in the input does not match number of columns in tensor\");\r\n    }\r\n    if (curr.size(curr.shape().length - 2) != curr.size(curr.shape().length - 1)) {\r\n        throw new AssertionError(\"Can only perform this operation on a SimpleTensor with square slices\");\r\n    }\r\n    INDArray ret = Nd4j.create(curr.slices(), 1);\r\n    INDArray inT = in.transpose();\r\n    for (int i = 0; i < curr.slices(); i++) {\r\n        INDArray slice = curr.slice(i);\r\n        INDArray inTTimesSlice = inT.mmul(slice);\r\n        ret.putScalar(i, Nd4j.getBlasWrapper().dot(inTTimesSlice, in));\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.ops.executioner.CudaGridExecutioner.isMatchingZXY",
	"Comment": "this method is additional check, basically it qualifies possibility of invertedpredicate metaop",
	"Method": "boolean isMatchingZXY(Op opA,Op opB){\r\n    if (opA.z() == opB.x() || opA.z() == opB.y())\r\n        return true;\r\n    return false;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.logic.completion.Clipboard.unpin",
	"Comment": "this method removes given voidaggregation from clipboard, and returns it",
	"Method": "VoidAggregation unpin(VoidAggregation aggregation,VoidAggregation unpin,long originatorId,long taskId){\r\n    RequestDescriptor descriptor = RequestDescriptor.createDescriptor(originatorId, taskId);\r\n    VoidAggregation aggregation;\r\n    if ((aggregation = clipboard.get(descriptor)) != null) {\r\n        clipboard.remove(descriptor);\r\n        trackingCounter.decrementAndGet();\r\n        return aggregation;\r\n    } else\r\n        return null;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.getSettingsAsync",
	"Comment": "asynchronously retrieve the settings of one or more indices.see indices get settings api on elastic.co",
	"Method": "void getSettingsAsync(GetSettingsRequest getSettingsRequest,RequestOptions options,ActionListener<GetSettingsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(getSettingsRequest, IndicesRequestConverters::getSettings, options, GetSettingsResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.deleteAsync",
	"Comment": "asynchronously deletes a document by id using the delete api.see delete api on elastic.co",
	"Method": "void deleteAsync(DeleteRequest deleteRequest,RequestOptions options,ActionListener<DeleteResponse> listener){\r\n    performRequestAsyncAndParseEntity(deleteRequest, RequestConverters::delete, options, DeleteResponse::fromXContent, listener, Collections.singleton(404));\r\n}"
}, {
	"Path": "org.elasticsearch.painless.api.Augmentation.collect",
	"Comment": "iterates through this map transforming each entry into a new value using the function, adding the values to the specified collection.",
	"Method": "List<U> collect(Collection<T> receiver,Function<T, U> function,Object collect,Collection<T> receiver,Collection<U> collection,Function<T, U> function,List<T> collect,Map<K, V> receiver,BiFunction<K, V, T> function,Object collect,Map<K, V> receiver,Collection<T> collection,BiFunction<K, V, T> function){\r\n    for (Map.Entry<K, V> kvPair : receiver.entrySet()) {\r\n        collection.add(function.apply(kvPair.getKey(), kvPair.getValue()));\r\n    }\r\n    return collection;\r\n}"
}, {
	"Path": "org.elasticsearch.client.sniff.ElasticsearchNodesSniffer.sniff",
	"Comment": "calls the elasticsearch nodes info api, parses the response and returns all the found http hosts",
	"Method": "List<Node> sniff(){\r\n    Response response = restClient.performRequest(request);\r\n    return readHosts(response.getEntity(), scheme, jsonFactory);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.startILM",
	"Comment": "start the index lifecycle management feature.see the docs for more.",
	"Method": "AcknowledgedResponse startILM(StartILMRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IndexLifecycleRequestConverters::startILM, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.rand",
	"Comment": "fill the given ndarray with random numbers drawn from a uniform distribution using the given randomgenerator",
	"Method": "INDArray rand(int[] shape,INDArray rand,long[] shape,INDArray rand,char order,int[] shape,INDArray rand,int rows,int columns,INDArray rand,char order,int rows,int columns,INDArray rand,int[] shape,long seed,INDArray rand,int rows,int columns,long seed,INDArray rand,int[] shape,org.nd4j.linalg.api.rng.Random rng,INDArray rand,int[] shape,Distribution dist,INDArray rand,long[] shape,Distribution dist,INDArray rand,int rows,int columns,org.nd4j.linalg.api.rng.Random rng,INDArray rand,int[] shape,double min,double max,org.nd4j.linalg.api.rng.Random rng,INDArray rand,long[] shape,double min,double max,org.nd4j.linalg.api.rng.Random rng,INDArray rand,int rows,int columns,double min,double max,org.nd4j.linalg.api.rng.Random rng,INDArray rand,INDArray target,INDArray rand,INDArray target,long seed,INDArray rand,INDArray target,org.nd4j.linalg.api.rng.Random rng,INDArray rand,INDArray target,Distribution dist,INDArray rand,INDArray target,double min,double max,org.nd4j.linalg.api.rng.Random rng){\r\n    if (min > max)\r\n        throw new IllegalArgumentException(\"the maximum value supplied is smaller than the minimum\");\r\n    return getExecutioner().exec(new UniformDistribution(target, min, max), rng);\r\n}"
}, {
	"Path": "org.nd4j.linalg.dimensionalityreduction.RandomProjection.project",
	"Comment": "create a copy random projection by using matrix product with a random matrix",
	"Method": "INDArray project(INDArray data,INDArray project,INDArray data,INDArray result){\r\n    long[] tShape = targetShape(data.shape(), eps, components, autoMode);\r\n    return data.mmuli(getProjectionMatrix(tShape, this.rng), result);\r\n}"
}, {
	"Path": "org.nd4j.jita.handler.impl.CudaZeroHandler.getDeviceTrackingPoints",
	"Comment": "this method returns set of allocation tracking ids for specific device",
	"Method": "Set<Long> getDeviceTrackingPoints(Integer deviceId){\r\n    return deviceAllocations.get(deviceId).keySet();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse.getActiveShardsPercent",
	"Comment": "the percentage of active shards, should be 100% in a green system",
	"Method": "double getActiveShardsPercent(){\r\n    return clusterStateHealth.getActiveShardsPercent();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.get.GetTaskRequest.setTimeout",
	"Comment": "timeout to wait for any async actions this request must take. it must take anywhere from 0 to 2.",
	"Method": "GetTaskRequest setTimeout(TimeValue timeout){\r\n    this.timeout = timeout;\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.shape.Shape.getMaxShape",
	"Comment": "return the shape of the largest length arraybased on the input",
	"Method": "long[] getMaxShape(INDArray inputs){\r\n    if (inputs == null)\r\n        return null;\r\n    else if (inputs.length < 2)\r\n        return inputs[0].shape();\r\n    else {\r\n        long[] currMax = inputs[0].shape();\r\n        for (int i = 1; i < inputs.length; i++) {\r\n            if (inputs[i] == null) {\r\n                continue;\r\n            }\r\n            if (ArrayUtil.prod(currMax) < inputs[i].length()) {\r\n                currMax = inputs[i].shape();\r\n            }\r\n        }\r\n        return currMax;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.CcrClient.putFollow",
	"Comment": "executes the put follow api, which creates a follower index and then the follower index starts followingthe leader index.see the docs for more.",
	"Method": "PutFollowResponse putFollow(PutFollowRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, CcrRequestConverters::putFollow, options, PutFollowResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.cli.Command.addShutdownHook",
	"Comment": "return whether or not to install the shutdown hook to cleanup resources on exit. this method should only be overridden in testclasses.",
	"Method": "boolean addShutdownHook(){\r\n    return true;\r\n}"
}, {
	"Path": "io.dropwizard.auth.AuthFilter.authenticate",
	"Comment": "authenticates a request with user credentials and setup the security context.",
	"Method": "boolean authenticate(ContainerRequestContext requestContext,C credentials,String scheme){\r\n    try {\r\n        if (credentials == null) {\r\n            return false;\r\n        }\r\n        final Optional<P> principal = authenticator.authenticate(credentials);\r\n        if (!principal.isPresent()) {\r\n            return false;\r\n        }\r\n        final SecurityContext securityContext = requestContext.getSecurityContext();\r\n        final boolean secure = securityContext != null && securityContext.isSecure();\r\n        requestContext.setSecurityContext(new SecurityContext() {\r\n            @Override\r\n            public Principal getUserPrincipal() {\r\n                return principal.get();\r\n            }\r\n            @Override\r\n            public boolean isUserInRole(String role) {\r\n                return authorizer.authorize(principal.get(), role);\r\n            }\r\n            @Override\r\n            public boolean isSecure() {\r\n                return secure;\r\n            }\r\n            @Override\r\n            public String getAuthenticationScheme() {\r\n                return scheme;\r\n            }\r\n        });\r\n        return true;\r\n    } catch (AuthenticationException e) {\r\n        logger.warn(\"Error authenticating credentials\", e);\r\n        throw new InternalServerErrorException();\r\n    }\r\n}"
}, {
	"Path": "io.dropwizard.auth.AuthFilter.authenticate",
	"Comment": "authenticates a request with user credentials and setup the security context.",
	"Method": "boolean authenticate(ContainerRequestContext requestContext,C credentials,String scheme){\r\n    return principal.get();\r\n}"
}, {
	"Path": "io.dropwizard.auth.AuthFilter.authenticate",
	"Comment": "authenticates a request with user credentials and setup the security context.",
	"Method": "boolean authenticate(ContainerRequestContext requestContext,C credentials,String scheme){\r\n    return authorizer.authorize(principal.get(), role);\r\n}"
}, {
	"Path": "io.dropwizard.auth.AuthFilter.authenticate",
	"Comment": "authenticates a request with user credentials and setup the security context.",
	"Method": "boolean authenticate(ContainerRequestContext requestContext,C credentials,String scheme){\r\n    return secure;\r\n}"
}, {
	"Path": "io.dropwizard.auth.AuthFilter.authenticate",
	"Comment": "authenticates a request with user credentials and setup the security context.",
	"Method": "boolean authenticate(ContainerRequestContext requestContext,C credentials,String scheme){\r\n    return scheme;\r\n}"
}, {
	"Path": "org.nd4j.linalg.dimensionalityreduction.PCA.reducedBasis",
	"Comment": "return a reduced basis set that covers a certain fraction of the variance of the data",
	"Method": "INDArray reducedBasis(double variance){\r\n    INDArray vars = Transforms.pow(eigenvalues, -0.5, true);\r\n    double res = vars.sumNumber().doubleValue();\r\n    double total = 0.0;\r\n    int ndims = 0;\r\n    for (int i = 0; i < vars.columns(); i++) {\r\n        ndims++;\r\n        total += vars.getDouble(i);\r\n        if (total / res > variance)\r\n            break;\r\n    }\r\n    INDArray result = Nd4j.create(eigenvectors.rows(), ndims);\r\n    for (int i = 0; i < ndims; i++) result.putColumn(i, eigenvectors.getColumn(i));\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getCalendarsAsync",
	"Comment": "gets a single or multiple calendars, notifies listener once the requested records are retrieved.for additional infosee ml get calendars documentation",
	"Method": "void getCalendarsAsync(GetCalendarsRequest request,RequestOptions options,ActionListener<GetCalendarsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getCalendars, options, GetCalendarsResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.DeleteJobRequest.setForce",
	"Comment": "used to forcefully delete an opened job.this method is quicker than closing and deleting the job.",
	"Method": "void setForce(Boolean force){\r\n    this.force = force;\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.registrationException",
	"Comment": "this method is called when an attempt to register a channel throws an exception.",
	"Method": "void registrationException(ChannelContext<?> context,Exception exception){\r\n    context.handleException(exception);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.putTemplateAsync",
	"Comment": "asynchronously puts an index template using the index templates api.seeindex templates apion elastic.co",
	"Method": "void putTemplateAsync(PutIndexTemplateRequest putIndexTemplateRequest,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(putIndexTemplateRequest, IndicesRequestConverters::putTemplate, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.Locals.getVariable",
	"Comment": "accesses a variable. this will throw iae if the variable does not exist",
	"Method": "Variable getVariable(Location location,String name){\r\n    Variable variable = lookupVariable(location, name);\r\n    if (variable != null) {\r\n        return variable;\r\n    }\r\n    if (parent != null) {\r\n        return parent.getVariable(location, name);\r\n    }\r\n    throw location.createError(new IllegalArgumentException(\"Variable [\" + name + \"] is not defined.\"));\r\n}"
}, {
	"Path": "org.nd4j.jita.handler.impl.CudaZeroHandler.getInitialLocation",
	"Comment": "this method returns initial allocation location. so, it can be host, or device if environment allows that.",
	"Method": "AllocationStatus getInitialLocation(){\r\n    return INITIAL_LOCATION;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.ModelParameterServer.addUpdaterParamsSubscriber",
	"Comment": "this method adds subcriber that will be called upon updater params receival",
	"Method": "void addUpdaterParamsSubscriber(Subscriber<INDArray> s){\r\n    updaterParamsSubscribers.add(s);\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.gce.GceDiscoverTests.assertNumberOfNodes",
	"Comment": "asserts that the cluster nodes info contains an expected number of node",
	"Method": "void assertNumberOfNodes(int expected){\r\n    assertEquals(expected, client().admin().cluster().prepareNodesInfo().clear().get().getNodes().size());\r\n}"
}, {
	"Path": "org.nd4j.linalg.workspace.EndlessWorkspaceTests.endlessTest1",
	"Comment": "this test checks for allocations within single workspace, without any spills",
	"Method": "void endlessTest1(){\r\n    Nd4j.getWorkspaceManager().setDefaultWorkspaceConfiguration(WorkspaceConfiguration.builder().initialSize(100 * 1024L * 1024L).build());\r\n    Nd4j.getMemoryManager().togglePeriodicGc(false);\r\n    AtomicLong counter = new AtomicLong(0);\r\n    while (true) {\r\n        try (MemoryWorkspace workspace = Nd4j.getWorkspaceManager().getAndActivateWorkspace()) {\r\n            long time1 = System.nanoTime();\r\n            INDArray array = Nd4j.create(1024 * 1024);\r\n            long time2 = System.nanoTime();\r\n            array.addi(1.0f);\r\n            assertEquals(1.0f, array.meanNumber().floatValue(), 0.1f);\r\n            if (counter.incrementAndGet() % 1000 == 0)\r\n                log.info(\"{} iterations passed... Allocation time: {} ns\", counter.get(), time2 - time1);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "io.dropwizard.Application.bootstrapLogLevel",
	"Comment": "the log level at which to bootstrap logging on application startup.",
	"Method": "Level bootstrapLogLevel(){\r\n    return Level.WARN;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.client.PreBuiltTransportClient.initializeNetty",
	"Comment": "netty wants to do some unwelcome things like use unsafe and replace a private field, or use a poorly considered buffer recycler. thismethod disables these things by default, but can be overridden by setting the corresponding system properties.",
	"Method": "void initializeNetty(){\r\n    setSystemPropertyIfUnset(\"io.netty.noUnsafe\", Boolean.toString(true));\r\n    setSystemPropertyIfUnset(\"io.netty.noKeySetOptimization\", Boolean.toString(true));\r\n    setSystemPropertyIfUnset(\"io.netty.recycler.maxCapacityPerThread\", Integer.toString(0));\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.shards.ClusterSearchShardsRequest.routing",
	"Comment": "the routing values to control the shards that the search will be executed on.",
	"Method": "String routing(ClusterSearchShardsRequest routing,String routing,ClusterSearchShardsRequest routing,String routings){\r\n    this.routing = Strings.arrayToCommaDelimitedString(routings);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.shape.Shape.createShapeInformation",
	"Comment": "creates the shape information buffergiven the shape,stride",
	"Method": "DataBuffer createShapeInformation(int[] shape,int[] stride,long offset,int elementWiseStride,char order,DataBuffer createShapeInformation,long[] shape,long[] stride,long offset,long elementWiseStride,char order){\r\n    offset = 0;\r\n    if (shape.length != stride.length)\r\n        throw new IllegalStateException(\"Shape and stride must be the same length\");\r\n    int rank = shape.length;\r\n    long[] shapeBuffer = new long[rank * 2 + 4];\r\n    shapeBuffer[0] = rank;\r\n    int count = 1;\r\n    for (int e = 0; e < shape.length; e++) shapeBuffer[count++] = shape[e];\r\n    for (int e = 0; e < stride.length; e++) shapeBuffer[count++] = stride[e];\r\n    shapeBuffer[count++] = (int) offset;\r\n    shapeBuffer[count++] = elementWiseStride;\r\n    shapeBuffer[count] = (int) order;\r\n    DataBuffer ret = Nd4j.createBufferDetached(shapeBuffer);\r\n    ret.setConstant(true);\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.refresh",
	"Comment": "refresh one or more indices using the refresh api.seerefresh api on elastic.co",
	"Method": "RefreshResponse refresh(RefreshRequest refreshRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(refreshRequest, IndicesRequestConverters::refresh, options, RefreshResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.deleteUser",
	"Comment": "removes user from the native realm synchronously.see the docs for more.",
	"Method": "DeleteUserResponse deleteUser(DeleteUserRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, SecurityRequestConverters::deleteUser, options, DeleteUserResponse::fromXContent, singleton(404));\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.deleteByQueryRethrottleAsync",
	"Comment": "asynchronously execute an delete by query rethrottle request.see delete by query api on elastic.co",
	"Method": "void deleteByQueryRethrottleAsync(RethrottleRequest rethrottleRequest,RequestOptions options,ActionListener<ListTasksResponse> listener){\r\n    performRequestAsyncAndParseEntity(rethrottleRequest, RequestConverters::rethrottleDeleteByQuery, options, ListTasksResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.Node.getBoundHosts",
	"Comment": "addresses on which the host is listening. these are useful to havearound because they allow you to find a host based on any address itis listening on.",
	"Method": "Set<HttpHost> getBoundHosts(){\r\n    return boundHosts;\r\n}"
}, {
	"Path": "org.nd4j.util.ArchiveUtils.zipExtractSingleFile",
	"Comment": "extract a single file from a .zip file. does not support directories",
	"Method": "void zipExtractSingleFile(File zipFile,File destination,String pathInZip){\r\n    try (ZipFile zf = new ZipFile(zipFile);\r\n        InputStream is = new BufferedInputStream(zf.getInputStream(zf.getEntry(pathInZip)));\r\n        OutputStream os = new BufferedOutputStream(new FileOutputStream(destination))) {\r\n        IOUtils.copy(is, os);\r\n    }\r\n}"
}, {
	"Path": "io.dropwizard.hibernate.AbstractDAO.uniqueResult",
	"Comment": "convenience method to return a single instance that matches the query, or null if the queryreturns no results.",
	"Method": "E uniqueResult(CriteriaQuery<E> criteriaQuery,E uniqueResult,Criteria criteria,E uniqueResult,Query<E> query){\r\n    return requireNonNull(query).uniqueResult();\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.searchTemplateAsync",
	"Comment": "asynchronously executes a request using the search template api.see search template apion elastic.co.",
	"Method": "void searchTemplateAsync(SearchTemplateRequest searchTemplateRequest,RequestOptions options,ActionListener<SearchTemplateResponse> listener){\r\n    performRequestAsyncAndParseEntity(searchTemplateRequest, RequestConverters::searchTemplate, options, SearchTemplateResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.process.DataCounts.getInvalidDateCount",
	"Comment": "the number of records with an invalid date field that couldnot be parsed or converted to epoch time.",
	"Method": "long getInvalidDateCount(){\r\n    return invalidDateCount;\r\n}"
}, {
	"Path": "io.dropwizard.testing.junit.ResourceTestRule.target",
	"Comment": "creates a web target to be sent to the resource under testing.",
	"Method": "WebTarget target(String path){\r\n    return resource.target(path);\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.storage.InMemoryUpdateStorage.doGetUpdate",
	"Comment": "a method for actually performing the implementationof retrieving the ndarray",
	"Method": "NDArrayMessage doGetUpdate(int index){\r\n    return updates.get(index);\r\n}"
}, {
	"Path": "org.nd4j.jita.flow.impl.SynchronousFlowController.synchronizeToHost",
	"Comment": "this method makes sure host memory contains latest data from gpu",
	"Method": "void synchronizeToHost(AllocationPoint point){\r\n    if (!point.isActualOnHostSide()) {\r\n        CudaContext context = (CudaContext) allocator.getDeviceContext().getContext();\r\n        if (!point.isConstant())\r\n            waitTillFinished(point);\r\n        if (point.getAllocationStatus() == AllocationStatus.DEVICE && !point.isActualOnHostSide()) {\r\n            long perfD = PerformanceTracker.getInstance().helperStartTransaction();\r\n            if (nativeOps.memcpyAsync(point.getHostPointer(), point.getDevicePointer(), AllocationUtils.getRequiredMemory(point.getShape()), CudaConstants.cudaMemcpyDeviceToHost, context.getSpecialStream()) == 0)\r\n                throw new IllegalStateException(\"MemcpyAsync failed: \" + point.getShape());\r\n            commitTransfer(context.getSpecialStream());\r\n            PerformanceTracker.getInstance().helperRegisterTransaction(point.getDeviceId(), perfD, point.getNumberOfBytes(), MemcpyDirection.DEVICE_TO_HOST);\r\n        }\r\n        point.tickHostRead();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.repositories.put.PutRepositoryRequest.verify",
	"Comment": "returns true if repository should be verified after creation",
	"Method": "PutRepositoryRequest verify(boolean verify,boolean verify){\r\n    return this.verify;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.ForecastJobRequest.setDuration",
	"Comment": "set the forecast durationa period of time that indicates how far into the future to forecast.the default value is 1 day. the forecast starts at the last record that was processed.",
	"Method": "void setDuration(TimeValue duration){\r\n    this.duration = duration;\r\n}"
}, {
	"Path": "org.elasticsearch.plugin.noop.action.search.NoopSearchRequestBuilder.setTerminateAfter",
	"Comment": "an optional document count, upon collecting which the searchquery will early terminate",
	"Method": "NoopSearchRequestBuilder setTerminateAfter(int terminateAfter){\r\n    sourceBuilder().terminateAfter(terminateAfter);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.flushSyncedAsync",
	"Comment": "asynchronously initiate a synced flush manually using the synced flush api.see synced flush api on elastic.co",
	"Method": "void flushSyncedAsync(SyncedFlushRequest syncedFlushRequest,RequestOptions options,ActionListener<SyncedFlushResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(syncedFlushRequest, IndicesRequestConverters::flushSynced, options, SyncedFlushResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.existsAsync",
	"Comment": "asynchronously checks for the existence of a document. returns true if it exists, false otherwise.see get api on elastic.co",
	"Method": "void existsAsync(GetRequest getRequest,RequestOptions options,ActionListener<Boolean> listener){\r\n    performRequestAsync(getRequest, RequestConverters::exists, options, RestHighLevelClient::convertExistsResponse, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.revertModelSnapshotAsync",
	"Comment": "reverts to a particular machine learning model snapshot asynchronously and notifies the listener on completionfor additional infosee ml revert model snapshot documentation",
	"Method": "void revertModelSnapshotAsync(RevertModelSnapshotRequest request,RequestOptions options,ActionListener<RevertModelSnapshotResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::revertModelSnapshot, options, RevertModelSnapshotResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.stopDatafeed",
	"Comment": "stops the given machine learning datafeedfor additional infosee ml stop datafeed documentation",
	"Method": "StopDatafeedResponse stopDatafeed(StopDatafeedRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::stopDatafeed, options, StopDatafeedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.api.Augmentation.each",
	"Comment": "iterates through a map, passing each item to the given consumer.",
	"Method": "Object each(Iterable<T> receiver,Consumer<T> consumer,Object each,Map<K, V> receiver,BiConsumer<K, V> consumer){\r\n    receiver.forEach(consumer);\r\n    return receiver;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.results.Bucket.getRecords",
	"Comment": "get all the anomaly records associated with this bucket.the records are not part of the bucket document. they willonly be present when the bucket was retrieved and expandedto contain the associated records.",
	"Method": "List<AnomalyRecord> getRecords(){\r\n    return records;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestClient.getNodes",
	"Comment": "get the list of nodes that the client knows about. the list isunmodifiable.",
	"Method": "List<Node> getNodes(){\r\n    return nodeTuple.nodes;\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.verifyRepository",
	"Comment": "verifies a snapshot repository.seesnapshot and restoreapi on elastic.co",
	"Method": "VerifyRepositoryResponse verifyRepository(VerifyRepositoryRequest verifyRepositoryRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(verifyRepositoryRequest, SnapshotRequestConverters::verifyRepository, options, VerifyRepositoryResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestClientSingleHostIntegTests.testPreemptiveAuthDisabled",
	"Comment": "verify that credentials are not sent on the first request with preemptive auth disabled.",
	"Method": "void testPreemptiveAuthDisabled(){\r\n    final String[] methods = { \"POST\", \"PUT\", \"GET\", \"DELETE\" };\r\n    try (RestClient restClient = createRestClient(true, false)) {\r\n        for (final String method : methods) {\r\n            final Response response = bodyTest(restClient, method);\r\n            assertThat(response.getHeader(\"Authorization\"), nullValue());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.rolloverAsync",
	"Comment": "asynchronously rolls over an index using the rollover index api.see rollover index api on elastic.co",
	"Method": "void rolloverAsync(RolloverRequest rolloverRequest,RequestOptions options,ActionListener<RolloverResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(rolloverRequest, IndicesRequestConverters::rollover, options, RolloverResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "io.dropwizard.auth.CachingAuthenticator.stats",
	"Comment": "returns a set of statistics about the cache contents and usage.",
	"Method": "CacheStats stats(){\r\n    return cache.stats();\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.performRequestAndParseOptionalEntity",
	"Comment": "defines a helper method for requests that can 404 and in which case will return an empty optional otherwise tries to parse the response body",
	"Method": "Optional<Resp> performRequestAndParseOptionalEntity(Req request,CheckedFunction<Req, Request, IOException> requestConverter,RequestOptions options,CheckedFunction<XContentParser, Resp, IOException> entityParser){\r\n    Optional<ValidationException> validationException = request.validate();\r\n    if (validationException != null && validationException.isPresent()) {\r\n        throw validationException.get();\r\n    }\r\n    Request req = requestConverter.apply(request);\r\n    req.setOptions(options);\r\n    Response response;\r\n    try {\r\n        response = client.performRequest(req);\r\n    } catch (ResponseException e) {\r\n        if (RestStatus.NOT_FOUND.getStatus() == e.getResponse().getStatusLine().getStatusCode()) {\r\n            return Optional.empty();\r\n        }\r\n        throw parseResponseException(e);\r\n    }\r\n    try {\r\n        return Optional.of(parseEntity(response.getEntity(), entityParser));\r\n    } catch (Exception e) {\r\n        throw new IOException(\"Unable to parse response body for \" + response, e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.deleteAsync",
	"Comment": "asynchronously deletes a snapshot.seesnapshot and restoreapi on elastic.co",
	"Method": "void deleteAsync(DeleteSnapshotRequest deleteSnapshotRequest,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(deleteSnapshotRequest, SnapshotRequestConverters::deleteSnapshot, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.MethodWriter.box",
	"Comment": "proxy the box method to use valueof instead to ensure that the modern boxing methods are used.",
	"Method": "void box(Type type){\r\n    valueOf(type);\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.toDecimal",
	"Comment": "this will convert the given binary string to a decimal basedinteger",
	"Method": "int toDecimal(String binary){\r\n    long num = Long.parseLong(binary);\r\n    long rem;\r\n    while (num > 0) {\r\n        rem = num % 10;\r\n        num = num / 10;\r\n        if (rem != 0 && rem != 1) {\r\n            System.out.println(\"This is not a binary number.\");\r\n            System.out.println(\"Please try once again.\");\r\n            return -1;\r\n        }\r\n    }\r\n    int i = Integer.parseInt(binary, 2);\r\n    return i;\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.InstallPluginCommand.loadPluginInfo",
	"Comment": "load information about the plugin, and verify it can be installed with no errors.",
	"Method": "PluginInfo loadPluginInfo(Terminal terminal,Path pluginRoot,Environment env){\r\n    final PluginInfo info = PluginInfo.readFromProperties(pluginRoot);\r\n    if (info.hasNativeController()) {\r\n        throw new IllegalStateException(\"plugins can not have native controllers\");\r\n    }\r\n    PluginsService.verifyCompatibility(info);\r\n    verifyPluginName(env.pluginsFile(), info.getName());\r\n    PluginsService.checkForFailedPluginRemovals(env.pluginsFile());\r\n    terminal.println(VERBOSE, info.toString());\r\n    jarHellCheck(info, pluginRoot, env.pluginsFile(), env.modulesFile());\r\n    return info;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.VoidParameterServerStressTest.testPerformanceUnicast3",
	"Comment": "this test checks for single shard scenario, when shard is also a client",
	"Method": "void testPerformanceUnicast3(){\r\n    VoidConfiguration voidConfiguration = VoidConfiguration.builder().numberOfShards(1).shardAddresses(Arrays.asList(\"127.0.0.1:49823\")).build();\r\n    voidConfiguration.setUnicastControllerPort(49823);\r\n    Transport transport = new RoutedTransport();\r\n    transport.setIpAndPort(\"127.0.0.1\", Integer.valueOf(\"49823\"));\r\n    VoidParameterServer parameterServer = new VoidParameterServer(NodeRole.SHARD);\r\n    parameterServer.setShardIndex((short) 0);\r\n    parameterServer.init(voidConfiguration, transport, new CbowTrainer());\r\n    parameterServer.initializeSeqVec(100, NUM_WORDS, 123L, 100, true, false);\r\n    final List<Long> times = new ArrayList();\r\n    log.info(\"Starting loop...\");\r\n    for (int i = 0; i < 200; i++) {\r\n        Frame<CbowRequestMessage> frame = new Frame(BasicSequenceProvider.getInstance().getNextValue());\r\n        for (int f = 0; f < 128; f++) {\r\n            frame.stackMessage(getCRM());\r\n        }\r\n        long time1 = System.nanoTime();\r\n        parameterServer.execDistributed(frame);\r\n        long time2 = System.nanoTime();\r\n        times.add(time2 - time1);\r\n        if (i % 50 == 0)\r\n            log.info(\"{} frames passed...\", i);\r\n    }\r\n    Collections.sort(times);\r\n    log.info(\"p50: {} us\", times.get(times.size() / 2) / 1000);\r\n    parameterServer.shutdown();\r\n}"
}, {
	"Path": "org.elasticsearch.index.rankeval.MeanReciprocalRank.evaluate",
	"Comment": "compute reciprocalrank based on provided relevant document ids.",
	"Method": "EvalQueryQuality evaluate(String taskId,SearchHit[] hits,List<RatedDocument> ratedDocs){\r\n    List<RatedSearchHit> ratedHits = joinHitsWithRatings(hits, ratedDocs);\r\n    int firstRelevant = -1;\r\n    int rank = 1;\r\n    for (RatedSearchHit hit : ratedHits) {\r\n        OptionalInt rating = hit.getRating();\r\n        if (rating.isPresent()) {\r\n            if (rating.getAsInt() >= this.relevantRatingThreshhold) {\r\n                firstRelevant = rank;\r\n                break;\r\n            }\r\n        }\r\n        rank++;\r\n    }\r\n    double reciprocalRank = (firstRelevant == -1) ? 0 : 1.0d / firstRelevant;\r\n    EvalQueryQuality evalQueryQuality = new EvalQueryQuality(taskId, reciprocalRank);\r\n    evalQueryQuality.setMetricDetails(new Detail(firstRelevant));\r\n    evalQueryQuality.addHitsAndRatings(ratedHits);\r\n    return evalQueryQuality;\r\n}"
}, {
	"Path": "org.elasticsearch.search.fetch.subphase.highlight.AnnotatedPassageFormatter.mergeAnnotations",
	"Comment": "merge original annotations and search hits into a single set of markups for each passage",
	"Method": "MarkupPassage mergeAnnotations(AnnotationToken[] annotations,Passage passage){\r\n    try {\r\n        MarkupPassage markupPassage = new MarkupPassage();\r\n        for (int i = 0; i < passage.getNumMatches(); i++) {\r\n            int start = passage.getMatchStarts()[i];\r\n            int end = passage.getMatchEnds()[i];\r\n            String searchTerm = passage.getMatchTerms()[i].utf8ToString();\r\n            Markup markup = new Markup(start, end, SEARCH_HIT_TYPE + \"=\" + URLEncoder.encode(searchTerm, StandardCharsets.UTF_8.name()));\r\n            markupPassage.addUnlessOverlapping(markup);\r\n        }\r\n        for (AnnotationToken token : annotations) {\r\n            int start = token.offset;\r\n            int end = token.endOffset;\r\n            if (start >= passage.getStartOffset() && end <= passage.getEndOffset()) {\r\n                String escapedValue = URLEncoder.encode(token.value, StandardCharsets.UTF_8.name());\r\n                Markup markup = new Markup(start, end, escapedValue);\r\n                markupPassage.addUnlessOverlapping(markup);\r\n            }\r\n        }\r\n        return markupPassage;\r\n    } catch (UnsupportedEncodingException e) {\r\n        throw new IllegalStateException(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.health.ClusterHealthRequestBuilder.setWaitForNoRelocatingShards",
	"Comment": "sets whether the request should wait for there to be no relocating shards beforeretrieving the cluster health status.defaults to false, meaning theoperation does not wait on there being no more relocating shards.set to trueto wait until the number of relocating shards in the cluster is 0.",
	"Method": "ClusterHealthRequestBuilder setWaitForNoRelocatingShards(boolean waitForRelocatingShards){\r\n    request.waitForNoRelocatingShards(waitForRelocatingShards);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.concurrency.RRWLock.attachObject",
	"Comment": "this method notifies locker, that specific object was added to tracking list",
	"Method": "void attachObject(Object object){\r\n    if (!objectLocks.containsKey(object))\r\n        objectLocks.put(object, new ReentrantReadWriteLock());\r\n}"
}, {
	"Path": "org.nd4j.camel.kafka.Nd4jKafkaProducer.publish",
	"Comment": "publish to a kafka topicbased on the connection information",
	"Method": "void publish(INDArray arr){\r\n    if (producerTemplate == null)\r\n        producerTemplate = camelContext.createProducerTemplate();\r\n    producerTemplate.sendBody(\"direct:start\", arr);\r\n}"
}, {
	"Path": "io.dropwizard.testing.common.DAOTest.getSessionFactory",
	"Comment": "returns the current active session factory for injecting to daos.",
	"Method": "SessionFactory getSessionFactory(){\r\n    return sessionFactory;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.messages.aggregations.VectorAggregation.processMessage",
	"Comment": "vector aggregations are saved only by shards started aggregation process. all other shards are ignoring this meesage",
	"Method": "void processMessage(){\r\n    if (clipboard.isTracking(this.originatorId, this.getTaskId())) {\r\n        clipboard.pin(this);\r\n        if (clipboard.isReady(this.originatorId, taskId)) {\r\n            VoidAggregation aggregation = clipboard.unpin(this.originatorId, taskId);\r\n            if (aggregation == null)\r\n                return;\r\n            VectorCompleteMessage msg = new VectorCompleteMessage(taskId, aggregation.getAccumulatedResult());\r\n            msg.setOriginatorId(aggregation.getOriginatorId());\r\n            transport.sendMessage(msg);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.transport.RoutedTransport.jointMessageHandler",
	"Comment": "this message handler is responsible for receiving messages on any side of p2p network",
	"Method": "void jointMessageHandler(DirectBuffer buffer,int offset,int length,Header header){\r\n    byte[] data = new byte[length];\r\n    buffer.getBytes(offset, data);\r\n    VoidMessage message = VoidMessage.fromBytes(data);\r\n    if (message instanceof MeaningfulMessage) {\r\n        MeaningfulMessage msg = (MeaningfulMessage) message;\r\n        completed.put(message.getTaskId(), msg);\r\n    } else if (message instanceof RequestMessage) {\r\n        try {\r\n            messages.put((RequestMessage) message);\r\n        } catch (InterruptedException e) {\r\n        } catch (Exception e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    } else if (message instanceof DistributedMessage) {\r\n        try {\r\n            messages.put((DistributedMessage) message);\r\n        } catch (InterruptedException e) {\r\n        } catch (Exception e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    } else if (message instanceof TrainingMessage) {\r\n        try {\r\n            messages.put((TrainingMessage) message);\r\n        } catch (InterruptedException e) {\r\n        } catch (Exception e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    } else if (message instanceof VoidAggregation) {\r\n        try {\r\n            messages.put((VoidAggregation) message);\r\n        } catch (InterruptedException e) {\r\n        } catch (Exception e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    } else if (message instanceof Frame) {\r\n        try {\r\n            messages.put((Frame) message);\r\n        } catch (InterruptedException e) {\r\n        } catch (Exception e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    } else {\r\n        log.info(\"Unknown message: {}\", message.getClass().getSimpleName());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.settings.ClusterUpdateSettingsRequestBuilder.setPersistentSettings",
	"Comment": "sets the persistent settings to be updated. they will get applied cross restarts",
	"Method": "ClusterUpdateSettingsRequestBuilder setPersistentSettings(Settings settings,ClusterUpdateSettingsRequestBuilder setPersistentSettings,Settings.Builder settings,ClusterUpdateSettingsRequestBuilder setPersistentSettings,String settings,XContentType xContentType,ClusterUpdateSettingsRequestBuilder setPersistentSettings,Map<String, ?> settings){\r\n    request.persistentSettings(settings);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.jdbc.mysql.MysqlLoaderTest.testMysqlLoader",
	"Comment": "simple litmus test, unfortunately relies on an external database",
	"Method": "void testMysqlLoader(){\r\n    ComboPooledDataSource ds = new ComboPooledDataSource();\r\n    ds.setJdbcUrl(\"jdbc:mysql://localhost:3306/nd4j?user=nd4j&password=nd4j\");\r\n    MysqlLoader loader = new MysqlLoader(ds, \"jdbc:mysql://localhost:3306/nd4j?user=nd4j&password=nd4j\", \"ndarrays\", \"array\");\r\n    loader.delete(\"1\");\r\n    INDArray load = loader.load(loader.loadForID(\"1\"));\r\n    if (load != null) {\r\n        loader.delete(\"1\");\r\n    }\r\n    loader.save(Nd4j.create(new float[] { 1, 2, 3 }), \"1\");\r\n    Blob b = loader.loadForID(\"1\");\r\n    INDArray loaded = loader.load(b);\r\n    assertEquals((Nd4j.create(new float[] { 1, 2, 3 })), loaded);\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteJobAsync",
	"Comment": "deletes the given machine learning job asynchronously and notifies the listener on completionfor additional infosee ml delete job documentation",
	"Method": "void deleteJobAsync(DeleteJobRequest request,RequestOptions options,ActionListener<DeleteJobResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::deleteJob, options, DeleteJobResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.process.ModelSizeStats.getLogTime",
	"Comment": "the wall clock time at the point when this instance was created.",
	"Method": "Date getLogTime(){\r\n    return logTime;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.messages.intercom.DistributedSolidMessage.processMessage",
	"Comment": "this method will be started in context of executor, either shard, client or backup node",
	"Method": "void processMessage(){\r\n    if (overwrite)\r\n        storage.setArray(key, payload);\r\n    else if (!storage.arrayExists(key))\r\n        storage.setArray(key, payload);\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.RevertModelSnapshotRequest.setDeleteInterveningResults",
	"Comment": "sets the request flag that indicates whether or not intervening results should be deleted.",
	"Method": "void setDeleteInterveningResults(Boolean deleteInterveningResults){\r\n    this.deleteInterveningResults = deleteInterveningResults;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.allUnique",
	"Comment": "returns true if all of the elements in thegiven int array are unique",
	"Method": "boolean allUnique(int[] toTest){\r\n    Set<Integer> set = new HashSet();\r\n    for (int i : toTest) {\r\n        if (!set.contains(i))\r\n            set.add(i);\r\n        else\r\n            return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "io.dropwizard.testing.junit5.DAOTestExtension.getSessionFactory",
	"Comment": "returns the current active session factory for injecting to daos.",
	"Method": "SessionFactory getSessionFactory(){\r\n    return daoTest.getSessionFactory();\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.uncaughtException",
	"Comment": "this method handles an exception that was uncaught during a select loop.",
	"Method": "void uncaughtException(Exception exception){\r\n    Thread thread = Thread.currentThread();\r\n    thread.getUncaughtExceptionHandler().uncaughtException(thread, exception);\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.DataSet.getColumnNames",
	"Comment": "optional column names of the data transform, this is mainly usedfor interpeting what columns are in the dataset",
	"Method": "List<String> getColumnNames(){\r\n    return columnNames;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.explainLifecycleAsync",
	"Comment": "asynchronously explain the lifecycle state for an indexsee the docs for more.",
	"Method": "void explainLifecycleAsync(ExplainLifecycleRequest request,RequestOptions options,ActionListener<ExplainLifecycleResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IndexLifecycleRequestConverters::explainLifecycle, options, ExplainLifecycleResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.upgrades.FullClusterRestartIT.testRecovery",
	"Comment": "tests recovery of an index with or without a translog and thestatistics we gather about that.",
	"Method": "void testRecovery(){\r\n    int count;\r\n    boolean shouldHaveTranslog;\r\n    if (isRunningAgainstOldCluster()) {\r\n        count = between(200, 300);\r\n        shouldHaveTranslog = randomBoolean();\r\n        indexRandomDocuments(count, true, true, i -> jsonBuilder().startObject().field(\"field\", \"value\").endObject());\r\n        ensureGreen(index);\r\n        if (randomBoolean()) {\r\n            assertBusy(() -> {\r\n                try {\r\n                    Response resp = client().performRequest(new Request(\"POST\", index + \"/_flush/synced\"));\r\n                    Map<String, Object> result = ObjectPath.createFromResponse(resp).evaluate(\"_shards\");\r\n                    assertThat(result.get(\"successful\"), equalTo(result.get(\"total\")));\r\n                    assertThat(result.get(\"failed\"), equalTo(0));\r\n                } catch (ResponseException ex) {\r\n                    throw new AssertionError(ex);\r\n                }\r\n            });\r\n        } else {\r\n            assertOK(client().performRequest(new Request(\"POST\", \"/_flush\")));\r\n        }\r\n        if (shouldHaveTranslog) {\r\n            indexRandomDocuments(count / 10, false, false, i -> jsonBuilder().startObject().field(\"field\", \"value\").endObject());\r\n        }\r\n        saveInfoDocument(\"should_have_translog\", Boolean.toString(shouldHaveTranslog));\r\n    } else {\r\n        count = countOfIndexedRandomDocuments();\r\n        shouldHaveTranslog = Booleans.parseBoolean(loadInfoDocument(\"should_have_translog\"));\r\n    }\r\n    Request countRequest = new Request(\"GET\", \"/\" + index + \"/_search\");\r\n    countRequest.addParameter(\"size\", \"0\");\r\n    String countResponse = toStr(client().performRequest(countRequest));\r\n    assertThat(countResponse, containsString(\"\\\"total\\\":\" + count));\r\n    if (false == isRunningAgainstOldCluster()) {\r\n        boolean restoredFromTranslog = false;\r\n        boolean foundPrimary = false;\r\n        Request recoveryRequest = new Request(\"GET\", \"/_cat/recovery/\" + index);\r\n        recoveryRequest.addParameter(\"h\", \"index,shard,type,stage,translog_ops_recovered\");\r\n        recoveryRequest.addParameter(\"s\", \"index,shard,type\");\r\n        String recoveryResponse = toStr(client().performRequest(recoveryRequest));\r\n        for (String line : recoveryResponse.split(\"\\n\")) {\r\n            foundPrimary = true;\r\n            if (false == line.contains(\"done\") && line.contains(\"existing_store\")) {\r\n                continue;\r\n            }\r\n            Matcher m = Pattern.compile(\"(\\\\d+)$\").matcher(line);\r\n            assertTrue(line, m.find());\r\n            int translogOps = Integer.parseInt(m.group(1));\r\n            if (translogOps > 0) {\r\n                restoredFromTranslog = true;\r\n            }\r\n        }\r\n        assertTrue(\"expected to find a primary but didn't\\n\" + recoveryResponse, foundPrimary);\r\n        assertEquals(\"mismatch while checking for translog recovery\\n\" + recoveryResponse, shouldHaveTranslog, restoredFromTranslog);\r\n        String currentLuceneVersion = Version.CURRENT.luceneVersion.toString();\r\n        String bwcLuceneVersion = getOldClusterVersion().luceneVersion.toString();\r\n        if (shouldHaveTranslog && false == currentLuceneVersion.equals(bwcLuceneVersion)) {\r\n            int numCurrentVersion = 0;\r\n            int numBwcVersion = 0;\r\n            Request segmentsRequest = new Request(\"GET\", \"/_cat/segments/\" + index);\r\n            segmentsRequest.addParameter(\"h\", \"prirep,shard,index,version\");\r\n            segmentsRequest.addParameter(\"s\", \"prirep,shard,index\");\r\n            String segmentsResponse = toStr(client().performRequest(segmentsRequest));\r\n            for (String line : segmentsResponse.split(\"\\n\")) {\r\n                if (false == line.startsWith(\"p\")) {\r\n                    continue;\r\n                }\r\n                Matcher m = Pattern.compile(\"(\\\\d+\\\\.\\\\d+\\\\.\\\\d+)$\").matcher(line);\r\n                assertTrue(line, m.find());\r\n                String version = m.group(1);\r\n                if (currentLuceneVersion.equals(version)) {\r\n                    numCurrentVersion++;\r\n                } else if (bwcLuceneVersion.equals(version)) {\r\n                    numBwcVersion++;\r\n                } else {\r\n                    fail(\"expected version to be one of [\" + currentLuceneVersion + \",\" + bwcLuceneVersion + \"] but was \" + line);\r\n                }\r\n            }\r\n            assertNotEquals(\"expected at least 1 current segment after translog recovery. segments:\\n\" + segmentsResponse, 0, numCurrentVersion);\r\n            assertNotEquals(\"expected at least 1 old segment. segments:\\n\" + segmentsResponse, 0, numBwcVersion);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.InstallPluginCommand.download",
	"Comment": "downloads the plugin and returns the file it was downloaded to.",
	"Method": "Path download(Terminal terminal,String pluginId,Path tmpDir){\r\n    if (OFFICIAL_PLUGINS.contains(pluginId)) {\r\n        final String url = getElasticUrl(terminal, getStagingHash(), Version.CURRENT, isSnapshot(), pluginId, Platforms.PLATFORM_NAME);\r\n        terminal.println(\"-> Downloading \" + pluginId + \" from elastic\");\r\n        return downloadAndValidate(terminal, url, tmpDir, true);\r\n    }\r\n    String[] coordinates = pluginId.split(\":\");\r\n    if (coordinates.length == 3 && pluginId.contains(\"/\") == false && pluginId.startsWith(\"file:\") == false) {\r\n        String mavenUrl = getMavenUrl(terminal, coordinates, Platforms.PLATFORM_NAME);\r\n        terminal.println(\"-> Downloading \" + pluginId + \" from maven central\");\r\n        return downloadAndValidate(terminal, mavenUrl, tmpDir, false);\r\n    }\r\n    if (pluginId.contains(\":\") == false) {\r\n        List<String> plugins = checkMisspelledPlugin(pluginId);\r\n        String msg = \"Unknown plugin \" + pluginId;\r\n        if (plugins.isEmpty() == false) {\r\n            msg += \", did you mean \" + (plugins.size() == 1 ? \"[\" + plugins.get(0) + \"]\" : \"any of \" + plugins.toString()) + \"?\";\r\n        }\r\n        throw new UserException(ExitCodes.USAGE, msg);\r\n    }\r\n    terminal.println(\"-> Downloading \" + URLDecoder.decode(pluginId, \"UTF-8\"));\r\n    return downloadZip(terminal, pluginId, tmpDir);\r\n}"
}, {
	"Path": "org.nd4j.versioncheck.VersionCheck.versionInfoString",
	"Comment": "get the version information for dependencies as a string with a specified amount of detail",
	"Method": "String versionInfoString(String versionInfoString,Detail detail){\r\n    StringBuilder sb = new StringBuilder();\r\n    for (VersionInfo grp : getVersionInfos()) {\r\n        sb.append(grp.getGroupId()).append(\" : \").append(grp.getArtifactId()).append(\" : \").append(grp.getBuildVersion());\r\n        switch(detail) {\r\n            case FULL:\r\n            case GAVC:\r\n                sb.append(\" - \").append(grp.getCommitIdAbbrev());\r\n                if (detail != Detail.FULL)\r\n                    break;\r\n                sb.append(\"buildTime=\").append(grp.getBuildTime()).append(\"branch=\").append(grp.getBranch()).append(\"commitMsg=\").append(grp.getCommitMessageShort());\r\n        }\r\n        sb.append(\"\\n\");\r\n    }\r\n    return sb.toString();\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.provider.BasicWorkspaceManager.getWorkspaceForCurrentThread",
	"Comment": "this method will return workspace with default configuration and default id.",
	"Method": "MemoryWorkspace getWorkspaceForCurrentThread(MemoryWorkspace getWorkspaceForCurrentThread,String id){\r\n    return getWorkspaceForCurrentThread(defaultConfiguration, id);\r\n}"
}, {
	"Path": "org.elasticsearch.plugin.noop.action.search.NoopSearchRequestBuilder.setMinScore",
	"Comment": "sets the minimum score below which docs will be filtered out.",
	"Method": "NoopSearchRequestBuilder setMinScore(float minScore){\r\n    sourceBuilder().minScore(minScore);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.RetryTests.nodeSettings",
	"Comment": "lower the queue sizes to be small enough that both bulk and searches will time out and have to be retried.",
	"Method": "Settings nodeSettings(int nodeOrdinal,Settings nodeSettings){\r\n    return // whitelist reindexing from the HTTP host we're going to use\r\n    Settings.builder().put(TransportReindexAction.REMOTE_CLUSTER_WHITELIST.getKey(), \"127.0.0.1:*\").build();\r\n}"
}, {
	"Path": "org.elasticsearch.index.rankeval.RatedRequest.getParams",
	"Comment": "return the parameters if this request uses a template, otherwise this will be empty.",
	"Method": "Map<String, Object> getParams(){\r\n    return Collections.unmodifiableMap(this.params);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.indexAsync",
	"Comment": "asynchronously index a document using the index api.see index api on elastic.co",
	"Method": "void indexAsync(IndexRequest indexRequest,RequestOptions options,ActionListener<IndexResponse> listener){\r\n    performRequestAsyncAndParseEntity(indexRequest, RequestConverters::index, options, IndexResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteJob",
	"Comment": "deletes the given machine learning jobfor additional infosee ml delete job documentation",
	"Method": "DeleteJobResponse deleteJob(DeleteJobRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::deleteJob, options, DeleteJobResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.CcrClient.unfollow",
	"Comment": "instructs a follower index to unfollow and become a regular index.note that index following needs to be paused and the follower index needs to be closed.see the docs for more.",
	"Method": "AcknowledgedResponse unfollow(UnfollowRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, CcrRequestConverters::unfollow, options, AcknowledgedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.primitives.Counter.normalize",
	"Comment": "this method will apply normalization to counter values and totals.",
	"Method": "void normalize(){\r\n    for (T key : keySet()) {\r\n        setCount(key, getCount(key) / totalCount.get());\r\n    }\r\n    rebuildTotals();\r\n}"
}, {
	"Path": "org.elasticsearch.client.IngestClient.simulate",
	"Comment": "simulate a pipeline on a set of documents provided in the requestseesimulate pipeline api on elastic.co",
	"Method": "SimulatePipelineResponse simulate(SimulatePipelineRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IngestRequestConverters::simulatePipeline, options, SimulatePipelineResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.security.user.privileges.IndicesPrivileges.getQuery",
	"Comment": "a query limiting the visible documents in the indices. can be null, in whichcase all documents are visible.",
	"Method": "String getQuery(){\r\n    return this.query;\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AllocationPoint.getAllocationStatus",
	"Comment": "this method returns current allocationstatus for this point",
	"Method": "AllocationStatus getAllocationStatus(){\r\n    return allocationStatus;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.CompilerSettings.setRegexesEnabled",
	"Comment": "are regexes enabled? they are currently disabled by default because they break out of the loop counter and even fairly simplelooking regexes can cause stack overflows.",
	"Method": "void setRegexesEnabled(boolean regexesEnabled){\r\n    this.regexesEnabled = regexesEnabled;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.settings.ClusterUpdateSettingsRequest.persistentSettings",
	"Comment": "sets the persistent settings to be updated. they will get applied cross restarts",
	"Method": "Settings persistentSettings(ClusterUpdateSettingsRequest persistentSettings,Settings settings,ClusterUpdateSettingsRequest persistentSettings,Settings.Builder settings,ClusterUpdateSettingsRequest persistentSettings,String source,XContentType xContentType,ClusterUpdateSettingsRequest persistentSettings,Map<String, ?> source){\r\n    try {\r\n        XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON);\r\n        builder.map(source);\r\n        persistentSettings(Strings.toString(builder), builder.contentType());\r\n    } catch (IOException e) {\r\n        throw new ElasticsearchGenerationException(\"Failed to generate [\" + source + \"]\", e);\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.jita.memory.impl.CudaFullCachingProvider.ensureDeviceCacheHolder",
	"Comment": "this method checks, if storage contains holder for specified shape",
	"Method": "void ensureDeviceCacheHolder(Integer deviceId,AllocationShape shape){\r\n    if (!deviceCache.containsKey(deviceId)) {\r\n        try {\r\n            synchronized (this) {\r\n                if (!deviceCache.containsKey(deviceId)) {\r\n                    deviceCache.put(deviceId, new ConcurrentHashMap<AllocationShape, CacheHolder>());\r\n                }\r\n            }\r\n        } catch (Exception e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n    if (!deviceCache.get(deviceId).containsKey(shape)) {\r\n        try {\r\n            singleLock.acquire();\r\n            if (!deviceCache.get(deviceId).containsKey(shape)) {\r\n                deviceCache.get(deviceId).put(shape, new CacheHolder(shape, deviceCachedAmount.get(deviceId)));\r\n            }\r\n        } catch (Exception e) {\r\n        } finally {\r\n            singleLock.release();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.scrollAsync",
	"Comment": "asynchronously executes a search using the search scroll api.see search scrollapi on elastic.co",
	"Method": "void scrollAsync(SearchScrollRequest searchScrollRequest,RequestOptions options,ActionListener<SearchResponse> listener){\r\n    performRequestAsyncAndParseEntity(searchScrollRequest, RequestConverters::searchScroll, options, SearchResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.jita.conf.Configuration.setMaximumHostCache",
	"Comment": "this method allows you to specify maximum memory cache for host memory",
	"Method": "Configuration setMaximumHostCache(long maxCache){\r\n    this.maximumHostCache = maxCache;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.api.Augmentation.any",
	"Comment": "iterates over the contents of an iterable, and checks whether a predicate is valid for at least one element.",
	"Method": "boolean any(Iterable<T> receiver,Predicate<T> predicate){\r\n    for (T t : receiver) {\r\n        if (predicate.test(t)) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "io.dropwizard.configuration.ConfigurationFactory.build",
	"Comment": "loads, parses, binds, and validates a configuration object from an empty document.",
	"Method": "T build(ConfigurationSourceProvider provider,String path,T build,File file,T build){\r\n    return build(new FileConfigurationSourceProvider(), file.toString());\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.storage.RocksDbStorage.doGetUpdate",
	"Comment": "a method for actually performing the implementationof retrieving the ndarray",
	"Method": "NDArrayMessage doGetUpdate(int index){\r\n    byte[] key = ByteBuffer.allocate(4).putInt(index).array();\r\n    try {\r\n        UnsafeBuffer unsafeBuffer = new UnsafeBuffer(db.get(key));\r\n        return NDArrayMessage.fromBuffer(unsafeBuffer, 0);\r\n    } catch (RocksDBException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.painless.lookup.PainlessCast.unboxOriginalType",
	"Comment": "create a cast where the original type will be unboxed, and then the cast will be performed.",
	"Method": "PainlessCast unboxOriginalType(Class<?> originalType,Class<?> targetType,boolean explicitCast,Class<?> unboxOriginalType){\r\n    Objects.requireNonNull(originalType);\r\n    Objects.requireNonNull(targetType);\r\n    Objects.requireNonNull(unboxOriginalType);\r\n    return new PainlessCast(originalType, targetType, explicitCast, unboxOriginalType, null, null, null);\r\n}"
}, {
	"Path": "org.elasticsearch.client.ValidationException.addValidationError",
	"Comment": "add a new validation error to the accumulating validation errors",
	"Method": "void addValidationError(String error){\r\n    validationErrors.add(error);\r\n}"
}, {
	"Path": "org.elasticsearch.tools.java_version_checker.JavaVersionChecker.main",
	"Comment": "the main entry point. the exit code is 0 if the java version is at least 1.8, otherwise the exit code is 1.",
	"Method": "void main(String[] args){\r\n    if (args.length != 0) {\r\n        throw new IllegalArgumentException(\"expected zero arguments but was \" + Arrays.toString(args));\r\n    }\r\n    if (JavaVersion.compare(JavaVersion.CURRENT, JavaVersion.JAVA_8) < 0) {\r\n        final String message = String.format(Locale.ROOT, \"the minimum required Java version is 8; your Java version from [%s] does not meet this requirement\", System.getProperty(\"java.home\"));\r\n        errPrintln(message);\r\n        exit(1);\r\n    }\r\n    exit(0);\r\n}"
}, {
	"Path": "org.deeplearning4j.rl4j.learning.sync.Transition.concat",
	"Comment": "concat an array history into a single indarry of as many channelas element in the history array",
	"Method": "INDArray concat(INDArray[] history){\r\n    INDArray arr = Nd4j.concat(0, history);\r\n    return arr;\r\n}"
}, {
	"Path": "org.elasticsearch.client.NodesResponse.getHeader",
	"Comment": "gets information about the number of total, successful and failed nodes the request was run on.also includes exceptions if relevant.",
	"Method": "NodesResponseHeader getHeader(){\r\n    return header;\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.Int8.getCompressionType",
	"Comment": "this method returns compression optype provided by specific ndarraycompressor implementation",
	"Method": "CompressionType getCompressionType(){\r\n    return CompressionType.LOSSY;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.updateAliases",
	"Comment": "updates aliases using the index aliases api.see index aliases api on elastic.co",
	"Method": "AcknowledgedResponse updateAliases(IndicesAliasesRequest indicesAliasesRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(indicesAliasesRequest, IndicesRequestConverters::updateAliases, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.logic.ClipboardTest.testPin3",
	"Comment": "this test checks how clipboard handles singular aggregations",
	"Method": "void testPin3(){\r\n    Clipboard clipboard = new Clipboard();\r\n    Random rng = new Random(12345L);\r\n    Long validId = 123L;\r\n    InitializationAggregation aggregation = new InitializationAggregation(1, 0);\r\n    clipboard.pin(aggregation);\r\n    assertTrue(clipboard.isTracking(0L, aggregation.getTaskId()));\r\n    assertTrue(clipboard.isReady(0L, aggregation.getTaskId()));\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.mget",
	"Comment": "retrieves multiple documents by id using the multi get api.see multi get api on elastic.co",
	"Method": "MultiGetResponse mget(MultiGetRequest multiGetRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(multiGetRequest, RequestConverters::multiGet, options, MultiGetResponse::fromXContent, singleton(404));\r\n}"
}, {
	"Path": "org.elasticsearch.client.core.TermVectorsRequest.getId",
	"Comment": "returns the id of the requestcan be null if there is no document id",
	"Method": "String getId(){\r\n    return id;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.updateAsync",
	"Comment": "asynchronously updates a document using the update api.see update api on elastic.co",
	"Method": "void updateAsync(UpdateRequest updateRequest,RequestOptions options,ActionListener<UpdateResponse> listener){\r\n    performRequestAsyncAndParseEntity(updateRequest, RequestConverters::update, options, UpdateResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.ScaledFloatFieldMapper.objectToDouble",
	"Comment": "converts an object to a double by checking it against known types first",
	"Method": "double objectToDouble(Object value){\r\n    double doubleValue;\r\n    if (value instanceof Number) {\r\n        doubleValue = ((Number) value).doubleValue();\r\n    } else if (value instanceof BytesRef) {\r\n        doubleValue = Double.parseDouble(((BytesRef) value).utf8ToString());\r\n    } else {\r\n        doubleValue = Double.parseDouble(value.toString());\r\n    }\r\n    return doubleValue;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.azure.classic.AzureDiscoveryClusterFormationTests.startHttpd",
	"Comment": "creates mock ec2 endpoint providing the list of started nodes to the describeinstances api call",
	"Method": "void startHttpd(){\r\n    logDir = createTempDir();\r\n    SSLContext sslContext = getSSLContext();\r\n    httpsServer = MockHttpServer.createHttps(new InetSocketAddress(InetAddress.getLoopbackAddress().getHostAddress(), 0), 0);\r\n    httpsServer.setHttpsConfigurator(new HttpsConfigurator(sslContext));\r\n    httpsServer.createContext(\"/subscription/services/hostedservices/myservice\", (s) -> {\r\n        Headers headers = s.getResponseHeaders();\r\n        headers.add(\"Content-Type\", \"text/xml; charset=UTF-8\");\r\n        XMLOutputFactory xmlOutputFactory = XMLOutputFactory.newFactory();\r\n        xmlOutputFactory.setProperty(XMLOutputFactory.IS_REPAIRING_NAMESPACES, true);\r\n        StringWriter out = new StringWriter();\r\n        XMLStreamWriter sw;\r\n        try {\r\n            sw = xmlOutputFactory.createXMLStreamWriter(out);\r\n            sw.writeStartDocument();\r\n            String namespace = \"http://schemas.microsoft.com/windowsazure\";\r\n            sw.setDefaultNamespace(namespace);\r\n            sw.writeStartElement(XMLConstants.DEFAULT_NS_PREFIX, \"HostedService\", namespace);\r\n            {\r\n                sw.writeStartElement(\"Deployments\");\r\n                {\r\n                    Path[] files = FileSystemUtils.files(logDir);\r\n                    for (int i = 0; i < files.length; i++) {\r\n                        Path resolve = files[i].resolve(\"transport.ports\");\r\n                        if (Files.exists(resolve)) {\r\n                            List<String> addresses = Files.readAllLines(resolve);\r\n                            Collections.shuffle(addresses, random());\r\n                            String address = addresses.get(0);\r\n                            int indexOfLastColon = address.lastIndexOf(':');\r\n                            String host = address.substring(0, indexOfLastColon);\r\n                            String port = address.substring(indexOfLastColon + 1);\r\n                            sw.writeStartElement(\"Deployment\");\r\n                            {\r\n                                sw.writeStartElement(\"Name\");\r\n                                sw.writeCharacters(\"mydeployment\");\r\n                                sw.writeEndElement();\r\n                                sw.writeStartElement(\"DeploymentSlot\");\r\n                                sw.writeCharacters(DeploymentSlot.Production.name());\r\n                                sw.writeEndElement();\r\n                                sw.writeStartElement(\"Status\");\r\n                                sw.writeCharacters(DeploymentStatus.Running.name());\r\n                                sw.writeEndElement();\r\n                                sw.writeStartElement(\"RoleInstanceList\");\r\n                                {\r\n                                    sw.writeStartElement(\"RoleInstance\");\r\n                                    {\r\n                                        sw.writeStartElement(\"RoleName\");\r\n                                        sw.writeCharacters(UUID.randomUUID().toString());\r\n                                        sw.writeEndElement();\r\n                                        sw.writeStartElement(\"IpAddress\");\r\n                                        sw.writeCharacters(host);\r\n                                        sw.writeEndElement();\r\n                                        sw.writeStartElement(\"InstanceEndpoints\");\r\n                                        {\r\n                                            sw.writeStartElement(\"InstanceEndpoint\");\r\n                                            {\r\n                                                sw.writeStartElement(\"Name\");\r\n                                                sw.writeCharacters(\"myendpoint\");\r\n                                                sw.writeEndElement();\r\n                                                sw.writeStartElement(\"Vip\");\r\n                                                sw.writeCharacters(host);\r\n                                                sw.writeEndElement();\r\n                                                sw.writeStartElement(\"PublicPort\");\r\n                                                sw.writeCharacters(port);\r\n                                                sw.writeEndElement();\r\n                                            }\r\n                                            sw.writeEndElement();\r\n                                        }\r\n                                        sw.writeEndElement();\r\n                                    }\r\n                                    sw.writeEndElement();\r\n                                }\r\n                                sw.writeEndElement();\r\n                            }\r\n                            sw.writeEndElement();\r\n                        }\r\n                    }\r\n                }\r\n                sw.writeEndElement();\r\n            }\r\n            sw.writeEndElement();\r\n            sw.writeEndDocument();\r\n            sw.flush();\r\n            final byte[] responseAsBytes = out.toString().getBytes(StandardCharsets.UTF_8);\r\n            s.sendResponseHeaders(200, responseAsBytes.length);\r\n            OutputStream responseBody = s.getResponseBody();\r\n            responseBody.write(responseAsBytes);\r\n            responseBody.close();\r\n        } catch (XMLStreamException e) {\r\n            LogManager.getLogger(AzureDiscoveryClusterFormationTests.class).error(\"Failed serializing XML\", e);\r\n            throw new RuntimeException(e);\r\n        }\r\n    });\r\n    httpsServer.start();\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.pile",
	"Comment": "this method stacks vertically examples with the same shape, increasing result dimensionality. i.e. if you provide bunch of 3d tensors, output will be 4d tensor. alignment is always applied to axis 0.",
	"Method": "INDArray pile(INDArray arrays,INDArray pile,Collection<INDArray> arrays){\r\n    return pile(arrays.toArray(new INDArray[0]));\r\n}"
}, {
	"Path": "org.elasticsearch.client.graph.GraphExploreRequest.maxDocsPerDiversityValue",
	"Comment": "optional number of permitted docs with same value in sampled searchresults. must also declare which field using samplediversityfield",
	"Method": "void maxDocsPerDiversityValue(int maxDocs,int maxDocsPerDiversityValue){\r\n    return maxDocsPerDiversityValue;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.putLifecyclePolicyAsync",
	"Comment": "asynchronously create or modify a lifecycle definition. see the docs for more.",
	"Method": "void putLifecyclePolicyAsync(PutLifecyclePolicyRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IndexLifecycleRequestConverters::putLifecyclePolicy, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.join.query.HasParentQueryBuilder.innerHit",
	"Comment": "returns inner hit definition in the scope of this query and reusing the defined type and query.",
	"Method": "InnerHitBuilder innerHit(HasParentQueryBuilder innerHit,InnerHitBuilder innerHit){\r\n    this.innerHitBuilder = innerHit;\r\n    innerHitBuilder.setIgnoreUnmapped(ignoreUnmapped);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.analysis.IndexableBinaryStringTools.getDecodedLength",
	"Comment": "returns the number of bytes required to decode the given char sequence.",
	"Method": "int getDecodedLength(char[] encoded,int offset,int length){\r\n    final int numChars = length - 1;\r\n    if (numChars <= 0) {\r\n        return 0;\r\n    } else {\r\n        final long numFullBytesInFinalChar = encoded[offset + length - 1];\r\n        final long numEncodedChars = numChars - 1;\r\n        return (int) ((numEncodedChars * 15L + 7L) / 8L + numFullBytesInFinalChar);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.getSpilledSize",
	"Comment": "this method returns number of bytes in spilled allocations.",
	"Method": "long getSpilledSize(){\r\n    return spilledAllocationsSize.get();\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteFilterAsync",
	"Comment": "deletes the given machine learning filter asynchronously and notifies the listener on completionfor additional infosee ml delete filter documentation",
	"Method": "void deleteFilterAsync(DeleteFilterRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::deleteFilter, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.plugin.noop.action.search.NoopSearchRequestBuilder.addIndexBoost",
	"Comment": "sets the boost a specific index will receive when the query is executed against it.",
	"Method": "NoopSearchRequestBuilder addIndexBoost(String index,float indexBoost){\r\n    sourceBuilder().indexBoost(index, indexBoost);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.get.GetTaskRequest.getTimeout",
	"Comment": "timeout to wait for any async actions this request must take. it must take anywhere from 0 to 2.",
	"Method": "TimeValue getTimeout(){\r\n    return timeout;\r\n}"
}, {
	"Path": "org.elasticsearch.nio.InboundChannelBuffer.release",
	"Comment": "this method will release bytes from the head of this buffer. if you release bytes past the currentindex the index is truncated to zero.",
	"Method": "void release(long bytesToRelease){\r\n    if (bytesToRelease > capacity) {\r\n        throw new IllegalArgumentException(\"Releasing more bytes [\" + bytesToRelease + \"] than buffer capacity [\" + capacity + \"].\");\r\n    }\r\n    int pagesToRelease = pageIndex(offset + bytesToRelease);\r\n    for (int i = 0; i < pagesToRelease; i++) {\r\n        pages.removeFirst().close();\r\n    }\r\n    capacity -= bytesToRelease;\r\n    internalIndex = Math.max(internalIndex - bytesToRelease, 0);\r\n    offset = indexInPage(bytesToRelease + offset);\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.MultiNormalizerHybrid.standardizeAllOutputs",
	"Comment": "apply standardization to all outputs, except the ones individually configured",
	"Method": "MultiNormalizerHybrid standardizeAllOutputs(){\r\n    globalOutputStrategy = new StandardizeStrategy();\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore.deleteBlobs",
	"Comment": "deletes multiple blobs from the specific bucket using a batch request",
	"Method": "void deleteBlobs(Collection<String> blobNames){\r\n    if (blobNames.isEmpty()) {\r\n        return;\r\n    }\r\n    if (blobNames.size() == 1) {\r\n        deleteBlob(blobNames.iterator().next());\r\n        return;\r\n    }\r\n    final List<BlobId> blobIdsToDelete = blobNames.stream().map(blob -> BlobId.of(bucketName, blob)).collect(Collectors.toList());\r\n    final List<Boolean> deletedStatuses = SocketAccess.doPrivilegedIOException(() -> client().delete(blobIdsToDelete));\r\n    assert blobIdsToDelete.size() == deletedStatuses.size();\r\n    boolean failed = false;\r\n    for (int i = 0; i < blobIdsToDelete.size(); i++) {\r\n        if (deletedStatuses.get(i) == false) {\r\n            logger.error(\"Failed to delete blob [{}] in bucket [{}]\", blobIdsToDelete.get(i).getName(), bucketName);\r\n            failed = true;\r\n        }\r\n    }\r\n    if (failed) {\r\n        throw new IOException(\"Failed to delete all [\" + blobIdsToDelete.size() + \"] blobs\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.analysis.common.KeywordMarkerFilterFactoryTests.testCannotSpecifyBothKeywordsAndPattern",
	"Comment": "verifies that both keywords and patterns cannot be specified together.",
	"Method": "void testCannotSpecifyBothKeywordsAndPattern(){\r\n    Settings settings = Settings.builder().put(\"index.analysis.filter.my_keyword.type\", \"keyword_marker\").put(\"index.analysis.filter.my_keyword.keywords\", \"running\").put(\"index.analysis.filter.my_keyword.keywords_pattern\", \"run[a-z]ing\").put(\"index.analysis.analyzer.my_keyword.type\", \"custom\").put(\"index.analysis.analyzer.my_keyword.tokenizer\", \"standard\").put(\"index.analysis.analyzer.my_keyword.filter\", \"my_keyword, porter_stem\").put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).build();\r\n    IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> AnalysisTestsHelper.createTestAnalysisFromSettings(settings, new CommonAnalysisPlugin()));\r\n    assertEquals(\"cannot specify both `keywords_pattern` and `keywords` or `keywords_path`\", e.getMessage());\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.testDefaultRetryTimes",
	"Comment": "the default retry time matches what we say it is in the javadoc for the request.",
	"Method": "void testDefaultRetryTimes(){\r\n    Iterator<TimeValue> policy = new DummyAsyncBulkByScrollAction().buildBackoffPolicy().iterator();\r\n    long millis = 0;\r\n    while (policy.hasNext()) {\r\n        millis += policy.next().millis();\r\n    }\r\n    int defaultBackoffBeforeFailing = 59460;\r\n    assertEquals(defaultBackoffBeforeFailing, millis);\r\n}"
}, {
	"Path": "io.dropwizard.testing.junit5.ResourceExtension.target",
	"Comment": "creates a web target to be sent to the resource under testing.",
	"Method": "WebTarget target(String path){\r\n    return resource.target(path);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.clearScroll",
	"Comment": "clears one or more scroll ids using the clear scroll api.see clear scroll api on elastic.co",
	"Method": "ClearScrollResponse clearScroll(ClearScrollRequest clearScrollRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(clearScrollRequest, RequestConverters::clearScroll, options, ClearScrollResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.SelectionKeyUtils.setAcceptInterested",
	"Comment": "adds an interest in accepts for this selection key while maintaining other interests.",
	"Method": "void setAcceptInterested(SelectionKey selectionKey){\r\n    selectionKey.interestOps(selectionKey.interestOps() | SelectionKey.OP_ACCEPT);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.putScriptAsync",
	"Comment": "asynchronously puts an stored script using the scripting api.seescripting apion elastic.co",
	"Method": "void putScriptAsync(PutStoredScriptRequest putStoredScriptRequest,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    performRequestAsyncAndParseEntity(putStoredScriptRequest, RequestConverters::putScript, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "com.googlecode.dex2jar.tools.DecryptStringCmd.findAnyMethodMatch",
	"Comment": "fix for issue 216, travel all the parent of class and use getdeclaredmethod to find methods",
	"Method": "Method findAnyMethodMatch(Class<?> clz,String name,Class<?>[] classes){\r\n    try {\r\n        Method m = clz.getDeclaredMethod(name, classes);\r\n        if (m != null) {\r\n            return m;\r\n        }\r\n    } catch (NoSuchMethodException ignored) {\r\n    }\r\n    Class<?> sup = clz.getSuperclass();\r\n    if (sup != null) {\r\n        Method m = findAnyMethodMatch(sup, name, classes);\r\n        if (m != null) {\r\n            return m;\r\n        }\r\n    }\r\n    Class<?>[] itfs = clz.getInterfaces();\r\n    if (itfs != null && itfs.length > 0) {\r\n        for (Class<?> itf : itfs) {\r\n            Method m = findAnyMethodMatch(itf, name, classes);\r\n            if (m != null) {\r\n                return m;\r\n            }\r\n        }\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteExpiredData",
	"Comment": "deletes expired data from machine learning jobsfor additional infosee ml delete expired datadocumentation",
	"Method": "DeleteExpiredDataResponse deleteExpiredData(DeleteExpiredDataRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::deleteExpiredData, options, DeleteExpiredDataResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.buffer.BaseDataBuffer.isAttached",
	"Comment": "this method returns true, if this databuffer is attached to some workspace. false otherwise",
	"Method": "boolean isAttached(){\r\n    return attached;\r\n}"
}, {
	"Path": "io.dropwizard.jersey.filter.RequestIdFilter.generateRandomUuid",
	"Comment": "generate a random uuid v4 that will perform reasonably when used bymultiple threads under load.",
	"Method": "UUID generateRandomUuid(){\r\n    final Random rnd = ThreadLocalRandom.current();\r\n    long mostSig = rnd.nextLong();\r\n    long leastSig = rnd.nextLong();\r\n    mostSig &= 0xffffffffffff0fffL;\r\n    mostSig |= 0x0000000000004000L;\r\n    leastSig &= 0x3fffffffffffffffL;\r\n    leastSig |= 0x8000000000000000L;\r\n    return new UUID(mostSig, leastSig);\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.testBulkRejectionsRetryWithEnoughRetries",
	"Comment": "mimicks bulk rejections. these should be retried and eventually succeed.",
	"Method": "void testBulkRejectionsRetryWithEnoughRetries(){\r\n    int bulksToTry = randomIntBetween(1, 10);\r\n    long retryAttempts = 0;\r\n    for (int i = 0; i < bulksToTry; i++) {\r\n        bulkRetryTestCase(false);\r\n        retryAttempts += testRequest.getMaxRetries();\r\n        assertEquals(retryAttempts, testTask.getStatus().getBulkRetries());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getBucketsAsync",
	"Comment": "gets the buckets for a machine learning job, notifies listener once the requested buckets are retrieved.for additional infosee ml get buckets documentation",
	"Method": "void getBucketsAsync(GetBucketsRequest request,RequestOptions options,ActionListener<GetBucketsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getBuckets, options, GetBucketsResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "io.dropwizard.auth.principal.NoAuthPrincipalEntityResource.annotatedPrincipalEntityWithoutAuth",
	"Comment": "annotated principal instance must be injected even when no authentication is required.",
	"Method": "String annotatedPrincipalEntityWithoutAuth(JsonPrincipal principal){\r\n    assertThat(principal).isNotNull();\r\n    return principal.getName();\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.reindexRethrottle",
	"Comment": "executes a reindex rethrottling request.see the reindex rethrottling api on elastic.co",
	"Method": "ListTasksResponse reindexRethrottle(RethrottleRequest rethrottleRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(rethrottleRequest, RequestConverters::rethrottleReindex, options, ListTasksResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.stats.MinMaxStats.getRange",
	"Comment": "get the feature wiserange for the statistics.note that this is a lazy getter.it is only computed when needed.",
	"Method": "INDArray getRange(){\r\n    if (range == null) {\r\n        try (MemoryWorkspace ws = Nd4j.getMemoryManager().scopeOutOfWorkspaces()) {\r\n            range = upper.sub(lower);\r\n        }\r\n    }\r\n    return range;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.existsAliasAsync",
	"Comment": "asynchronously checks if one or more aliases exist using the aliases exist api.see indices aliases api on elastic.co",
	"Method": "void existsAliasAsync(GetAliasesRequest getAliasesRequest,RequestOptions options,ActionListener<Boolean> listener){\r\n    restHighLevelClient.performRequestAsync(getAliasesRequest, IndicesRequestConverters::existsAlias, options, RestHighLevelClient::convertExistsResponse, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.startILMAsync",
	"Comment": "asynchronously start the index lifecycle management feature.see the docs for more.",
	"Method": "void startILMAsync(StartILMRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IndexLifecycleRequestConverters::startILM, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.CublasPointer.setResultPointer",
	"Comment": "sets whether this is a result pointer or nota result pointer means that thispointer should not automatically be freedbut instead wait for results to accumulateso they can be returned fromthe gpu first",
	"Method": "void setResultPointer(boolean resultPointer){\r\n    this.resultPointer = resultPointer;\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.getHostOffset",
	"Comment": "this method returns current host memory offset within workspace",
	"Method": "long getHostOffset(){\r\n    return hostOffset.get();\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.node.ParameterServerNode.subscriberLaunched",
	"Comment": "returns true if all susbcribers in thesubscriber pool have been launched",
	"Method": "boolean subscriberLaunched(){\r\n    boolean launched = true;\r\n    for (int i = 0; i < numWorkers; i++) {\r\n        launched = launched && subscriber[i].subscriberLaunched();\r\n    }\r\n    return launched;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.analyzeAsync",
	"Comment": "asynchronously calls the analyze apisee analyze api on elastic.co",
	"Method": "void analyzeAsync(AnalyzeRequest request,RequestOptions options,ActionListener<AnalyzeResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IndicesRequestConverters::analyze, options, AnalyzeResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.WatcherClient.getWatchAsync",
	"Comment": "asynchronously gets a watch into the clustersee the docs for more.",
	"Method": "void getWatchAsync(GetWatchRequest request,RequestOptions options,ActionListener<GetWatchResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, WatcherRequestConverters::getWatch, options, GetWatchResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "com.googlecode.d2j.reader.zip.ZipUtil.readDex",
	"Comment": "read the dex file from byte array, if the byte array is a zip stream, it will return the content of classes.dexin the zip stream.",
	"Method": "byte[] readDex(File file,byte[] readDex,Path file,byte[] readDex,InputStream in,byte[] readDex,byte[] data){\r\n    if (data.length < 3) {\r\n        throw new IOException(\"File too small to be a dex/zip\");\r\n    }\r\n    if (\"dex\".equals(new String(data, 0, 3, StandardCharsets.ISO_8859_1))) {\r\n        return data;\r\n    } else if (\"PK\".equals(new String(data, 0, 2, StandardCharsets.ISO_8859_1))) {\r\n        try (ZipFile zipFile = new ZipFile(data)) {\r\n            ZipEntry classes = zipFile.findFirstEntry(\"classes.dex\");\r\n            if (classes != null) {\r\n                return toByteArray(zipFile.getInputStream(classes));\r\n            } else {\r\n                throw new IOException(\"Can not find classes.dex in zip file\");\r\n            }\r\n        }\r\n    }\r\n    throw new IOException(\"the src file not a .dex or zip file\");\r\n}"
}, {
	"Path": "com.example.helloworld.resources.ProtectedClassResource.showBasicUserSecret",
	"Comment": "access to this method is authorized by the class level annotation",
	"Method": "String showBasicUserSecret(SecurityContext context){\r\n    User user = (User) context.getUserPrincipal();\r\n    return String.format(\"Hey there, %s. You seem to be a basic user. %d\", user.getName(), user.getId());\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.BaseNDArrayFactory.rand",
	"Comment": "create a random ndarray with the given shape usingthe current time as the seed",
	"Method": "INDArray rand(long[] shape,double min,double max,org.nd4j.linalg.api.rng.Random rng,INDArray rand,int[] shape,double min,double max,org.nd4j.linalg.api.rng.Random rng,INDArray rand,long rows,long columns,double min,double max,org.nd4j.linalg.api.rng.Random rng,INDArray rand,int[] shape,float min,float max,org.nd4j.linalg.api.rng.Random rng,INDArray rand,long[] shape,float min,float max,org.nd4j.linalg.api.rng.Random rng,INDArray rand,long rows,long columns,float min,float max,org.nd4j.linalg.api.rng.Random rng,INDArray rand,long rows,long columns,org.nd4j.linalg.api.rng.Random r,INDArray rand,long rows,long columns,long seed,INDArray rand,long rows,long columns,INDArray rand,char order,long rows,long columns,INDArray rand,int[] shape,Distribution r,INDArray rand,int[] shape,org.nd4j.linalg.api.rng.Random r,INDArray rand,long[] shape,org.nd4j.linalg.api.rng.Random r,INDArray rand,int[] shape,long seed,INDArray rand,long[] shape,long seed,INDArray rand,int[] shape,INDArray rand,long[] shape,INDArray rand,char order,int[] shape,INDArray rand,char order,long[] shape){\r\n    Shape.assertValidOrder(order);\r\n    return Nd4j.getRandom().nextDouble(order, shape);\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.xVals",
	"Comment": "this returns the x values of the given vector.these are assumed to be the even values of the vector.",
	"Method": "double[] xVals(double[] vector){\r\n    if (vector == null)\r\n        return null;\r\n    double[] x = new double[vector.length / 2];\r\n    int count = 0;\r\n    for (int i = 0; i < vector.length; i++) {\r\n        if (i % 2 != 0)\r\n            x[count++] = vector[i];\r\n    }\r\n    return x;\r\n}"
}, {
	"Path": "org.nd4j.jita.memory.impl.CudaDirectProvider.pingDeviceForFreeMemory",
	"Comment": "this method checks specified device for specified amount of memory",
	"Method": "boolean pingDeviceForFreeMemory(Integer deviceId,long requiredMemory){\r\n    long freeMem = nativeOps.getDeviceFreeMemory(new CudaPointer(-1));\r\n    if (freeMem - requiredMemory < DEVICE_RESERVED_SPACE)\r\n        return false;\r\n    else\r\n        return true;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.multiGetAsync",
	"Comment": "asynchronously retrieves multiple documents by id using the multi get api.see multi get api on elastic.co",
	"Method": "void multiGetAsync(MultiGetRequest multiGetRequest,RequestOptions options,ActionListener<MultiGetResponse> listener){\r\n    mgetAsync(multiGetRequest, options, listener);\r\n}"
}, {
	"Path": "com.jakewharton.disklrucache.DiskLruCache.journalRebuildRequired",
	"Comment": "we only rebuild the journal when it will halve the size of the journaland eliminate at least 2000 ops.",
	"Method": "boolean journalRebuildRequired(){\r\n    final int redundantOpCompactThreshold = 2000;\r\n    return redundantOpCount >= redundantOpCompactThreshold && redundantOpCount >= lruEntries.size();\r\n}"
}, {
	"Path": "org.elasticsearch.plugin.noop.action.search.NoopSearchRequestBuilder.setRouting",
	"Comment": "the routing values to control the shards that the search will be executed on.",
	"Method": "NoopSearchRequestBuilder setRouting(String routing,NoopSearchRequestBuilder setRouting,String routing){\r\n    request.routing(routing);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.putMappingAsync",
	"Comment": "asynchronously updates the mappings on an index using the put mapping api.see put mapping api on elastic.co",
	"Method": "void putMappingAsync(PutMappingRequest putMappingRequest,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(putMappingRequest, IndicesRequestConverters::putMapping, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getCalendars",
	"Comment": "gets a single or multiple calendars.for additional infosee ml get calendars documentation",
	"Method": "GetCalendarsResponse getCalendars(GetCalendarsRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getCalendars, options, GetCalendarsResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.settings.SettingsUpdater.partitionKnownAndValidSettings",
	"Comment": "partitions the settings into those that are known and valid versus those that are unknown or invalid. the resulting tuple containsthe known and valid settings in the first component and the unknown or invalid settings in the second component. note that archivedsettings contained in the settings to partition are included in the first component.",
	"Method": "Tuple<Settings, Settings> partitionKnownAndValidSettings(Settings settings,String settingsType,Logger logger){\r\n    final Settings existingArchivedSettings = settings.filter(k -> k.startsWith(ARCHIVED_SETTINGS_PREFIX));\r\n    final Settings settingsExcludingExistingArchivedSettings = settings.filter(k -> k.startsWith(ARCHIVED_SETTINGS_PREFIX) == false);\r\n    final Settings settingsWithUnknownOrInvalidArchived = clusterSettings.archiveUnknownOrInvalidSettings(settingsExcludingExistingArchivedSettings, e -> logUnknownSetting(settingsType, e, logger), (e, ex) -> logInvalidSetting(settingsType, e, ex, logger));\r\n    return Tuple.tuple(Settings.builder().put(settingsWithUnknownOrInvalidArchived.filter(k -> k.startsWith(ARCHIVED_SETTINGS_PREFIX) == false)).put(existingArchivedSettings).build(), settingsWithUnknownOrInvalidArchived.filter(k -> k.startsWith(ARCHIVED_SETTINGS_PREFIX)));\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.sumOfMeanDifferencesOnePoint",
	"Comment": "used for calculating top part of simple regression forbeta 1",
	"Method": "double sumOfMeanDifferencesOnePoint(double[] vector){\r\n    double mean = sum(vector) / vector.length;\r\n    double ret = 0;\r\n    for (int i = 0; i < vector.length; i++) {\r\n        double vec1Diff = Math.pow(vector[i] - mean, 2);\r\n        ret += vec1Diff;\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteModelSnapshot",
	"Comment": "deletes machine learning model snapshotsfor additional infosee ml delete model snapshot documentation",
	"Method": "AcknowledgedResponse deleteModelSnapshot(DeleteModelSnapshotRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::deleteModelSnapshot, options, AcknowledgedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.StartDatafeedRequest.setTimeout",
	"Comment": "indicates how long to wait for the cluster to respond to the request.",
	"Method": "void setTimeout(TimeValue timeout){\r\n    this.timeout = timeout;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.deleteScriptAsync",
	"Comment": "asynchronously delete stored script by id.see how to use scripts on elastic.co",
	"Method": "void deleteScriptAsync(DeleteStoredScriptRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    performRequestAsyncAndParseEntity(request, RequestConverters::deleteScript, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.util.ArchiveUtils.zipListFiles",
	"Comment": "list all of the files and directories in the specified .zip file",
	"Method": "List<String> zipListFiles(File zipFile){\r\n    List<String> out = new ArrayList();\r\n    try (ZipFile zf = new ZipFile(zipFile)) {\r\n        Enumeration entries = zf.entries();\r\n        while (entries.hasMoreElements()) {\r\n            ZipEntry ze = (ZipEntry) entries.nextElement();\r\n            out.add(ze.getName());\r\n        }\r\n    }\r\n    return out;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.getSettings",
	"Comment": "retrieve the settings of one or more indices.see indices get settings api on elastic.co",
	"Method": "GetSettingsResponse getSettings(GetSettingsRequest getSettingsRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(getSettingsRequest, IndicesRequestConverters::getSettings, options, GetSettingsResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.utils.AllocationUtils.buildAllocationShape",
	"Comment": "this method returns allocationshape for the whole databuffer.",
	"Method": "AllocationShape buildAllocationShape(INDArray array,AllocationShape buildAllocationShape,DataBuffer buffer,AllocationShape buildAllocationShape,JCudaBuffer buffer){\r\n    AllocationShape shape = new AllocationShape();\r\n    shape.setStride(1);\r\n    shape.setOffset(buffer.originalOffset());\r\n    shape.setDataType(buffer.dataType());\r\n    shape.setLength(buffer.length());\r\n    return shape;\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.createRepositoryAsync",
	"Comment": "asynchronously creates a snapshot repository.seesnapshot and restoreapi on elastic.co",
	"Method": "void createRepositoryAsync(PutRepositoryRequest putRepositoryRequest,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(putRepositoryRequest, SnapshotRequestConverters::createRepository, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.StartDatafeedRequest.setStart",
	"Comment": "the time that the datafeed should begin. this value is inclusive.if you specify a start value that is earlier than the timestamp of the latest processed record,the datafeed continues from 1 millisecond after the timestamp of the latest processed record.if you do not specify a start time and the datafeed is associated with a new job,the analysis starts from the earliest time for which data is available.",
	"Method": "void setStart(String start){\r\n    this.start = start;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.isInverse",
	"Comment": "returns true if the giventwo arrays are reverse copies of each other",
	"Method": "boolean isInverse(int[] first,int[] second){\r\n    int backWardCount = second.length - 1;\r\n    for (int i = 0; i < first.length; i++) {\r\n        if (first[i] != second[backWardCount--])\r\n            return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.shrinkAsync",
	"Comment": "asynchronously shrinks an index using the shrink index api.see shrink index api on elastic.co",
	"Method": "void shrinkAsync(ResizeRequest resizeRequest,RequestOptions options,ActionListener<ResizeResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(resizeRequest, IndicesRequestConverters::shrink, options, ResizeResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.searchAsync",
	"Comment": "asynchronously executes a search using the search api.see search api on elastic.co",
	"Method": "void searchAsync(SearchRequest searchRequest,RequestOptions options,ActionListener<SearchResponse> listener){\r\n    performRequestAsyncAndParseEntity(searchRequest, RequestConverters::search, options, SearchResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.CublasPointer.close",
	"Comment": "frees the underlyingdevice memory allocated for this pointer",
	"Method": "void close(){\r\n    if (!isResultPointer()) {\r\n        destroy();\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.AbstractDataSetNormalizer.isFitLabel",
	"Comment": "whether normalization for the labels is also enabled. most commonly used for regression, not classification.",
	"Method": "boolean isFitLabel(){\r\n    return this.fitLabels;\r\n}"
}, {
	"Path": "org.elasticsearch.percolator.PercolatorFieldMapperTests.testNestedPercolatorField",
	"Comment": "percolator field can be nested under an object field, but only one query can be specified per document",
	"Method": "void testNestedPercolatorField(){\r\n    String typeName = \"doc\";\r\n    String percolatorMapper = Strings.toString(// makes testing easier\r\n    XContentFactory.jsonBuilder().startObject().startObject(typeName).startObject(\"_field_names\").field(\"enabled\", false).endObject().startObject(\"properties\").startObject(\"object_field\").field(\"type\", \"object\").startObject(\"properties\").startObject(\"query_field\").field(\"type\", \"percolator\").endObject().endObject().endObject().endObject().endObject().endObject());\r\n    mapperService.merge(typeName, new CompressedXContent(percolatorMapper), MapperService.MergeReason.MAPPING_UPDATE);\r\n    QueryBuilder queryBuilder = matchQuery(\"field\", \"value\");\r\n    ParsedDocument doc = mapperService.documentMapper(typeName).parse(SourceToParse.source(\"test\", typeName, \"1\", BytesReference.bytes(jsonBuilder().startObject().startObject(\"object_field\").field(\"query_field\", queryBuilder).endObject().endObject()), XContentType.JSON));\r\n    assertThat(doc.rootDoc().getFields().size(), equalTo(10));\r\n    BytesRef queryBuilderAsBytes = doc.rootDoc().getField(\"object_field.query_field.query_builder_field\").binaryValue();\r\n    assertQueryBuilder(queryBuilderAsBytes, queryBuilder);\r\n    doc = mapperService.documentMapper(typeName).parse(SourceToParse.source(\"test\", typeName, \"1\", BytesReference.bytes(jsonBuilder().startObject().startArray(\"object_field\").startObject().field(\"query_field\", queryBuilder).endObject().endArray().endObject()), XContentType.JSON));\r\n    assertThat(doc.rootDoc().getFields().size(), equalTo(10));\r\n    queryBuilderAsBytes = doc.rootDoc().getField(\"object_field.query_field.query_builder_field\").binaryValue();\r\n    assertQueryBuilder(queryBuilderAsBytes, queryBuilder);\r\n    MapperParsingException e = expectThrows(MapperParsingException.class, () -> {\r\n        mapperService.documentMapper(typeName).parse(SourceToParse.source(\"test\", typeName, \"1\", BytesReference.bytes(jsonBuilder().startObject().startArray(\"object_field\").startObject().field(\"query_field\", queryBuilder).endObject().startObject().field(\"query_field\", queryBuilder).endObject().endArray().endObject()), XContentType.JSON));\r\n    });\r\n    assertThat(e.getCause(), instanceOf(IllegalArgumentException.class));\r\n    assertThat(e.getCause().getMessage(), equalTo(\"a document can only contain one percolator query\"));\r\n}"
}, {
	"Path": "org.nd4j.jita.handler.impl.CudaZeroHandler.copyback",
	"Comment": "copies memory from device to host, if needed.device copy is preserved as is.",
	"Method": "void copyback(AllocationPoint point,AllocationShape shape){\r\n    throw new UnsupportedOperationException(\"Deprecated call\");\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.BaseParameterUpdater.isAsync",
	"Comment": "returns true if thegiven updater is asyncor synchronousupdates",
	"Method": "boolean isAsync(){\r\n    return true;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.buffer.BaseDataBuffer.originalDataBuffer",
	"Comment": "original databuffer.in case if we have a view derived from another view, derived from some other view, original databuffer will point to the originating databuffer, where all views come from.",
	"Method": "DataBuffer originalDataBuffer(){\r\n    return originalBuffer;\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.support.filtering.FilterPathBasedFilter.evaluate",
	"Comment": "evaluates if a property name matches one of the given filter paths.",
	"Method": "TokenFilter evaluate(String name,FilterPath[] filters){\r\n    if (filters != null) {\r\n        List<FilterPath> nextFilters = null;\r\n        for (FilterPath filter : filters) {\r\n            FilterPath next = filter.matchProperty(name);\r\n            if (next != null) {\r\n                if (next.matches()) {\r\n                    return MATCHING;\r\n                } else {\r\n                    if (nextFilters == null) {\r\n                        nextFilters = new ArrayList();\r\n                    }\r\n                    if (filter.isDoubleWildcard()) {\r\n                        nextFilters.add(filter);\r\n                    }\r\n                    nextFilters.add(next);\r\n                }\r\n            }\r\n        }\r\n        if ((nextFilters != null) && (nextFilters.isEmpty() == false)) {\r\n            return new FilterPathBasedFilter(nextFilters.toArray(new FilterPath[nextFilters.size()]), inclusive);\r\n        }\r\n    }\r\n    return NO_MATCHING;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractAsyncBulkByScrollAction.prepareBulkRequest",
	"Comment": "prepare the bulk request. called on the generic thread pool after some preflight checks have been done one the searchresponse and anydelay has been slept. uses the generic thread pool because reindex is rare enough not to need its own thread pool and because thethread may be blocked by the user script.",
	"Method": "void prepareBulkRequest(TimeValue thisBatchStartTime,ScrollableHitSource.Response response){\r\n    logger.debug(\"[{}]: preparing bulk request\", task.getId());\r\n    if (task.isCancelled()) {\r\n        logger.debug(\"[{}]: finishing early because the task was cancelled\", task.getId());\r\n        finishHim(null);\r\n        return;\r\n    }\r\n    if (response.getHits().isEmpty()) {\r\n        refreshAndFinish(emptyList(), emptyList(), false);\r\n        return;\r\n    }\r\n    worker.countBatch();\r\n    List<? extends ScrollableHitSource.Hit> hits = response.getHits();\r\n    if (mainRequest.getSize() != SIZE_ALL_MATCHES) {\r\n        long remaining = max(0, mainRequest.getSize() - worker.getSuccessfullyProcessed());\r\n        if (remaining < hits.size()) {\r\n            hits = hits.subList(0, (int) remaining);\r\n        }\r\n    }\r\n    BulkRequest request = buildBulk(hits);\r\n    if (request.requests().isEmpty()) {\r\n        startNextScroll(thisBatchStartTime, timeValueNanos(System.nanoTime()), 0);\r\n        return;\r\n    }\r\n    request.timeout(mainRequest.getTimeout());\r\n    request.waitForActiveShards(mainRequest.getWaitForActiveShards());\r\n    sendBulkRequest(thisBatchStartTime, request);\r\n}"
}, {
	"Path": "org.elasticsearch.client.GraphClient.exploreAsync",
	"Comment": "asynchronously executes an exploration request using the graph api.see graph apion elastic.co.",
	"Method": "void exploreAsync(GraphExploreRequest graphExploreRequest,RequestOptions options,ActionListener<GraphExploreResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(graphExploreRequest, GraphRequestConverters::explore, options, GraphExploreResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.deleteAsync",
	"Comment": "asynchronously deletes an index using the delete index api.see delete index api on elastic.co",
	"Method": "void deleteAsync(DeleteIndexRequest deleteIndexRequest,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(deleteIndexRequest, IndicesRequestConverters::deleteIndex, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.WatcherClient.startWatchServiceAsync",
	"Comment": "asynchronously start the watch servicesee the docs for more.",
	"Method": "void startWatchServiceAsync(StartWatchServiceRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, WatcherRequestConverters::startWatchService, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.updateModelSnapshotAsync",
	"Comment": "updates a snapshot for a machine learning job, notifies listener once the requested snapshots are retrieved.for additional infosee ml update model snapshots documentation",
	"Method": "void updateModelSnapshotAsync(UpdateModelSnapshotRequest request,RequestOptions options,ActionListener<UpdateModelSnapshotResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::updateModelSnapshot, options, UpdateModelSnapshotResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.VoidParameterServer.isInit",
	"Comment": "this method returns true if initialization was started and was finished, false otherwise",
	"Method": "boolean isInit(){\r\n    return initFinished.get();\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.createRepository",
	"Comment": "creates a snapshot repository.seesnapshot and restoreapi on elastic.co",
	"Method": "AcknowledgedResponse createRepository(PutRepositoryRequest putRepositoryRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(putRepositoryRequest, SnapshotRequestConverters::createRepository, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getOverallBucketsAsync",
	"Comment": "gets overall buckets for a set of machine learning jobs, notifies listener once the requested buckets are retrieved.for additional infosee ml get overall buckets documentation",
	"Method": "void getOverallBucketsAsync(GetOverallBucketsRequest request,RequestOptions options,ActionListener<GetOverallBucketsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getOverallBuckets, options, GetOverallBucketsResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.common.Booleans.parseBoolean",
	"Comment": "parses a string representation of a boolean value to boolean.",
	"Method": "boolean parseBoolean(char[] text,int offset,int length,boolean defaultValue,boolean parseBoolean,String value,boolean parseBoolean,String value,boolean defaultValue,Boolean parseBoolean,String value,Boolean defaultValue){\r\n    if (hasText(value)) {\r\n        return parseBoolean(value);\r\n    }\r\n    return defaultValue;\r\n}"
}, {
	"Path": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests.testNormalization",
	"Comment": "test usage of the decomposition option for unicode normalization.",
	"Method": "void testNormalization(){\r\n    Settings settings = Settings.builder().put(\"index.analysis.filter.myCollator.type\", \"icu_collation\").put(\"index.analysis.filter.myCollator.language\", \"tr\").put(\"index.analysis.filter.myCollator.strength\", \"primary\").put(\"index.analysis.filter.myCollator.decomposition\", \"canonical\").build();\r\n    TestAnalysis analysis = createTestAnalysis(new Index(\"test\", \"_na_\"), settings, new AnalysisICUPlugin());\r\n    TokenFilterFactory filterFactory = analysis.tokenFilter.get(\"myCollator\");\r\n    assertCollatesToSame(filterFactory, \"I WI?LL USE TURKİSH CASING\", \"ı will use turkish casıng\");\r\n}"
}, {
	"Path": "com.jakewharton.disklrucache.DiskLruCache.processJournal",
	"Comment": "computes the initial size and collects garbage as a part of opening thecache. dirty entries are assumed to be inconsistent and will be deleted.",
	"Method": "void processJournal(){\r\n    deleteIfExists(journalFileTmp);\r\n    for (Iterator<Entry> i = lruEntries.values().iterator(); i.hasNext(); ) {\r\n        Entry entry = i.next();\r\n        if (entry.currentEditor == null) {\r\n            for (int t = 0; t < valueCount; t++) {\r\n                size += entry.lengths[t];\r\n            }\r\n        } else {\r\n            entry.currentEditor = null;\r\n            for (int t = 0; t < valueCount; t++) {\r\n                deleteIfExists(entry.getCleanFile(t));\r\n                deleteIfExists(entry.getDirtyFile(t));\r\n            }\r\n            i.remove();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getInfluencersAsync",
	"Comment": "gets the influencers for a machine learning job, notifies listener once the requested influencers are retrieved.for additional infosee ml get influencers documentation",
	"Method": "void getInfluencersAsync(GetInfluencersRequest request,RequestOptions options,ActionListener<GetInfluencersResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getInfluencers, options, GetInfluencersResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.util.MeshOrganizer.flatSize",
	"Comment": "this method returns size of flattened map of nodes.suited for tests.",
	"Method": "long flatSize(){\r\n    return (long) nodeMap.size();\r\n}"
}, {
	"Path": "org.nd4j.aeron.ipc.chunk.InMemoryChunkAccumulator.numChunksSoFar",
	"Comment": "returns the number of chunksaccumulated for a given id so far",
	"Method": "int numChunksSoFar(String id){\r\n    if (!chunks.containsKey(id))\r\n        return 0;\r\n    return chunks.get(id).size();\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.open",
	"Comment": "opens an index using the open index api.see open index api on elastic.co",
	"Method": "OpenIndexResponse open(OpenIndexRequest openIndexRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(openIndexRequest, IndicesRequestConverters::openIndex, options, OpenIndexResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.termvectorsAsync",
	"Comment": "asynchronously calls the term vectors apisee term vectors api onelastic.co",
	"Method": "void termvectorsAsync(TermVectorsRequest request,RequestOptions options,ActionListener<TermVectorsResponse> listener){\r\n    performRequestAsyncAndParseEntity(request, RequestConverters::termVectors, options, TermVectorsResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.DummyWorkspace.getCurrentSize",
	"Comment": "this methos returns current allocated size of this workspace",
	"Method": "long getCurrentSize(){\r\n    return 0;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.findFileStructure",
	"Comment": "finds the structure of a filefor additional infosee ml find file structure documentation",
	"Method": "FindFileStructureResponse findFileStructure(FindFileStructureRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::findFileStructure, options, FindFileStructureResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.tools.PropertyParser.toString",
	"Comment": "get property. the method returns the default value if the property is not parsed.",
	"Method": "String toString(String name,String toString,String name,String defaultValue){\r\n    try {\r\n        return parseString(name);\r\n    } catch (Exception e) {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.AtomicThrowable.isTriggered",
	"Comment": "this method returns true if internal state holds error, false otherwise",
	"Method": "boolean isTriggered(){\r\n    try {\r\n        lock.readLock().lock();\r\n        return t != null;\r\n    } finally {\r\n        lock.readLock().unlock();\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AllocationPoint.setAllocationStatus",
	"Comment": "this method sets specified allocationstatus for this point",
	"Method": "void setAllocationStatus(AllocationStatus status){\r\n    allocationStatus = status;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.rng.distribution.impl.SaddlePointExpansion.logBinomialProbability",
	"Comment": "compute the logarithm of the pmf for a binomial distributionusing the saddle point expansion.",
	"Method": "double logBinomialProbability(int x,int n,double p,double q){\r\n    double ret;\r\n    if (x == 0) {\r\n        if (p < 0.1) {\r\n            ret = -getDeviancePart(n, n * q) - n * p;\r\n        } else {\r\n            ret = n * FastMath.log(q);\r\n        }\r\n    } else if (x == n) {\r\n        if (q < 0.1) {\r\n            ret = -getDeviancePart(n, n * p) - n * q;\r\n        } else {\r\n            ret = n * FastMath.log(p);\r\n        }\r\n    } else {\r\n        ret = getStirlingError(n) - getStirlingError(x) - getStirlingError(n - x) - getDeviancePart(x, n * p) - getDeviancePart(n - x, n * q);\r\n        double f = (MathUtils.TWO_PI * x * (n - x)) / n;\r\n        ret = -0.5 * FastMath.log(f) + ret;\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests.testZipFileDoesNotHang",
	"Comment": "about the issue that causes a zip file to hang in tika versions prior to 1.18.",
	"Method": "void testZipFileDoesNotHang(){\r\n    expectThrows(Exception.class, () -> parseDocument(\"bad_tika.zip\", processor));\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.settings.ClusterUpdateSettingsRequest.transientSettings",
	"Comment": "sets the transient settings to be updated. they will not survive a full cluster restart",
	"Method": "Settings transientSettings(ClusterUpdateSettingsRequest transientSettings,Settings settings,ClusterUpdateSettingsRequest transientSettings,Settings.Builder settings,ClusterUpdateSettingsRequest transientSettings,String source,XContentType xContentType,ClusterUpdateSettingsRequest transientSettings,Map<String, ?> source){\r\n    try {\r\n        XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON);\r\n        builder.map(source);\r\n        transientSettings(Strings.toString(builder), builder.contentType());\r\n    } catch (IOException e) {\r\n        throw new ElasticsearchGenerationException(\"Failed to generate [\" + source + \"]\", e);\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.msearchTemplateAsync",
	"Comment": "asynchronously executes a request using the multi search template apisee multi search template apion elastic.co.",
	"Method": "void msearchTemplateAsync(MultiSearchTemplateRequest multiSearchTemplateRequest,RequestOptions options,ActionListener<MultiSearchTemplateResponse> listener){\r\n    performRequestAsyncAndParseEntity(multiSearchTemplateRequest, RequestConverters::multiSearchTemplate, options, MultiSearchTemplateResponse::fromXContext, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory.pullRows",
	"Comment": "this method produces concatenated array, that consist from tensors, fetched from source array, against some dimension and specified indexes",
	"Method": "INDArray pullRows(INDArray source,int sourceDimension,int[] indexes,INDArray pullRows,INDArray source,int sourceDimension,long[] indexes,INDArray pullRows,INDArray source,int sourceDimension,long[] indexes,char order,INDArray pullRows,INDArray source,int sourceDimension,int[] indexes,char order,INDArray pullRows,INDArray source,INDArray destination,int sourceDimension,int[] indexes,INDArray pullRows,INDArray source,INDArray destination,int sourceDimension,long[] indexes){\r\n    if (indexes == null || indexes.length < 1)\r\n        throw new IllegalStateException(\"Indexes can't be null or zero-length\");\r\n    long[] shape = null;\r\n    if (sourceDimension == 1)\r\n        shape = new long[] { indexes.length, source.shape()[sourceDimension] };\r\n    else if (sourceDimension == 0)\r\n        shape = new long[] { source.shape()[sourceDimension], indexes.length };\r\n    else\r\n        throw new UnsupportedOperationException(\"2D input is expected\");\r\n    INDArray ret = destination;\r\n    if (ret == null) {\r\n        ret = Nd4j.createUninitialized(shape, order);\r\n    } else {\r\n        if (!Arrays.equals(shape, destination.shape())) {\r\n            throw new IllegalStateException(\"Cannot pull rows into destination array: expected destination array of\" + \" shape \" + Arrays.toString(shape) + \" but got destination array of shape \" + Arrays.toString(destination.shape()));\r\n        }\r\n    }\r\n    Nd4j.getCompressor().autoDecompress(source);\r\n    PointerPointer dummy = new PointerPointer(new Pointer[] { null });\r\n    TADManager tadManager = Nd4j.getExecutioner().getTADManager();\r\n    Pair<DataBuffer, DataBuffer> tadBuffers = tadManager.getTADOnlyShapeInfo(source, new int[] { sourceDimension });\r\n    Pair<DataBuffer, DataBuffer> zTadBuffers = tadManager.getTADOnlyShapeInfo(ret, new int[] { sourceDimension });\r\n    Pointer hostTadShapeInfo = tadBuffers.getFirst().addressPointer();\r\n    Pointer zTadShapeInfo = zTadBuffers.getFirst().addressPointer();\r\n    LongPointer pIndex = new LongPointer(indexes);\r\n    DataBuffer offsets = tadBuffers.getSecond();\r\n    Pointer hostTadOffsets = offsets == null ? null : offsets.addressPointer();\r\n    DataBuffer zOffsets = zTadBuffers.getSecond();\r\n    Pointer zTadOffsets = zOffsets == null ? null : zOffsets.addressPointer();\r\n    if (ret.data().dataType() == DataBuffer.Type.DOUBLE) {\r\n        nativeOps.pullRowsDouble(dummy, (DoublePointer) source.data().addressPointer(), (LongPointer) source.shapeInfoDataBuffer().addressPointer(), (DoublePointer) ret.data().addressPointer(), (LongPointer) ret.shapeInfoDataBuffer().addressPointer(), indexes.length, pIndex, (LongPointer) hostTadShapeInfo, new LongPointerWrapper(hostTadOffsets), (LongPointer) zTadShapeInfo, new LongPointerWrapper(zTadOffsets));\r\n    } else if (ret.data().dataType() == DataBuffer.Type.FLOAT) {\r\n        nativeOps.pullRowsFloat(dummy, (FloatPointer) source.data().addressPointer(), (LongPointer) source.shapeInfoDataBuffer().addressPointer(), (FloatPointer) ret.data().addressPointer(), (LongPointer) ret.shapeInfoDataBuffer().addressPointer(), indexes.length, pIndex, (LongPointer) hostTadShapeInfo, new LongPointerWrapper(hostTadOffsets), (LongPointer) zTadShapeInfo, new LongPointerWrapper(zTadOffsets));\r\n    } else {\r\n        nativeOps.pullRowsHalf(dummy, (ShortPointer) source.data().addressPointer(), (LongPointer) source.shapeInfoDataBuffer().addressPointer(), (ShortPointer) ret.data().addressPointer(), (LongPointer) ret.shapeInfoDataBuffer().addressPointer(), indexes.length, pIndex, (LongPointer) hostTadShapeInfo, new LongPointerWrapper(hostTadOffsets), (LongPointer) zTadShapeInfo, new LongPointerWrapper(zTadOffsets));\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.linalg.primitives.CounterMap.size",
	"Comment": "this method returns number of first elements in this countermap",
	"Method": "int size(){\r\n    return maps.size();\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.SynchronousParameterUpdater.isAsync",
	"Comment": "returns true if thegiven updater is asyncor synchronousupdates",
	"Method": "boolean isAsync(){\r\n    return false;\r\n}"
}, {
	"Path": "org.nd4j.jita.handler.impl.CudaZeroHandler.getAllocatedDeviceMemory",
	"Comment": "this method returns total amount of memory allocated at specified device",
	"Method": "long getAllocatedDeviceMemory(Integer device){\r\n    return deviceMemoryTracker.getAllocatedSize(device);\r\n}"
}, {
	"Path": "io.dropwizard.auth.principal.NoAuthPolymorphicPrincipalEntityResource.annotatedPrincipalEntityWithoutAuth",
	"Comment": "annotated principal instance must be injected even when no authentication is required.",
	"Method": "String annotatedPrincipalEntityWithoutAuth(JsonPrincipal principal,String annotatedPrincipalEntityWithoutAuth,NullPrincipal principal){\r\n    assertThat(principal).isNotNull();\r\n    return principal.getName();\r\n}"
}, {
	"Path": "org.nd4j.linalg.cpu.nativecpu.compression.CpuFlexibleThreshold.getDescriptor",
	"Comment": "this method returns compression descriptor. it should be unique for any compressor implementation",
	"Method": "String getDescriptor(){\r\n    return \"FTHRESHOLD\";\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.updateByQueryRethrottleAsync",
	"Comment": "asynchronously execute an update by query rethrottle request.see update by query api on elastic.co",
	"Method": "void updateByQueryRethrottleAsync(RethrottleRequest rethrottleRequest,RequestOptions options,ActionListener<ListTasksResponse> listener){\r\n    performRequestAsyncAndParseEntity(rethrottleRequest, RequestConverters::rethrottleUpdateByQuery, options, ListTasksResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.join.query.HasChildQueryBuilder.scoreMode",
	"Comment": "returns how the scores from the matching child documents are mapped into the parent document.",
	"Method": "ScoreMode scoreMode(){\r\n    return scoreMode;\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.serializer.MultiHybridSerializerStrategy.restore",
	"Comment": "restore a multinormalizerhybrid that was previously serialized by this strategy",
	"Method": "MultiNormalizerHybrid restore(InputStream stream){\r\n    DataInputStream dis = new DataInputStream(stream);\r\n    MultiNormalizerHybrid result = new MultiNormalizerHybrid();\r\n    result.setInputStats(readStatsMap(dis));\r\n    result.setOutputStats(readStatsMap(dis));\r\n    result.setGlobalInputStrategy(readStrategy(dis));\r\n    result.setGlobalOutputStrategy(readStrategy(dis));\r\n    result.setPerInputStrategies(readStrategyMap(dis));\r\n    result.setPerOutputStrategies(readStrategyMap(dis));\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.hasPrivilegesAsync",
	"Comment": "asynchronously determine whether the current user has a specified list of privilegessee the docs for more.",
	"Method": "void hasPrivilegesAsync(HasPrivilegesRequest request,RequestOptions options,ActionListener<HasPrivilegesResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, SecurityRequestConverters::hasPrivileges, options, HasPrivilegesResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.matrix.stats.MatrixStatsResults.getValFromUpperTriangularMatrix",
	"Comment": "return the value for two fields in an upper triangular matrix, regardless of row col location.",
	"Method": "double getValFromUpperTriangularMatrix(Map<String, M> map,String fieldX,String fieldY){\r\n    if (map.containsKey(fieldX) == false && map.containsKey(fieldY) == false) {\r\n        throw new IllegalArgumentException(\"neither field \" + fieldX + \" nor \" + fieldY + \" exist\");\r\n    } else if (map.containsKey(fieldX)) {\r\n        if (map.get(fieldX).containsKey(fieldY)) {\r\n            return map.get(fieldX).get(fieldY);\r\n        } else {\r\n            return map.get(fieldY).get(fieldX);\r\n        }\r\n    } else if (map.containsKey(fieldY)) {\r\n        return map.get(fieldY).get(fieldX);\r\n    }\r\n    throw new IllegalArgumentException(\"Coefficient not computed between fields: \" + fieldX + \" and \" + fieldY);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.list.ListTasksResponse.toXContentGroupedByNode",
	"Comment": "convert this task response to xcontent grouping by executing nodes.",
	"Method": "XContentBuilder toXContentGroupedByNode(XContentBuilder builder,Params params,DiscoveryNodes discoveryNodes){\r\n    toXContentCommon(builder, params);\r\n    builder.startObject(\"nodes\");\r\n    for (Map.Entry<String, List<TaskInfo>> entry : getPerNodeTasks().entrySet()) {\r\n        DiscoveryNode node = discoveryNodes.get(entry.getKey());\r\n        builder.startObject(entry.getKey());\r\n        if (node != null) {\r\n            builder.field(\"name\", node.getName());\r\n            builder.field(\"transport_address\", node.getAddress().toString());\r\n            builder.field(\"host\", node.getHostName());\r\n            builder.field(\"ip\", node.getAddress());\r\n            builder.startArray(\"roles\");\r\n            for (DiscoveryNode.Role role : node.getRoles()) {\r\n                builder.value(role.getRoleName());\r\n            }\r\n            builder.endArray();\r\n            if (!node.getAttributes().isEmpty()) {\r\n                builder.startObject(\"attributes\");\r\n                for (Map.Entry<String, String> attrEntry : node.getAttributes().entrySet()) {\r\n                    builder.field(attrEntry.getKey(), attrEntry.getValue());\r\n                }\r\n                builder.endObject();\r\n            }\r\n        }\r\n        builder.startObject(TASKS);\r\n        for (TaskInfo task : entry.getValue()) {\r\n            builder.startObject(task.getTaskId().toString());\r\n            task.toXContent(builder, params);\r\n            builder.endObject();\r\n        }\r\n        builder.endObject();\r\n        builder.endObject();\r\n    }\r\n    builder.endObject();\r\n    return builder;\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.MultiNormalizerHybrid.standardizeInput",
	"Comment": "apply standardization to a specific input, overriding the global input strategy if any",
	"Method": "MultiNormalizerHybrid standardizeInput(int input){\r\n    perInputStrategies.put(input, new StandardizeStrategy());\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.stats.JobStats.getAssignmentExplanation",
	"Comment": "for open jobs only, contains messages relating to the selection of a node to run the job.",
	"Method": "String getAssignmentExplanation(){\r\n    return assignmentExplanation;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.shape.Shape.shapeEquals",
	"Comment": "returns whether 2 shapes are equals by checking for dimension semanticsas well as array equality",
	"Method": "boolean shapeEquals(int[] shape1,int[] shape2,boolean shapeEquals,long[] shape1,long[] shape2){\r\n    if (isColumnVectorShape(shape1) && isColumnVectorShape(shape2)) {\r\n        return Arrays.equals(shape1, shape2);\r\n    }\r\n    if (isRowVectorShape(shape1) && isRowVectorShape(shape2)) {\r\n        long[] shape1Comp = squeeze(shape1);\r\n        long[] shape2Comp = squeeze(shape2);\r\n        return Arrays.equals(shape1Comp, shape2Comp);\r\n    }\r\n    if (shape1.length == 0 || shape2.length == 0) {\r\n        if (shape1.length == 0 && shapeIsScalar(shape2)) {\r\n            return true;\r\n        }\r\n        if (shape2.length == 0 && shapeIsScalar(shape1)) {\r\n            return true;\r\n        }\r\n    }\r\n    shape1 = squeeze(shape1);\r\n    shape2 = squeeze(shape2);\r\n    return scalarEquals(shape1, shape2) || Arrays.equals(shape1, shape2);\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteDatafeed",
	"Comment": "deletes the given machine learning datafeedfor additional infosee ml delete datafeed documentation",
	"Method": "AcknowledgedResponse deleteDatafeed(DeleteDatafeedRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::deleteDatafeed, options, AcknowledgedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.client.ParameterServerClient.masterStarted",
	"Comment": "sends a post request to thestatus server to determine if the master node is started.",
	"Method": "boolean masterStarted(){\r\n    if (objectMapper == null)\r\n        objectMapper = new ObjectMapper();\r\n    try {\r\n        String type = objectMapper.readValue(Unirest.get(String.format(\"http://%s:%d/opType\", masterStatusHost, masterStatusPort)).asJson().getBody().toString(), ServerTypeJson.class).getType();\r\n        if (!type.equals(\"master\"))\r\n            throw new IllegalStateException(\"Wrong opType \" + type);\r\n        Unirest.get(String.format(\"http://%s:%d/started\", masterStatusHost, masterStatusPort)).asJson().getBody();\r\n        return objectMapper.readValue(Unirest.get(String.format(\"http://%s:%d/started\", masterStatusHost, masterStatusPort)).asJson().getBody().toString(), MasterStatus.class).started();\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.permute",
	"Comment": "permute the given inputswitching the dimensions of the input shapearray with in the order of the specifieddimensions",
	"Method": "int[] permute(int[] shape,int[] dimensions,long[] permute,long[] shape,int[] dimensions){\r\n    val ret = new long[shape.length];\r\n    for (int i = 0; i < shape.length; i++) {\r\n        ret[i] = shape[dimensions[i]];\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.clearScrollAsync",
	"Comment": "asynchronously clears one or more scroll ids using the clear scroll api.see clear scroll api on elastic.co",
	"Method": "void clearScrollAsync(ClearScrollRequest clearScrollRequest,RequestOptions options,ActionListener<ClearScrollResponse> listener){\r\n    performRequestAsyncAndParseEntity(clearScrollRequest, RequestConverters::clearScroll, options, ClearScrollResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.restore",
	"Comment": "restores a snapshot.seesnapshot and restoreapi on elastic.co",
	"Method": "RestoreSnapshotResponse restore(RestoreSnapshotRequest restoreSnapshotRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(restoreSnapshotRequest, SnapshotRequestConverters::restoreSnapshot, options, RestoreSnapshotResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.dissect.DissectMatch.getResults",
	"Comment": "gets all the current matches. pass the results of this to isvalid to determine if a fully successful match has occured.",
	"Method": "Map<String, String> getResults(){\r\n    results.clear();\r\n    if (simpleCount > 0) {\r\n        results.putAll(simpleResults);\r\n    }\r\n    if (referenceCount > 0) {\r\n        referenceResults.forEach((k, v) -> results.put(v.getKey(), v.getValue()));\r\n    }\r\n    if (appendCount > 0) {\r\n        appendResults.forEach((k, v) -> results.put(k, v.getAppendResult()));\r\n    }\r\n    return results;\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.XContentBuilder.rawValue",
	"Comment": "writes a value with the source coming directly from the bytes in the stream",
	"Method": "Object rawValue(Object value,XContentBuilder rawValue,InputStream stream,XContentType contentType){\r\n    generator.writeRawValue(stream, contentType);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.verifyRepositoryAsync",
	"Comment": "asynchronously verifies a snapshot repository.seesnapshot and restoreapi on elastic.co",
	"Method": "void verifyRepositoryAsync(VerifyRepositoryRequest verifyRepositoryRequest,RequestOptions options,ActionListener<VerifyRepositoryResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(verifyRepositoryRequest, SnapshotRequestConverters::verifyRepository, options, VerifyRepositoryResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.combineFloat",
	"Comment": "combines a applytransformtodestination of int arrays in to one flat int array",
	"Method": "float[] combineFloat(List<float[]> nums){\r\n    int length = 0;\r\n    for (int i = 0; i < nums.size(); i++) length += nums.get(i).length;\r\n    float[] ret = new float[length];\r\n    int count = 0;\r\n    for (float[] i : nums) {\r\n        for (int j = 0; j < i.length; j++) {\r\n            ret[count++] = i[j];\r\n        }\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.DeleteDatafeedRequest.setForce",
	"Comment": "used to forcefully delete a started datafeed.this method is quicker than stopping and deleting the datafeed.",
	"Method": "void setForce(Boolean force){\r\n    this.force = force;\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.clearRealmCacheAsync",
	"Comment": "clears the cache in one or more realms asynchronously.see the docs for more.",
	"Method": "void clearRealmCacheAsync(ClearRealmCacheRequest request,RequestOptions options,ActionListener<ClearRealmCacheResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, SecurityRequestConverters::clearRealmCache, options, ClearRealmCacheResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.function.FunctionalUtils.cogroup",
	"Comment": "for each key in left and right, cogroup returns the list of valuesas a pair for each value present in left as well as right.",
	"Method": "Map<K, Pair<List<V>, List<V>>> cogroup(List<Pair<K, V>> left,List<Pair<K, V>> right){\r\n    Map<K, Pair<List<V>, List<V>>> ret = new HashMap();\r\n    Map<K, List<V>> leftMap = groupByKey(left);\r\n    Map<K, List<V>> rightMap = groupByKey(right);\r\n    for (Map.Entry<K, List<V>> entry : leftMap.entrySet()) {\r\n        K key = entry.getKey();\r\n        if (!ret.containsKey(key)) {\r\n            List<V> leftListPair = new ArrayList();\r\n            List<V> rightListPair = new ArrayList();\r\n            Pair<List<V>, List<V>> p = Pair.of(leftListPair, rightListPair);\r\n            ret.put(key, p);\r\n        }\r\n        Pair<List<V>, List<V>> p = ret.get(key);\r\n        p.getFirst().addAll(entry.getValue());\r\n    }\r\n    for (Map.Entry<K, List<V>> entry : rightMap.entrySet()) {\r\n        K key = entry.getKey();\r\n        if (!ret.containsKey(key)) {\r\n            List<V> leftListPair = new ArrayList();\r\n            List<V> rightListPair = new ArrayList();\r\n            Pair<List<V>, List<V>> p = Pair.of(leftListPair, rightListPair);\r\n            ret.put(key, p);\r\n        }\r\n        Pair<List<V>, List<V>> p = ret.get(key);\r\n        p.getSecond().addAll(entry.getValue());\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.isScopeActive",
	"Comment": "this method returns true if scope was opened, and not closed yet.",
	"Method": "boolean isScopeActive(){\r\n    return isOpen.get();\r\n}"
}, {
	"Path": "org.elasticsearch.grok.Grok.match",
	"Comment": "checks whether a specific text matches the defined grok expression.",
	"Method": "boolean match(String text){\r\n    Matcher matcher = compiledExpression.matcher(text.getBytes(StandardCharsets.UTF_8));\r\n    int result;\r\n    try {\r\n        threadWatchdog.register();\r\n        result = matcher.search(0, text.length(), Option.DEFAULT);\r\n    } finally {\r\n        threadWatchdog.unregister();\r\n    }\r\n    return (result != -1);\r\n}"
}, {
	"Path": "org.elasticsearch.client.benchmark.AbstractBenchmark.runGc",
	"Comment": "requests a full gc and checks whether the gc did actually run after a request. it retries up to 5 times in case the gc did notrun in time.",
	"Method": "void runGc(){\r\n    long previousCollections = getTotalGcCount();\r\n    int attempts = 0;\r\n    do {\r\n        System.gc();\r\n        try {\r\n            Thread.sleep(2000);\r\n        } catch (InterruptedException e) {\r\n            Thread.currentThread().interrupt();\r\n            return;\r\n        }\r\n        attempts++;\r\n    } while (previousCollections == getTotalGcCount() || attempts < 5);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.delete",
	"Comment": "deletes an index using the delete index api.see delete index api on elastic.co",
	"Method": "AcknowledgedResponse delete(DeleteIndexRequest deleteIndexRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(deleteIndexRequest, IndicesRequestConverters::deleteIndex, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.shape.Shape.strideUnsafe",
	"Comment": "get the stride of the specified dimension, without any input validation",
	"Method": "int strideUnsafe(DataBuffer buffer,int dimension,int rank,int strideUnsafe,int[] buffer,int dimension,int rank,long strideUnsafe,long[] buffer,int dimension,int rank){\r\n    return buffer[1 + rank + dimension];\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.retryLifecyclePolicyAsync",
	"Comment": "asynchronously retry the lifecycle step for given indicessee the docs for more.",
	"Method": "void retryLifecyclePolicyAsync(RetryLifecyclePolicyRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IndexLifecycleRequestConverters::retryLifecycle, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.WatcherClient.deleteWatchAsync",
	"Comment": "asynchronously deletes a watch from the clustersee the docs for more.",
	"Method": "void deleteWatchAsync(DeleteWatchRequest request,RequestOptions options,ActionListener<DeleteWatchResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, WatcherRequestConverters::deleteWatch, options, DeleteWatchResponse::fromXContent, listener, singleton(404));\r\n}"
}, {
	"Path": "org.nd4j.linalg.primitives.Counter.keepTopNElements",
	"Comment": "this method removes all elements except of top n by counter values",
	"Method": "void keepTopNElements(int N){\r\n    PriorityQueue<Pair<T, Double>> queue = asPriorityQueue();\r\n    clear();\r\n    for (int e = 0; e < N; e++) {\r\n        Pair<T, Double> pair = queue.poll();\r\n        if (pair != null)\r\n            incrementCount(pair.getFirst(), pair.getSecond());\r\n    }\r\n}"
}, {
	"Path": "io.dropwizard.client.HttpClientBuilder.createUserAgent",
	"Comment": "create a user agent string using the configured user agent if defined, otherwiseusing a combination of the environment name and this client name",
	"Method": "String createUserAgent(String name){\r\n    final String defaultUserAgent = environmentName == null ? name : String.format(\"%s (%s)\", environmentName, name);\r\n    return configuration.getUserAgent().orElse(defaultUserAgent);\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.Indices.isContiguous",
	"Comment": "returns whether indices are contiguousby a certain amount or not",
	"Method": "boolean isContiguous(int[] indices,int diff){\r\n    if (indices.length < 1)\r\n        return true;\r\n    for (int i = 1; i < indices.length; i++) {\r\n        if (Math.abs(indices[i] - indices[i - 1]) > diff)\r\n            return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.handleWrite",
	"Comment": "this method is called when a channel signals it is ready to receive writes. all of the write logicshould occur in this call.",
	"Method": "void handleWrite(SocketChannelContext context){\r\n    context.flushChannel();\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.rng.distribution.BaseDistribution.getSolverAbsoluteAccuracy",
	"Comment": "returns the solver absolute accuracy for inverse cumulative computation.you can override this method in order to use a brent solver with anabsolute accuracy different from the default.",
	"Method": "double getSolverAbsoluteAccuracy(){\r\n    return solverAbsoluteAccuracy;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.CastTests.testIllegalConversionsDef",
	"Comment": "test that without a cast, we fail when conversions would narrow.",
	"Method": "void testIllegalConversionsDef(){\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"def x = 5L; int y = +x; return y\");\r\n    });\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"def x = 5L; int y = (x + x); return y\");\r\n    });\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"def x = true; int y = +x; return y\");\r\n    });\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"def x = true; int y = (x ^ false); return y\");\r\n    });\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"def x = 5L; boolean y = +x; return y\");\r\n    });\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"def x = 5L; boolean y = (x + x); return y\");\r\n    });\r\n}"
}, {
	"Path": "org.nd4j.jita.handler.impl.CudaZeroHandler.copyforward",
	"Comment": "copies memory from host buffer to device.host copy is preserved as is.",
	"Method": "void copyforward(AllocationPoint point,AllocationShape shape){\r\n    log.info(\"copyforward() called on tp[\" + point.getObjectId() + \"], shape: \" + point.getShape());\r\n    throw new UnsupportedOperationException(\"Deprecated call\");\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.XContentFactory.smileBuilder",
	"Comment": "constructs a new json builder that will output the result into the provided output stream.",
	"Method": "XContentBuilder smileBuilder(XContentBuilder smileBuilder,OutputStream os){\r\n    return new XContentBuilder(SmileXContent.smileXContent, os);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.migration",
	"Comment": "provides methods for accessing the elastic licensed licensing apis thatare shipped with the default distribution of elasticsearch. all ofthese apis will 404 if run against the oss distribution of elasticsearch.see the migration apis on elastic.co for more information.",
	"Method": "MigrationClient migration(){\r\n    return migrationClient;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.sumOfMeanDifferences",
	"Comment": "used for calculating top part of simple regression forbeta 1",
	"Method": "double sumOfMeanDifferences(double[] vector,double[] vector2){\r\n    double mean = sum(vector) / vector.length;\r\n    double mean2 = sum(vector2) / vector2.length;\r\n    double ret = 0;\r\n    for (int i = 0; i < vector.length; i++) {\r\n        double vec1Diff = vector[i] - mean;\r\n        double vec2Diff = vector2[i] - mean2;\r\n        ret += vec1Diff * vec2Diff;\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.statusAsync",
	"Comment": "asynchronously gets the status of requested snapshots.seesnapshot and restoreapi on elastic.co",
	"Method": "void statusAsync(SnapshotsStatusRequest snapshotsStatusRequest,RequestOptions options,ActionListener<SnapshotsStatusResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(snapshotsStatusRequest, SnapshotRequestConverters::snapshotsStatus, options, SnapshotsStatusResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.BaseNDArrayFactory.valueArrayOf",
	"Comment": "creates an ndarray with the specified valueas theonly value in the ndarray",
	"Method": "INDArray valueArrayOf(int[] shape,double value,INDArray valueArrayOf,long[] shape,double value,INDArray valueArrayOf,long rows,long columns,double value){\r\n    INDArray create = createUninitialized(new long[] { rows, columns }, Nd4j.order());\r\n    create.assign(value);\r\n    return create;\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AtomicAllocator.getHostPointer",
	"Comment": "this method returns actual host pointer valid for current object",
	"Method": "Pointer getHostPointer(INDArray array,Pointer getHostPointer,DataBuffer buffer){\r\n    return memoryHandler.getHostPointer(buffer);\r\n}"
}, {
	"Path": "org.elasticsearch.client.license.StartTrialResponse.isAcknowledged",
	"Comment": "returns true if the request that corresponds to this response acknowledged license changes that would occur as a result of startinga trial license",
	"Method": "boolean isAcknowledged(){\r\n    return acknowledged;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.NodeAttributes.getTransportAddress",
	"Comment": "the host and port where transport http connections are accepted.",
	"Method": "String getTransportAddress(){\r\n    return transportAddress;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.putMapping",
	"Comment": "updates the mappings on an index using the put mapping api.see put mapping api on elastic.co",
	"Method": "AcknowledgedResponse putMapping(PutMappingRequest putMappingRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(putMappingRequest, IndicesRequestConverters::putMapping, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.api.Augmentation.count",
	"Comment": "counts the number of occurrences which satisfy the given predicate from inside this map",
	"Method": "int count(Iterable<T> receiver,Predicate<T> predicate,int count,Map<K, V> receiver,BiPredicate<K, V> predicate){\r\n    int count = 0;\r\n    for (Map.Entry<K, V> kvPair : receiver.entrySet()) {\r\n        if (predicate.test(kvPair.getKey(), kvPair.getValue())) {\r\n            count++;\r\n        }\r\n    }\r\n    return count;\r\n}"
}, {
	"Path": "io.dropwizard.servlets.tasks.LogConfigurationTask.getTimer",
	"Comment": "lazy create the timer to avoid unnecessary thread creation unless an expirable log configuration task is submittedmethod synchronization is acceptable since a log level task does not need to be highly performant",
	"Method": "Timer getTimer(){\r\n    if (timer == null) {\r\n        timer = new Timer(LogConfigurationTask.class.getSimpleName(), true);\r\n    }\r\n    return timer;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.clearCacheAsync",
	"Comment": "asynchronously clears the cache of one or more indices using the clear cache api.see clear cache api on elastic.co",
	"Method": "void clearCacheAsync(ClearIndicesCacheRequest clearIndicesCacheRequest,RequestOptions options,ActionListener<ClearIndicesCacheResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(clearIndicesCacheRequest, IndicesRequestConverters::clearCache, options, ClearIndicesCacheResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.Indices.adjustIndices",
	"Comment": "prunes indices of greater length than the shapeand fills in missing indices if there are any",
	"Method": "INDArrayIndex[] adjustIndices(int[] originalShape,INDArrayIndex indexes){\r\n    if (Shape.isVector(originalShape) && indexes.length == 1)\r\n        return indexes;\r\n    if (indexes.length < originalShape.length)\r\n        indexes = fillIn(originalShape, indexes);\r\n    if (indexes.length > originalShape.length) {\r\n        INDArrayIndex[] ret = new INDArrayIndex[originalShape.length];\r\n        System.arraycopy(indexes, 0, ret, 0, originalShape.length);\r\n        return ret;\r\n    }\r\n    if (indexes.length == originalShape.length)\r\n        return indexes;\r\n    for (int i = 0; i < indexes.length; i++) {\r\n        if (indexes[i].end() >= originalShape[i] || indexes[i] instanceof NDArrayIndexAll)\r\n            indexes[i] = NDArrayIndex.interval(0, originalShape[i] - 1);\r\n    }\r\n    return indexes;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteFilter",
	"Comment": "deletes the given machine learning filterfor additional infosee ml delete filter documentation",
	"Method": "AcknowledgedResponse deleteFilter(DeleteFilterRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::deleteFilter, options, AcknowledgedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.startDatafeed",
	"Comment": "starts the given machine learning datafeedfor additional infosee ml start datafeed documentation",
	"Method": "StartDatafeedResponse startDatafeed(StartDatafeedRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::startDatafeed, options, StartDatafeedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.lossfunctions.impl.LossMixtureDensity.computeScore",
	"Comment": "computes the aggregate score as a sum of all of the individual scores ofeach of the labels against each of the outputs of the network.forthe mixture density network, this is the negative log likelihood thatthe given labels fall within the probability distribution described bythe mixture of gaussians of the network output.",
	"Method": "double computeScore(INDArray labels,INDArray preOutput,IActivation activationFn,INDArray mask,boolean average){\r\n    INDArray scoreArr = computeScoreArray(labels, preOutput, activationFn, mask);\r\n    double score = scoreArr.sumNumber().doubleValue();\r\n    if (average) {\r\n        score /= scoreArr.size(0);\r\n    }\r\n    return score;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.multiGet",
	"Comment": "retrieves multiple documents by id using the multi get api.see multi get api on elastic.co",
	"Method": "MultiGetResponse multiGet(MultiGetRequest multiGetRequest,RequestOptions options){\r\n    return mget(multiGetRequest, options);\r\n}"
}, {
	"Path": "org.nd4j.jita.constant.ProtectedCudaShapeInfoProvider.purgeCache",
	"Comment": "this method forces cache purge, if cache is available for specific implementation",
	"Method": "void purgeCache(){\r\n    protector.purgeProtector();\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.fieldCapsAsync",
	"Comment": "asynchronously executes a request using the field capabilities api.see field capabilities apion elastic.co.",
	"Method": "void fieldCapsAsync(FieldCapabilitiesRequest fieldCapabilitiesRequest,RequestOptions options,ActionListener<FieldCapabilitiesResponse> listener){\r\n    performRequestAsyncAndParseEntity(fieldCapabilitiesRequest, RequestConverters::fieldCaps, options, FieldCapabilitiesResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.VoidParameterServerStressTest.getSGRM",
	"Comment": "this method just produces random sgrm requests, fot testing purposes.no real sense could be found here.",
	"Method": "SkipGramRequestMessage getSGRM(){\r\n    int w1 = RandomUtils.nextInt(0, NUM_WORDS);\r\n    int w2 = RandomUtils.nextInt(0, NUM_WORDS);\r\n    byte[] codes = new byte[RandomUtils.nextInt(15, 45)];\r\n    int[] points = new int[codes.length];\r\n    for (int e = 0; e < codes.length; e++) {\r\n        codes[e] = (byte) (e % 2 == 0 ? 0 : 1);\r\n        points[e] = RandomUtils.nextInt(0, NUM_WORDS);\r\n    }\r\n    return new SkipGramRequestMessage(w1, w2, points, codes, (short) 0, 0.025, 213412L);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.flushSynced",
	"Comment": "initiate a synced flush manually using the synced flush api.see synced flush api on elastic.co",
	"Method": "SyncedFlushResponse flushSynced(SyncedFlushRequest syncedFlushRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(syncedFlushRequest, IndicesRequestConverters::flushSynced, options, SyncedFlushResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.rankEval",
	"Comment": "executes a request using the ranking evaluation api.see ranking evaluation apion elastic.co",
	"Method": "RankEvalResponse rankEval(RankEvalRequest rankEvalRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(rankEvalRequest, RequestConverters::rankEval, options, RankEvalResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.GetDatafeedRequest.setAllowNoDatafeeds",
	"Comment": "whether to ignore if a wildcard expression matches no datafeeds.",
	"Method": "void setAllowNoDatafeeds(boolean allowNoDatafeeds){\r\n    this.allowNoDatafeeds = allowNoDatafeeds;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.CastTests.testIllegalConversions",
	"Comment": "test that without a cast, we fail when conversions would narrow.",
	"Method": "void testIllegalConversions(){\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"long x = 5L; int y = +x; return y\");\r\n    });\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"long x = 5L; int y = (x + x); return y\");\r\n    });\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"boolean x = true; int y = +x; return y\");\r\n    });\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"boolean x = true; int y = (x ^ false); return y\");\r\n    });\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"long x = 5L; boolean y = +x; return y\");\r\n    });\r\n    expectScriptThrows(ClassCastException.class, () -> {\r\n        exec(\"long x = 5L; boolean y = (x + x); return y\");\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.putTemplate",
	"Comment": "puts an index template using the index templates api.seeindex templates apion elastic.co",
	"Method": "AcknowledgedResponse putTemplate(PutIndexTemplateRequest putIndexTemplateRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(putIndexTemplateRequest, IndicesRequestConverters::putTemplate, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.deleteUserAsync",
	"Comment": "asynchronously deletes a user in the native realm.see the docs for more.",
	"Method": "void deleteUserAsync(DeleteUserRequest request,RequestOptions options,ActionListener<DeleteUserResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, SecurityRequestConverters::deleteUser, options, DeleteUserResponse::fromXContent, listener, singleton(404));\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.VoidParameterServer.getRole",
	"Comment": "this method checks for designated role, according to local ip addresses and configuration passed into method",
	"Method": "Pair<NodeRole, String> getRole(VoidConfiguration voidConfiguration,Collection<String> localIPs){\r\n    NodeRole result = NodeRole.CLIENT;\r\n    for (String ip : voidConfiguration.getShardAddresses()) {\r\n        String cleansed = ip.replaceAll(\":.*\", \"\");\r\n        if (localIPs.contains(cleansed))\r\n            return Pair.create(NodeRole.SHARD, ip);\r\n    }\r\n    if (voidConfiguration.getBackupAddresses() != null)\r\n        for (String ip : voidConfiguration.getBackupAddresses()) {\r\n            String cleansed = ip.replaceAll(\":.*\", \"\");\r\n            if (localIPs.contains(cleansed))\r\n                return Pair.create(NodeRole.BACKUP, ip);\r\n        }\r\n    String sparkIp = null;\r\n    if (sparkIp == null && voidConfiguration.getNetworkMask() != null) {\r\n        NetworkOrganizer organizer = new NetworkOrganizer(voidConfiguration.getNetworkMask());\r\n        sparkIp = organizer.getMatchingAddress();\r\n    }\r\n    if (sparkIp == null)\r\n        sparkIp = System.getenv(ND4JEnvironmentVars.DL4J_VOID_IP);\r\n    log.info(\"Got [{}] as sparkIp\", sparkIp);\r\n    if (sparkIp == null)\r\n        throw new ND4JIllegalStateException(\"Can't get IP address for UDP communcation\");\r\n    return Pair.create(result, sparkIp + \":\" + voidConfiguration.getUnicastControllerPort());\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AllocationPoint.getHostReadTime",
	"Comment": "returns time, in milliseconds, when this point was accessed on host side",
	"Method": "long getHostReadTime(){\r\n    return accessHostRead.get();\r\n}"
}, {
	"Path": "org.elasticsearch.http.nio.cors.NioCorsConfig.isOriginAllowed",
	"Comment": "returns whether the input origin is allowed by this configuration.",
	"Method": "boolean isOriginAllowed(String origin){\r\n    if (origins.isPresent()) {\r\n        return origins.get().contains(origin);\r\n    } else if (pattern.isPresent()) {\r\n        return pattern.get().matcher(origin).matches();\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.client.CcrClient.resumeFollow",
	"Comment": "instructs a follower index to resume the following of a leader index.see the docs for more.",
	"Method": "AcknowledgedResponse resumeFollow(ResumeFollowRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, CcrRequestConverters::resumeFollow, options, AcknowledgedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AtomicAllocator.setMemoryHandler",
	"Comment": "this method specifies mover implementation to be used internally",
	"Method": "void setMemoryHandler(MemoryHandler memoryHandler){\r\n    globalLock.writeLock().lock();\r\n    this.memoryHandler = memoryHandler;\r\n    this.memoryHandler.init(configuration, this);\r\n    globalLock.writeLock().unlock();\r\n}"
}, {
	"Path": "org.nd4j.linalg.learning.legacy.AdaGrad.getGradient",
	"Comment": "gets feature specific learning ratesadagrad keeps a history of gradients being passed in.note that each gradient passed in becomes adapted over time, hencethe opname adagrad",
	"Method": "INDArray getGradient(INDArray gradient,int iteration,double getGradient,double gradient,int column,int[] shape,INDArray getGradient,INDArray gradient,int slice,int[] shape){\r\n    boolean historicalInitialized = false;\r\n    INDArray sqrtHistory;\r\n    if (this.historicalGradient == null) {\r\n        this.historicalGradient = Nd4j.zeros(shape).add(epsilon);\r\n        historicalInitialized = true;\r\n    } else if (!this.historicalGradient.isVector() && this.historicalGradient.slice(slice).length() != gradient.length())\r\n        throw new IllegalArgumentException(\"Illegal gradient\");\r\n    if (historicalGradient.isVector())\r\n        sqrtHistory = sqrt(historicalGradient);\r\n    else\r\n        sqrtHistory = !historicalInitialized ? sqrt(historicalGradient.slice(slice)) : historicalGradient;\r\n    INDArray learningRates;\r\n    try {\r\n        learningRates = sqrtHistory.rdivi(learningRate);\r\n    } catch (ArithmeticException ae) {\r\n        learningRates = sqrtHistory.rdivi(learningRate + epsilon);\r\n    }\r\n    if (gradient.length() != learningRates.length())\r\n        gradient.muli(learningRates.slice(slice));\r\n    else\r\n        gradient.muli(learningRates);\r\n    this.historicalGradient.slice(slice).addi(gradient.mul(gradient));\r\n    numIterations++;\r\n    return gradient;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.VoidParameterServerStressTest.testPerformanceStandalone1",
	"Comment": "this test measures performance of blocking messages processing, vectorrequestmessage in this case",
	"Method": "void testPerformanceStandalone1(){\r\n    VoidConfiguration voidConfiguration = VoidConfiguration.builder().networkMask(\"192.168.0.0/16\").numberOfShards(1).build();\r\n    voidConfiguration.setShardAddresses(\"192.168.1.35\");\r\n    VoidParameterServer parameterServer = new VoidParameterServer();\r\n    parameterServer.init(voidConfiguration);\r\n    parameterServer.initializeSeqVec(100, NUM_WORDS, 123, 10, true, false);\r\n    final List<Long> times = new CopyOnWriteArrayList();\r\n    Thread[] threads = new Thread[8];\r\n    for (int t = 0; t < threads.length; t++) {\r\n        final int e = t;\r\n        threads[t] = new Thread(() -> {\r\n            List<Long> results = new ArrayList();\r\n            int chunk = NUM_WORDS / threads.length;\r\n            int start = e * chunk;\r\n            int end = (e + 1) * chunk;\r\n            for (int i = 0; i < 1000000; i++) {\r\n                long time1 = System.nanoTime();\r\n                INDArray array = parameterServer.getVector(RandomUtils.nextInt(start, end));\r\n                long time2 = System.nanoTime();\r\n                results.add(time2 - time1);\r\n                if ((i + 1) % 1000 == 0)\r\n                    log.info(\"Thread {} cnt {}\", e, i + 1);\r\n            }\r\n            times.addAll(results);\r\n        });\r\n        threads[t].setDaemon(true);\r\n        threads[t].start();\r\n    }\r\n    for (int t = 0; t < threads.length; t++) {\r\n        try {\r\n            threads[t].join();\r\n        } catch (Exception e) {\r\n        }\r\n    }\r\n    List<Long> newTimes = new ArrayList(times);\r\n    Collections.sort(newTimes);\r\n    log.info(\"p50: {} us\", newTimes.get(newTimes.size() / 2) / 1000);\r\n    parameterServer.shutdown();\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AtomicAllocator.getAllocationPoint",
	"Comment": "this method returns allocationpoint pojo for specified tracking id",
	"Method": "AllocationPoint getAllocationPoint(Long objectId,AllocationPoint getAllocationPoint,INDArray array,AllocationPoint getAllocationPoint,DataBuffer buffer){\r\n    if (buffer instanceof CompressedDataBuffer) {\r\n        log.warn(\"Trying to get AllocationPoint from CompressedDataBuffer\");\r\n        throw new RuntimeException(\"AP CDB\");\r\n    }\r\n    return getAllocationPoint(buffer.getTrackingPoint());\r\n}"
}, {
	"Path": "org.elasticsearch.core.internal.io.Streams.copy",
	"Comment": "copy the contents of the given inputstream to the given outputstream.closes both streams when done.",
	"Method": "long copy(InputStream in,OutputStream out){\r\n    Exception err = null;\r\n    try {\r\n        final long byteCount = in.transferTo(out);\r\n        out.flush();\r\n        return byteCount;\r\n    } catch (IOException | RuntimeException e) {\r\n        err = e;\r\n        throw e;\r\n    } finally {\r\n        IOUtils.close(err, in, out);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.multiSearchAsync",
	"Comment": "asynchronously executes a multi search using the msearch api.see multi search api onelastic.co",
	"Method": "void multiSearchAsync(MultiSearchRequest searchRequest,RequestOptions options,ActionListener<MultiSearchResponse> listener){\r\n    msearchAsync(searchRequest, options, listener);\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.TimeDelayedParameterUpdater.shouldReplicate",
	"Comment": "returns true ifthe updater has accumulated enough ndarrays toreplicate to the workers",
	"Method": "boolean shouldReplicate(){\r\n    long now = System.currentTimeMillis();\r\n    long diff = Math.abs(now - lastSynced);\r\n    return diff > syncTime;\r\n}"
}, {
	"Path": "org.elasticsearch.plugin.noop.action.search.NoopSearchRequestBuilder.storedFields",
	"Comment": "sets the fields to load and return as part of the search request. if noneare specified, the source of the document will be returned.",
	"Method": "NoopSearchRequestBuilder storedFields(String fields){\r\n    sourceBuilder().storedFields(Arrays.asList(fields));\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.postCalendarEvent",
	"Comment": "creates new events for a a machine learning calendarfor additional infoseeadd events to calendar api",
	"Method": "PostCalendarEventResponse postCalendarEvent(PostCalendarEventRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::postCalendarEvents, options, PostCalendarEventResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.splitAsync",
	"Comment": "asynchronously splits an index using the split index api.see split index api on elastic.co",
	"Method": "void splitAsync(ResizeRequest resizeRequest,RequestOptions options,ActionListener<ResizeResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(resizeRequest, IndicesRequestConverters::split, options, ResizeResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.Float8.getCompressionType",
	"Comment": "this method returns compression optype provided by specific ndarraycompressor implementation",
	"Method": "CompressionType getCompressionType(){\r\n    return CompressionType.LOSSY;\r\n}"
}, {
	"Path": "org.elasticsearch.client.license.GetBasicStatusResponse.isEligibleToStartBasic",
	"Comment": "returns whether the license is eligible to start basic or not",
	"Method": "boolean isEligibleToStartBasic(){\r\n    return eligibleToStartBasic;\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore.listBlobs",
	"Comment": "list blobs in the specific bucket under the specified path. the path root is removed.",
	"Method": "Map<String, BlobMetaData> listBlobs(String path){\r\n    return listBlobsByPrefix(path, \"\");\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.shape.Shape.isColumnVectorShape",
	"Comment": "returns true if the given shape length is 2and the size at element 1 is 1",
	"Method": "boolean isColumnVectorShape(int[] shape,boolean isColumnVectorShape,long[] shape){\r\n    return (shape.length == 2 && shape[1] == 1);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.stopILMAsync",
	"Comment": "asynchronously stop the index lifecycle management feature.see the docs for more.",
	"Method": "void stopILMAsync(StopILMRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IndexLifecycleRequestConverters::stopILM, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.azure.AzureStorageSettings.load",
	"Comment": "parse and read all settings available under the azure.client. namespace",
	"Method": "Map<String, AzureStorageSettings> load(Settings settings){\r\n    final Map<String, AzureStorageSettings> storageSettings = new HashMap();\r\n    for (final String clientName : ACCOUNT_SETTING.getNamespaces(settings)) {\r\n        storageSettings.put(clientName, getClientSettings(settings, clientName));\r\n    }\r\n    if (false == storageSettings.containsKey(\"default\") && false == storageSettings.isEmpty()) {\r\n        final AzureStorageSettings defaultSettings = storageSettings.values().iterator().next();\r\n        storageSettings.put(\"default\", defaultSettings);\r\n    }\r\n    assert storageSettings.containsKey(\"default\") || storageSettings.isEmpty() : \"always have 'default' if any\";\r\n    return Collections.unmodifiableMap(storageSettings);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestClientBuilder.setStrictDeprecationMode",
	"Comment": "whether the rest client should return any response containing at leastone warning header as a failure.",
	"Method": "RestClientBuilder setStrictDeprecationMode(boolean strictDeprecationMode){\r\n    this.strictDeprecationMode = strictDeprecationMode;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests.testBasicUsage",
	"Comment": "turkish has some funny casing. this test shows how you can solve this kind of thing easily with collation. instead of using lowercasefilter, use a turkish collator with primary strength. then things will sort and match correctly.",
	"Method": "void testBasicUsage(){\r\n    Settings settings = Settings.builder().put(\"index.analysis.filter.myCollator.type\", \"icu_collation\").put(\"index.analysis.filter.myCollator.language\", \"tr\").put(\"index.analysis.filter.myCollator.strength\", \"primary\").build();\r\n    TestAnalysis analysis = createTestAnalysis(new Index(\"test\", \"_na_\"), settings, new AnalysisICUPlugin());\r\n    TokenFilterFactory filterFactory = analysis.tokenFilter.get(\"myCollator\");\r\n    assertCollatesToSame(filterFactory, \"I WİLL USE TURKİSH CASING\", \"ı will use turkish casıng\");\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.JCublasNDArrayFactory.average",
	"Comment": "this method averages input arrays, and returns averaged array",
	"Method": "INDArray average(INDArray target,INDArray[] arrays,INDArray average,Collection<INDArray> arrays,INDArray average,INDArray[] arrays,INDArray average,INDArray target,Collection<INDArray> arrays){\r\n    return average(target, arrays.toArray(new INDArray[0]));\r\n}"
}, {
	"Path": "org.nd4j.jita.concurrency.CudaAffinityManager.replicateToDevice",
	"Comment": "this method replicates given databuffer, and places it to target device.",
	"Method": "INDArray replicateToDevice(Integer deviceId,INDArray array,DataBuffer replicateToDevice,Integer deviceId,DataBuffer buffer){\r\n    if (buffer == null)\r\n        return null;\r\n    int currentDeviceId = AtomicAllocator.getInstance().getDeviceId();\r\n    if (currentDeviceId != deviceId) {\r\n        NativeOpsHolder.getInstance().getDeviceNativeOps().setDevice(new CudaPointer(deviceId));\r\n        Nd4j.getAffinityManager().attachThreadToDevice(Thread.currentThread().getId(), deviceId);\r\n    }\r\n    DataBuffer dstBuffer = Nd4j.createBuffer(buffer.length(), false);\r\n    AtomicAllocator.getInstance().memcpy(dstBuffer, buffer);\r\n    if (currentDeviceId != deviceId) {\r\n        NativeOpsHolder.getInstance().getDeviceNativeOps().setDevice(new CudaPointer(currentDeviceId));\r\n        Nd4j.getAffinityManager().attachThreadToDevice(Thread.currentThread().getId(), currentDeviceId);\r\n    }\r\n    return dstBuffer;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.testScrollResponseBatchingBehavior",
	"Comment": "tests that each scroll response is a batch and that the batch is launched properly.",
	"Method": "void testScrollResponseBatchingBehavior(){\r\n    int maxBatches = randomIntBetween(0, 100);\r\n    for (int batches = 1; batches < maxBatches; batches++) {\r\n        Hit hit = new ScrollableHitSource.BasicHit(\"index\", \"type\", \"id\", 0);\r\n        ScrollableHitSource.Response response = new ScrollableHitSource.Response(false, emptyList(), 1, singletonList(hit), null);\r\n        DummyAsyncBulkByScrollAction action = new DummyAsyncBulkByScrollAction();\r\n        simulateScrollResponse(action, timeValueNanos(System.nanoTime()), 0, response);\r\n        final int expectedBatches = batches;\r\n        assertBusy(() -> assertEquals(expectedBatches, testTask.getStatus().getBatches()));\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.BaseNd4jTest.ordering",
	"Comment": "the ordering for this testthis test will only be invoked forthe given testand ignored for others",
	"Method": "char ordering(){\r\n    return 'a';\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.s3.S3Service.client",
	"Comment": "attempts to retrieve a client by name from the cache. if the client does notexist it will be created.",
	"Method": "AmazonS3Reference client(String clientName){\r\n    AmazonS3Reference clientReference = clientsCache.get(clientName);\r\n    if ((clientReference != null) && clientReference.tryIncRef()) {\r\n        return clientReference;\r\n    }\r\n    synchronized (this) {\r\n        clientReference = clientsCache.get(clientName);\r\n        if ((clientReference != null) && clientReference.tryIncRef()) {\r\n            return clientReference;\r\n        }\r\n        final S3ClientSettings clientSettings = clientsSettings.get(clientName);\r\n        if (clientSettings == null) {\r\n            throw new IllegalArgumentException(\"Unknown s3 client name [\" + clientName + \"]. Existing client configs: \" + Strings.collectionToDelimitedString(clientsSettings.keySet(), \",\"));\r\n        }\r\n        logger.debug(\"creating S3 client with client_name [{}], endpoint [{}]\", clientName, clientSettings.endpoint);\r\n        clientReference = new AmazonS3Reference(buildClient(clientSettings));\r\n        clientReference.incRef();\r\n        clientsCache = MapBuilder.newMapBuilder(clientsCache).put(clientName, clientReference).immutableMap();\r\n        return clientReference;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.painless.lookup.PainlessCast.boxTargetType",
	"Comment": "create a cast where the target type will be boxed, and then the cast will be performed.",
	"Method": "PainlessCast boxTargetType(Class<?> originalType,Class<?> targetType,boolean explicitCast,Class<?> boxTargetType){\r\n    Objects.requireNonNull(originalType);\r\n    Objects.requireNonNull(targetType);\r\n    Objects.requireNonNull(boxTargetType);\r\n    return new PainlessCast(originalType, targetType, explicitCast, null, null, null, boxTargetType);\r\n}"
}, {
	"Path": "org.elasticsearch.client.tasks.GetTaskRequest.setTimeout",
	"Comment": "timeout to wait for any async actions this request must take.",
	"Method": "GetTaskRequest setTimeout(TimeValue timeout){\r\n    this.timeout = timeout;\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.util.MessageSplitter.split",
	"Comment": "this method splits voidmessage into chunks, and returns them as collection",
	"Method": "Collection<VoidChunk> split(VoidMessage message,int maxBytes){\r\n    if (maxBytes <= 0)\r\n        throw new ND4JIllegalStateException(\"MaxBytes must be > 0\");\r\n    val tempFile = ND4JFileUtils.createTempFile(\"messageSplitter\", \"temp\");\r\n    val result = new ArrayList<VoidChunk>();\r\n    try (val fos = new FileOutputStream(tempFile);\r\n        val bos = new BufferedOutputStream(fos)) {\r\n        SerializationUtils.serialize(message, fos);\r\n        val length = tempFile.length();\r\n        int numChunks = (int) (length / maxBytes + (length % maxBytes > 0 ? 1 : 0));\r\n        try (val fis = new FileInputStream(tempFile);\r\n            val bis = new BufferedInputStream(fis)) {\r\n            val bytes = new byte[maxBytes];\r\n            int cnt = 0;\r\n            int id = 0;\r\n            while (cnt < length) {\r\n                val c = bis.read(bytes);\r\n                val tmp = Arrays.copyOf(bytes, c);\r\n                val msg = VoidChunk.builder().messageId(java.util.UUID.randomUUID().toString()).originalId(message.getMessageId()).chunkId(id++).numberOfChunks(numChunks).splitSize(maxBytes).payload(tmp).totalSize(length).build();\r\n                result.add(msg);\r\n                cnt += c;\r\n            }\r\n        }\r\n    }\r\n    tempFile.delete();\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.hasPrivileges",
	"Comment": "determine whether the current user has a specified list of privilegessee the docs for more.",
	"Method": "HasPrivilegesResponse hasPrivileges(HasPrivilegesRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, SecurityRequestConverters::hasPrivileges, options, HasPrivilegesResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.closeException",
	"Comment": "this method is called when an attempt to close a channel throws an exception.",
	"Method": "void closeException(ChannelContext<?> channel,Exception exception){\r\n    channel.handleException(exception);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.validateQuery",
	"Comment": "validate a potentially expensive query without executing it.seevalidate query apion elastic.co",
	"Method": "ValidateQueryResponse validateQuery(ValidateQueryRequest validateQueryRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(validateQueryRequest, IndicesRequestConverters::validateQuery, options, ValidateQueryResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.createAsync",
	"Comment": "asynchronously creates a snapshot.seesnapshot and restoreapi on elastic.co",
	"Method": "void createAsync(CreateSnapshotRequest createSnapshotRequest,RequestOptions options,ActionListener<CreateSnapshotResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(createSnapshotRequest, SnapshotRequestConverters::createSnapshot, options, CreateSnapshotResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.process.DataCounts.getInputBytes",
	"Comment": "the total number of bytes sent to this job.this value includes the bytes from anyrecordsthat have been discarded for anyreasone.g. because the date cannot be read",
	"Method": "long getInputBytes(){\r\n    return inputBytes;\r\n}"
}, {
	"Path": "org.nd4j.jita.flow.impl.AsynchronousFlowController.sweepTail",
	"Comment": "this method ensures the events in the beginning of fifo queues are finished",
	"Method": "void sweepTail(){\r\n    Integer deviceId = allocator.getDeviceId();\r\n    int cnt = 0;\r\n    long lastCommandId = deviceClocks.get(deviceId).get();\r\n    for (int l = 0; l < configuration.getCommandLanesNumber(); l++) {\r\n        Queue<cudaEvent_t> queue = eventsBarrier.get(deviceId).get(l);\r\n        if (queue.size() >= MAX_EXECUTION_QUEUE || laneClocks.get(deviceId).get(l).get() < lastCommandId - MAX_EXECUTION_QUEUE) {\r\n            cudaEvent_t event = queue.poll();\r\n            if (event != null && !event.isDestroyed()) {\r\n                event.synchronize();\r\n                event.destroy();\r\n                cnt++;\r\n            }\r\n        }\r\n    }\r\n    deviceClocks.get(deviceId).incrementAndGet();\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.ModelParameterServer.addModelParamsSubscriber",
	"Comment": "this method adds subcriber that will be called upon model params receival",
	"Method": "void addModelParamsSubscriber(Subscriber<INDArray> s){\r\n    modelParamsSubsribers.add(s);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.searchScrollAsync",
	"Comment": "asynchronously executes a search using the search scroll api.see search scrollapi on elastic.co",
	"Method": "void searchScrollAsync(SearchScrollRequest searchScrollRequest,RequestOptions options,ActionListener<SearchResponse> listener){\r\n    scrollAsync(searchScrollRequest, options, listener);\r\n}"
}, {
	"Path": "org.nd4j.linalg.compression.CompressionDescriptor.fromByteBuffer",
	"Comment": "instantiate a compression descriptor fromthe given bytebuffer",
	"Method": "CompressionDescriptor fromByteBuffer(ByteBuffer byteBuffer){\r\n    CompressionDescriptor compressionDescriptor = new CompressionDescriptor();\r\n    int compressionTypeOrdinal = byteBuffer.getInt();\r\n    CompressionType compressionType = CompressionType.values()[compressionTypeOrdinal];\r\n    compressionDescriptor.setCompressionType(compressionType);\r\n    int compressionAlgoOrdinal = byteBuffer.getInt();\r\n    CompressionAlgorithm compressionAlgorithm = CompressionAlgorithm.values()[compressionAlgoOrdinal];\r\n    compressionDescriptor.setCompressionAlgorithm(compressionAlgorithm.name());\r\n    compressionDescriptor.setOriginalLength(byteBuffer.getLong());\r\n    compressionDescriptor.setCompressedLength(byteBuffer.getLong());\r\n    compressionDescriptor.setNumberOfElements(byteBuffer.getLong());\r\n    compressionDescriptor.setOriginalElementSize(byteBuffer.getLong());\r\n    return compressionDescriptor;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.convertNegativeIndices",
	"Comment": "convert all dimensions in the specifiedaxes array to be positivebased on the specified range of values",
	"Method": "int[] convertNegativeIndices(int range,int[] axes){\r\n    int[] axesRet = ArrayUtil.range(0, range);\r\n    int[] newAxes = ArrayUtil.copy(axes);\r\n    for (int i = 0; i < axes.length; i++) {\r\n        newAxes[i] = axes[axesRet[i]];\r\n    }\r\n    return newAxes;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.config.Detector.getPartitionFieldName",
	"Comment": "segments the analysis along another field to have completelyindependent baselines for each instance of partitionfield",
	"Method": "String getPartitionFieldName(){\r\n    return partitionFieldName;\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.getInitialBlockSize",
	"Comment": "this method returns number of bytes for first block of circular workspace.",
	"Method": "long getInitialBlockSize(){\r\n    return initialBlockSize.get();\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.getTemplateAsync",
	"Comment": "asynchronously gets index templates using the index templates apiseeindex templates apion elastic.co",
	"Method": "void getTemplateAsync(GetIndexTemplatesRequest getIndexTemplatesRequest,RequestOptions options,ActionListener<GetIndexTemplatesResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(getIndexTemplatesRequest, IndicesRequestConverters::getTemplates, options, GetIndexTemplatesResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.Locals.lookupVariable",
	"Comment": "looks up a variable at this scope only. returns null if the variable does not exist.",
	"Method": "Variable lookupVariable(Location location,String name){\r\n    if (variables == null) {\r\n        return null;\r\n    }\r\n    return variables.get(name);\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.findFileStructureAsync",
	"Comment": "finds the structure of a file asynchronously and notifies the listener on completionfor additional infosee ml find file structure documentation",
	"Method": "void findFileStructureAsync(FindFileStructureRequest request,RequestOptions options,ActionListener<FindFileStructureResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::findFileStructure, options, FindFileStructureResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RollupClient.deleteRollupJobAsync",
	"Comment": "asynchronously delete a rollup job from the clustersee the docs for details.",
	"Method": "void deleteRollupJobAsync(DeleteRollupJobRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, RollupRequestConverters::deleteJob, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.lossfunctions.impl.LossMixtureDensity.negativeLogLikelihood",
	"Comment": "this method returns an array consisting of each of the training samples,for each label in each sample, the negative log likelihood of thatvalue falling within the given gaussian mixtures.",
	"Method": "INDArray negativeLogLikelihood(INDArray labels,INDArray alpha,INDArray mu,INDArray sigma){\r\n    INDArray labelsMinusMu = labelsMinusMu(labels, mu);\r\n    INDArray diffsquared = labelsMinusMu.mul(labelsMinusMu).sum(2);\r\n    INDArray phitimesalphasum = phi(diffsquared, sigma).muli(alpha).sum(1);\r\n    INDArray result = Transforms.log(phitimesalphasum).negi();\r\n    return result;\r\n}"
}, {
	"Path": "org.apache.lucene.search.uhighlight.CustomUnifiedHighlighter.rewriteCustomQuery",
	"Comment": "translate custom queries in queries that are supported by the unified highlighter.",
	"Method": "Collection<Query> rewriteCustomQuery(Query query){\r\n    if (query instanceof MultiPhrasePrefixQuery) {\r\n        MultiPhrasePrefixQuery mpq = (MultiPhrasePrefixQuery) query;\r\n        Term[][] terms = mpq.getTerms();\r\n        int[] positions = mpq.getPositions();\r\n        SpanQuery[] positionSpanQueries = new SpanQuery[positions.length];\r\n        int sizeMinus1 = terms.length - 1;\r\n        for (int i = 0; i < positions.length; i++) {\r\n            SpanQuery[] innerQueries = new SpanQuery[terms[i].length];\r\n            for (int j = 0; j < terms[i].length; j++) {\r\n                if (i == sizeMinus1) {\r\n                    innerQueries[j] = new SpanMultiTermQueryWrapper<PrefixQuery>(new PrefixQuery(terms[i][j]));\r\n                } else {\r\n                    innerQueries[j] = new SpanTermQuery(terms[i][j]);\r\n                }\r\n            }\r\n            if (innerQueries.length > 1) {\r\n                positionSpanQueries[i] = new SpanOrQuery(innerQueries);\r\n            } else {\r\n                positionSpanQueries[i] = innerQueries[0];\r\n            }\r\n        }\r\n        if (positionSpanQueries.length == 1) {\r\n            return Collections.singletonList(positionSpanQueries[0]);\r\n        }\r\n        int positionGaps = 0;\r\n        if (positions.length >= 2) {\r\n            positionGaps = Math.max(0, positions[positions.length - 1] - positions[0] - positions.length + 1);\r\n        }\r\n        boolean inorder = (mpq.getSlop() == 0);\r\n        return Collections.singletonList(new SpanNearQuery(positionSpanQueries, mpq.getSlop() + positionGaps, inorder));\r\n    } else if (query instanceof CommonTermsQuery) {\r\n        CommonTermsQuery ctq = (CommonTermsQuery) query;\r\n        List<Query> tqs = new ArrayList();\r\n        for (Term term : ctq.getTerms()) {\r\n            tqs.add(new TermQuery(term));\r\n        }\r\n        return tqs;\r\n    } else if (query instanceof FunctionScoreQuery) {\r\n        return Collections.singletonList(((FunctionScoreQuery) query).getSubQuery());\r\n    } else if (query instanceof ESToParentBlockJoinQuery) {\r\n        return Collections.singletonList(((ESToParentBlockJoinQuery) query).getChildQuery());\r\n    } else {\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.script.mustache.SearchTemplateIT.testTemplateQueryAsEscapedString",
	"Comment": "test that template can be expressed as a single escaped string.",
	"Method": "void testTemplateQueryAsEscapedString(){\r\n    SearchRequest searchRequest = new SearchRequest();\r\n    searchRequest.indices(\"_all\");\r\n    String query = \"{\" + \"  \\\"source\\\" : \\\"{ \\\\\\\"size\\\\\\\": \\\\\\\"{{size}}\\\\\\\", \\\\\\\"query\\\\\\\":{\\\\\\\"match_all\\\\\\\":{}}}\\\",\" + \"  \\\"params\\\":{\" + \"    \\\"size\\\": 1\" + \"  }\" + \"}\";\r\n    SearchTemplateRequest request = SearchTemplateRequest.fromXContent(createParser(JsonXContent.jsonXContent, query));\r\n    request.setRequest(searchRequest);\r\n    SearchTemplateResponse searchResponse = client().execute(SearchTemplateAction.INSTANCE, request).get();\r\n    assertThat(searchResponse.getResponse().getHits().getHits().length, equalTo(1));\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.AbstractObjectParser.declareFieldArray",
	"Comment": "declares a field that can contain an array of elements listed in the type valuetype enum",
	"Method": "void declareFieldArray(BiConsumer<Value, List<T>> consumer,ContextParser<Context, T> itemParser,ParseField field,ValueType type){\r\n    declareField(consumer, (p, c) -> parseArray(p, () -> itemParser.parse(p, c)), field, type);\r\n}"
}, {
	"Path": "org.nd4j.jita.memory.impl.CudaDirectProvider.free",
	"Comment": "this method frees specific chunk of memory, described by allocationpoint passed in",
	"Method": "void free(AllocationPoint point){\r\n    switch(point.getAllocationStatus()) {\r\n        case HOST:\r\n            {\r\n                long reqMem = AllocationUtils.getRequiredMemory(point.getShape());\r\n                NativeOps nativeOps = NativeOpsHolder.getInstance().getDeviceNativeOps();\r\n                long result = nativeOps.freeHost(point.getPointers().getHostPointer());\r\n                if (result == 0)\r\n                    throw new RuntimeException(\"Can't deallocate [HOST] memory...\");\r\n            }\r\n            break;\r\n        case DEVICE:\r\n            {\r\n                if (point.isConstant())\r\n                    return;\r\n                long reqMem = AllocationUtils.getRequiredMemory(point.getShape());\r\n                NativeOps nativeOps = NativeOpsHolder.getInstance().getDeviceNativeOps();\r\n                long result = nativeOps.freeDevice(point.getPointers().getDevicePointer(), new CudaPointer(0));\r\n                if (result == 0)\r\n                    throw new RuntimeException(\"Can't deallocate [DEVICE] memory...\");\r\n            }\r\n            break;\r\n        default:\r\n            throw new IllegalStateException(\"Can't free memory on target [\" + point.getAllocationStatus() + \"]\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.putCalendar",
	"Comment": "create a new machine learning calendarfor additional infosee ml create calendar documentation",
	"Method": "PutCalendarResponse putCalendar(PutCalendarRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::putCalendar, options, PutCalendarResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.msearchTemplate",
	"Comment": "executes a request using the multi search template api.see multi search template apion elastic.co.",
	"Method": "MultiSearchTemplateResponse msearchTemplate(MultiSearchTemplateRequest multiSearchTemplateRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(multiSearchTemplateRequest, RequestConverters::multiSearchTemplate, options, MultiSearchTemplateResponse::fromXContext, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.BaseNDArrayFactory.toFlattened",
	"Comment": "returns a vector with all of the elements in every nd arrayequal to the sum of the lengths of the ndarrays",
	"Method": "INDArray toFlattened(Collection<INDArray> matrices,INDArray toFlattened,int length,Iterator<? extends INDArray> matrices,INDArray toFlattened,INDArray matrices,INDArray toFlattened,char order,INDArray matrices){\r\n    return toFlattened(order, Arrays.asList(matrices));\r\n}"
}, {
	"Path": "org.elasticsearch.painless.Def.lookupReference",
	"Comment": "returns an implementation of interfaceclass that calls receiverclass.namethis is just like lambdametafactory, only with a dynamic type. the interface type is known, so we simply need to lookup the matching implementation method based on receiver type.",
	"Method": "MethodHandle lookupReference(PainlessLookup painlessLookup,Map<String, LocalMethod> localMethods,MethodHandles.Lookup methodHandlesLookup,String interfaceClass,Class<?> receiverClass,String name){\r\n    Class<?> interfaceType = painlessLookup.canonicalTypeNameToType(interfaceClass);\r\n    if (interfaceType == null) {\r\n        throw new IllegalArgumentException(\"type [\" + interfaceClass + \"] not found\");\r\n    }\r\n    PainlessMethod interfaceMethod = painlessLookup.lookupFunctionalInterfacePainlessMethod(interfaceType);\r\n    if (interfaceMethod == null) {\r\n        throw new IllegalArgumentException(\"Class [\" + interfaceClass + \"] is not a functional interface\");\r\n    }\r\n    int arity = interfaceMethod.typeParameters.size();\r\n    PainlessMethod implMethod = painlessLookup.lookupRuntimePainlessMethod(receiverClass, name, arity);\r\n    if (implMethod == null) {\r\n        throw new IllegalArgumentException(\"dynamic method [\" + typeToCanonicalTypeName(receiverClass) + \", \" + name + \"/\" + arity + \"] not found\");\r\n    }\r\n    return lookupReferenceInternal(painlessLookup, localMethods, methodHandlesLookup, interfaceType, PainlessLookupUtility.typeToCanonicalTypeName(implMethod.targetClass), implMethod.javaMethod.getName(), 1);\r\n}"
}, {
	"Path": "org.elasticsearch.analysis.common.MappingCharFilterFactory.parseRules",
	"Comment": "parses a list of mappingcharfilter style rules into a normalize char map",
	"Method": "void parseRules(List<String> rules,NormalizeCharMap.Builder map){\r\n    for (String rule : rules) {\r\n        Matcher m = rulePattern.matcher(rule);\r\n        if (!m.find())\r\n            throw new RuntimeException(\"Invalid Mapping Rule : [\" + rule + \"]\");\r\n        String lhs = parseString(m.group(1).trim());\r\n        String rhs = parseString(m.group(2).trim());\r\n        if (lhs == null || rhs == null)\r\n            throw new RuntimeException(\"Invalid Mapping Rule : [\" + rule + \"]. Illegal mapping.\");\r\n        map.add(lhs, rhs);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.updateAliasesAsync",
	"Comment": "asynchronously updates aliases using the index aliases api.see index aliases api on elastic.co",
	"Method": "void updateAliasesAsync(IndicesAliasesRequest indicesAliasesRequest,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(indicesAliasesRequest, IndicesRequestConverters::updateAliases, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.CublasPointer.isResultPointer",
	"Comment": "whether this is a result pointer or nota result pointer means that thispointer should not automatically be freedbut instead wait for results to accumulateso they can be returned fromthe gpu first",
	"Method": "boolean isResultPointer(){\r\n    return resultPointer;\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.XContentFactory.contentBuilder",
	"Comment": "returns a binary content builder for the provided content type.",
	"Method": "XContentBuilder contentBuilder(XContentType type,OutputStream outputStream,XContentBuilder contentBuilder,XContentType type){\r\n    if (type == XContentType.JSON) {\r\n        return JsonXContent.contentBuilder();\r\n    } else if (type == XContentType.SMILE) {\r\n        return SmileXContent.contentBuilder();\r\n    } else if (type == XContentType.YAML) {\r\n        return YamlXContent.contentBuilder();\r\n    } else if (type == XContentType.CBOR) {\r\n        return CborXContent.contentBuilder();\r\n    }\r\n    throw new IllegalArgumentException(\"No matching content type for \" + type);\r\n}"
}, {
	"Path": "org.nd4j.versioncheck.VersionCheck.checkVersions",
	"Comment": "perform a check of the versions of nd4j, dl4j, datavec, rl4j and arbiter dependencies, logging a warningif necessary.",
	"Method": "void checkVersions(){\r\n    boolean doCheck = Boolean.parseBoolean(System.getProperty(ND4JSystemProperties.VERSION_CHECK_PROPERTY, \"true\"));\r\n    if (!doCheck) {\r\n        return;\r\n    }\r\n    if (classExists(ND4J_JBLAS_CLASS)) {\r\n        log.error(\"Found incompatible/obsolete backend and version (nd4j-jblas) on classpath. ND4J is unlikely to\" + \" function correctly with nd4j-jblas on the classpath. JVM will now exit.\");\r\n        System.exit(1);\r\n    }\r\n    if (classExists(CANOVA_CLASS)) {\r\n        log.error(\"Found incompatible/obsolete library Canova on classpath. ND4J is unlikely to\" + \" function correctly with this library on the classpath. JVM will now exit.\");\r\n        System.exit(1);\r\n    }\r\n    List<VersionInfo> dependencies = getVersionInfos();\r\n    if (dependencies.size() <= 2) {\r\n        if (dependencies.size() == 0) {\r\n            return;\r\n        }\r\n        boolean dl4jViaClass = false;\r\n        boolean datavecViaClass = false;\r\n        for (VersionInfo vi : dependencies) {\r\n            if (DL4J_GROUPID.equals(vi.getGroupId()) && DL4J_ARTIFACT.equals(vi.getArtifactId()) && (UNKNOWN_VERSION.equals(vi.getBuildVersion()))) {\r\n                dl4jViaClass = true;\r\n            } else if (DATAVEC_GROUPID.equals(vi.getGroupId()) && DATAVEC_ARTIFACT.equals(vi.getArtifactId()) && (UNKNOWN_VERSION.equals(vi.getBuildVersion()))) {\r\n                datavecViaClass = true;\r\n            }\r\n        }\r\n        if (dependencies.size() == 1 && (dl4jViaClass || datavecViaClass)) {\r\n            return;\r\n        } else if (dependencies.size() == 2 && dl4jViaClass && datavecViaClass) {\r\n            return;\r\n        }\r\n    }\r\n    Set<String> foundVersions = new HashSet();\r\n    for (VersionInfo vi : dependencies) {\r\n        String g = vi.getGroupId();\r\n        if (g != null && GROUPIDS_TO_CHECK.contains(g)) {\r\n            String version = vi.getBuildVersion();\r\n            if (version.contains(\"_spark_\")) {\r\n                version = version.replaceAll(\"_spark_1\", \"\");\r\n                version = version.replaceAll(\"_spark_2\", \"\");\r\n            }\r\n            foundVersions.add(version);\r\n        }\r\n    }\r\n    boolean logVersions = false;\r\n    if (foundVersions.size() > 1) {\r\n        log.warn(\"*** ND4J VERSION CHECK FAILED - INCOMPATIBLE VERSIONS FOUND ***\");\r\n        log.warn(\"Incompatible versions (different version number) of DL4J, ND4J, RL4J, DataVec, Arbiter are unlikely to function correctly\");\r\n        logVersions = true;\r\n    }\r\n    boolean scala210 = false;\r\n    boolean scala211 = false;\r\n    boolean spark1 = false;\r\n    boolean spark2 = false;\r\n    for (VersionInfo vi : dependencies) {\r\n        String artifact = vi.getArtifactId();\r\n        if (!scala210 && artifact.contains(SCALA_210_SUFFIX)) {\r\n            scala210 = true;\r\n        }\r\n        if (!scala211 && artifact.contains(SCALA_211_SUFFIX)) {\r\n            scala211 = true;\r\n        }\r\n        String version = vi.getBuildVersion();\r\n        if (!spark1 && version.contains(SPARK_1_VER_STRING)) {\r\n            spark1 = true;\r\n        }\r\n        if (!spark2 && version.contains(SPARK_2_VER_STRING)) {\r\n            spark2 = true;\r\n        }\r\n    }\r\n    if (scala210 && scala211) {\r\n        log.warn(\"*** ND4J VERSION CHECK FAILED - FOUND BOTH SCALA VERSION 2.10 AND 2.11 ARTIFACTS ***\");\r\n        log.warn(\"Projects with mixed Scala versions (2.10/2.11) are unlikely to function correctly\");\r\n        logVersions = true;\r\n    }\r\n    if (spark1 && spark2) {\r\n        log.warn(\"*** ND4J VERSION CHECK FAILED - FOUND BOTH SPARK VERSION 1 AND 2 ARTIFACTS ***\");\r\n        log.warn(\"Projects with mixed Spark versions (1 and 2) are unlikely to function correctly\");\r\n        logVersions = true;\r\n    }\r\n    if (logVersions) {\r\n        log.info(\"Versions of artifacts found on classpath:\");\r\n        logVersionInfo();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getRecords",
	"Comment": "gets the records for a machine learning job.for additional infosee ml get records documentation",
	"Method": "GetRecordsResponse getRecords(GetRecordsRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getRecords, options, GetRecordsResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.lookup.PainlessLookupUtility.buildPainlessFieldKey",
	"Comment": "constructs a painless field key used to lookup painless fields from a painless class.",
	"Method": "String buildPainlessFieldKey(String fieldName){\r\n    return fieldName;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getJob",
	"Comment": "gets one or more machine learning job configuration info.for additional infosee ml get job documentation",
	"Method": "GetJobResponse getJob(GetJobRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getJob, options, GetJobResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.TasksClient.getAsync",
	"Comment": "get a task using the task management api.see task management api on elastic.co",
	"Method": "void getAsync(GetTaskRequest request,RequestOptions options,ActionListener<Optional<GetTaskResponse>> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseOptionalEntity(request, TasksRequestConverters::getTask, options, GetTaskResponse::fromXContent, listener);\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.process.ModelSizeStats.getTimestamp",
	"Comment": "the timestamp of the last processed record when this instance was created.",
	"Method": "Date getTimestamp(){\r\n    return timestamp;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.calcStridesFortran",
	"Comment": "computes the standard packed array strides for a given shape.",
	"Method": "int[] calcStridesFortran(int[] shape,int startNum,long[] calcStridesFortran,long[] shape,int startNum,int[] calcStridesFortran,int[] shape,long[] calcStridesFortran,long[] shape){\r\n    return calcStridesFortran(shape, 1);\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.compression.CudaThreshold.getDescriptor",
	"Comment": "this method returns compression descriptor. it should be unique for any compressor implementation",
	"Method": "String getDescriptor(){\r\n    return \"THRESHOLD\";\r\n}"
}, {
	"Path": "org.elasticsearch.index.rankeval.RankEvalSpec.getRatedRequests",
	"Comment": "returns a list of intent to query translation specifications to evaluate.",
	"Method": "List<RatedRequest> getRatedRequests(){\r\n    return Collections.unmodifiableList(ratedRequests);\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.logs2probs",
	"Comment": "converts an array containing the natural logarithms ofprobabilities stored in a vector back into probabilities.the probabilities are assumed to sum to one.",
	"Method": "double[] logs2probs(double[] a){\r\n    double max = a[maxIndex(a)];\r\n    double sum = 0.0;\r\n    double[] result = new double[a.length];\r\n    for (int i = 0; i < a.length; i++) {\r\n        result[i] = Math.exp(a[i] - max);\r\n        sum += result[i];\r\n    }\r\n    normalize(result, sum);\r\n    return result;\r\n}"
}, {
	"Path": "org.nd4j.aeron.ipc.response.AeronNDArrayResponder.launch",
	"Comment": "launch a background threadthat subscribes tothe aeron context",
	"Method": "void launch(){\r\n    if (init.get())\r\n        return;\r\n    if (!init.get())\r\n        init();\r\n    log.info(\"Subscribing to \" + channel + \" on stream Id \" + streamId);\r\n    SigInt.register(() -> running.set(false));\r\n    boolean started = false;\r\n    int numTries = 0;\r\n    while (!started && numTries < 3) {\r\n        try {\r\n            try (final Subscription subscription = aeron.addSubscription(channel, streamId)) {\r\n                log.info(\"Beginning subscribe on channel \" + channel + \" and stream \" + streamId);\r\n                AeronUtil.subscriberLoop(new FragmentAssembler(NDArrayResponseFragmentHandler.builder().aeron(aeron).context(ctx).streamId(responseStreamId).holder(ndArrayHolder).build()), fragmentLimitCount, running, launched).accept(subscription);\r\n                started = true;\r\n            }\r\n        } catch (Exception e) {\r\n            numTries++;\r\n            log.warn(\"Failed to connect..trying again\", e);\r\n        }\r\n    }\r\n    if (numTries >= 3)\r\n        throw new IllegalStateException(\"Was unable to start responder after \" + numTries + \"tries\");\r\n}"
}, {
	"Path": "org.nd4j.tools.PropertyParser.toFloat",
	"Comment": "get property. the method returns the default value if the property is not parsed.",
	"Method": "float toFloat(String name,float toFloat,String name,float defaultValue){\r\n    try {\r\n        return parseFloat(name);\r\n    } catch (Exception e) {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteCalendarEventAsync",
	"Comment": "removes a scheduled event from a calendar, notifies listener when completedfor additional infosee ml delete calendar event documentation",
	"Method": "void deleteCalendarEventAsync(DeleteCalendarEventRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::deleteCalendarEvent, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.api.Augmentation.asCollection",
	"Comment": "converts this iterable to a collection. returns the original iterable if it is already a collection.",
	"Method": "Collection<T> asCollection(Iterable<T> receiver){\r\n    if (receiver instanceof Collection) {\r\n        return (Collection<T>) receiver;\r\n    }\r\n    List<T> list = new ArrayList();\r\n    for (T t : receiver) {\r\n        list.add(t);\r\n    }\r\n    return list;\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.append",
	"Comment": "append the givenarray with the specified value sizealong a particular axis",
	"Method": "INDArray append(INDArray arr,int padAmount,double val,int axis){\r\n    if (padAmount == 0)\r\n        return arr;\r\n    long[] paShape = ArrayUtil.copy(arr.shape());\r\n    if (axis < 0)\r\n        axis = axis + arr.shape().length;\r\n    paShape[axis] = padAmount;\r\n    INDArray concatArray = Nd4j.valueArrayOf(paShape, val);\r\n    return Nd4j.concat(axis, arr, concatArray);\r\n}"
}, {
	"Path": "io.dropwizard.jersey.DropwizardResourceConfigTest.runJersey",
	"Comment": "successfully hooks into the jersey start up application event",
	"Method": "void runJersey(){\r\n    try {\r\n        jerseyTest.setUp();\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(\"Could not start jersey\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.datafeed.DatafeedConfig.hashCode",
	"Comment": "note this could be a heavy operation when a query or aggregationsare set as we need to convert the bytes references into maps tocompute a stable hash code.",
	"Method": "int hashCode(){\r\n    return Objects.hash(id, jobId, frequency, queryDelay, indices, types, asMap(query), scrollSize, asMap(aggregations), scriptFields, chunkingConfig, delayedDataCheckConfig);\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.FlushJobRequest.setSkipTime",
	"Comment": "specifies to skip to a particular time value.results are not generated and the model is not updated for data from the specified time interval.",
	"Method": "void setSkipTime(String skipTime){\r\n    this.skipTime = skipTime;\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.XContentBuilder.rawField",
	"Comment": "writes a raw field with the value taken from the bytes in the stream",
	"Method": "XContentBuilder rawField(String name,InputStream value,XContentBuilder rawField,String name,InputStream value,XContentType contentType){\r\n    generator.writeRawField(name, value, contentType);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.serde.base64.Nd4jBase64.arraysToBase64",
	"Comment": "returns a tab delimited base 64representation of the given arrays",
	"Method": "String arraysToBase64(INDArray[] arrays){\r\n    StringBuilder sb = new StringBuilder();\r\n    for (INDArray outputArr : arrays) {\r\n        ByteArrayOutputStream bos = new ByteArrayOutputStream();\r\n        DataOutputStream dos = new DataOutputStream(bos);\r\n        Nd4j.write(outputArr, dos);\r\n        String base64 = Base64.encodeBase64String(bos.toByteArray());\r\n        sb.append(base64);\r\n        sb.append(\"\\t\");\r\n    }\r\n    return sb.toString();\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.getWorkspaceManager",
	"Comment": "this method returns workspacemanager implementation to be used within this jvm process",
	"Method": "MemoryWorkspaceManager getWorkspaceManager(){\r\n    return workspaceManager;\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.time.impl.SimpleTimer.getNumberOfEvents",
	"Comment": "this method returns total number of events happened withing predefined timeframe",
	"Method": "long getNumberOfEvents(){\r\n    try {\r\n        lock.readLock().lock();\r\n        long currentTime = System.currentTimeMillis();\r\n        actualizeCounts(currentTime);\r\n        return sumCounts();\r\n    } finally {\r\n        lock.readLock().unlock();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.previewDatafeedAsync",
	"Comment": "previews the given machine learning datafeed asynchronously and notifies the listener on completionfor additional infosee ml preview datafeed documentation",
	"Method": "void previewDatafeedAsync(PreviewDatafeedRequest request,RequestOptions options,ActionListener<PreviewDatafeedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::previewDatafeed, options, PreviewDatafeedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.NioSelector.executeFailedListener",
	"Comment": "executes a failed listener with consistent exception handling. this can only be called from currentselector thread.",
	"Method": "void executeFailedListener(BiConsumer<V, Exception> listener,Exception exception){\r\n    assertOnSelectorThread();\r\n    try {\r\n        listener.accept(null, exception);\r\n    } catch (Exception e) {\r\n        eventHandler.listenerException(e);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.removeIndex",
	"Comment": "returns the array with the item in indexremoved, if the array is empty it will return the array itself",
	"Method": "int[] removeIndex(int[] data,int index,long[] removeIndex,long[] data,int index,int[] removeIndex,int[] data,int index,long[] removeIndex,long[] data,int index,Integer[] removeIndex,Integer[] data,int index){\r\n    if (data == null)\r\n        return null;\r\n    if (data.length < 1)\r\n        return data;\r\n    int len = data.length;\r\n    Integer[] result = new Integer[len - 1];\r\n    System.arraycopy(data, 0, result, 0, index);\r\n    System.arraycopy(data, index + 1, result, index, len - index - 1);\r\n    return result;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.shape.Shape.lengthOfBuffer",
	"Comment": "calculate the length of the buffer required to store the given shape with the given strides",
	"Method": "long lengthOfBuffer(long[] shape,long[] stride,long lengthOfBuffer,int[] shape,int[] stride){\r\n    Preconditions.checkArgument(shape.length == stride.length, \"Shape and strides must be same length: shape %s, stride %s\", shape, stride);\r\n    long length = 1;\r\n    for (int i = 0; i < shape.length; i++) {\r\n        length += (shape[i] - 1) * stride[i];\r\n    }\r\n    return length;\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.getRolesAsync",
	"Comment": "asynchronously retrieves roles from the native roles store.see the docs for more.",
	"Method": "void getRolesAsync(GetRolesRequest request,RequestOptions options,ActionListener<GetRolesResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, SecurityRequestConverters::getRoles, options, GetRolesResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.postCalendarEventAsync",
	"Comment": "creates new events for a a machine learning calendar asynchronously, notifies the listener on completionfor additional infoseeadd events to calendar api",
	"Method": "void postCalendarEventAsync(PostCalendarEventRequest request,RequestOptions options,ActionListener<PostCalendarEventResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::postCalendarEvents, options, PostCalendarEventResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "com.jakewharton.disklrucache.DiskLruCacheTest.readAndWriteOverlapsMaintainConsistency",
	"Comment": "each read sees a snapshot of the file at the time read was called.this means that two reads of the same key can see different data.",
	"Method": "void readAndWriteOverlapsMaintainConsistency(){\r\n    DiskLruCache.Editor v1Creator = cache.edit(\"k1\");\r\n    v1Creator.set(0, \"AAaa\");\r\n    v1Creator.set(1, \"BBbb\");\r\n    v1Creator.commit();\r\n    DiskLruCache.Snapshot snapshot1 = cache.get(\"k1\");\r\n    InputStream inV1 = snapshot1.getInputStream(0);\r\n    assertThat(inV1.read()).isEqualTo('A');\r\n    assertThat(inV1.read()).isEqualTo('A');\r\n    DiskLruCache.Editor v1Updater = cache.edit(\"k1\");\r\n    v1Updater.set(0, \"CCcc\");\r\n    v1Updater.set(1, \"DDdd\");\r\n    v1Updater.commit();\r\n    DiskLruCache.Snapshot snapshot2 = cache.get(\"k1\");\r\n    assertThat(snapshot2.getString(0)).isEqualTo(\"CCcc\");\r\n    assertThat(snapshot2.getLength(0)).isEqualTo(4);\r\n    assertThat(snapshot2.getString(1)).isEqualTo(\"DDdd\");\r\n    assertThat(snapshot2.getLength(1)).isEqualTo(4);\r\n    snapshot2.close();\r\n    assertThat(inV1.read()).isEqualTo('a');\r\n    assertThat(inV1.read()).isEqualTo('a');\r\n    assertThat(snapshot1.getString(1)).isEqualTo(\"BBbb\");\r\n    assertThat(snapshot1.getLength(1)).isEqualTo(4);\r\n    snapshot1.close();\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.forecastJob",
	"Comment": "creates a forecast of an existing, opened machine learning jobthis predicts the future behavior of a time series by using its historical behavior.for additional infosee forecast ml job documentation",
	"Method": "ForecastJobResponse forecastJob(ForecastJobRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::forecastJob, options, ForecastJobResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.testShardFailuresAbortRequest",
	"Comment": "mimicks shard search failures usually caused by the data node serving thescroll request going down.",
	"Method": "void testShardFailuresAbortRequest(){\r\n    SearchFailure shardFailure = new SearchFailure(new RuntimeException(\"test\"));\r\n    ScrollableHitSource.Response scrollResponse = new ScrollableHitSource.Response(false, singletonList(shardFailure), 0, emptyList(), null);\r\n    simulateScrollResponse(new DummyAsyncBulkByScrollAction(), timeValueNanos(System.nanoTime()), 0, scrollResponse);\r\n    BulkByScrollResponse response = listener.get();\r\n    assertThat(response.getBulkFailures(), empty());\r\n    assertThat(response.getSearchFailures(), contains(shardFailure));\r\n    assertFalse(response.isTimedOut());\r\n    assertNull(response.getReasonCancelled());\r\n    assertThat(client.scrollsCleared, contains(scrollId));\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.putDatafeed",
	"Comment": "creates a new machine learning datafeedfor additional infosee ml put datafeed documentation",
	"Method": "PutDatafeedResponse putDatafeed(PutDatafeedRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::putDatafeed, options, PutDatafeedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.index.analysis.IndexableBinaryStringTools.getEncodedLength",
	"Comment": "returns the number of chars required to encode the given bytes.",
	"Method": "int getEncodedLength(byte[] inputArray,int inputOffset,int inputLength){\r\n    return (int) ((8L * inputLength + 14L) / 15L) + 1;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.putJobAsync",
	"Comment": "creates a new machine learning job asynchronously and notifies listener on completionfor additional infosee ml put job documentation",
	"Method": "void putJobAsync(PutJobRequest request,RequestOptions options,ActionListener<PutJobResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::putJob, options, PutJobResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.apache.lucene.analysis.miscellaneous.DuplicateByteSequenceSpotter.startNewSequence",
	"Comment": "reset the sequence detection logic to avoid any continuation of theimmediately previous bytes. a minimum of dupsequencesize bytes need to beadded before any new duplicate sequences will be reported.hit counts are not reset by calling this method.",
	"Method": "void startNewSequence(){\r\n    sequenceBufferFilled = false;\r\n    nextFreePos = 0;\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.json.JsonXContentGenerator.copyStream",
	"Comment": "copy the contents of the given inputstream to the given outputstream.closes both streams when done.",
	"Method": "long copyStream(InputStream in,OutputStream out){\r\n    Objects.requireNonNull(in, \"No InputStream specified\");\r\n    Objects.requireNonNull(out, \"No OutputStream specified\");\r\n    final byte[] buffer = new byte[8192];\r\n    boolean success = false;\r\n    try {\r\n        long byteCount = 0;\r\n        int bytesRead;\r\n        while ((bytesRead = in.read(buffer)) != -1) {\r\n            out.write(buffer, 0, bytesRead);\r\n            byteCount += bytesRead;\r\n        }\r\n        out.flush();\r\n        success = true;\r\n        return byteCount;\r\n    } finally {\r\n        if (success) {\r\n            IOUtils.close(in, out);\r\n        } else {\r\n            IOUtils.closeWhileHandlingException(in, out);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.putSettingsAsync",
	"Comment": "asynchronously updates specific index level settings using the update indices settings api.seeupdate indices settingsapi on elastic.co",
	"Method": "void putSettingsAsync(UpdateSettingsRequest updateSettingsRequest,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(updateSettingsRequest, IndicesRequestConverters::indexPutSettings, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.previewDatafeed",
	"Comment": "previews the given machine learning datafeedfor additional infosee ml preview datafeed documentation",
	"Method": "PreviewDatafeedResponse previewDatafeed(PreviewDatafeedRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::previewDatafeed, options, PreviewDatafeedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.jita.handler.impl.CudaZeroHandler.getHostTrackingPoints",
	"Comment": "this method returns sets of allocation tracking ids for specific bucket",
	"Method": "Set<Long> getHostTrackingPoints(Long bucketId){\r\n    if (!zeroAllocations.containsKey(bucketId)) {\r\n        return new HashSet();\r\n    }\r\n    return zeroAllocations.get(bucketId).keySet();\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.ExtendedPluginsClassLoader.create",
	"Comment": "return a new classloader across the parent and extended loaders.",
	"Method": "ExtendedPluginsClassLoader create(ClassLoader parent,List<ClassLoader> extendedLoaders){\r\n    return AccessController.doPrivileged((PrivilegedAction<ExtendedPluginsClassLoader>) () -> new ExtendedPluginsClassLoader(parent, extendedLoaders));\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.ops.executioner.CudaGridExecutioner.isMatchingZX",
	"Comment": "this method checks, if opa and opb are sharing the same operands",
	"Method": "boolean isMatchingZX(Op opA,Op opB){\r\n    if (opA.x() == opB.x() && opA.z() == opB.z() && opA.x() == opB.z())\r\n        return true;\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.DefMath.getNumber",
	"Comment": "slowly returns a number for o. just for supporting dynamiccast",
	"Method": "Number getNumber(Object o){\r\n    if (o instanceof Number) {\r\n        return (Number) o;\r\n    } else if (o instanceof Character) {\r\n        return Integer.valueOf((char) o);\r\n    } else {\r\n        throw new ClassCastException(\"Cannot convert [\" + o.getClass() + \"] to a Number\");\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.stringSimilarity",
	"Comment": "calculate string similarity with tfidf weights relative to each characterfrequency and how many times a character appears in a given string",
	"Method": "double stringSimilarity(String strings){\r\n    if (strings == null)\r\n        return 0;\r\n    Counter<String> counter = new Counter();\r\n    Counter<String> counter2 = new Counter();\r\n    for (int i = 0; i < strings[0].length(); i++) counter.incrementCount(String.valueOf(strings[0].charAt(i)), 1.0f);\r\n    for (int i = 0; i < strings[1].length(); i++) counter2.incrementCount(String.valueOf(strings[1].charAt(i)), 1.0f);\r\n    Set<String> v1 = counter.keySet();\r\n    Set<String> v2 = counter2.keySet();\r\n    Set<String> both = SetUtils.intersection(v1, v2);\r\n    double sclar = 0, norm1 = 0, norm2 = 0;\r\n    for (String k : both) sclar += counter.getCount(k) * counter2.getCount(k);\r\n    for (String k : v1) norm1 += counter.getCount(k) * counter.getCount(k);\r\n    for (String k : v2) norm2 += counter2.getCount(k) * counter2.getCount(k);\r\n    return sclar / Math.sqrt(norm1 * norm2);\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.DummyWorkspace.getMaxCycleAllocations",
	"Comment": "this method returns amount of memory consumed by largest successful cycle, in bytes",
	"Method": "long getMaxCycleAllocations(){\r\n    return 0;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.config.MlFilter.builder",
	"Comment": "creates a new builder object for creating an mlfilter object",
	"Method": "Builder builder(String filterId){\r\n    return new Builder().setId(filterId);\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.url.URLRepository.checkURL",
	"Comment": "makes sure that the url is white listed or if it points to the local file system it matches one on of the root path in path.repo",
	"Method": "URL checkURL(URL url){\r\n    String protocol = url.getProtocol();\r\n    if (protocol == null) {\r\n        throw new RepositoryException(getMetadata().name(), \"unknown url protocol from URL [\" + url + \"]\");\r\n    }\r\n    for (String supportedProtocol : supportedProtocols) {\r\n        if (supportedProtocol.equals(protocol)) {\r\n            try {\r\n                if (URIPattern.match(urlWhiteList, url.toURI())) {\r\n                    return url;\r\n                }\r\n            } catch (URISyntaxException ex) {\r\n                logger.warn(\"cannot parse the specified url [{}]\", url);\r\n                throw new RepositoryException(getMetadata().name(), \"cannot parse the specified url [\" + url + \"]\");\r\n            }\r\n            URL normalizedUrl = environment.resolveRepoURL(url);\r\n            if (normalizedUrl == null) {\r\n                String logMessage = \"The specified url [{}] doesn't start with any repository paths specified by the \" + \"path.repo setting or by {} setting: [{}] \";\r\n                logger.warn(logMessage, url, ALLOWED_URLS_SETTING.getKey(), environment.repoFiles());\r\n                String exceptionMessage = \"file url [\" + url + \"] doesn't match any of the locations specified by path.repo or \" + ALLOWED_URLS_SETTING.getKey();\r\n                throw new RepositoryException(getMetadata().name(), exceptionMessage);\r\n            }\r\n            return normalizedUrl;\r\n        }\r\n    }\r\n    throw new RepositoryException(getMetadata().name(), \"unsupported url protocol [\" + protocol + \"] from URL [\" + url + \"]\");\r\n}"
}, {
	"Path": "io.dropwizard.testing.common.Resource.target",
	"Comment": "creates a web target to be sent to the resource under testing.",
	"Method": "WebTarget target(String path){\r\n    return getJerseyTest().target(path);\r\n}"
}, {
	"Path": "com.jakewharton.disklrucache.DiskLruCache.rebuildJournal",
	"Comment": "creates a new journal that omits redundant information. this replaces thecurrent journal if it exists.",
	"Method": "void rebuildJournal(){\r\n    if (journalWriter != null) {\r\n        journalWriter.close();\r\n    }\r\n    Writer writer = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(journalFileTmp), Util.US_ASCII));\r\n    try {\r\n        writer.write(MAGIC);\r\n        writer.write(\"\\n\");\r\n        writer.write(VERSION_1);\r\n        writer.write(\"\\n\");\r\n        writer.write(Integer.toString(appVersion));\r\n        writer.write(\"\\n\");\r\n        writer.write(Integer.toString(valueCount));\r\n        writer.write(\"\\n\");\r\n        writer.write(\"\\n\");\r\n        for (Entry entry : lruEntries.values()) {\r\n            if (entry.currentEditor != null) {\r\n                writer.write(DIRTY + ' ' + entry.key + '\\n');\r\n            } else {\r\n                writer.write(CLEAN + ' ' + entry.key + entry.getLengths() + '\\n');\r\n            }\r\n        }\r\n    } finally {\r\n        writer.close();\r\n    }\r\n    if (journalFile.exists()) {\r\n        renameTo(journalFile, journalFileBackup, true);\r\n    }\r\n    renameTo(journalFileTmp, journalFile, false);\r\n    journalFileBackup.delete();\r\n    journalWriter = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(journalFile, true), Util.US_ASCII));\r\n}"
}, {
	"Path": "org.elasticsearch.plugin.noop.action.search.NoopSearchRequestBuilder.setScroll",
	"Comment": "if set, will enable scrolling of the search request for the specified timeout.",
	"Method": "NoopSearchRequestBuilder setScroll(Scroll scroll,NoopSearchRequestBuilder setScroll,TimeValue keepAlive,NoopSearchRequestBuilder setScroll,String keepAlive){\r\n    request.scroll(keepAlive);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestClient.onResponse",
	"Comment": "called after each successful request call.receives as an argument the host that was used for the successful request.",
	"Method": "void onResponse(Node node){\r\n    DeadHostState removedHost = this.blacklist.remove(node.getHost());\r\n    if (logger.isDebugEnabled() && removedHost != null) {\r\n        logger.debug(\"removed [\" + node + \"] from blacklist\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.painless.node.ANode.createError",
	"Comment": "create an error with location information pointing to this node.",
	"Method": "RuntimeException createError(RuntimeException exception){\r\n    return location.createError(exception);\r\n}"
}, {
	"Path": "io.dropwizard.servlets.tasks.TaskServletTest.verifyTaskExecuteMethod",
	"Comment": "add a test to make sure the signature of the task class does not change as the taskservletdepends on this to perform record metrics on tasks",
	"Method": "void verifyTaskExecuteMethod(){\r\n    assertThatCode(() -> Task.class.getMethod(\"execute\", Map.class, PrintWriter.class)).doesNotThrowAnyException();\r\n}"
}, {
	"Path": "org.nd4j.graph.GraphInferenceGrpcClient.output",
	"Comment": "this method sends inference request to the graphserver instance, and returns result as array of indarrays",
	"Method": "INDArray[] output(Pair<String, INDArray> inputs,T output,long graphId,T value,OperandsAdapter<T> adapter,Operands output,long graphId,Operands operands,INDArray[] output,long graphId,Pair<String, INDArray> inputs){\r\n    val operands = new Operands();\r\n    for (val in : inputs) operands.addArgument(in.getFirst(), in.getSecond());\r\n    return output(graphId, operands).asArray();\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.handleRead",
	"Comment": "this method is called when a channel signals it is ready for be read. all of the read logic shouldoccur in this call.",
	"Method": "void handleRead(SocketChannelContext context){\r\n    context.read();\r\n}"
}, {
	"Path": "org.elasticsearch.client.security.DeletePrivilegesResponse.isFound",
	"Comment": "indicates if the given privilege was successfully found and deleted from the list of application privileges.",
	"Method": "boolean isFound(String privilege){\r\n    return privileges.contains(privilege);\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.transport.impl.AeronUdpTransport.jointMessageHandler",
	"Comment": "this method converts aeron buffer into voidmessage and puts into temp queue for further processing",
	"Method": "void jointMessageHandler(DirectBuffer buffer,int offset,int length,Header header){\r\n    byte[] data = new byte[length];\r\n    buffer.getBytes(offset, data);\r\n    val message = VoidMessage.fromBytes(data);\r\n    if (!remoteConnections.containsKey(message.getOriginatorId()))\r\n        addConnection(message.getOriginatorId());\r\n    log.debug(\"Got [{}] message from [{}]\", message.getClass().getSimpleName(), message.getOriginatorId());\r\n    try {\r\n        messageQueue.put(message);\r\n    } catch (InterruptedException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.apache.lucene.queries.SearchAfterSortedDocQuery.searchAfterDoc",
	"Comment": "returns the first doc id greater than the provided after doc.",
	"Method": "int searchAfterDoc(TopComparator comparator,int from,int to){\r\n    int low = from;\r\n    int high = to - 1;\r\n    while (low <= high) {\r\n        int mid = (low + high) >>> 1;\r\n        if (comparator.lessThanTop(mid)) {\r\n            high = mid - 1;\r\n        } else {\r\n            low = mid + 1;\r\n        }\r\n    }\r\n    return low;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.shape.Shape.wholeArrayDimension",
	"Comment": "returns true if the given arrayis meant for the whole dimension",
	"Method": "boolean wholeArrayDimension(int arr){\r\n    return arr.length == 1 && arr[0] == Integer.MAX_VALUE;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.getAsync",
	"Comment": "retrieve information about one or more indexessee indices get index api on elastic.co",
	"Method": "void getAsync(GetIndexRequest getIndexRequest,RequestOptions options,ActionListener<GetIndexResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(getIndexRequest, IndicesRequestConverters::getIndex, options, GetIndexResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.TokenCountFieldMapper.countPositions",
	"Comment": "count position increments in a token stream.package private for testing.",
	"Method": "int countPositions(Analyzer analyzer,String fieldName,String fieldValue,boolean enablePositionIncrements){\r\n    try (TokenStream tokenStream = analyzer.tokenStream(fieldName, fieldValue)) {\r\n        int count = 0;\r\n        PositionIncrementAttribute position = tokenStream.addAttribute(PositionIncrementAttribute.class);\r\n        tokenStream.reset();\r\n        while (tokenStream.incrementToken()) {\r\n            if (enablePositionIncrements) {\r\n                count += position.getPositionIncrement();\r\n            } else {\r\n                count += Math.min(1, position.getPositionIncrement());\r\n            }\r\n        }\r\n        tokenStream.end();\r\n        if (enablePositionIncrements) {\r\n            count += position.getPositionIncrement();\r\n        }\r\n        return count;\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.RandomFactory.getRandom",
	"Comment": "this method returns random implementation instance associated with calling thread",
	"Method": "Random getRandom(){\r\n    try {\r\n        if (threadRandom.get() == null) {\r\n            Random t = (Random) randomClass.newInstance();\r\n            if (t.getStatePointer() != null) {\r\n            }\r\n            threadRandom.set(t);\r\n            return t;\r\n        }\r\n        return threadRandom.get();\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.create",
	"Comment": "creates a snapshot.seesnapshot and restoreapi on elastic.co",
	"Method": "CreateSnapshotResponse create(CreateSnapshotRequest createSnapshotRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(createSnapshotRequest, SnapshotRequestConverters::createSnapshot, options, CreateSnapshotResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.get.GetTaskRequestBuilder.setTimeout",
	"Comment": "timeout to wait for any async actions this request must take. it must take anywhere from 0 to 2.",
	"Method": "GetTaskRequestBuilder setTimeout(TimeValue timeout){\r\n    request.setTimeout(timeout);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.randn",
	"Comment": "fill the given ndarray with random numbers drawn from a normal distribution utilizing the given random generator",
	"Method": "INDArray randn(INDArray target,INDArray randn,int[] shape,INDArray randn,long[] shape,INDArray randn,char order,int[] shape,INDArray randn,char order,long[] shape,INDArray randn,int[] shape,long seed,INDArray randn,long rows,long columns,INDArray randn,char order,long rows,long columns,INDArray randn,long rows,long columns,long seed,INDArray randn,long rows,long columns,org.nd4j.linalg.api.rng.Random r,INDArray randn,int[] shape,org.nd4j.linalg.api.rng.Random r,INDArray randn,long[] shape,org.nd4j.linalg.api.rng.Random r,INDArray randn,INDArray target,long seed,INDArray randn,INDArray target,org.nd4j.linalg.api.rng.Random rng){\r\n    return getExecutioner().exec(new GaussianDistribution(target), rng);\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.averageAndPropagate",
	"Comment": "this method averages input arrays, and returns averaged array.on top of that, averaged array is propagated to all input arrays",
	"Method": "INDArray averageAndPropagate(INDArray target,INDArray[] arrays,INDArray averageAndPropagate,INDArray[] arrays,INDArray averageAndPropagate,Collection<INDArray> arrays,INDArray averageAndPropagate,INDArray target,Collection<INDArray> arrays){\r\n    INDArray ret = INSTANCE.average(target, arrays);\r\n    logCreationIfNecessary(ret);\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.Gzip.getDescriptor",
	"Comment": "this method returns compression descriptor. it should be unique for any compressor implementation",
	"Method": "String getDescriptor(){\r\n    return \"GZIP\";\r\n}"
}, {
	"Path": "org.elasticsearch.client.CcrClient.putAutoFollowPatternAsync",
	"Comment": "asynchronously stores an auto follow pattern.see the docs for more.",
	"Method": "void putAutoFollowPatternAsync(PutAutoFollowPatternRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, CcrRequestConverters::putAutoFollowPattern, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "io.dropwizard.client.JerseyClientBuilder.name",
	"Comment": "use the given environment name. this is used in the user agent.",
	"Method": "JerseyClientBuilder name(String environmentName){\r\n    apacheHttpClientBuilder.name(environmentName);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.util.TimeUtil.parseTimeField",
	"Comment": "parse out a date object given the current parser and field name.",
	"Method": "Date parseTimeField(XContentParser parser,String fieldName){\r\n    if (parser.currentToken() == XContentParser.Token.VALUE_NUMBER) {\r\n        return new Date(parser.longValue());\r\n    } else if (parser.currentToken() == XContentParser.Token.VALUE_STRING) {\r\n        return new Date(DateFormatters.toZonedDateTime(DateTimeFormatter.ISO_INSTANT.parse(parser.text())).toInstant().toEpochMilli());\r\n    }\r\n    throw new IllegalArgumentException(\"unexpected token [\" + parser.currentToken() + \"] for [\" + fieldName + \"]\");\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.DummyWorkspace.notifyScopeBorrowed",
	"Comment": "this method temporary enters this workspace, without reset applied",
	"Method": "MemoryWorkspace notifyScopeBorrowed(){\r\n    return null;\r\n}"
}, {
	"Path": "org.nd4j.linalg.compression.BasicNDArrayCompressor.getDefaultCompression",
	"Comment": "get the default compression algorithm as a string.this is an all caps algorithm with a representation in thecompressionalgorithm enum",
	"Method": "String getDefaultCompression(){\r\n    synchronized (this) {\r\n        return defaultCompression;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.retryLifecyclePolicy",
	"Comment": "retry lifecycle step for given indicessee the docs for more.",
	"Method": "AcknowledgedResponse retryLifecyclePolicy(RetryLifecyclePolicyRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IndexLifecycleRequestConverters::retryLifecycle, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.DefBootstrapTests.testMegamorphic",
	"Comment": "test that we revert to the megamorphic classvalue cache and that it works as expected",
	"Method": "void testMegamorphic(){\r\n    DefBootstrap.PIC site = (DefBootstrap.PIC) DefBootstrap.bootstrap(painlessLookup, Collections.emptyMap(), MethodHandles.publicLookup(), \"size\", MethodType.methodType(int.class, Object.class), 0, DefBootstrap.METHOD_CALL, \"\");\r\n    site.depth = DefBootstrap.PIC.MAX_DEPTH;\r\n    MethodHandle handle = site.dynamicInvoker();\r\n    assertEquals(2, (int) handle.invokeExact((Object) Arrays.asList(\"1\", \"2\")));\r\n    assertEquals(1, (int) handle.invokeExact((Object) Collections.singletonMap(\"a\", \"b\")));\r\n    assertEquals(3, (int) handle.invokeExact((Object) Arrays.asList(\"x\", \"y\", \"z\")));\r\n    assertEquals(2, (int) handle.invokeExact((Object) Arrays.asList(\"u\", \"v\")));\r\n    final HashMap<String, String> map = new HashMap();\r\n    map.put(\"x\", \"y\");\r\n    map.put(\"a\", \"b\");\r\n    assertEquals(2, (int) handle.invokeExact((Object) map));\r\n    final IllegalArgumentException iae = expectThrows(IllegalArgumentException.class, () -> {\r\n        Integer.toString((int) handle.invokeExact(new Object()));\r\n    });\r\n    assertEquals(\"dynamic method [java.lang.Object, size/0] not found\", iae.getMessage());\r\n    assertTrue(\"Does not fail inside ClassValue.computeValue()\", Arrays.stream(iae.getStackTrace()).anyMatch(e -> {\r\n        return e.getMethodName().equals(\"computeValue\") && e.getClassName().startsWith(\"org.elasticsearch.painless.DefBootstrap$PIC$\");\r\n    }));\r\n}"
}, {
	"Path": "org.nd4j.linalg.lossfunctions.impl.LossMixtureDensity.getNMixtures",
	"Comment": "returns the number of gaussians this loss functionwill attempt to find.",
	"Method": "int getNMixtures(){\r\n    return mMixtures;\r\n}"
}, {
	"Path": "org.nd4j.util.StringUtils.hexStringToByte",
	"Comment": "given a hexstring this will return the byte array corresponding to thestring",
	"Method": "byte[] hexStringToByte(String hex){\r\n    byte[] bts = new byte[hex.length() / 2];\r\n    for (int i = 0; i < bts.length; i++) {\r\n        bts[i] = (byte) Integer.parseInt(hex.substring(2 * i, 2 * i + 2), 16);\r\n    }\r\n    return bts;\r\n}"
}, {
	"Path": "org.elasticsearch.client.core.TermVectorsResponse.getFound",
	"Comment": "returns if the document is foundalways true for artificial documents",
	"Method": "boolean getFound(){\r\n    return found;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.multiSearch",
	"Comment": "executes a multi search using the msearch api.see multi search api onelastic.co",
	"Method": "MultiSearchResponse multiSearch(MultiSearchRequest multiSearchRequest,RequestOptions options){\r\n    return msearch(multiSearchRequest, options);\r\n}"
}, {
	"Path": "org.elasticsearch.index.rankeval.EvaluationMetric.forcedSearchSize",
	"Comment": "metrics can define a size of the search hits windows they want to retrieve by overwritingthis method. the default implementation returns an empty optional.",
	"Method": "OptionalInt forcedSearchSize(){\r\n    return OptionalInt.empty();\r\n}"
}, {
	"Path": "org.elasticsearch.upgrades.FullClusterRestartIT.testSingleDoc",
	"Comment": "tests that a single document survives. super basic smoke test.",
	"Method": "void testSingleDoc(){\r\n    String docLocation = \"/\" + index + \"/doc/1\";\r\n    String doc = \"{\\\"test\\\": \\\"test\\\"}\";\r\n    if (isRunningAgainstOldCluster()) {\r\n        Request createDoc = new Request(\"PUT\", docLocation);\r\n        createDoc.setJsonEntity(doc);\r\n        client().performRequest(createDoc);\r\n    }\r\n    assertThat(toStr(client().performRequest(new Request(\"GET\", docLocation))), containsString(doc));\r\n}"
}, {
	"Path": "org.nd4j.linalg.cpu.nativecpu.blas.SparseCpuLevel1.droti",
	"Comment": "applies givens rotation to sparse vectors one of which is in compressed form.",
	"Method": "void droti(long N,INDArray X,DataBuffer indexes,INDArray Y,double c,double s){\r\n    cblas_droti((int) N, (DoublePointer) X.data().addressPointer(), (IntPointer) indexes.addressPointer(), (DoublePointer) Y.data().addressPointer(), c, s);\r\n}"
}, {
	"Path": "org.elasticsearch.client.indices.UnfreezeIndexRequest.getWaitForActiveShards",
	"Comment": "returns the wait for active shard cound or null if the default should be used",
	"Method": "ActiveShardCount getWaitForActiveShards(){\r\n    return waitForActiveShards;\r\n}"
}, {
	"Path": "io.dropwizard.logging.json.layout.MapBuilder.add",
	"Comment": "adds the string value to the provided map under the provided field name,if it should be included. the supplier is only invoked if the field is to be included.",
	"Method": "MapBuilder add(String fieldName,boolean include,String value,MapBuilder add,String fieldName,boolean include,Supplier<String> supplier,MapBuilder add,String fieldName,boolean include,Map<String, ?> mapValue){\r\n    if (include && mapValue != null && !mapValue.isEmpty()) {\r\n        map.put(getFieldName(fieldName), mapValue);\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.forecastJobAsync",
	"Comment": "creates a forecast of an existing, opened machine learning job asynchronouslythis predicts the future behavior of a time series by using its historical behavior.for additional infosee forecast ml job documentation",
	"Method": "void forecastJobAsync(ForecastJobRequest request,RequestOptions options,ActionListener<ForecastJobResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::forecastJob, options, ForecastJobResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.core.TermVectorsRequest.setFields",
	"Comment": "sets the fields for which term vectors information should be retrieved",
	"Method": "void setFields(String fields){\r\n    this.fields = fields;\r\n}"
}, {
	"Path": "io.dropwizard.setup.AdminEnvironment.addTask",
	"Comment": "adds the given task to the set of tasks exposed via the admin interface.",
	"Method": "void addTask(Task task){\r\n    tasks.add(requireNonNull(task));\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getBuckets",
	"Comment": "gets the buckets for a machine learning job.for additional infosee ml get buckets documentation",
	"Method": "GetBucketsResponse getBuckets(GetBucketsRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getBuckets, options, GetBucketsResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.updateModelSnapshot",
	"Comment": "updates a snapshot for a machine learning job.for additional infosee ml update model snapshots documentation",
	"Method": "UpdateModelSnapshotResponse updateModelSnapshot(UpdateModelSnapshotRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::updateModelSnapshot, options, UpdateModelSnapshotResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.SelectionKeyUtils.setConnectReadAndWriteInterested",
	"Comment": "removes an interest in connects, reads, and writes for this selection key while maintaining otherinterests.",
	"Method": "void setConnectReadAndWriteInterested(SelectionKey selectionKey){\r\n    selectionKey.interestOps(selectionKey.interestOps() | SelectionKey.OP_CONNECT | SelectionKey.OP_READ | SelectionKey.OP_WRITE);\r\n}"
}, {
	"Path": "org.nd4j.util.StringUtils.formatTimeDiff",
	"Comment": "given a finish and start time in long milliseconds, returns astring in the format xhrs, ymins, z sec, for the time difference between two times.if finish time comes before start time then negative valeus of x, y and z wil return.",
	"Method": "String formatTimeDiff(long finishTime,long startTime){\r\n    long timeDiff = finishTime - startTime;\r\n    return formatTime(timeDiff);\r\n}"
}, {
	"Path": "org.elasticsearch.client.TimedRequest.masterNodeTimeout",
	"Comment": "returns the timeout for the request to be completed on the master node",
	"Method": "TimeValue masterNodeTimeout(){\r\n    return masterTimeout;\r\n}"
}, {
	"Path": "org.elasticsearch.client.CcrClient.resumeFollowAsync",
	"Comment": "asynchronously instruct a follower index to resume the following of a leader index.see the docs for more.",
	"Method": "void resumeFollowAsync(ResumeFollowRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, CcrRequestConverters::resumeFollow, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.license.StartTrialResponse.isTrialWasStarted",
	"Comment": "returns true if a trial license was started as a result of the request corresponding to this response. returns false if the clusterdid not start a trial, or a trial had already been started before the corresponding request was made",
	"Method": "boolean isTrialWasStarted(){\r\n    return trialWasStarted;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.shape.Shape.toOffsetZeroCopy",
	"Comment": "create a copy of the ndarray where the new offset is zero, and has specified order",
	"Method": "INDArray toOffsetZeroCopy(INDArray arr,INDArray toOffsetZeroCopy,INDArray arr,char order){\r\n    return toOffsetZeroCopyHelper(arr, order, false);\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.datafeed.DatafeedConfig.equals",
	"Comment": "the lists of indices and types are compared for equality but they are notsorted first so this test could fail simply because the indices and typeslists are in different orders.also note this could be a heavy operation when a query or aggregationsare set as we need to convert the bytes references into maps to correctlycompare them.",
	"Method": "boolean equals(Object other){\r\n    if (this == other) {\r\n        return true;\r\n    }\r\n    if (other == null || getClass() != other.getClass()) {\r\n        return false;\r\n    }\r\n    DatafeedConfig that = (DatafeedConfig) other;\r\n    return Objects.equals(this.id, that.id) && Objects.equals(this.jobId, that.jobId) && Objects.equals(this.frequency, that.frequency) && Objects.equals(this.queryDelay, that.queryDelay) && Objects.equals(this.indices, that.indices) && Objects.equals(this.types, that.types) && Objects.equals(asMap(this.query), asMap(that.query)) && Objects.equals(this.scrollSize, that.scrollSize) && Objects.equals(asMap(this.aggregations), asMap(that.aggregations)) && Objects.equals(this.scriptFields, that.scriptFields) && Objects.equals(this.chunkingConfig, that.chunkingConfig) && Objects.equals(this.delayedDataCheckConfig, that.delayedDataCheckConfig);\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.DummyWorkspace.getThisCycleAllocations",
	"Comment": "this method returns amount of memory consumed in last successful cycle, in bytes",
	"Method": "long getThisCycleAllocations(){\r\n    return 0;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.search",
	"Comment": "executes a search request using the search api.see search api on elastic.co",
	"Method": "SearchResponse search(SearchRequest searchRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(searchRequest, RequestConverters::search, options, SearchResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.BaseNDArrayFactory.vstack",
	"Comment": "concatenates two matrices vertically. matrices must have identicalnumbers of columns.",
	"Method": "INDArray vstack(INDArray arrs){\r\n    return Nd4j.concat(0, arrs);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.split",
	"Comment": "splits an index using the split index api.see split index api on elastic.co",
	"Method": "ResizeResponse split(ResizeRequest resizeRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(resizeRequest, IndicesRequestConverters::split, options, ResizeResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.IngestClient.getPipelineAsync",
	"Comment": "asynchronously get an existing pipeline.see get pipeline api on elastic.co",
	"Method": "void getPipelineAsync(GetPipelineRequest request,RequestOptions options,ActionListener<GetPipelineResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IngestRequestConverters::getPipeline, options, GetPipelineResponse::fromXContent, listener, Collections.singleton(404));\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.util.NetworkOrganizer.getSubset",
	"Comment": "this method returns specified number of ip addresses from original list of addresses, that are not listen in primary collection",
	"Method": "List<String> getSubset(int numShards,List<String> getSubset,int numShards,Collection<String> primary){\r\n    if (networkMask == null)\r\n        return getIntersections(numShards, primary);\r\n    List<String> addresses = new ArrayList();\r\n    SubnetUtils utils = new SubnetUtils(networkMask);\r\n    Collections.shuffle(informationCollection);\r\n    for (NetworkInformation information : informationCollection) {\r\n        for (String ip : information.getIpAddresses()) {\r\n            if (primary != null && primary.contains(ip))\r\n                continue;\r\n            if (utils.getInfo().isInRange(ip)) {\r\n                log.debug(\"Picked {} as {}\", ip, primary == null ? \"Shard\" : \"Backup\");\r\n                addresses.add(ip);\r\n            }\r\n            if (addresses.size() >= numShards)\r\n                break;\r\n        }\r\n        if (addresses.size() >= numShards)\r\n            break;\r\n    }\r\n    return addresses;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.rollover",
	"Comment": "rolls over an index using the rollover index api.see rollover index api on elastic.co",
	"Method": "RolloverResponse rollover(RolloverRequest rolloverRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(rolloverRequest, IndicesRequestConverters::rollover, options, RolloverResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.putDatafeedAsync",
	"Comment": "creates a new machine learning datafeed asynchronously and notifies listener on completionfor additional infosee ml put datafeed documentation",
	"Method": "void putDatafeedAsync(PutDatafeedRequest request,RequestOptions options,ActionListener<PutDatafeedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::putDatafeed, options, PutDatafeedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.CachedShapeInfoProvider.purgeCache",
	"Comment": "this method forces cache purge, if cache is available for specific implementation",
	"Method": "void purgeCache(){\r\n    provider.purgeCache();\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.PluginSecurityTests.testParseTwoPermissions",
	"Comment": "test that we can parse the set of permissions correctly for a complex policy",
	"Method": "void testParseTwoPermissions(){\r\n    assumeTrue(\"test cannot run with security manager enabled\", System.getSecurityManager() == null);\r\n    Path scratch = createTempDir();\r\n    Path testFile = this.getDataPath(\"security/complex-plugin-security.policy\");\r\n    Set<String> actual = PluginSecurity.parsePermissions(testFile, scratch);\r\n    assertThat(actual, containsInAnyOrder(PluginSecurity.formatPermission(new RuntimePermission(\"getClassLoader\")), PluginSecurity.formatPermission(new RuntimePermission(\"closeClassLoader\"))));\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.SerializationUtils.serialize",
	"Comment": "writes the object to the output streamthis does not flush the stream",
	"Method": "void serialize(Serializable object,OutputStream os){\r\n    writeObject(object, os);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.watcher",
	"Comment": "provides methods for accessing the elastic licensed watcher apis thatare shipped with the default distribution of elasticsearch. all ofthese apis will 404 if run against the oss distribution of elasticsearch.see the watcher apis on elastic.co for more information.",
	"Method": "WatcherClient watcher(){\r\n    return watcherClient;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.shape.Shape.ind2subC",
	"Comment": "convert a linear index tothe equivalent nd index based on the shape of the specified ndarray.infers the number of indices from the specified shape.",
	"Method": "int[] ind2subC(int[] shape,long index,long numIndices,long[] ind2subC,long[] shape,long index,long numIndices,int[] ind2subC,int[] shape,long index,long[] ind2subC,long[] shape,long index,long[] ind2subC,INDArray arr,long index){\r\n    if (arr.rank() == 1)\r\n        return new long[] { index };\r\n    return ind2subC(arr.shape(), index, ArrayUtil.prodLong(arr.shape()));\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.CloseJobRequest.setTimeout",
	"Comment": "how long to wait for the close request to complete before timing out.",
	"Method": "void setTimeout(TimeValue timeout){\r\n    this.timeout = timeout;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.messages.intercom.DistributedAssignMessage.processMessage",
	"Comment": "this method assigns specific value to either specific row, or whole array.array is identified by key",
	"Method": "void processMessage(){\r\n    if (payload != null) {\r\n        if (storage.arrayExists(key) && storage.getArray(key).length() == payload.length())\r\n            storage.getArray(key).assign(payload);\r\n        else\r\n            storage.setArray(key, payload);\r\n    } else {\r\n        if (index >= 0) {\r\n            if (storage.getArray(key) == null)\r\n                throw new RuntimeException(\"Init wasn't called before for key [\" + key + \"]\");\r\n            storage.getArray(key).getRow(index).assign(value);\r\n        } else\r\n            storage.getArray(key).assign(value);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.stopDatafeedAsync",
	"Comment": "stops the given machine learning datafeed asynchronously and notifies the listener on completionfor additional infosee ml stop datafeed documentation",
	"Method": "void stopDatafeedAsync(StopDatafeedRequest request,RequestOptions options,ActionListener<StopDatafeedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::stopDatafeed, options, StopDatafeedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.tools.InfoValues.getValues",
	"Comment": "returns values.\tthis method use class infoline.\tthis method is not intended for external use.",
	"Method": "String getValues(){\r\n    String info = \"\";\r\n    for (int i = 0; i < vsL.size(); i++) {\r\n        info += vsL.get(i) + \"|\";\r\n    }\r\n    return info;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestClientSingleHostTests.testBody",
	"Comment": "end to end test for request and response body. exercises the mock http client ability to send backwhatever body it has received.",
	"Method": "void testBody(){\r\n    String body = \"{ \\\"field\\\": \\\"value\\\" }\";\r\n    StringEntity entity = new StringEntity(body, ContentType.APPLICATION_JSON);\r\n    for (String method : Arrays.asList(\"DELETE\", \"GET\", \"PATCH\", \"POST\", \"PUT\")) {\r\n        for (int okStatusCode : getOkStatusCodes()) {\r\n            Request request = new Request(method, \"/\" + okStatusCode);\r\n            request.setEntity(entity);\r\n            Response response = restClient.performRequest(request);\r\n            assertThat(response.getStatusLine().getStatusCode(), equalTo(okStatusCode));\r\n            assertThat(EntityUtils.toString(response.getEntity()), equalTo(body));\r\n        }\r\n        for (int errorStatusCode : getAllErrorStatusCodes()) {\r\n            Request request = new Request(method, \"/\" + errorStatusCode);\r\n            request.setEntity(entity);\r\n            try {\r\n                restClient.performRequest(request);\r\n                fail(\"request should have failed\");\r\n            } catch (ResponseException e) {\r\n                Response response = e.getResponse();\r\n                assertThat(response.getStatusLine().getStatusCode(), equalTo(errorStatusCode));\r\n                assertThat(EntityUtils.toString(response.getEntity()), equalTo(body));\r\n                assertExceptionStackContainsCallingMethod(e);\r\n            }\r\n        }\r\n    }\r\n    for (String method : Arrays.asList(\"HEAD\", \"OPTIONS\", \"TRACE\")) {\r\n        Request request = new Request(method, \"/\" + randomStatusCode(getRandom()));\r\n        request.setEntity(entity);\r\n        try {\r\n            restClient.performRequest(request);\r\n            fail(\"request should have failed\");\r\n        } catch (UnsupportedOperationException e) {\r\n            assertThat(e.getMessage(), equalTo(method + \" with body is not supported\"));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.io.PathUtils.get",
	"Comment": "tries to resolve the given file uri against the list of available roots.if uri starts with one of the listed roots, it returned back by this method, otherwise null is returned.",
	"Method": "Path get(String first,String more,Path get,URI uri,Path get,Path[] roots,String path,Path get,Path[] roots,URI uri){\r\n    return get(roots, PathUtils.get(uri).normalize().toString());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.mtermvectorsAsync",
	"Comment": "asynchronously calls the multi term vectors apisee multi term vectors apion elastic.co",
	"Method": "void mtermvectorsAsync(MultiTermVectorsRequest request,RequestOptions options,ActionListener<MultiTermVectorsResponse> listener){\r\n    performRequestAsyncAndParseEntity(request, RequestConverters::mtermVectors, options, MultiTermVectorsResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.settings.ClusterGetSettingsResponse.getSetting",
	"Comment": "returns the string value of the setting for the specified index. the order of search is firstin persistent settings the transient settings and finally the default settings.",
	"Method": "String getSetting(String setting){\r\n    if (persistentSettings.hasValue(setting)) {\r\n        return persistentSettings.get(setting);\r\n    } else if (transientSettings.hasValue(setting)) {\r\n        return transientSettings.get(setting);\r\n    } else if (defaultSettings.hasValue(setting)) {\r\n        return defaultSettings.get(setting);\r\n    } else {\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.closeJobAsync",
	"Comment": "closes one or more machine learning jobs asynchronously, notifies listener on completiona closed job cannot receive data or perform analysis operations, but you can still explore and navigate results.for additional infosee ml close job documentation",
	"Method": "void closeJobAsync(CloseJobRequest request,RequestOptions options,ActionListener<CloseJobResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::closeJob, options, CloseJobResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.buffer.factory.DefaultDataBufferFactory.createSame",
	"Comment": "this method will create new databuffer of the same datatype & same length",
	"Method": "DataBuffer createSame(DataBuffer buffer,boolean init,DataBuffer createSame,DataBuffer buffer,boolean init,MemoryWorkspace workspace){\r\n    switch(buffer.dataType()) {\r\n        case INT:\r\n            return createInt(buffer.length(), init, workspace);\r\n        case FLOAT:\r\n            return createFloat(buffer.length(), init, workspace);\r\n        case DOUBLE:\r\n            return createDouble(buffer.length(), init, workspace);\r\n        case HALF:\r\n            return createHalf(buffer.length(), init, workspace);\r\n        default:\r\n            throw new UnsupportedOperationException(\"Unknown dataType: \" + buffer.dataType());\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.tools.PropertyParser.toDouble",
	"Comment": "get property. the method returns the default value if the property is not parsed.",
	"Method": "double toDouble(String name,double toDouble,String name,double defaultValue){\r\n    try {\r\n        return parseDouble(name);\r\n    } catch (Exception e) {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.painless.api.Augmentation.find",
	"Comment": "finds the first entry matching the predicate, or returns null.",
	"Method": "T find(Collection<T> receiver,Predicate<T> predicate,Map.Entry<K, V> find,Map<K, V> receiver,BiPredicate<K, V> predicate){\r\n    for (Map.Entry<K, V> kvPair : receiver.entrySet()) {\r\n        if (predicate.test(kvPair.getKey(), kvPair.getValue())) {\r\n            return kvPair;\r\n        }\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getModelSnapshotsAsync",
	"Comment": "gets the snapshots for a machine learning job, notifies listener once the requested snapshots are retrieved.for additional infosee ml get model snapshots documentation",
	"Method": "void getModelSnapshotsAsync(GetModelSnapshotsRequest request,RequestOptions options,ActionListener<GetModelSnapshotsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getModelSnapshots, options, GetModelSnapshotsResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.primitives.Counter.isEmpty",
	"Comment": "this method returns true if counter has no elements, false otherwise",
	"Method": "boolean isEmpty(){\r\n    return map.size() == 0;\r\n}"
}, {
	"Path": "org.elasticsearch.grok.Grok.captures",
	"Comment": "matches and returns any named captures within a compiled grok expression that matchedwithin the provided text.",
	"Method": "Map<String, Object> captures(String text){\r\n    byte[] textAsBytes = text.getBytes(StandardCharsets.UTF_8);\r\n    Map<String, Object> fields = new HashMap();\r\n    Matcher matcher = compiledExpression.matcher(textAsBytes);\r\n    int result;\r\n    try {\r\n        threadWatchdog.register();\r\n        result = matcher.search(0, textAsBytes.length, Option.DEFAULT);\r\n    } finally {\r\n        threadWatchdog.unregister();\r\n    }\r\n    if (result == Matcher.INTERRUPTED) {\r\n        throw new RuntimeException(\"grok pattern matching was interrupted after [\" + threadWatchdog.maxExecutionTimeInMillis() + \"] ms\");\r\n    } else if (result == Matcher.FAILED) {\r\n        return null;\r\n    } else if (compiledExpression.numberOfNames() > 0) {\r\n        Region region = matcher.getEagerRegion();\r\n        for (Iterator<NameEntry> entry = compiledExpression.namedBackrefIterator(); entry.hasNext(); ) {\r\n            NameEntry e = entry.next();\r\n            String groupName = new String(e.name, e.nameP, e.nameEnd - e.nameP, StandardCharsets.UTF_8);\r\n            for (int number : e.getBackRefs()) {\r\n                if (region.beg[number] >= 0) {\r\n                    String matchValue = new String(textAsBytes, region.beg[number], region.end[number] - region.beg[number], StandardCharsets.UTF_8);\r\n                    GrokMatchGroup match = new GrokMatchGroup(groupName, matchValue);\r\n                    fields.put(match.getName(), match.getValue());\r\n                    break;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return fields;\r\n}"
}, {
	"Path": "org.elasticsearch.common.logging.EvilLoggerConfigurationTests.testResolveOrder",
	"Comment": "tests that custom settings are not overwritten by settings in the config file",
	"Method": "void testResolveOrder(){\r\n    final Path configDir = getDataPath(\"config\");\r\n    final Settings settings = Settings.builder().put(Environment.PATH_HOME_SETTING.getKey(), createTempDir().toString()).put(\"logger.test_resolve_order\", \"TRACE\").build();\r\n    final Environment environment = new Environment(settings, configDir);\r\n    LogConfigurator.configure(environment);\r\n    final String loggerName = \"test_resolve_order\";\r\n    final Logger logger = LogManager.getLogger(loggerName);\r\n    assertTrue(logger.isTraceEnabled());\r\n}"
}, {
	"Path": "io.dropwizard.util.DirectExecutorService.startTask",
	"Comment": "checks if the executor has been shut down and increments the running task count.",
	"Method": "void startTask(){\r\n    synchronized (lock) {\r\n        if (shutdown) {\r\n            throw new RejectedExecutionException(\"Executor already shutdown\");\r\n        }\r\n        runningTasks++;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.WatcherClient.putWatchAsync",
	"Comment": "asynchronously put a watch into the clustersee the docs for more.",
	"Method": "void putWatchAsync(PutWatchRequest request,RequestOptions options,ActionListener<PutWatchResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, WatcherRequestConverters::putWatch, options, PutWatchResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.ChannelContext.addCloseListener",
	"Comment": "add a listener that will be called when the channel is closed.",
	"Method": "void addCloseListener(BiConsumer<Void, Exception> listener){\r\n    closeContext.addListener(listener);\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.argMax",
	"Comment": "returns the index of the maximum value in the array.if two entries have same maximum value, index of the first one is returned.",
	"Method": "int argMax(int[] in,int argMax,long[] in){\r\n    int maxIdx = 0;\r\n    for (int i = 1; i < in.length; i++) if (in[i] > in[maxIdx])\r\n        maxIdx = i;\r\n    return maxIdx;\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.serializer.MultiMinMaxSerializerStrategy.write",
	"Comment": "serialize a multinormalizerminmaxscaler to a output stream",
	"Method": "void write(MultiNormalizerMinMaxScaler normalizer,OutputStream stream){\r\n    try (DataOutputStream dos = new DataOutputStream(stream)) {\r\n        dos.writeBoolean(normalizer.isFitLabel());\r\n        dos.writeInt(normalizer.numInputs());\r\n        dos.writeInt(normalizer.isFitLabel() ? normalizer.numOutputs() : -1);\r\n        dos.writeDouble(normalizer.getTargetMin());\r\n        dos.writeDouble(normalizer.getTargetMax());\r\n        for (int i = 0; i < normalizer.numInputs(); i++) {\r\n            Nd4j.write(normalizer.getMin(i), dos);\r\n            Nd4j.write(normalizer.getMax(i), dos);\r\n        }\r\n        if (normalizer.isFitLabel()) {\r\n            for (int i = 0; i < normalizer.numOutputs(); i++) {\r\n                Nd4j.write(normalizer.getLabelMin(i), dos);\r\n                Nd4j.write(normalizer.getLabelMax(i), dos);\r\n            }\r\n        }\r\n        dos.flush();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.datafeed.DatafeedUpdate.hashCode",
	"Comment": "note this could be a heavy operation when a query or aggregationsare set as we need to convert the bytes references into maps tocompute a stable hash code.",
	"Method": "int hashCode(){\r\n    return Objects.hash(id, jobId, frequency, queryDelay, indices, types, asMap(query), scrollSize, asMap(aggregations), scriptFields, chunkingConfig, delayedDataCheckConfig);\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.getSslCertificatesAsync",
	"Comment": "asynchronously retrieve the x.509 certificates that are used to encrypt communications in an elasticsearch cluster.see the docs for more.",
	"Method": "void getSslCertificatesAsync(RequestOptions options,ActionListener<GetSslCertificatesResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(GetSslCertificatesRequest.INSTANCE, GetSslCertificatesRequest::getRequest, options, GetSslCertificatesResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.PainlessExecuteRequestTests.testFromXContent",
	"Comment": "as i know this request class is the only case where this is a problem.",
	"Method": "void testFromXContent(){\r\n    for (int i = 0; i < 20; i++) {\r\n        PainlessExecuteAction.Request testInstance = createTestInstance();\r\n        ContextSetup contextSetup = testInstance.getContextSetup();\r\n        XContent xContent = randomFrom(XContentType.values()).xContent();\r\n        if (contextSetup != null && contextSetup.getXContentType() != null) {\r\n            xContent = contextSetup.getXContentType().xContent();\r\n        }\r\n        try (XContentBuilder builder = XContentBuilder.builder(xContent)) {\r\n            builder.value(testInstance);\r\n            StreamInput instanceInput = BytesReference.bytes(builder).streamInput();\r\n            try (XContentParser parser = xContent.createParser(xContentRegistry(), LoggingDeprecationHandler.INSTANCE, instanceInput)) {\r\n                PainlessExecuteAction.Request result = PainlessExecuteAction.Request.parse(parser);\r\n                assertThat(result, equalTo(testInstance));\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.pullRows",
	"Comment": "this method produces concatenated array, that consist from tensors, fetched from source array, against somedimension and specified indexes.the concatenated arrays are placed in the specified array.",
	"Method": "INDArray pullRows(INDArray source,int sourceDimension,int[] indexes,INDArray pullRows,INDArray source,int sourceDimension,int[] indexes,char order,INDArray pullRows,INDArray source,INDArray destination,int sourceDimension,int[] indexes){\r\n    if (sourceDimension >= source.rank())\r\n        throw new IllegalStateException(\"Source dimension can't be higher the rank of source tensor\");\r\n    if (indexes == null || indexes.length == 0)\r\n        throw new IllegalStateException(\"Indexes shouldn't be empty\");\r\n    for (int idx : indexes) {\r\n        if (idx < 0 || idx >= source.shape()[source.rank() - sourceDimension - 1]) {\r\n            throw new IllegalStateException(\"Index can't be < 0 and >= \" + source.shape()[source.rank() - sourceDimension - 1]);\r\n        }\r\n    }\r\n    INDArray ret = INSTANCE.pullRows(source, destination, sourceDimension, indexes);\r\n    logCreationIfNecessary(ret);\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.msearch",
	"Comment": "executes a multi search using the msearch api.see multi search api onelastic.co",
	"Method": "MultiSearchResponse msearch(MultiSearchRequest multiSearchRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(multiSearchRequest, RequestConverters::multiSearch, options, MultiSearchResponse::fromXContext, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.NDArrayIndex.numNewAxis",
	"Comment": "given an array of indexesreturn the number of new axis elementsin teh array",
	"Method": "int numNewAxis(INDArrayIndex axes){\r\n    int ret = 0;\r\n    for (INDArrayIndex index : axes) if (index instanceof NewAxis)\r\n        ret++;\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestClientTestCase.assertHeaders",
	"Comment": "assert that the actual headers are the expected ones given the original default and request headers. some headers can be ignored,for instance in case the http client is adding its own automatically.",
	"Method": "void assertHeaders(Header[] defaultHeaders,Header[] requestHeaders,Header[] actualHeaders,Set<String> ignoreHeaders){\r\n    final Map<String, List<String>> expectedHeaders = new HashMap();\r\n    final Set<String> requestHeaderKeys = new HashSet();\r\n    for (final Header header : requestHeaders) {\r\n        final String name = header.getName();\r\n        addValueToListEntry(expectedHeaders, name, header.getValue());\r\n        requestHeaderKeys.add(name);\r\n    }\r\n    for (final Header defaultHeader : defaultHeaders) {\r\n        final String name = defaultHeader.getName();\r\n        if (requestHeaderKeys.contains(name) == false) {\r\n            addValueToListEntry(expectedHeaders, name, defaultHeader.getValue());\r\n        }\r\n    }\r\n    Set<String> actualIgnoredHeaders = new HashSet();\r\n    for (Header responseHeader : actualHeaders) {\r\n        final String name = responseHeader.getName();\r\n        if (ignoreHeaders.contains(name)) {\r\n            expectedHeaders.remove(name);\r\n            actualIgnoredHeaders.add(name);\r\n            continue;\r\n        }\r\n        final String value = responseHeader.getValue();\r\n        final List<String> values = expectedHeaders.get(name);\r\n        assertNotNull(\"found response header [\" + name + \"] that wasn't originally sent: \" + value, values);\r\n        assertTrue(\"found incorrect response header [\" + name + \"]: \" + value, values.remove(value));\r\n        if (values.isEmpty()) {\r\n            expectedHeaders.remove(name);\r\n        }\r\n    }\r\n    assertEquals(\"some headers meant to be ignored were not part of the actual headers\", ignoreHeaders, actualIgnoredHeaders);\r\n    assertTrue(\"some headers that were sent weren't returned \" + expectedHeaders, expectedHeaders.isEmpty());\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.ReindexFailureTests.testResponseOnSearchFailure",
	"Comment": "make sure that search failures get pushed back to the user as failures ofthe whole process. we do lose some information about how far along theprocess got, but its important that they see these failures.",
	"Method": "void testResponseOnSearchFailure(){\r\n    int attempt = 1;\r\n    while (attempt < 5) {\r\n        indexDocs(100);\r\n        ReindexRequestBuilder copy = reindex().source(\"source\").destination(\"dest\");\r\n        copy.source().setSize(10);\r\n        Future<BulkByScrollResponse> response = copy.execute();\r\n        client().admin().indices().prepareDelete(\"source\").get();\r\n        try {\r\n            response.get();\r\n            logger.info(\"Didn't trigger a reindex failure on the {} attempt\", attempt);\r\n            attempt++;\r\n            assertBusy(() -> assertFalse(client().admin().indices().prepareExists(\"source\").get().isExists()));\r\n        } catch (ExecutionException e) {\r\n            logger.info(\"Triggered a reindex failure on the {} attempt: {}\", attempt, e.getMessage());\r\n            assertThat(e.getMessage(), either(containsString(\"all shards failed\")).or(containsString(\"No search context found\")).or(containsString(\"no such index [source]\")));\r\n            return;\r\n        }\r\n    }\r\n    assumeFalse(\"Wasn't able to trigger a reindex failure in \" + attempt + \" attempts.\", true);\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.DummyWorkspace.getWorkspaceConfiguration",
	"Comment": "this method returns workspaceconfiguration bean that was used for given workspace instance",
	"Method": "WorkspaceConfiguration getWorkspaceConfiguration(){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.client.license.GetTrialStatusResponse.isEligibleToStartTrial",
	"Comment": "returns whether the license is eligible to start trial or not",
	"Method": "boolean isEligibleToStartTrial(){\r\n    return eligibleToStartTrial;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.CastTests.testMethodCallDef",
	"Comment": "currently these do not adopt the return value, we issue a separate cast!",
	"Method": "void testMethodCallDef(){\r\n    assertEquals(5, exec(\"def x = 5; return (int)x.longValue();\"));\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.transport.impl.BaseTransport.forwardToParameterServer",
	"Comment": "this method puts indarray to the flow read by parameter server",
	"Method": "void forwardToParameterServer(INDArrayMessage message){\r\n    try {\r\n        incomingFlow.accept(message);\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.tools.PropertyParser.toChar",
	"Comment": "get property. the method returns the default value if the property is not parsed.",
	"Method": "char toChar(String name,char toChar,String name,char defaultValue){\r\n    try {\r\n        return parseChar(name);\r\n    } catch (Exception e) {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.updateDatafeedAsync",
	"Comment": "updates a machine learning datafeed asynchronously and notifies listener on completionfor additional infosee ml update datafeed documentation",
	"Method": "void updateDatafeedAsync(UpdateDatafeedRequest request,RequestOptions options,ActionListener<PutDatafeedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::updateDatafeed, options, PutDatafeedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.deletePrivilegesAsync",
	"Comment": "asynchronously removes an application privilegesee the docs for more.",
	"Method": "void deletePrivilegesAsync(DeletePrivilegesRequest request,RequestOptions options,ActionListener<DeletePrivilegesResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, SecurityRequestConverters::deletePrivileges, options, DeletePrivilegesResponse::fromXContent, listener, singleton(404));\r\n}"
}, {
	"Path": "org.elasticsearch.painless.DefMath.unbox",
	"Comment": "unboxes a class to its primitive type, or returns the originalclass if its not a boxed type.",
	"Method": "Class<?> unbox(Class<?> clazz){\r\n    return MethodType.methodType(clazz).unwrap().returnType();\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.authenticateAsync",
	"Comment": "authenticate the current user asynchronously and return all the information about the authenticated user.see the docs for more.",
	"Method": "void authenticateAsync(RequestOptions options,ActionListener<AuthenticateResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(AuthenticateRequest.INSTANCE, AuthenticateRequest::getRequest, options, AuthenticateResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.openJobAsync",
	"Comment": "opens a machine learning job asynchronously, notifies listener on completion.when you open a new job, it starts with an empty model.when you open an existing job, the most recent model state is automatically loaded.the job is ready to resume its analysis from where it left off, once new data is received.for additional infosee ml open job documentation",
	"Method": "void openJobAsync(OpenJobRequest request,RequestOptions options,ActionListener<OpenJobResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::openJob, options, OpenJobResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "io.dropwizard.cli.ConfiguredCommand.addFileArgument",
	"Comment": "adds the configuration file argument for the configured command.",
	"Method": "Argument addFileArgument(Subparser subparser){\r\n    return subparser.addArgument(\"file\").nargs(\"?\").help(\"application configuration file\");\r\n}"
}, {
	"Path": "org.nd4j.linalg.cpu.nativecpu.blas.SparseCpuLevel1.sroti",
	"Comment": "applies givens rotation to sparse vectors one of which is in compressed form.",
	"Method": "void sroti(long N,INDArray X,DataBuffer indexes,INDArray Y,double c,double s){\r\n    cblas_sroti((int) N, (FloatPointer) X.data().addressPointer(), (IntPointer) indexes.addressPointer().capacity(X.columns()), (FloatPointer) Y.data().addressPointer(), (float) c, (float) s);\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.messages.intercom.DistributedCbowDotMessage.processMessage",
	"Comment": "this method calculates dot of gives rows, with averaging applied to rowsa, as required by cbow",
	"Method": "void processMessage(){\r\n    CbowRequestMessage cbrm = new CbowRequestMessage(rowsA, rowsB, w1, codes, negSamples, alpha, 119);\r\n    if (negSamples > 0) {\r\n        int[] negatives = Arrays.copyOfRange(rowsB, codes.length, rowsB.length);\r\n        cbrm.setNegatives(negatives);\r\n    }\r\n    cbrm.setFrameId(-119L);\r\n    cbrm.setTaskId(this.taskId);\r\n    cbrm.setOriginatorId(this.getOriginatorId());\r\n    CbowTrainer cbt = (CbowTrainer) trainer;\r\n    cbt.pickTraining(cbrm);\r\n    INDArray words = Nd4j.pullRows(storage.getArray(WordVectorStorage.SYN_0), 1, rowsA, 'c');\r\n    INDArray mean = words.mean(0);\r\n    int resultLength = codes.length + (negSamples > 0 ? (negSamples + 1) : 0);\r\n    INDArray result = Nd4j.createUninitialized(resultLength, 1);\r\n    int e = 0;\r\n    for (; e < codes.length; e++) {\r\n        double dot = Nd4j.getBlasWrapper().dot(mean, storage.getArray(WordVectorStorage.SYN_1).getRow(rowsB[e]));\r\n        result.putScalar(e, dot);\r\n    }\r\n    for (; e < resultLength; e++) {\r\n        double dot = Nd4j.getBlasWrapper().dot(mean, storage.getArray(WordVectorStorage.SYN_1_NEGATIVE).getRow(rowsB[e]));\r\n        result.putScalar(e, dot);\r\n    }\r\n    if (voidConfiguration.getExecutionMode() == ExecutionMode.AVERAGING) {\r\n        DotAggregation dot = new DotAggregation(taskId, (short) 1, shardIndex, result);\r\n        dot.setTargetId((short) -1);\r\n        dot.setOriginatorId(getOriginatorId());\r\n        transport.putMessage(dot);\r\n    } else if (voidConfiguration.getExecutionMode() == ExecutionMode.SHARDED) {\r\n        DotAggregation dot = new DotAggregation(taskId, (short) voidConfiguration.getNumberOfShards(), shardIndex, result);\r\n        dot.setTargetId((short) -1);\r\n        dot.setOriginatorId(getOriginatorId());\r\n        transport.sendMessage(dot);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.expandDims",
	"Comment": "expand the array dimensions.this is equivalent toadding a new axis dimension",
	"Method": "INDArray expandDims(INDArray input,int dimension){\r\n    if (dimension < 0)\r\n        dimension += input.rank();\r\n    long[] shape = input.shape();\r\n    long[] indexes = new long[input.rank() + 1];\r\n    for (int i = 0; i < indexes.length; i++) indexes[i] = i < dimension ? shape[i] : i == dimension ? 1 : shape[i - 1];\r\n    return input.reshape(input.ordering(), indexes);\r\n}"
}, {
	"Path": "org.elasticsearch.nio.NioSelector.scheduleForRegistration",
	"Comment": "schedules a niochannel to be registered with this selector. the channel will by queued andeventually registered next time through the event loop.",
	"Method": "void scheduleForRegistration(NioChannel channel){\r\n    ChannelContext<?> context = channel.getContext();\r\n    channelsToRegister.add(context);\r\n    ensureSelectorOpenForEnqueuing(channelsToRegister, context);\r\n    wakeup();\r\n}"
}, {
	"Path": "org.elasticsearch.script.expression.MoreExpressionTests.testInvalidUpdateScript",
	"Comment": "test to make sure expressions are not allowed to be used as update scripts",
	"Method": "void testInvalidUpdateScript(){\r\n    try {\r\n        createIndex(\"test_index\");\r\n        ensureGreen(\"test_index\");\r\n        indexRandom(true, client().prepareIndex(\"test_index\", \"doc\", \"1\").setSource(\"text_field\", \"text\"));\r\n        UpdateRequestBuilder urb = client().prepareUpdate().setIndex(\"test_index\");\r\n        urb.setType(\"doc\");\r\n        urb.setId(\"1\");\r\n        urb.setScript(new Script(ScriptType.INLINE, ExpressionScriptEngine.NAME, \"0\", Collections.emptyMap()));\r\n        urb.get();\r\n        fail(\"Expression scripts should not be allowed to run as update scripts.\");\r\n    } catch (Exception e) {\r\n        String message = e.getMessage();\r\n        assertThat(message + \" should have contained failed to execute\", message.contains(\"failed to execute\"), equalTo(true));\r\n        message = e.getCause().getMessage();\r\n        assertThat(message, equalTo(\"Failed to compile inline script [0] using lang [expression]\"));\r\n    }\r\n}"
}, {
	"Path": "io.dropwizard.auth.CachingAuthorizer.stats",
	"Comment": "returns a set of statistics about the cache contents and usage.",
	"Method": "CacheStats stats(){\r\n    return cache.stats();\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AllocationPoint.getDeviceAccessTime",
	"Comment": "returns time, in milliseconds, when this point was accessed on device side",
	"Method": "long getDeviceAccessTime(){\r\n    return accessDeviceRead.get();\r\n}"
}, {
	"Path": "org.elasticsearch.client.ClusterClient.putSettings",
	"Comment": "updates cluster wide specific settings using the cluster update settings api.seecluster update settingsapi on elastic.co",
	"Method": "ClusterUpdateSettingsResponse putSettings(ClusterUpdateSettingsRequest clusterUpdateSettingsRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(clusterUpdateSettingsRequest, ClusterRequestConverters::clusterPutSettings, options, ClusterUpdateSettingsResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.validateQueryAsync",
	"Comment": "asynchronously validate a potentially expensive query without executing it.seevalidate query apion elastic.co",
	"Method": "void validateQueryAsync(ValidateQueryRequest validateQueryRequest,RequestOptions options,ActionListener<ValidateQueryResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(validateQueryRequest, IndicesRequestConverters::validateQuery, options, ValidateQueryResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore.deleteBlobsByPrefix",
	"Comment": "deletes multiple blobs from the specific bucket all of which have prefixed names",
	"Method": "void deleteBlobsByPrefix(String prefix){\r\n    deleteBlobs(listBlobsByPrefix(\"\", prefix).keySet());\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AtomicAllocator.getPointer",
	"Comment": "this method returns actual device pointer valid for specified indarray",
	"Method": "Pointer getPointer(DataBuffer buffer,CudaContext context,Pointer getPointer,DataBuffer buffer,Pointer getPointer,DataBuffer buffer,AllocationShape shape,boolean isView,CudaContext context,Pointer getPointer,INDArray array,CudaContext context){\r\n    return memoryHandler.getDevicePointer(array.data(), context);\r\n}"
}, {
	"Path": "org.nd4j.aeron.ipc.response.AeronNDArrayResponder.startSubscriber",
	"Comment": "start a subscriber in another threadbased on the given parameters",
	"Method": "AeronNDArrayResponder startSubscriber(Aeron aeron,String host,int port,NDArrayHolder callback,int streamId,AeronNDArrayResponder startSubscriber,Aeron.Context context,String host,int port,NDArrayHolder callback,int streamId){\r\n    if (callback == null)\r\n        throw new IllegalArgumentException(\"NDArrayHolder must be specified\");\r\n    final AtomicBoolean running = new AtomicBoolean(true);\r\n    AeronNDArrayResponder subscriber = AeronNDArrayResponder.builder().streamId(streamId).ctx(context).channel(String.format(\"aeron:udp?endpoint=%s:%d\", host, port)).running(running).ndArrayHolder(callback).build();\r\n    Thread t = new Thread(() -> {\r\n        try {\r\n            subscriber.launch();\r\n        } catch (Exception e) {\r\n            e.printStackTrace();\r\n        }\r\n    });\r\n    t.start();\r\n    return subscriber;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.ForecastJobRequest.setExpiresIn",
	"Comment": "set the forecast expirationthe period of time that forecast results are retained.after a forecast expires, the results are deleted. the default value is 14 days.if set to a value of 0, the forecast is never automatically deleted.",
	"Method": "void setExpiresIn(TimeValue expiresIn){\r\n    this.expiresIn = expiresIn;\r\n}"
}, {
	"Path": "io.dropwizard.jetty.HttpsConnectorFactory.logSslInfoOnStart",
	"Comment": "register a listener that waits until the ssl context factory has started. once it has started we can grab the fully initialized context so we can log the parameters.",
	"Method": "AbstractLifeCycle.AbstractLifeCycleListener logSslInfoOnStart(SslContextFactory sslContextFactory){\r\n    return new AbstractLifeCycle.AbstractLifeCycleListener() {\r\n        @Override\r\n        public void lifeCycleStarted(LifeCycle event) {\r\n            logSupportedParameters(sslContextFactory);\r\n        }\r\n    };\r\n}"
}, {
	"Path": "io.dropwizard.jetty.HttpsConnectorFactory.logSslInfoOnStart",
	"Comment": "register a listener that waits until the ssl context factory has started. once it has started we can grab the fully initialized context so we can log the parameters.",
	"Method": "AbstractLifeCycle.AbstractLifeCycleListener logSslInfoOnStart(SslContextFactory sslContextFactory){\r\n    logSupportedParameters(sslContextFactory);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.rankEvalAsync",
	"Comment": "asynchronously executes a request using the ranking evaluation api.see ranking evaluation apion elastic.co",
	"Method": "void rankEvalAsync(RankEvalRequest rankEvalRequest,RequestOptions options,ActionListener<RankEvalResponse> listener){\r\n    performRequestAsyncAndParseEntity(rankEvalRequest, RequestConverters::rankEval, options, RankEvalResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteExpiredDataAsync",
	"Comment": "deletes expired data from machine learning jobs asynchronously and notifies the listener on completionfor additional infosee ml delete expired datadocumentation",
	"Method": "void deleteExpiredDataAsync(DeleteExpiredDataRequest request,RequestOptions options,ActionListener<DeleteExpiredDataResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::deleteExpiredData, options, DeleteExpiredDataResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.getThisCycleAllocations",
	"Comment": "this method returns number of bytes allocated during this cycle",
	"Method": "long getThisCycleAllocations(){\r\n    return cycleAllocations.get();\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getJobStats",
	"Comment": "gets usage statistics for one or more machine learning jobsfor additional infosee get job stats docs",
	"Method": "GetJobStatsResponse getJobStats(GetJobStatsRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getJobStats, options, GetJobStatsResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.DataSet.merge",
	"Comment": "merge the list of datasets in to one list.all the rows are merged in to one dataset",
	"Method": "DataSet merge(List<? extends org.nd4j.linalg.dataset.api.DataSet> data){\r\n    if (data.isEmpty())\r\n        throw new IllegalArgumentException(\"Unable to merge empty dataset\");\r\n    int nonEmpty = 0;\r\n    boolean anyFeaturesPreset = false;\r\n    boolean anyLabelsPreset = false;\r\n    boolean first = true;\r\n    for (org.nd4j.linalg.dataset.api.DataSet ds : data) {\r\n        if (ds.isEmpty()) {\r\n            continue;\r\n        }\r\n        nonEmpty++;\r\n        if (anyFeaturesPreset && ds.getFeatures() == null || (!first && !anyFeaturesPreset && ds.getFeatures() != null)) {\r\n            throw new IllegalStateException(\"Cannot merge features: encountered null features in one or more DataSets\");\r\n        }\r\n        if (anyLabelsPreset && ds.getLabels() == null || (!first && !anyLabelsPreset && ds.getLabels() != null)) {\r\n            throw new IllegalStateException(\"Cannot merge labels: enountered null labels in one or more DataSets\");\r\n        }\r\n        anyFeaturesPreset |= ds.getFeatures() != null;\r\n        anyLabelsPreset |= ds.getLabels() != null;\r\n        first = false;\r\n    }\r\n    INDArray[] featuresToMerge = new INDArray[nonEmpty];\r\n    INDArray[] labelsToMerge = new INDArray[nonEmpty];\r\n    INDArray[] featuresMasksToMerge = null;\r\n    INDArray[] labelsMasksToMerge = null;\r\n    int count = 0;\r\n    for (org.nd4j.linalg.dataset.api.DataSet ds : data) {\r\n        if (ds.isEmpty())\r\n            continue;\r\n        featuresToMerge[count] = ds.getFeatures();\r\n        labelsToMerge[count] = ds.getLabels();\r\n        if (ds.getFeaturesMaskArray() != null) {\r\n            if (featuresMasksToMerge == null) {\r\n                featuresMasksToMerge = new INDArray[nonEmpty];\r\n            }\r\n            featuresMasksToMerge[count] = ds.getFeaturesMaskArray();\r\n        }\r\n        if (ds.getLabelsMaskArray() != null) {\r\n            if (labelsMasksToMerge == null) {\r\n                labelsMasksToMerge = new INDArray[nonEmpty];\r\n            }\r\n            labelsMasksToMerge[count] = ds.getLabelsMaskArray();\r\n        }\r\n        count++;\r\n    }\r\n    INDArray featuresOut;\r\n    INDArray labelsOut;\r\n    INDArray featuresMaskOut;\r\n    INDArray labelsMaskOut;\r\n    Pair<INDArray, INDArray> fp = DataSetUtil.mergeFeatures(featuresToMerge, featuresMasksToMerge);\r\n    featuresOut = fp.getFirst();\r\n    featuresMaskOut = fp.getSecond();\r\n    Pair<INDArray, INDArray> lp = DataSetUtil.mergeLabels(labelsToMerge, labelsMasksToMerge);\r\n    labelsOut = lp.getFirst();\r\n    labelsMaskOut = lp.getSecond();\r\n    DataSet dataset = new DataSet(featuresOut, labelsOut, featuresMaskOut, labelsMaskOut);\r\n    List<Serializable> meta = null;\r\n    for (org.nd4j.linalg.dataset.api.DataSet ds : data) {\r\n        if (ds.getExampleMetaData() == null || ds.getExampleMetaData().size() != ds.numExamples()) {\r\n            meta = null;\r\n            break;\r\n        }\r\n        if (meta == null)\r\n            meta = new ArrayList();\r\n        meta.addAll(ds.getExampleMetaData());\r\n    }\r\n    if (meta != null) {\r\n        dataset.setExampleMetaData(meta);\r\n    }\r\n    return dataset;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.Def.arrayLengthGetter",
	"Comment": "returns an array length getter methodhandle for the given array type",
	"Method": "MethodHandle arrayLengthGetter(Class<?> arrayType,MethodHandle arrayLengthGetter,Class<?> arrayType){\r\n    if (JAVA9_ARRAY_LENGTH_MH_FACTORY != null) {\r\n        try {\r\n            return (MethodHandle) JAVA9_ARRAY_LENGTH_MH_FACTORY.invokeExact(arrayType);\r\n        } catch (Throwable t) {\r\n            rethrow(t);\r\n            throw new AssertionError(t);\r\n        }\r\n    } else {\r\n        return ArrayLengthHelper.arrayLengthGetter(arrayType);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.CharArrays.constantTimeEquals",
	"Comment": "constant time equality check of strings to avoid potential timing attacks.",
	"Method": "boolean constantTimeEquals(char[] a,char[] b,boolean constantTimeEquals,String a,String b){\r\n    Objects.requireNonNull(a, \"strings must not be null for constantTimeEquals\");\r\n    Objects.requireNonNull(b, \"strings must not be null for constantTimeEquals\");\r\n    if (a.length() != b.length()) {\r\n        return false;\r\n    }\r\n    int equals = 0;\r\n    for (int i = 0; i < a.length(); i++) {\r\n        equals |= a.charAt(i) ^ b.charAt(i);\r\n    }\r\n    return equals == 0;\r\n}"
}, {
	"Path": "org.nd4j.linalg.primitives.Counter.dropElementsBelowThreshold",
	"Comment": "this method will remove all elements with counts below given threshold from counter",
	"Method": "void dropElementsBelowThreshold(double threshold){\r\n    Iterator<T> iterator = keySet().iterator();\r\n    while (iterator.hasNext()) {\r\n        T element = iterator.next();\r\n        double val = map.get(element).get();\r\n        if (val < threshold) {\r\n            iterator.remove();\r\n            dirty.set(true);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.sniff.SnifferBuilder.setSniffIntervalMillis",
	"Comment": "sets the interval between consecutive ordinary sniff executions in milliseconds. will be honoured whensniffonfailure is disabled or when there are no failures between consecutive sniff executions.",
	"Method": "SnifferBuilder setSniffIntervalMillis(int sniffIntervalMillis){\r\n    if (sniffIntervalMillis <= 0) {\r\n        throw new IllegalArgumentException(\"sniffIntervalMillis must be greater than 0\");\r\n    }\r\n    this.sniffIntervalMillis = sniffIntervalMillis;\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.jita.memory.CudaMemoryManager.memcpy",
	"Comment": "this method provides basic memcpy functionality with respect to target environment",
	"Method": "void memcpy(DataBuffer dstBuffer,DataBuffer srcBuffer){\r\n    CudaContext context = (CudaContext) AtomicAllocator.getInstance().getDeviceContext().getContext();\r\n    if (dstBuffer instanceof CompressedDataBuffer && !(srcBuffer instanceof CompressedDataBuffer)) {\r\n        AllocationPoint srcPoint = AtomicAllocator.getInstance().getAllocationPoint(srcBuffer);\r\n        long size = srcBuffer.getElementSize() * srcBuffer.length();\r\n        if (!srcPoint.isActualOnHostSide()) {\r\n            AtomicAllocator.getInstance().synchronizeHostData(srcBuffer);\r\n        }\r\n        Pointer src = AtomicAllocator.getInstance().getHostPointer(srcBuffer);\r\n        Pointer.memcpy(dstBuffer.addressPointer(), src, size);\r\n    } else if (!(dstBuffer instanceof CompressedDataBuffer) && srcBuffer instanceof CompressedDataBuffer) {\r\n        AllocationPoint dstPoint = AtomicAllocator.getInstance().getAllocationPoint(dstBuffer);\r\n        long size = srcBuffer.getElementSize() * srcBuffer.length();\r\n        Pointer.memcpy(dstBuffer.addressPointer(), srcBuffer.addressPointer(), size);\r\n        dstPoint.tickHostWrite();\r\n    } else if (dstBuffer instanceof CompressedDataBuffer && srcBuffer instanceof CompressedDataBuffer) {\r\n        Pointer.memcpy(dstBuffer.addressPointer(), srcBuffer.addressPointer(), srcBuffer.length() * srcBuffer.getElementSize());\r\n    } else {\r\n        AtomicAllocator.getInstance().memcpy(dstBuffer, srcBuffer);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.BaseNDArrayFactory.hstack",
	"Comment": "concatenates two matrices horizontally.matrices must have identicalnumbers of rows.",
	"Method": "INDArray hstack(INDArray arrs){\r\n    return Nd4j.concat(1, arrs);\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.serializer.MultiStandardizeSerializerStrategy.write",
	"Comment": "serialize a multinormalizerstandardize to a output stream",
	"Method": "void write(MultiNormalizerStandardize normalizer,OutputStream stream){\r\n    try (DataOutputStream dos = new DataOutputStream(stream)) {\r\n        dos.writeBoolean(normalizer.isFitLabel());\r\n        dos.writeInt(normalizer.numInputs());\r\n        dos.writeInt(normalizer.isFitLabel() ? normalizer.numOutputs() : -1);\r\n        for (int i = 0; i < normalizer.numInputs(); i++) {\r\n            Nd4j.write(normalizer.getFeatureMean(i), dos);\r\n            Nd4j.write(normalizer.getFeatureStd(i), dos);\r\n        }\r\n        if (normalizer.isFitLabel()) {\r\n            for (int i = 0; i < normalizer.numOutputs(); i++) {\r\n                Nd4j.write(normalizer.getLabelMean(i), dos);\r\n                Nd4j.write(normalizer.getLabelStd(i), dos);\r\n            }\r\n        }\r\n        dos.flush();\r\n    }\r\n}"
}, {
	"Path": "org.deeplearning4j.rl4j.util.DataManager.createSubdir",
	"Comment": "fixme race condition if you create them at the same time where checking if dir exists is not atomic with the creation",
	"Method": "String createSubdir(){\r\n    if (!saveData)\r\n        return \"\";\r\n    File dr = new File(dataRoot);\r\n    dr.mkdirs();\r\n    File[] rootChildren = dr.listFiles();\r\n    int i = 1;\r\n    while (childrenExist(rootChildren, i + \"\")) i++;\r\n    File f = new File(dataRoot + \"/\" + i);\r\n    f.mkdirs();\r\n    currentDir = f.getAbsolutePath();\r\n    log.info(\"Created training data directory: \" + currentDir);\r\n    File mov = new File(getVideoDir());\r\n    mov.mkdirs();\r\n    File model = new File(getModelDir());\r\n    model.mkdirs();\r\n    File stat = new File(getStat());\r\n    File info = new File(getInfo());\r\n    stat.createNewFile();\r\n    info.createNewFile();\r\n    return f.getAbsolutePath();\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.deleteRoleMappingAsync",
	"Comment": "asynchronously delete a role mapping.see the docs for more.",
	"Method": "void deleteRoleMappingAsync(DeleteRoleMappingRequest request,RequestOptions options,ActionListener<DeleteRoleMappingResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, SecurityRequestConverters::deleteRoleMapping, options, DeleteRoleMappingResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.util.CudaArgs.convertMPtoCores",
	"Comment": "returns number of sms, based on device compute capability and number of processors.",
	"Method": "int convertMPtoCores(int ccMajor,int ccMinor,int numberOfProcessors){\r\n    if (ccMajor == 1)\r\n        return 8;\r\n    if (ccMajor == 2 && ccMinor == 1)\r\n        return 48;\r\n    if (ccMajor == 2)\r\n        return 32;\r\n    if (ccMajor == 3)\r\n        return 192;\r\n    if (ccMajor == 5)\r\n        return 128;\r\n    return -1;\r\n}"
}, {
	"Path": "io.dropwizard.logging.json.layout.MapBuilder.addNumber",
	"Comment": "adds the number value to the provided map under the provided field name,if it should be included. the supplier is only invoked if the field is to be included.",
	"Method": "MapBuilder addNumber(String fieldName,boolean include,Number number,MapBuilder addNumber,String fieldName,boolean include,Supplier<Number> supplier){\r\n    if (include) {\r\n        Number value = supplier.get();\r\n        if (value != null) {\r\n            map.put(getFieldName(fieldName), value);\r\n        }\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.linalg.shape.indexing.IndexingTests.testSimplePoint",
	"Comment": "simple test that checks indexing through different ways that fails",
	"Method": "void testSimplePoint(){\r\n    INDArray A = Nd4j.linspace(1, 3 * 3 * 3, 3 * 3 * 3).reshape(3, 3, 3);\r\n    INDArray viewOne = A.get(NDArrayIndex.point(1), NDArrayIndex.interval(0, 2), NDArrayIndex.interval(1, 3));\r\n    INDArray viewTwo = A.get(NDArrayIndex.point(1)).get(NDArrayIndex.interval(0, 2), NDArrayIndex.interval(1, 3));\r\n    INDArray expected = Nd4j.zeros(2, 2);\r\n    expected.putScalar(0, 0, 11);\r\n    expected.putScalar(0, 1, 20);\r\n    expected.putScalar(1, 0, 14);\r\n    expected.putScalar(1, 1, 23);\r\n    assertEquals(\"View with two get\", expected, viewTwo);\r\n    assertEquals(\"View with one get\", expected, viewOne);\r\n    assertEquals(\"Two views should be the same\", viewOne, viewTwo);\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.util.MeshOrganizer.clone",
	"Comment": "this method returns absolutely independent copy of this mesh",
	"Method": "MeshOrganizer clone(){\r\n    val b = SerializationUtils.toByteArray(this);\r\n    return SerializationUtils.fromByteArray(b);\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.s3.S3Service.refreshAndClearCache",
	"Comment": "refreshes the settings for the amazons3 clients and clears the cache ofexisting clients. new clients will be build using these new settings. oldclients are usable until released. on release they will be destroyed insteadto being returned to the cache.",
	"Method": "Map<String, S3ClientSettings> refreshAndClearCache(Map<String, S3ClientSettings> clientsSettings){\r\n    releaseCachedClients();\r\n    final Map<String, S3ClientSettings> prevSettings = this.clientsSettings;\r\n    this.clientsSettings = MapBuilder.newMapBuilder(clientsSettings).immutableMap();\r\n    assert this.clientsSettings.containsKey(\"default\") : \"always at least have 'default'\";\r\n    return prevSettings;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.transport.BaseTransport.shardMessageHandler",
	"Comment": "this message handler is responsible for receiving messages on shard side",
	"Method": "void shardMessageHandler(DirectBuffer buffer,int offset,int length,Header header){\r\n    byte[] data = new byte[length];\r\n    buffer.getBytes(offset, data);\r\n    VoidMessage message = VoidMessage.fromBytes(data);\r\n    if (message.getMessageType() == 7) {\r\n        messages.add(message);\r\n    } else {\r\n        publicationForShards.offer(buffer, offset, length);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.clearCache",
	"Comment": "clears the cache of one or more indices using the clear cache api.see clear cache api on elastic.co",
	"Method": "ClearIndicesCacheResponse clearCache(ClearIndicesCacheRequest clearIndicesCacheRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(clearIndicesCacheRequest, IndicesRequestConverters::clearCache, options, ClearIndicesCacheResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.getRoles",
	"Comment": "retrieves roles from the native roles store.see the docs for more.",
	"Method": "GetRolesResponse getRoles(GetRolesRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, SecurityRequestConverters::getRoles, options, GetRolesResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.script.mustache.SearchTemplateIT.testTemplateQueryAsEscapedStringStartingWithConditionalClause",
	"Comment": "test that template can contain conditional clause. in this case it is atthe beginning of the string.",
	"Method": "void testTemplateQueryAsEscapedStringStartingWithConditionalClause(){\r\n    SearchRequest searchRequest = new SearchRequest();\r\n    searchRequest.indices(\"_all\");\r\n    String templateString = \"{\" + \"  \\\"source\\\" : \\\"{ {{#use_size}} \\\\\\\"size\\\\\\\": \\\\\\\"{{size}}\\\\\\\", {{/use_size}} \\\\\\\"query\\\\\\\":{\\\\\\\"match_all\\\\\\\":{}}}\\\",\" + \"  \\\"params\\\":{\" + \"    \\\"size\\\": 1,\" + \"    \\\"use_size\\\": true\" + \"  }\" + \"}\";\r\n    SearchTemplateRequest request = SearchTemplateRequest.fromXContent(createParser(JsonXContent.jsonXContent, templateString));\r\n    request.setRequest(searchRequest);\r\n    SearchTemplateResponse searchResponse = client().execute(SearchTemplateAction.INSTANCE, request).get();\r\n    assertThat(searchResponse.getResponse().getHits().getHits().length, equalTo(1));\r\n}"
}, {
	"Path": "org.nd4j.linalg.primitives.Optional.empty",
	"Comment": "returns an empty optional instance. no value is present for this optional.",
	"Method": "Optional<T> empty(){\r\n    return (Optional<T>) EMPTY;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.closeJob",
	"Comment": "closes one or more machine learning jobs. a job can be opened and closed multiple times throughout its lifecycle.a closed job cannot receive data or perform analysis operations, but you can still explore and navigate results.for additional infosee ml close job documentation",
	"Method": "CloseJobResponse closeJob(CloseJobRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::closeJob, options, CloseJobResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.BooleanIndexing.chooseFrom",
	"Comment": "choose from the inputs based on the given condition.this returns a row vector of all elements fulfilling thecondition listed within the array for input",
	"Method": "INDArray chooseFrom(INDArray[] input,Condition condition,INDArray chooseFrom,INDArray[] input,List<Double> tArgs,List<Integer> iArgs,Condition condition){\r\n    Choose choose = new Choose(input, iArgs, tArgs, condition);\r\n    Nd4j.getExecutioner().exec(choose);\r\n    int secondOutput = choose.getOutputArgument(1).getInt(0);\r\n    if (secondOutput < 1) {\r\n        return null;\r\n    }\r\n    INDArray ret = choose.getOutputArgument(0).get(NDArrayIndex.interval(0, secondOutput));\r\n    ret = ret.reshape(ret.length());\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.ops.executioner.CudaGridExecutioner.buildGrid",
	"Comment": "this method bundless all ops available in queue into single gridop",
	"Method": "GridOp buildGrid(){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.http.netty4.cors.Netty4CorsConfig.preflightResponseHeaders",
	"Comment": "returns http response headers that should be added to a cors preflight response.",
	"Method": "HttpHeaders preflightResponseHeaders(){\r\n    if (preflightHeaders.isEmpty()) {\r\n        return EmptyHttpHeaders.INSTANCE;\r\n    }\r\n    final HttpHeaders preflightHeaders = new DefaultHttpHeaders();\r\n    for (Map.Entry<CharSequence, Callable<?>> entry : this.preflightHeaders.entrySet()) {\r\n        final Object value = getValue(entry.getValue());\r\n        if (value instanceof Iterable) {\r\n            preflightHeaders.add(entry.getKey().toString(), (Iterable<?>) value);\r\n        } else {\r\n            preflightHeaders.add(entry.getKey().toString(), value);\r\n        }\r\n    }\r\n    return preflightHeaders;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.machineLearning",
	"Comment": "provides methods for accessing the elastic licensed machine learning apis thatare shipped with the elastic stack distribution of elasticsearch. all ofthese apis will 404 if run against the oss distribution of elasticsearch.see the machine learning apis on elastic.co for more information.",
	"Method": "MachineLearningClient machineLearning(){\r\n    return machineLearningClient;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IngestClient.deletePipeline",
	"Comment": "delete an existing pipeline.seedelete pipeline api on elastic.co",
	"Method": "AcknowledgedResponse deletePipeline(DeletePipelineRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IngestRequestConverters::deletePipeline, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.backwards.IndexingIT.indexDocWithConcurrentUpdates",
	"Comment": "indexes a document in index with docid then concurrently updates the same documentnupdates times",
	"Method": "int indexDocWithConcurrentUpdates(String index,int docId,int nUpdates){\r\n    indexDocs(index, docId, 1);\r\n    Thread[] indexThreads = new Thread[nUpdates];\r\n    for (int i = 0; i < nUpdates; i++) {\r\n        indexThreads[i] = new Thread(() -> {\r\n            try {\r\n                indexDocs(index, docId, 1);\r\n            } catch (IOException e) {\r\n                throw new AssertionError(\"failed while indexing [\" + e.getMessage() + \"]\");\r\n            }\r\n        });\r\n        indexThreads[i].start();\r\n    }\r\n    for (Thread indexThread : indexThreads) {\r\n        indexThread.join();\r\n    }\r\n    return nUpdates + 1;\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.getDeviceOffset",
	"Comment": "this method returns current device memory offset within workspace",
	"Method": "long getDeviceOffset(){\r\n    return deviceOffset.get();\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.flush",
	"Comment": "flush one or more indices using the flush api.seeflush api on elastic.co",
	"Method": "FlushResponse flush(FlushRequest flushRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(flushRequest, IndicesRequestConverters::flush, options, FlushResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.createTokenAsync",
	"Comment": "asynchronously creates an oauth2 token.see the docs for more.",
	"Method": "void createTokenAsync(CreateTokenRequest request,RequestOptions options,ActionListener<CreateTokenResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, SecurityRequestConverters::createToken, options, CreateTokenResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.clearRolesCache",
	"Comment": "clears the roles cache for a set of roles.see the docs for more.",
	"Method": "ClearRolesCacheResponse clearRolesCache(ClearRolesCacheRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, SecurityRequestConverters::clearRolesCache, options, ClearRolesCacheResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.InstallPluginCommand.checkMisspelledPlugin",
	"Comment": "returns all the official plugin names that look similar to pluginid.",
	"Method": "List<String> checkMisspelledPlugin(String pluginId){\r\n    LevenshteinDistance ld = new LevenshteinDistance();\r\n    List<Tuple<Float, String>> scoredKeys = new ArrayList();\r\n    for (String officialPlugin : OFFICIAL_PLUGINS) {\r\n        float distance = ld.getDistance(pluginId, officialPlugin);\r\n        if (distance > 0.7f) {\r\n            scoredKeys.add(new Tuple(distance, officialPlugin));\r\n        }\r\n    }\r\n    CollectionUtil.timSort(scoredKeys, (a, b) -> b.v1().compareTo(a.v1()));\r\n    return scoredKeys.stream().map((a) -> a.v2()).collect(Collectors.toList());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getJobStatsAsync",
	"Comment": "gets usage statistics for one or more machine learning jobs, asynchronously.for additional infosee get job stats docs",
	"Method": "void getJobStatsAsync(GetJobStatsRequest request,RequestOptions options,ActionListener<GetJobStatsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getJobStats, options, GetJobStatsResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.stats.ForecastStats.getTotal",
	"Comment": "the number of forecasts currently available for this model.",
	"Method": "long getTotal(){\r\n    return total;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity",
	"Comment": "defines a helper method for performing a request and then parsing the returned entity using the provided entityparser.",
	"Method": "Resp performRequestAndParseEntity(Req request,CheckedFunction<Req, Request, IOException> requestConverter,RequestOptions options,CheckedFunction<XContentParser, Resp, IOException> entityParser,Set<Integer> ignores,Resp performRequestAndParseEntity,Req request,CheckedFunction<Req, Request, IOException> requestConverter,RequestOptions options,CheckedFunction<XContentParser, Resp, IOException> entityParser,Set<Integer> ignores){\r\n    return performRequest(request, requestConverter, options, response -> parseEntity(response.getEntity(), entityParser), ignores);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.reroute.ClusterRerouteRequest.dryRun",
	"Comment": "returns the current dry run flag which allows to run the commands without actually applying them,just to get back the resulting cluster state back.",
	"Method": "ClusterRerouteRequest dryRun(boolean dryRun,boolean dryRun){\r\n    return this.dryRun;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.StopDatafeedRequest.setForce",
	"Comment": "should the stopping be forced.use to forcefully stop a datafeed",
	"Method": "void setForce(boolean force){\r\n    this.force = force;\r\n}"
}, {
	"Path": "org.elasticsearch.nio.SelectionKeyUtils.removeWriteInterested",
	"Comment": "removes an interest in writes for this selection key while maintaining other interests.",
	"Method": "void removeWriteInterested(SelectionKey selectionKey){\r\n    selectionKey.interestOps(selectionKey.interestOps() & ~SelectionKey.OP_WRITE);\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.BooleanIndexing.or",
	"Comment": "or over the whole ndarray given some condition, with respect to dimensions",
	"Method": "boolean[] or(INDArray n,Condition condition,int dimension,boolean or,INDArray n,Condition cond){\r\n    if (cond instanceof BaseCondition) {\r\n        long val = (long) Nd4j.getExecutioner().exec(new MatchCondition(n, cond), Integer.MAX_VALUE).getDouble(0);\r\n        if (val > 0)\r\n            return true;\r\n        else\r\n            return false;\r\n    } else {\r\n        boolean ret = false;\r\n        final AtomicBoolean a = new AtomicBoolean(ret);\r\n        Shape.iterate(n, new CoordinateFunction() {\r\n            @Override\r\n            public void process(long[]... coord) {\r\n                if (!a.get())\r\n                    a.compareAndSet(false, a.get() || cond.apply(n.getDouble(coord[0])));\r\n            }\r\n        });\r\n        return a.get();\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.BooleanIndexing.or",
	"Comment": "or over the whole ndarray given some condition, with respect to dimensions",
	"Method": "boolean[] or(INDArray n,Condition condition,int dimension,boolean or,INDArray n,Condition cond){\r\n    if (!a.get())\r\n        a.compareAndSet(false, a.get() || cond.apply(n.getDouble(coord[0])));\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.startDatafeedAsync",
	"Comment": "starts the given machine learning datafeed asynchronously and notifies the listener on completionfor additional infosee ml start datafeed documentation",
	"Method": "void startDatafeedAsync(StartDatafeedRequest request,RequestOptions options,ActionListener<StartDatafeedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::startDatafeed, options, StartDatafeedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.apache.lucene.search.uhighlight.CustomUnifiedHighlighter.highlightField",
	"Comment": "highlights terms extracted from the provided query within the content of the provided field name",
	"Method": "Snippet[] highlightField(String field,Query query,int docId,int maxPassages){\r\n    Map<String, Object[]> fieldsAsObjects = super.highlightFieldsAsObjects(new String[] { field }, query, new int[] { docId }, new int[] { maxPassages });\r\n    Object[] snippetObjects = fieldsAsObjects.get(field);\r\n    if (snippetObjects != null) {\r\n        assert snippetObjects.length == 1;\r\n        Object snippetObject = snippetObjects[0];\r\n        if (snippetObject != null && snippetObject instanceof Snippet[]) {\r\n            return (Snippet[]) snippetObject;\r\n        }\r\n    }\r\n    return EMPTY_SNIPPET;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.argMin",
	"Comment": "returns the index of the minimum value in the array.if two entries have same minimum value, index of the first one is returned.",
	"Method": "int argMin(int[] in,int argMin,long[] in){\r\n    int minIdx = 0;\r\n    for (int i = 1; i < in.length; i++) if (in[i] < in[minIdx])\r\n        minIdx = i;\r\n    return minIdx;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.SoftSyncParameterUpdater.reset",
	"Comment": "reset internal counterssuch as number of updates accumulated.",
	"Method": "void reset(){\r\n    currentVersion++;\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.provider.BasicWorkspaceManager.getAndActivateWorkspace",
	"Comment": "this method gets & activates default with a given configuration and id",
	"Method": "MemoryWorkspace getAndActivateWorkspace(MemoryWorkspace getAndActivateWorkspace,String id,MemoryWorkspace getAndActivateWorkspace,WorkspaceConfiguration configuration,String id){\r\n    return getWorkspaceForCurrentThread(configuration, id).notifyScopeEntered();\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteCalendarAsync",
	"Comment": "deletes the given machine learning job asynchronously and notifies the listener on completionfor additional info seeml delete calendar documentation",
	"Method": "void deleteCalendarAsync(DeleteCalendarRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::deleteCalendar, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.prepend",
	"Comment": "append the givenarray with the specified value sizealong a particular axis",
	"Method": "INDArray prepend(INDArray arr,int padAmount,double val,int axis){\r\n    if (padAmount == 0)\r\n        return arr;\r\n    long[] paShape = ArrayUtil.copy(arr.shape());\r\n    if (axis < 0)\r\n        axis = axis + arr.shape().length;\r\n    paShape[axis] = padAmount;\r\n    INDArray concatArr = Nd4j.valueArrayOf(paShape, val);\r\n    return Nd4j.concat(axis, concatArr, arr);\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteCalendarEvent",
	"Comment": "removes a scheduled event from a calendarfor additional infosee ml delete calendar event documentation",
	"Method": "AcknowledgedResponse deleteCalendarEvent(DeleteCalendarEventRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::deleteCalendarEvent, options, AcknowledgedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ClusterClient.getSettings",
	"Comment": "get the cluster wide settings using the cluster get settings api.seecluster get settingsapi on elastic.co",
	"Method": "ClusterGetSettingsResponse getSettings(ClusterGetSettingsRequest clusterGetSettingsRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(clusterGetSettingsRequest, ClusterRequestConverters::clusterGetSettings, options, ClusterGetSettingsResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RollupClient.getRollupIndexCapabilitiesAsync",
	"Comment": "asynchronously get the rollup index capabilities of a rollup index or patternsee the docs for more.",
	"Method": "void getRollupIndexCapabilitiesAsync(GetRollupIndexCapsRequest request,RequestOptions options,ActionListener<GetRollupIndexCapsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, RollupRequestConverters::getRollupIndexCaps, options, GetRollupIndexCapsResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.JCublasNDArrayFactory.pullRows",
	"Comment": "this method produces concatenated array, that consist from tensors, fetched from source array, against some dimension and specified indexes",
	"Method": "INDArray pullRows(INDArray source,int sourceDimension,int[] indexes,INDArray pullRows,INDArray source,int sourceDimension,long[] indexes,INDArray pullRows,INDArray source,int sourceDimension,int[] indexes,char order,INDArray pullRows,INDArray source,INDArray destination,int sourceDimension,int[] indexes){\r\n    if (Nd4j.getExecutioner() instanceof GridExecutioner)\r\n        ((GridExecutioner) Nd4j.getExecutioner()).flushQueue();\r\n    if (indexes == null || indexes.length < 1)\r\n        throw new IllegalStateException(\"Indexes can't be null or zero-length\");\r\n    long[] shape = null;\r\n    if (sourceDimension == 1)\r\n        shape = new long[] { indexes.length, source.shape()[sourceDimension] };\r\n    else if (sourceDimension == 0)\r\n        shape = new long[] { source.shape()[sourceDimension], indexes.length };\r\n    else\r\n        throw new UnsupportedOperationException(\"2D input is expected\");\r\n    INDArray ret = destination;\r\n    if (ret == null) {\r\n        ret = Nd4j.createUninitialized(shape, order);\r\n    } else {\r\n        if (!Arrays.equals(shape, destination.shape())) {\r\n            throw new IllegalStateException(\"Cannot pull rows into destination array: expected destination array of\" + \" shape \" + Arrays.toString(shape) + \" but got destination array of shape \" + Arrays.toString(destination.shape()));\r\n        }\r\n    }\r\n    AtomicAllocator allocator = AtomicAllocator.getInstance();\r\n    CudaContext context = allocator.getFlowController().prepareAction(ret, source);\r\n    Pointer x = AtomicAllocator.getInstance().getPointer(source, context);\r\n    Pointer xShape = AtomicAllocator.getInstance().getPointer(source.shapeInfoDataBuffer(), context);\r\n    Pointer z = AtomicAllocator.getInstance().getPointer(ret, context);\r\n    Pointer zShape = AtomicAllocator.getInstance().getPointer(ret.shapeInfoDataBuffer(), context);\r\n    PointerPointer extras = new PointerPointer(AddressRetriever.retrieveHostPointer(ret.shapeInfoDataBuffer()), context.getOldStream(), allocator.getDeviceIdPointer());\r\n    val tempIndexes = new CudaLongDataBuffer(indexes.length);\r\n    AtomicAllocator.getInstance().memcpyBlocking(tempIndexes, new LongPointer(ArrayUtil.toLongArray(indexes)), indexes.length * 8, 0);\r\n    Pointer pIndex = AtomicAllocator.getInstance().getPointer(tempIndexes, context);\r\n    TADManager tadManager = Nd4j.getExecutioner().getTADManager();\r\n    Pair<DataBuffer, DataBuffer> tadBuffers = tadManager.getTADOnlyShapeInfo(source, new int[] { sourceDimension });\r\n    Pair<DataBuffer, DataBuffer> zTadBuffers = tadManager.getTADOnlyShapeInfo(ret, new int[] { sourceDimension });\r\n    Pointer tadShapeInfo = AtomicAllocator.getInstance().getPointer(tadBuffers.getFirst(), context);\r\n    Pointer zTadShapeInfo = AtomicAllocator.getInstance().getPointer(zTadBuffers.getFirst(), context);\r\n    DataBuffer offsets = tadBuffers.getSecond();\r\n    Pointer tadOffsets = AtomicAllocator.getInstance().getPointer(offsets, context);\r\n    Pointer zTadOffsets = AtomicAllocator.getInstance().getPointer(zTadBuffers.getSecond(), context);\r\n    if (ret.data().dataType() == DataBuffer.Type.DOUBLE) {\r\n        nativeOps.pullRowsDouble(extras, (DoublePointer) x, (LongPointer) xShape, (DoublePointer) z, (LongPointer) zShape, indexes.length, (LongPointer) pIndex, (LongPointer) tadShapeInfo, new LongPointerWrapper(tadOffsets), (LongPointer) zTadShapeInfo, new LongPointerWrapper(zTadOffsets));\r\n    } else if (ret.data().dataType() == DataBuffer.Type.FLOAT) {\r\n        nativeOps.pullRowsFloat(extras, (FloatPointer) x, (LongPointer) xShape, (FloatPointer) z, (LongPointer) zShape, indexes.length, (LongPointer) pIndex, (LongPointer) tadShapeInfo, new LongPointerWrapper(tadOffsets), (LongPointer) zTadShapeInfo, new LongPointerWrapper(zTadOffsets));\r\n    } else {\r\n        nativeOps.pullRowsHalf(extras, (ShortPointer) x, (LongPointer) xShape, (ShortPointer) z, (LongPointer) zShape, indexes.length, (LongPointer) pIndex, (LongPointer) tadShapeInfo, new LongPointerWrapper(tadOffsets), (LongPointer) zTadShapeInfo, new LongPointerWrapper(zTadOffsets));\r\n    }\r\n    allocator.registerAction(context, ret, source);\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.XContentFactory.xContentType",
	"Comment": "guesses the content type based on the provided input stream without consuming it.",
	"Method": "XContentType xContentType(CharSequence content,XContentType xContentType,InputStream si,XContentType xContentType,byte[] bytes,XContentType xContentType,byte[] bytes,int offset,int length){\r\n    int totalLength = bytes.length;\r\n    if (totalLength == 0 || length == 0) {\r\n        return null;\r\n    } else if ((offset + length) > totalLength) {\r\n        return null;\r\n    }\r\n    byte first = bytes[offset];\r\n    if (first == '{') {\r\n        return XContentType.JSON;\r\n    }\r\n    if (length > 2 && first == SmileConstants.HEADER_BYTE_1 && bytes[offset + 1] == SmileConstants.HEADER_BYTE_2 && bytes[offset + 2] == SmileConstants.HEADER_BYTE_3) {\r\n        return XContentType.SMILE;\r\n    }\r\n    if (length > 2 && first == '-' && bytes[offset + 1] == '-' && bytes[offset + 2] == '-') {\r\n        return XContentType.YAML;\r\n    }\r\n    if (first == CBORConstants.BYTE_OBJECT_INDEFINITE && length > 1) {\r\n        return XContentType.CBOR;\r\n    }\r\n    if (CBORConstants.hasMajorType(CBORConstants.MAJOR_TYPE_TAG, first) && length > 2) {\r\n        if (first == (byte) 0xD9 && bytes[offset + 1] == (byte) 0xD9 && bytes[offset + 2] == (byte) 0xF7) {\r\n            return XContentType.CBOR;\r\n        }\r\n    }\r\n    if (CBORConstants.hasMajorType(CBORConstants.MAJOR_TYPE_OBJECT, first)) {\r\n        return XContentType.CBOR;\r\n    }\r\n    int jsonStart = 0;\r\n    if (length > 3 && first == (byte) 0xEF && bytes[offset + 1] == (byte) 0xBB && bytes[offset + 2] == (byte) 0xBF) {\r\n        jsonStart = 3;\r\n    }\r\n    for (int i = jsonStart; i < length; i++) {\r\n        byte b = bytes[offset + i];\r\n        if (b == '{') {\r\n            return XContentType.JSON;\r\n        }\r\n        if (Character.isWhitespace(b) == false) {\r\n            break;\r\n        }\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.list.ListTasksResponse.toXContentGroupedByParents",
	"Comment": "convert this response to xcontent grouping by parent tasks.",
	"Method": "XContentBuilder toXContentGroupedByParents(XContentBuilder builder,Params params){\r\n    toXContentCommon(builder, params);\r\n    builder.startObject(TASKS);\r\n    for (TaskGroup group : getTaskGroups()) {\r\n        builder.field(group.getTaskInfo().getTaskId().toString());\r\n        group.toXContent(builder, params);\r\n    }\r\n    builder.endObject();\r\n    return builder;\r\n}"
}, {
	"Path": "io.dropwizard.client.HttpClientBuilder.createConnectionManager",
	"Comment": "create a instrumentedhttpclientconnectionmanager based on thehttpclientconfiguration. it sets the maximum connections per route andthe maximum total connections that the connection manager can create",
	"Method": "InstrumentedHttpClientConnectionManager createConnectionManager(Registry<ConnectionSocketFactory> registry,String name){\r\n    final Duration ttl = configuration.getTimeToLive();\r\n    final InstrumentedHttpClientConnectionManager manager = new InstrumentedHttpClientConnectionManager(metricRegistry, registry, null, null, resolver, ttl.getQuantity(), ttl.getUnit(), name);\r\n    return configureConnectionManager(manager);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.refreshAsync",
	"Comment": "asynchronously refresh one or more indices using the refresh api.seerefresh api on elastic.co",
	"Method": "void refreshAsync(RefreshRequest refreshRequest,RequestOptions options,ActionListener<RefreshResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(refreshRequest, IndicesRequestConverters::refresh, options, RefreshResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.Response.getHeader",
	"Comment": "returns the value of the first header with a specified name of this message.if there is more than one matching header in the message the first element is returned.if there is no matching header in the message null is returned.",
	"Method": "String getHeader(String name){\r\n    Header header = response.getFirstHeader(name);\r\n    if (header == null) {\r\n        return null;\r\n    }\r\n    return header.getValue();\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.util.MeshOrganizer.getDownstreamsForNode",
	"Comment": "this method returns downstream connections for a given node",
	"Method": "Collection<Node> getDownstreamsForNode(String ip){\r\n    val node = getNodeById(ip);\r\n    return node.getDownstreamNodes();\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettingsTests.randomClient",
	"Comment": "generates a random googlecloudstorageclientsettings along with the settings to build it",
	"Method": "GoogleCloudStorageClientSettings randomClient(String clientName,Settings.Builder settings,MockSecureSettings secureSettings,List<Setting<?>> deprecationWarnings){\r\n    final Tuple<ServiceAccountCredentials, byte[]> credentials = randomCredential(clientName);\r\n    final ServiceAccountCredentials credential = credentials.v1();\r\n    secureSettings.setFile(CREDENTIALS_FILE_SETTING.getConcreteSettingForNamespace(clientName).getKey(), credentials.v2());\r\n    String endpoint;\r\n    if (randomBoolean()) {\r\n        endpoint = randomFrom(\"http://www.elastic.co\", \"http://metadata.google.com:88/oauth\", \"https://www.googleapis.com\", \"https://www.elastic.co:443\", \"http://localhost:8443\", \"https://www.googleapis.com/oauth/token\");\r\n        settings.put(ENDPOINT_SETTING.getConcreteSettingForNamespace(clientName).getKey(), endpoint);\r\n    } else {\r\n        endpoint = ENDPOINT_SETTING.getDefault(Settings.EMPTY);\r\n    }\r\n    String projectId;\r\n    if (randomBoolean()) {\r\n        projectId = randomAlphaOfLength(5);\r\n        settings.put(PROJECT_ID_SETTING.getConcreteSettingForNamespace(clientName).getKey(), projectId);\r\n    } else {\r\n        projectId = PROJECT_ID_SETTING.getDefault(Settings.EMPTY);\r\n    }\r\n    TimeValue connectTimeout;\r\n    if (randomBoolean()) {\r\n        connectTimeout = randomTimeout();\r\n        settings.put(CONNECT_TIMEOUT_SETTING.getConcreteSettingForNamespace(clientName).getKey(), connectTimeout.getStringRep());\r\n    } else {\r\n        connectTimeout = CONNECT_TIMEOUT_SETTING.getDefault(Settings.EMPTY);\r\n    }\r\n    TimeValue readTimeout;\r\n    if (randomBoolean()) {\r\n        readTimeout = randomTimeout();\r\n        settings.put(READ_TIMEOUT_SETTING.getConcreteSettingForNamespace(clientName).getKey(), readTimeout.getStringRep());\r\n    } else {\r\n        readTimeout = READ_TIMEOUT_SETTING.getDefault(Settings.EMPTY);\r\n    }\r\n    String applicationName;\r\n    if (randomBoolean()) {\r\n        applicationName = randomAlphaOfLength(5);\r\n        settings.put(APPLICATION_NAME_SETTING.getConcreteSettingForNamespace(clientName).getKey(), applicationName);\r\n        deprecationWarnings.add(APPLICATION_NAME_SETTING.getConcreteSettingForNamespace(clientName));\r\n    } else {\r\n        applicationName = APPLICATION_NAME_SETTING.getDefault(Settings.EMPTY);\r\n    }\r\n    return new GoogleCloudStorageClientSettings(credential, endpoint, projectId, connectTimeout, readTimeout, applicationName, new URI(\"\"));\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.testBuildRequestThrowsException",
	"Comment": "mimicks script failures or general wrongness by implementers.",
	"Method": "void testBuildRequestThrowsException(){\r\n    DummyAsyncBulkByScrollAction action = new DummyAsyncBulkByScrollAction() {\r\n        @Override\r\n        protected AbstractAsyncBulkByScrollAction.RequestWrapper<?> buildRequest(Hit doc) {\r\n            throw new RuntimeException(\"surprise\");\r\n        }\r\n    };\r\n    ScrollableHitSource.BasicHit hit = new ScrollableHitSource.BasicHit(\"index\", \"type\", \"id\", 0);\r\n    hit.setSource(new BytesArray(\"{}\"), XContentType.JSON);\r\n    ScrollableHitSource.Response response = new ScrollableHitSource.Response(false, emptyList(), 1, singletonList(hit), null);\r\n    simulateScrollResponse(action, timeValueNanos(System.nanoTime()), 0, response);\r\n    ExecutionException e = expectThrows(ExecutionException.class, () -> listener.get());\r\n    assertThat(e.getCause(), instanceOf(RuntimeException.class));\r\n    assertThat(e.getCause().getMessage(), equalTo(\"surprise\"));\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.testBuildRequestThrowsException",
	"Comment": "mimicks script failures or general wrongness by implementers.",
	"Method": "void testBuildRequestThrowsException(){\r\n    throw new RuntimeException(\"surprise\");\r\n}"
}, {
	"Path": "org.nd4j.util.StringUtils.getTrimmedStrings",
	"Comment": "splits a comma or newline separated value string, trimmingleading and trailing whitespace on each value.",
	"Method": "String[] getTrimmedStrings(String str){\r\n    if (null == str || str.trim().isEmpty()) {\r\n        return emptyStringArray;\r\n    }\r\n    return str.trim().split(\"\\\\s*[,\\n]\\\\s*\");\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.SoftSyncParameterUpdater.status",
	"Comment": "returns the current status of this parameter serverupdater",
	"Method": "Map<String, Number> status(){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ClusterClient.getSettingsAsync",
	"Comment": "asynchronously get the cluster wide settings using the cluster get settings api.seecluster get settingsapi on elastic.co",
	"Method": "void getSettingsAsync(ClusterGetSettingsRequest clusterGetSettingsRequest,RequestOptions options,ActionListener<ClusterGetSettingsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(clusterGetSettingsRequest, ClusterRequestConverters::clusterGetSettings, options, ClusterGetSettingsResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.script.expression.MoreExpressionTests.testPipelineAggregationScript",
	"Comment": "test to make sure expressions are allowed to be used for reduce in pipeline aggregations",
	"Method": "void testPipelineAggregationScript(){\r\n    createIndex(\"agg_index\");\r\n    ensureGreen(\"agg_index\");\r\n    indexRandom(true, client().prepareIndex(\"agg_index\", \"doc\", \"1\").setSource(\"one\", 1.0, \"two\", 2.0, \"three\", 3.0, \"four\", 4.0), client().prepareIndex(\"agg_index\", \"doc\", \"2\").setSource(\"one\", 2.0, \"two\", 2.0, \"three\", 3.0, \"four\", 4.0), client().prepareIndex(\"agg_index\", \"doc\", \"3\").setSource(\"one\", 3.0, \"two\", 2.0, \"three\", 3.0, \"four\", 4.0), client().prepareIndex(\"agg_index\", \"doc\", \"4\").setSource(\"one\", 4.0, \"two\", 2.0, \"three\", 3.0, \"four\", 4.0), client().prepareIndex(\"agg_index\", \"doc\", \"5\").setSource(\"one\", 5.0, \"two\", 2.0, \"three\", 3.0, \"four\", 4.0));\r\n    SearchResponse response = client().prepareSearch(\"agg_index\").addAggregation(histogram(\"histogram\").field(\"one\").interval(2).subAggregation(sum(\"twoSum\").field(\"two\")).subAggregation(sum(\"threeSum\").field(\"three\")).subAggregation(sum(\"fourSum\").field(\"four\")).subAggregation(bucketScript(\"totalSum\", new Script(ScriptType.INLINE, ExpressionScriptEngine.NAME, \"_value0 + _value1 + _value2\", Collections.emptyMap()), \"twoSum\", \"threeSum\", \"fourSum\"))).execute().actionGet();\r\n    Histogram histogram = response.getAggregations().get(\"histogram\");\r\n    assertThat(histogram, notNullValue());\r\n    assertThat(histogram.getName(), equalTo(\"histogram\"));\r\n    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\r\n    for (int bucketCount = 0; bucketCount < buckets.size(); ++bucketCount) {\r\n        Histogram.Bucket bucket = buckets.get(bucketCount);\r\n        if (bucket.getDocCount() == 1) {\r\n            SimpleValue seriesArithmetic = bucket.getAggregations().get(\"totalSum\");\r\n            assertThat(seriesArithmetic, notNullValue());\r\n            double seriesArithmeticValue = seriesArithmetic.value();\r\n            assertEquals(9.0, seriesArithmeticValue, 0.001);\r\n        } else if (bucket.getDocCount() == 2) {\r\n            SimpleValue seriesArithmetic = bucket.getAggregations().get(\"totalSum\");\r\n            assertThat(seriesArithmetic, notNullValue());\r\n            double seriesArithmeticValue = seriesArithmetic.value();\r\n            assertEquals(18.0, seriesArithmeticValue, 0.001);\r\n        } else {\r\n            fail(\"Incorrect number of documents in a bucket in the histogram.\");\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.util.StringUtils.formatTime",
	"Comment": "given the time in long milliseconds, returns astring in the format xhrs, ymins, z sec.",
	"Method": "String formatTime(long timeDiff){\r\n    StringBuilder buf = new StringBuilder();\r\n    long hours = timeDiff / (60 * 60 * 1000);\r\n    long rem = (timeDiff % (60 * 60 * 1000));\r\n    long minutes = rem / (60 * 1000);\r\n    rem = rem % (60 * 1000);\r\n    long seconds = rem / 1000;\r\n    if (hours != 0) {\r\n        buf.append(hours);\r\n        buf.append(\"hrs, \");\r\n    }\r\n    if (minutes != 0) {\r\n        buf.append(minutes);\r\n        buf.append(\"mins, \");\r\n    }\r\n    buf.append(seconds);\r\n    buf.append(\"sec\");\r\n    return buf.toString();\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getOverallBuckets",
	"Comment": "gets overall buckets for a set of machine learning jobs.for additional infosee ml get overall buckets documentation",
	"Method": "GetOverallBucketsResponse getOverallBuckets(GetOverallBucketsRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getOverallBuckets, options, GetOverallBucketsResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.index.rankeval.DiscountedCumulativeGainTests.testNoResults",
	"Comment": "test that metric returns 0.0 when there are no search results",
	"Method": "void testNoResults(){\r\n    Integer[] relevanceRatings = new Integer[] { 3, 2, 3, null, 1, null };\r\n    List<RatedDocument> ratedDocs = new ArrayList();\r\n    for (int i = 0; i < 6; i++) {\r\n        if (i < relevanceRatings.length) {\r\n            if (relevanceRatings[i] != null) {\r\n                ratedDocs.add(new RatedDocument(\"index\", Integer.toString(i), relevanceRatings[i]));\r\n            }\r\n        }\r\n    }\r\n    SearchHit[] hits = new SearchHit[0];\r\n    DiscountedCumulativeGain dcg = new DiscountedCumulativeGain();\r\n    EvalQueryQuality result = dcg.evaluate(\"id\", hits, ratedDocs);\r\n    assertEquals(0.0d, result.metricScore(), DELTA);\r\n    assertEquals(0, filterUnratedDocuments(result.getHitsAndRatings()).size());\r\n    dcg = new DiscountedCumulativeGain(true, null, 10);\r\n    result = dcg.evaluate(\"id\", hits, ratedDocs);\r\n    assertEquals(0.0d, result.metricScore(), DELTA);\r\n    assertEquals(0, filterUnratedDocuments(result.getHitsAndRatings()).size());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.StopDatafeedRequest.setTimeout",
	"Comment": "how long to wait for the stop request to complete before timing out.",
	"Method": "void setTimeout(TimeValue timeout){\r\n    this.timeout = timeout;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.scroll",
	"Comment": "executes a search using the search scroll api.see search scrollapi on elastic.co",
	"Method": "SearchResponse scroll(SearchScrollRequest searchScrollRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(searchScrollRequest, RequestConverters::searchScroll, options, SearchResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "com.googlecode.d2j.signapk.AbstractJarSign.addDigestsToManifest",
	"Comment": "add the sha1 of every file to the manifest, creating it if necessary.",
	"Method": "Manifest addDigestsToManifest(JarFile jar){\r\n    Manifest input = jar.getManifest();\r\n    Manifest output = new Manifest();\r\n    Attributes main = output.getMainAttributes();\r\n    if (input != null) {\r\n        main.putAll(input.getMainAttributes());\r\n    }\r\n    main.putValue(\"Manifest-Version\", \"1.0\");\r\n    main.putValue(\"Created-By\", \"1.6.0_21 (d2j-\" + AbstractJarSign.class.getPackage().getImplementationVersion() + \")\");\r\n    MessageDigest md = MessageDigest.getInstance(digestAlg);\r\n    byte[] buffer = new byte[4096];\r\n    int num;\r\n    TreeMap<String, JarEntry> byName = new TreeMap<String, JarEntry>();\r\n    for (Enumeration<JarEntry> e = jar.entries(); e.hasMoreElements(); ) {\r\n        JarEntry entry = e.nextElement();\r\n        byName.put(entry.getName(), entry);\r\n    }\r\n    String digName = digestAlg + \"-Digest\";\r\n    for (JarEntry entry : byName.values()) {\r\n        String name = entry.getName();\r\n        if (!entry.isDirectory() && !name.equals(JarFile.MANIFEST_NAME) && !stripPattern.matcher(name).matches()) {\r\n            InputStream data = jar.getInputStream(entry);\r\n            while ((num = data.read(buffer)) > 0) {\r\n                md.update(buffer, 0, num);\r\n            }\r\n            Attributes attr = null;\r\n            if (input != null) {\r\n                attr = input.getAttributes(name);\r\n            }\r\n            attr = attr != null ? new Attributes(attr) : new Attributes();\r\n            attr.putValue(digName, encodeBase64(md.digest()));\r\n            output.getEntries().put(name, attr);\r\n        }\r\n    }\r\n    return output;\r\n}"
}, {
	"Path": "io.dropwizard.setup.Bootstrap.registerMetrics",
	"Comment": "registers the jvm metrics to the metric registry and start to reportthe registry metrics via jmx.",
	"Method": "void registerMetrics(){\r\n    if (metricsAreRegistered) {\r\n        return;\r\n    }\r\n    getMetricRegistry().register(\"jvm.attribute\", new JvmAttributeGaugeSet());\r\n    getMetricRegistry().register(\"jvm.buffers\", new BufferPoolMetricSet(ManagementFactory.getPlatformMBeanServer()));\r\n    getMetricRegistry().register(\"jvm.classloader\", new ClassLoadingGaugeSet());\r\n    getMetricRegistry().register(\"jvm.filedescriptor\", new FileDescriptorRatioGauge());\r\n    getMetricRegistry().register(\"jvm.gc\", new GarbageCollectorMetricSet());\r\n    getMetricRegistry().register(\"jvm.memory\", new MemoryUsageGaugeSet());\r\n    getMetricRegistry().register(\"jvm.threads\", new ThreadStatesGaugeSet());\r\n    JmxReporter.forRegistry(metricRegistry).build().start();\r\n    metricsAreRegistered = true;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.GetCalendarEventsRequest.setEnd",
	"Comment": "specifies to get events with timestamps earlier than this time.",
	"Method": "void setEnd(String end){\r\n    this.end = end;\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.MultiNormalizerHybrid.standardizeOutput",
	"Comment": "apply standardization to a specific output, overriding the global output strategy if any",
	"Method": "MultiNormalizerHybrid standardizeOutput(int output){\r\n    perOutputStrategies.put(output, new StandardizeStrategy());\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.WatcherClient.ackWatchAsync",
	"Comment": "asynchronously acknowledges a watch.see the docs for more information.",
	"Method": "void ackWatchAsync(AckWatchRequest request,RequestOptions options,ActionListener<AckWatchResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, WatcherRequestConverters::ackWatch, options, AckWatchResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.rollup",
	"Comment": "provides methods for accessing the elastic licensed rollup apis thatare shipped with the default distribution of elasticsearch. all ofthese apis will 404 if run against the oss distribution of elasticsearch.see the watcher apis on elastic.co for more information.",
	"Method": "RollupClient rollup(){\r\n    return rollupClient;\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.valueArrayOf",
	"Comment": "creates a row vector with the specified number of columnssome people may know this as np.full",
	"Method": "INDArray valueArrayOf(int[] shape,double value,INDArray valueArrayOf,long[] shape,double value,INDArray valueArrayOf,long num,double value,INDArray valueArrayOf,long rows,long columns,double value){\r\n    if (rows < 1 || columns < 1)\r\n        throw new ND4JIllegalStateException(\"Number of rows and columns should be positive for new INDArray\");\r\n    INDArray ret = INSTANCE.valueArrayOf(rows, columns, value);\r\n    logCreationIfNecessary(ret);\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.config.Job.getResultsIndexNameNoPrefix",
	"Comment": "private version of getresultsindexname so that a job can be built from anotherjob and pass index name validation",
	"Method": "String getResultsIndexNameNoPrefix(){\r\n    return resultsIndexName;\r\n}"
}, {
	"Path": "org.nd4j.util.StringUtils.getTrimmedStringCollection",
	"Comment": "splits a comma separated value string, trimming leading andtrailing whitespace on each value. duplicate and empty values are removed.",
	"Method": "Collection<String> getTrimmedStringCollection(String str){\r\n    Set<String> set = new LinkedHashSet<String>(Arrays.asList(getTrimmedStrings(str)));\r\n    set.remove(\"\");\r\n    return set;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.scrollId",
	"Comment": "generates a random scrollid and registers it so that when the testfinishes we check that it was cleared. subsequent calls reregister a newrandom scroll id so it is checked instead.",
	"Method": "String scrollId(){\r\n    scrollId = randomSimpleString(random(), 1, 10);\r\n    return scrollId;\r\n}"
}, {
	"Path": "org.elasticsearch.client.tasks.GetTaskRequest.getTimeout",
	"Comment": "timeout to wait for any async actions this request must take. it must take anywhere from 0 to 2.",
	"Method": "TimeValue getTimeout(){\r\n    return timeout;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.fromString",
	"Comment": "this will take a given string and separator and convert it to an equivalentdouble array.",
	"Method": "double[] fromString(String data,String separator){\r\n    String[] split = data.split(separator);\r\n    double[] ret = new double[split.length];\r\n    for (int i = 0; i < split.length; i++) {\r\n        ret[i] = Double.parseDouble(split[i]);\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.SynchronousParameterUpdater.status",
	"Comment": "returns the current status of this parameter serverupdater",
	"Method": "Map<String, Number> status(){\r\n    Map<String, Number> ret = new HashMap();\r\n    ret.put(\"workers\", workers);\r\n    ret.put(\"accumulatedUpdates\", numUpdates());\r\n    return ret;\r\n}"
}, {
	"Path": "io.dropwizard.logging.json.layout.MapBuilder.addMap",
	"Comment": "adds the map value to the provided map under the provided field name, if it should be included. the supplier is only invoked if the field is to be included.",
	"Method": "MapBuilder addMap(String fieldName,boolean include,Supplier<Map<String, ?>> supplier){\r\n    if (include) {\r\n        Map<String, ?> value = supplier.get();\r\n        if (value != null && !value.isEmpty()) {\r\n            map.put(getFieldName(fieldName), value);\r\n        }\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.explainAsync",
	"Comment": "asynchronously executes a request using the explain api.see explain api on elastic.co",
	"Method": "void explainAsync(ExplainRequest explainRequest,RequestOptions options,ActionListener<ExplainResponse> listener){\r\n    performRequestAsync(explainRequest, RequestConverters::explain, options, response -> {\r\n        CheckedFunction<XContentParser, ExplainResponse, IOException> entityParser = parser -> ExplainResponse.fromXContent(parser, convertExistsResponse(response));\r\n        return parseEntity(response.getEntity(), entityParser);\r\n    }, listener, singleton(404));\r\n}"
}, {
	"Path": "org.elasticsearch.packaging.util.FileUtils.getFileOwner",
	"Comment": "gets the owner of a file in a way that should be supported by all filesystems that have a concept of file owner",
	"Method": "String getFileOwner(Path path){\r\n    try {\r\n        FileOwnerAttributeView view = Files.getFileAttributeView(path, FileOwnerAttributeView.class);\r\n        return view.getOwner().getName();\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.searchScroll",
	"Comment": "executes a search using the search scroll api.see search scrollapi on elastic.co",
	"Method": "SearchResponse searchScroll(SearchScrollRequest searchScrollRequest,RequestOptions options){\r\n    return scroll(searchScrollRequest, options);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.bulkAsync",
	"Comment": "asynchronously executes a bulk request using the bulk api.see bulk api on elastic.co",
	"Method": "void bulkAsync(BulkRequest bulkRequest,RequestOptions options,ActionListener<BulkResponse> listener){\r\n    performRequestAsyncAndParseEntity(bulkRequest, RequestConverters::bulk, options, BulkResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "io.dropwizard.auth.principal.NoAuthPrincipalEntityResource.principalEntityWithoutAuth",
	"Comment": "principal instance must be injected even when no authentication is required.",
	"Method": "String principalEntityWithoutAuth(JsonPrincipal principal){\r\n    assertThat(principal).isNotNull();\r\n    return principal.getName();\r\n}"
}, {
	"Path": "org.elasticsearch.painless.Locals.hasVariable",
	"Comment": "checks if a variable exists or not, in this scope or any parents.",
	"Method": "boolean hasVariable(String name){\r\n    Variable variable = lookupVariable(null, name);\r\n    if (variable != null) {\r\n        return true;\r\n    }\r\n    if (parent != null) {\r\n        return parent.hasVariable(name);\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.attachment.TikaImpl.parse",
	"Comment": "parses with tika, throwing any exception hit while parsing the document",
	"Method": "String parse(byte content,Metadata metadata,int limit){\r\n    SpecialPermission.check();\r\n    try {\r\n        return AccessController.doPrivileged((PrivilegedExceptionAction<String>) () -> TIKA_INSTANCE.parseToString(new ByteArrayInputStream(content), metadata, limit), RESTRICTED_CONTEXT);\r\n    } catch (PrivilegedActionException e) {\r\n        Throwable cause = e.getCause();\r\n        if (cause instanceof TikaException) {\r\n            throw (TikaException) cause;\r\n        } else if (cause instanceof IOException) {\r\n            throw (IOException) cause;\r\n        } else {\r\n            throw new AssertionError(cause);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.getLifecyclePolicy",
	"Comment": "retrieve one or more lifecycle policy definition. seethe docs for more.",
	"Method": "GetLifecyclePolicyResponse getLifecyclePolicy(GetLifecyclePolicyRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IndexLifecycleRequestConverters::getLifecyclePolicy, options, GetLifecyclePolicyResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.deleteByQueryRethrottle",
	"Comment": "executes a delete by query rethrottle request.see delete by query api on elastic.co",
	"Method": "ListTasksResponse deleteByQueryRethrottle(RethrottleRequest rethrottleRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(rethrottleRequest, RequestConverters::rethrottleDeleteByQuery, options, ListTasksResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RollupClient.getRollupIndexCapabilities",
	"Comment": "get the rollup index capabilities of a rollup index or patternsee the docs for more.",
	"Method": "GetRollupIndexCapsResponse getRollupIndexCapabilities(GetRollupIndexCapsRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, RollupRequestConverters::getRollupIndexCaps, options, GetRollupIndexCapsResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.create",
	"Comment": "creates an index using the create index api.see create index api on elastic.co",
	"Method": "CreateIndexResponse create(CreateIndexRequest createIndexRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(createIndexRequest, IndicesRequestConverters::createIndex, options, CreateIndexResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.CcrClient.unfollowAsync",
	"Comment": "asynchronously instructs a follower index to unfollow and become a regular index.note that index following needs to be paused and the follower index needs to be closed.see the docs for more.",
	"Method": "void unfollowAsync(UnfollowRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, CcrRequestConverters::unfollow, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "io.dropwizard.util.DirectExecutorService.shutdownNow",
	"Comment": "see newdirectexecutorservice javadoc for unusual behavior of this method.",
	"Method": "List<Runnable> shutdownNow(){\r\n    shutdown();\r\n    return Collections.emptyList();\r\n}"
}, {
	"Path": "org.elasticsearch.client.RankEvalIT.testMetrics",
	"Comment": "test case checks that the default metrics are registered and usable",
	"Method": "void testMetrics(){\r\n    List<RatedRequest> specifications = createTestEvaluationSpec();\r\n    List<Supplier<EvaluationMetric>> metrics = Arrays.asList(PrecisionAtK::new, MeanReciprocalRank::new, DiscountedCumulativeGain::new, () -> new ExpectedReciprocalRank(1));\r\n    double[] expectedScores = new double[] { 0.4285714285714286, 0.75, 1.6408962261063627, 0.4407738095238095 };\r\n    int i = 0;\r\n    for (Supplier<EvaluationMetric> metricSupplier : metrics) {\r\n        RankEvalSpec spec = new RankEvalSpec(specifications, metricSupplier.get());\r\n        RankEvalRequest rankEvalRequest = new RankEvalRequest(spec, new String[] { \"index\", \"index2\" });\r\n        RankEvalResponse response = execute(rankEvalRequest, highLevelClient()::rankEval, highLevelClient()::rankEvalAsync);\r\n        assertEquals(expectedScores[i], response.getMetricScore(), Double.MIN_VALUE);\r\n        i++;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.painless.DefMath.lookupBinary",
	"Comment": "returns an appropriate method handle for a binary operator, based on promotion of the lhs and rhs arguments",
	"Method": "MethodHandle lookupBinary(Class<?> classA,Class<?> classB,String name){\r\n    MethodHandle handle = TYPE_OP_MAPPING.get(promote(promote(unbox(classA)), promote(unbox(classB)))).get(name);\r\n    if (handle == null) {\r\n        throw new ClassCastException(\"Cannot apply operator [\" + name + \"] to types [\" + classA + \"] and [\" + classB + \"]\");\r\n    }\r\n    return handle;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.netty4.Netty4Utils.toBytesReference",
	"Comment": "wraps the given channelbuffer with a bytesreference of a given size",
	"Method": "BytesReference toBytesReference(ByteBuf buffer,BytesReference toBytesReference,ByteBuf buffer,int size){\r\n    return new ByteBufBytesReference(buffer, size);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.getMappingAsync",
	"Comment": "asynchronously retrieves the mappings on an index on indices using the get mapping api.see get mapping api on elastic.co",
	"Method": "void getMappingAsync(GetMappingsRequest getMappingsRequest,RequestOptions options,ActionListener<GetMappingsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(getMappingsRequest, IndicesRequestConverters::getMappings, options, GetMappingsResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.MultiNormalizerHybrid.standardizeAllInputs",
	"Comment": "apply standardization to all inputs, except the ones individually configured",
	"Method": "MultiNormalizerHybrid standardizeAllInputs(){\r\n    globalInputStrategy = new StandardizeStrategy();\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.bernoullis",
	"Comment": "this will return the bernoulli trial for the given event.a bernoulli trial is a mechanism for detecting the probabilityof a given event occurring k times in n independent trials",
	"Method": "double bernoullis(double n,double k,double successProb){\r\n    double combo = MathUtils.combination(n, k);\r\n    double p = successProb;\r\n    double q = 1 - successProb;\r\n    return combo * Math.pow(p, k) * Math.pow(q, n - k);\r\n}"
}, {
	"Path": "com.googlecode.d2j.signapk.AbstractJarSign.copyFiles",
	"Comment": "copy all the files in a manifest from input to output. we set themodification times in the output to a fixed time, so as to reducevariation in the output file and make incremental otas more efficient.",
	"Method": "void copyFiles(Manifest manifest,JarFile in,JarOutputStream out,long timestamp){\r\n    byte[] buffer = new byte[4096];\r\n    int num;\r\n    Map<String, Attributes> entries = manifest.getEntries();\r\n    List<String> names = new ArrayList(entries.keySet());\r\n    Collections.sort(names);\r\n    for (String name : names) {\r\n        JarEntry inEntry = in.getJarEntry(name);\r\n        JarEntry outEntry = null;\r\n        if (inEntry.getMethod() == JarEntry.STORED) {\r\n            outEntry = new JarEntry(inEntry);\r\n        } else {\r\n            outEntry = new JarEntry(name);\r\n        }\r\n        outEntry.setTime(timestamp);\r\n        out.putNextEntry(outEntry);\r\n        InputStream data = in.getInputStream(inEntry);\r\n        while ((num = data.read(buffer)) > 0) {\r\n            out.write(buffer, 0, num);\r\n        }\r\n        out.flush();\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.NDArrayIndex.resolve",
	"Comment": "given an all index andthe intended indexes, return anindex array containing a combination of all elementsfor slicing and overriding particular indexes where necessary",
	"Method": "INDArrayIndex[] resolve(INDArray arr,INDArrayIndex intendedIndexes,INDArrayIndex[] resolve,DataBuffer shapeInfo,INDArrayIndex intendedIndexes,INDArrayIndex[] resolve,int[] shape,INDArrayIndex intendedIndexes,INDArrayIndex[] resolve,long[] shape,INDArrayIndex intendedIndexes,INDArrayIndex[] resolve,INDArrayIndex[] allIndex,INDArrayIndex intendedIndexes){\r\n    int numNewAxes = numNewAxis(intendedIndexes);\r\n    INDArrayIndex[] all = new INDArrayIndex[allIndex.length + numNewAxes];\r\n    Arrays.fill(all, NDArrayIndex.all());\r\n    for (int i = 0; i < allIndex.length; i++) {\r\n        if (i >= intendedIndexes.length)\r\n            break;\r\n        if (intendedIndexes[i] instanceof NDArrayIndex) {\r\n            NDArrayIndex idx = (NDArrayIndex) intendedIndexes[i];\r\n            if (idx.indices.length == 1)\r\n                intendedIndexes[i] = new PointIndex(idx.indices[0]);\r\n        }\r\n        all[i] = intendedIndexes[i];\r\n    }\r\n    return all;\r\n}"
}, {
	"Path": "org.elasticsearch.client.TasksClient.cancel",
	"Comment": "cancel one or more cluster tasks using the task management api.see task management api on elastic.co",
	"Method": "CancelTasksResponse cancel(CancelTasksRequest cancelTasksRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(cancelTasksRequest, TasksRequestConverters::cancelTasks, options, CancelTasksResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.api.Augmentation.sum",
	"Comment": "sums the result of applying a function to each item of an iterable.",
	"Method": "double sum(Iterable<T> receiver,double sum,Iterable<T> receiver,ToDoubleFunction<T> function){\r\n    double sum = 0;\r\n    for (T t : receiver) {\r\n        sum += function.applyAsDouble(t);\r\n    }\r\n    return sum;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.messages.intercom.DistributedVectorMessage.processMessage",
	"Comment": "this method will be started in context of executor, either shard, client or backup node",
	"Method": "void processMessage(){\r\n    VectorAggregation aggregation = new VectorAggregation(rowIndex, (short) voidConfiguration.getNumberOfShards(), shardIndex, storage.getArray(key).getRow(rowIndex).dup());\r\n    aggregation.setOriginatorId(this.getOriginatorId());\r\n    transport.sendMessage(aggregation);\r\n}"
}, {
	"Path": "org.elasticsearch.client.IngestClient.putPipelineAsync",
	"Comment": "asynchronously add a pipeline or update an existing pipeline.see put pipeline api on elastic.co",
	"Method": "void putPipelineAsync(PutPipelineRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IngestRequestConverters::putPipeline, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.putCalendarAsync",
	"Comment": "create a new machine learning calendar, notifies listener with the created calendarfor additional infosee ml create calendar documentation",
	"Method": "void putCalendarAsync(PutCalendarRequest request,RequestOptions options,ActionListener<PutCalendarResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::putCalendar, options, PutCalendarResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.removeIndexLifecyclePolicy",
	"Comment": "remove the index lifecycle policy for an indexsee the docs for more.",
	"Method": "RemoveIndexLifecyclePolicyResponse removeIndexLifecyclePolicy(RemoveIndexLifecyclePolicyRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IndexLifecycleRequestConverters::removeIndexLifecyclePolicy, options, RemoveIndexLifecyclePolicyResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.handleRegistration",
	"Comment": "this method is called when a niochannel is being registered with the selector. it shouldonly be called once per channel.",
	"Method": "void handleRegistration(ChannelContext<?> context){\r\n    context.register();\r\n    SelectionKey selectionKey = context.getSelectionKey();\r\n    selectionKey.attach(context);\r\n    if (context instanceof SocketChannelContext) {\r\n        if (((SocketChannelContext) context).readyForFlush()) {\r\n            SelectionKeyUtils.setConnectReadAndWriteInterested(context.getSelectionKey());\r\n        } else {\r\n            SelectionKeyUtils.setConnectAndReadInterested(context.getSelectionKey());\r\n        }\r\n    } else {\r\n        assert context instanceof ServerChannelContext : \"If not SocketChannelContext the context must be a ServerChannelContext\";\r\n        SelectionKeyUtils.setAcceptInterested(context.getSelectionKey());\r\n    }\r\n}"
}, {
	"Path": "io.dropwizard.cli.CheckCommand.onError",
	"Comment": "the stacktrace is redundant as the message contains the yaml error location",
	"Method": "void onError(Cli cli,Namespace namespace,Throwable e){\r\n    cli.getStdErr().println(e.getMessage());\r\n}"
}, {
	"Path": "org.elasticsearch.client.security.PutUserRequest.updateUser",
	"Comment": "update an existing user in the native realm without modifying their password.",
	"Method": "PutUserRequest updateUser(User user,boolean enabled,RefreshPolicy refreshPolicy){\r\n    return new PutUserRequest(user, null, null, enabled, refreshPolicy);\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.process.DataCounts.getOutOfOrderTimeStampCount",
	"Comment": "the number of records with a timestamp that isbefore the time of the latest record. records shouldbe in ascending chronological order",
	"Method": "long getOutOfOrderTimeStampCount(){\r\n    return outOfOrderTimeStampCount;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.putScript",
	"Comment": "puts an stored script using the scripting api.seescripting apion elastic.co",
	"Method": "AcknowledgedResponse putScript(PutStoredScriptRequest putStoredScriptRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(putStoredScriptRequest, RequestConverters::putScript, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.cpu.nativecpu.compression.CpuThreshold.getDescriptor",
	"Comment": "this method returns compression descriptor. it should be unique for any compressor implementation",
	"Method": "String getDescriptor(){\r\n    return \"THRESHOLD\";\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.TimeDelayedParameterUpdater.status",
	"Comment": "returns the current status of this parameter serverupdater",
	"Method": "Map<String, Number> status(){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.azure.AzureStorageService.refreshAndClearCache",
	"Comment": "updates settings for building clients. any client cache is cleared. futureclient requests will use the new refreshed settings.",
	"Method": "Map<String, AzureStorageSettings> refreshAndClearCache(Map<String, AzureStorageSettings> clientsSettings){\r\n    final Map<String, AzureStorageSettings> prevSettings = this.storageSettings;\r\n    this.storageSettings = MapBuilder.newMapBuilder(clientsSettings).immutableMap();\r\n    return prevSettings;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.deleteScript",
	"Comment": "delete stored script by id.see how to use scripts on elastic.co",
	"Method": "AcknowledgedResponse deleteScript(DeleteStoredScriptRequest request,RequestOptions options){\r\n    return performRequestAndParseEntity(request, RequestConverters::deleteScript, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "io.dropwizard.validation.selfvalidating.ViolationCollector.addViolation",
	"Comment": "adds a new violation to this collector. this also sets violationoccurred to true.",
	"Method": "void addViolation(String msg){\r\n    violationOccurred = true;\r\n    context.buildConstraintViolationWithTemplate(msg).addConstraintViolation();\r\n}"
}, {
	"Path": "io.dropwizard.lifecycle.setup.ExecutorServiceBuilderTest.shouldBeAbleToExecute2TasksAtOnceWithLargeMaxThreadsOrBeWarnedOtherwise",
	"Comment": "setting large max threads without large min threads is misleading on the default queue implementationit should warn or work",
	"Method": "void shouldBeAbleToExecute2TasksAtOnceWithLargeMaxThreadsOrBeWarnedOtherwise(){\r\n    ExecutorService exe = executorServiceBuilder.maxThreads(Integer.MAX_VALUE).build();\r\n    try {\r\n        verify(log).warn(anyString());\r\n    } catch (WantedButNotInvoked error) {\r\n        assertCanExecuteAtLeast2ConcurrentTasks(exe);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.script.mustache.SearchTemplateRequestBuilder.setExplain",
	"Comment": "enables explanation for each hit on how its score was computed. disabled by default",
	"Method": "SearchTemplateRequestBuilder setExplain(boolean explain){\r\n    request.setExplain(explain);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.logic.completion.Clipboard.hasCandidates",
	"Comment": "this method checks, if clipboard has ready aggregations available",
	"Method": "boolean hasCandidates(){\r\n    return completedCounter.get() > 0;\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.BaseNDArrayFactory.order",
	"Comment": "returns the order for this ndarray for internal data storage",
	"Method": "char order(){\r\n    return order;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.NDArrayMath.numVectors",
	"Comment": "return the number of vectors for an arraythe number of vectors for an array",
	"Method": "long numVectors(INDArray arr){\r\n    if (arr.rank() == 1)\r\n        return 1;\r\n    else if (arr.rank() == 2)\r\n        return arr.size(0);\r\n    else {\r\n        int prod = 1;\r\n        for (int i = 0; i < arr.rank() - 1; i++) {\r\n            prod *= arr.size(i);\r\n        }\r\n        return prod;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.EvilSecurityTests.assertExactPermissions",
	"Comment": "checks exact file permissions, meaning those and only those for that path.",
	"Method": "void assertExactPermissions(FilePermission expected,PermissionCollection actual){\r\n    String target = expected.getName();\r\n    Set<String> permissionSet = asSet(expected.getActions().split(\",\"));\r\n    boolean read = permissionSet.remove(\"read\");\r\n    boolean readlink = permissionSet.remove(\"readlink\");\r\n    boolean write = permissionSet.remove(\"write\");\r\n    boolean delete = permissionSet.remove(\"delete\");\r\n    boolean execute = permissionSet.remove(\"execute\");\r\n    assertTrue(\"unrecognized permission: \" + permissionSet, permissionSet.isEmpty());\r\n    assertEquals(read, actual.implies(new FilePermission(target, \"read\")));\r\n    assertEquals(readlink, actual.implies(new FilePermission(target, \"readlink\")));\r\n    assertEquals(write, actual.implies(new FilePermission(target, \"write\")));\r\n    assertEquals(delete, actual.implies(new FilePermission(target, \"delete\")));\r\n    assertEquals(execute, actual.implies(new FilePermission(target, \"execute\")));\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.serializer.NormalizerSerializer.strategySupportsNormalizer",
	"Comment": "check if a serializer strategy supports a normalizer. if the normalizer is a custom optype, it checks if thesupported normalizer class matches.",
	"Method": "boolean strategySupportsNormalizer(NormalizerSerializerStrategy strategy,NormalizerType normalizerType,Class<? extends Normalizer> normalizerClass){\r\n    if (!strategy.getSupportedType().equals(normalizerType)) {\r\n        return false;\r\n    }\r\n    if (strategy.getSupportedType().equals(NormalizerType.CUSTOM)) {\r\n        if (!(strategy instanceof CustomSerializerStrategy)) {\r\n            throw new IllegalArgumentException(\"Strategies supporting CUSTOM opType must be instance of CustomSerializerStrategy, got\" + strategy.getClass());\r\n        }\r\n        return ((CustomSerializerStrategy) strategy).getSupportedClass().equals(normalizerClass);\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.flushJobAsync",
	"Comment": "flushes internally buffered data for the given machine learning job asynchronously ensuring all data sent to the has been processed.this may cause new results to be calculated depending on the contents of the bufferboth flush and close operations are similar,however the flush is more efficient if you are expecting to send more data for analysis.when flushing, the job remains open and is available to continue analyzing data.a close operation additionally prunes and persists the model state to disk and thejob must be opened again before analyzing further data.for additional infosee flush ml job documentation",
	"Method": "void flushJobAsync(FlushJobRequest request,RequestOptions options,ActionListener<FlushJobResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::flushJob, options, FlushJobResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.NioChannel.close",
	"Comment": "schedules channel for close. this process is asynchronous.",
	"Method": "void close(){\r\n    getContext().closeChannel();\r\n}"
}, {
	"Path": "io.dropwizard.jersey.validation.ConstraintMessage.getMethodReturnValueName",
	"Comment": "gets the method return value name, if the violation is raised in it",
	"Method": "Optional<String> getMethodReturnValueName(ConstraintViolation<?> violation){\r\n    int returnValueNames = -1;\r\n    final StringBuilder result = new StringBuilder(\"server response\");\r\n    for (Path.Node node : violation.getPropertyPath()) {\r\n        if (node.getKind().equals(ElementKind.RETURN_VALUE)) {\r\n            returnValueNames = 0;\r\n        } else if (returnValueNames >= 0) {\r\n            result.append(returnValueNames++ == 0 ? \" \" : \".\").append(node);\r\n        }\r\n    }\r\n    return returnValueNames >= 0 ? Optional.of(result.toString()) : Optional.empty();\r\n}"
}, {
	"Path": "org.nd4j.context.Nd4jContext.updateProperties",
	"Comment": "load the additional properties from an input stream and load all system properties",
	"Method": "void updateProperties(InputStream inputStream){\r\n    try {\r\n        conf.load(inputStream);\r\n        conf.putAll(System.getProperties());\r\n    } catch (IOException e) {\r\n        log.warn(\"Error loading system properties from input stream\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.plugin.noop.action.search.NoopSearchRequestBuilder.setTimeout",
	"Comment": "an optional timeout to control how long search is allowed to take.",
	"Method": "NoopSearchRequestBuilder setTimeout(TimeValue timeout){\r\n    sourceBuilder().timeout(timeout);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.linalg.checkutil.NDArrayCreationUtil.broadcastToShape",
	"Comment": "generate a random shape tobroadcast togiven a randomly generatedshape with 1s in it as inputs",
	"Method": "int[] broadcastToShape(int[] inputShapeWithOnes,long seed,long[] broadcastToShape,long[] inputShapeWithOnes,long seed){\r\n    Nd4j.getRandom().setSeed(seed);\r\n    val shape = new long[inputShapeWithOnes.length];\r\n    for (int i = 0; i < shape.length; i++) {\r\n        if (inputShapeWithOnes[i] == 1) {\r\n            shape[i] = Nd4j.getRandom().nextInt(9) + 1;\r\n        } else\r\n            shape[i] = inputShapeWithOnes[i];\r\n    }\r\n    return shape;\r\n}"
}, {
	"Path": "org.elasticsearch.tools.launchers.JvmOptionsParser.main",
	"Comment": "the main entry point. the exit code is 0 if the jvm options were successfully parsed, otherwise the exit code is 1. if an improperlyformatted line is discovered, the line is output to standard error.",
	"Method": "void main(String[] args){\r\n    if (args.length != 1) {\r\n        throw new IllegalArgumentException(\"expected one argument specifying path to jvm.options but was \" + Arrays.toString(args));\r\n    }\r\n    final List<String> jvmOptions = new ArrayList();\r\n    final SortedMap<Integer, String> invalidLines = new TreeMap();\r\n    try (InputStream is = Files.newInputStream(Paths.get(args[0]));\r\n        Reader reader = new InputStreamReader(is, Charset.forName(\"UTF-8\"));\r\n        BufferedReader br = new BufferedReader(reader)) {\r\n        parse(JavaVersion.majorVersion(JavaVersion.CURRENT), br, new JvmOptionConsumer() {\r\n            @Override\r\n            public void accept(final String jvmOption) {\r\n                jvmOptions.add(jvmOption);\r\n            }\r\n        }, new InvalidLineConsumer() {\r\n            @Override\r\n            public void accept(final int lineNumber, final String line) {\r\n                invalidLines.put(lineNumber, line);\r\n            }\r\n        });\r\n    }\r\n    if (invalidLines.isEmpty()) {\r\n        List<String> ergonomicJvmOptions = JvmErgonomics.choose(jvmOptions);\r\n        jvmOptions.addAll(ergonomicJvmOptions);\r\n        final String spaceDelimitedJvmOptions = spaceDelimitJvmOptions(jvmOptions);\r\n        Launchers.outPrintln(spaceDelimitedJvmOptions);\r\n        Launchers.exit(0);\r\n    } else {\r\n        final String errorMessage = String.format(Locale.ROOT, \"encountered [%d] error%s parsing [%s]\", invalidLines.size(), invalidLines.size() == 1 ? \"\" : \"s\", args[0]);\r\n        Launchers.errPrintln(errorMessage);\r\n        int count = 0;\r\n        for (final Map.Entry<Integer, String> entry : invalidLines.entrySet()) {\r\n            count++;\r\n            final String message = String.format(Locale.ROOT, \"[%d]: encountered improperly formatted JVM option line [%s] on line number [%d]\", count, entry.getValue(), entry.getKey());\r\n            Launchers.errPrintln(message);\r\n        }\r\n        Launchers.exit(1);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.tools.launchers.JvmOptionsParser.main",
	"Comment": "the main entry point. the exit code is 0 if the jvm options were successfully parsed, otherwise the exit code is 1. if an improperlyformatted line is discovered, the line is output to standard error.",
	"Method": "void main(String[] args){\r\n    jvmOptions.add(jvmOption);\r\n}"
}, {
	"Path": "org.elasticsearch.tools.launchers.JvmOptionsParser.main",
	"Comment": "the main entry point. the exit code is 0 if the jvm options were successfully parsed, otherwise the exit code is 1. if an improperlyformatted line is discovered, the line is output to standard error.",
	"Method": "void main(String[] args){\r\n    invalidLines.put(lineNumber, line);\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.SoftSyncParameterUpdater.shouldReplicate",
	"Comment": "returns true ifthe updater has accumulated enough ndarrays toreplicate to the workers",
	"Method": "boolean shouldReplicate(){\r\n    return accumulatedUpdates == s;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.Locals.buildLocalMethodKey",
	"Comment": "constructs a local method key used to lookup local methods from a painless class.",
	"Method": "String buildLocalMethodKey(String methodName,int methodArity){\r\n    return methodName + \"/\" + methodArity;\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.s3.S3BlobContainer.writeBlob",
	"Comment": "this implementation ignores the failifalreadyexists flag as the s3 api has no way to enforce this due to its weak consistency model.",
	"Method": "void writeBlob(String blobName,InputStream inputStream,long blobSize,boolean failIfAlreadyExists){\r\n    SocketAccess.doPrivilegedIOException(() -> {\r\n        if (blobSize <= blobStore.bufferSizeInBytes()) {\r\n            executeSingleUpload(blobStore, buildKey(blobName), inputStream, blobSize);\r\n        } else {\r\n            executeMultipartUpload(blobStore, buildKey(blobName), inputStream, blobSize);\r\n        }\r\n        return null;\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestClientSingleHostTests.testHeaders",
	"Comment": "end to end test for request and response headers. exercises the mock http client ability to send backwhatever headers it has received.",
	"Method": "void testHeaders(){\r\n    for (String method : getHttpMethods()) {\r\n        final Header[] requestHeaders = RestClientTestUtil.randomHeaders(getRandom(), \"Header\");\r\n        final int statusCode = randomStatusCode(getRandom());\r\n        Request request = new Request(method, \"/\" + statusCode);\r\n        RequestOptions.Builder options = request.getOptions().toBuilder();\r\n        for (Header requestHeader : requestHeaders) {\r\n            options.addHeader(requestHeader.getName(), requestHeader.getValue());\r\n        }\r\n        request.setOptions(options);\r\n        Response esResponse;\r\n        try {\r\n            esResponse = restClient.performRequest(request);\r\n        } catch (ResponseException e) {\r\n            esResponse = e.getResponse();\r\n        }\r\n        assertThat(esResponse.getStatusLine().getStatusCode(), equalTo(statusCode));\r\n        assertHeaders(defaultHeaders, requestHeaders, esResponse.getHeaders(), Collections.<String>emptySet());\r\n        assertFalse(esResponse.hasWarnings());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.delete",
	"Comment": "deletes a document by id using the delete api.see delete api on elastic.co",
	"Method": "DeleteResponse delete(DeleteRequest deleteRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(deleteRequest, RequestConverters::delete, options, DeleteResponse::fromXContent, singleton(404));\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT.testSecondaryStrength",
	"Comment": "test secondary strength, for english case is not significant.",
	"Method": "void testSecondaryStrength(){\r\n    String index = \"foo\";\r\n    String type = \"mytype\";\r\n    String[] equivalent = { \"TESTING\", \"testing\" };\r\n    XContentBuilder builder = jsonBuilder().startObject().startObject(\"properties\").startObject(\"collate\").field(\"type\", \"icu_collation_keyword\").field(\"language\", \"en\").field(\"strength\", \"secondary\").field(\"decomposition\", \"no\").endObject().endObject().endObject();\r\n    assertAcked(client().admin().indices().prepareCreate(index).addMapping(type, builder));\r\n    indexRandom(true, client().prepareIndex(index, type, \"1\").setSource(\"{\\\"collate\\\":\\\"\" + equivalent[0] + \"\\\"}\", XContentType.JSON), client().prepareIndex(index, type, \"2\").setSource(\"{\\\"collate\\\":\\\"\" + equivalent[1] + \"\\\"}\", XContentType.JSON));\r\n    SearchRequest request = new SearchRequest().indices(index).types(type).source(// secondary sort should kick in because both will collate to same value\r\n    new SearchSourceBuilder().fetchSource(false).query(QueryBuilders.termQuery(\"collate\", randomBoolean() ? equivalent[0] : equivalent[1])).sort(\"collate\").sort(\"_id\", SortOrder.DESC));\r\n    SearchResponse response = client().search(request).actionGet();\r\n    assertNoFailures(response);\r\n    assertHitCount(response, 2L);\r\n    assertOrderedSearchHits(response, \"2\", \"1\");\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.ModelParameterServer.addUpdatesSubscriber",
	"Comment": "this method adds subcriber that will be called upon gradients update receival",
	"Method": "void addUpdatesSubscriber(UpdatesHandler s){\r\n    updatesSubscribers.add(s);\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImpl.refreshAndClearCache",
	"Comment": "refreshes the settings for the amazonec2 client. the new client will be buildusing these new settings. the old client is usable until released. on release itwill be destroyed instead of being returned to the cache.",
	"Method": "void refreshAndClearCache(Ec2ClientSettings clientSettings){\r\n    final LazyInitializable<AmazonEc2Reference, ElasticsearchException> newClient = new LazyInitializable(() -> new AmazonEc2Reference(buildClient(clientSettings)), clientReference -> clientReference.incRef(), clientReference -> clientReference.decRef());\r\n    final LazyInitializable<AmazonEc2Reference, ElasticsearchException> oldClient = this.lazyClientReference.getAndSet(newClient);\r\n    if (oldClient != null) {\r\n        oldClient.reset();\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.aeron.ipc.NDArrayFragmentHandler.onFragment",
	"Comment": "callback for handlingfragments of data being read from a log.",
	"Method": "void onFragment(DirectBuffer buffer,int offset,int length,Header header){\r\n    ByteBuffer byteBuffer = buffer.byteBuffer();\r\n    boolean byteArrayInput = false;\r\n    if (byteBuffer == null) {\r\n        byteArrayInput = true;\r\n        byte[] destination = new byte[length];\r\n        ByteBuffer wrap = ByteBuffer.wrap(buffer.byteArray());\r\n        wrap.get(destination, offset, length);\r\n        byteBuffer = ByteBuffer.wrap(destination).order(ByteOrder.nativeOrder());\r\n    }\r\n    if (!byteArrayInput) {\r\n        byteBuffer.position(offset);\r\n        byteBuffer.order(ByteOrder.nativeOrder());\r\n    }\r\n    int messageTypeIndex = byteBuffer.getInt();\r\n    if (messageTypeIndex >= NDArrayMessage.MessageType.values().length)\r\n        throw new IllegalStateException(\"Illegal index on message opType. Likely corrupt message. Please check the serialization of the bytebuffer. Input was bytebuffer: \" + byteArrayInput);\r\n    NDArrayMessage.MessageType messageType = NDArrayMessage.MessageType.values()[messageTypeIndex];\r\n    if (messageType == NDArrayMessage.MessageType.CHUNKED) {\r\n        NDArrayMessageChunk chunk = NDArrayMessageChunk.fromBuffer(byteBuffer, messageType);\r\n        if (chunk.getNumChunks() < 1)\r\n            throw new IllegalStateException(\"Found invalid number of chunks \" + chunk.getNumChunks() + \" on chunk index \" + chunk.getChunkIndex());\r\n        chunkAccumulator.accumulateChunk(chunk);\r\n        log.info(\"Number of chunks \" + chunk.getNumChunks() + \" and number of chunks \" + chunk.getNumChunks() + \" for id \" + chunk.getId() + \" is \" + chunkAccumulator.numChunksSoFar(chunk.getId()));\r\n        if (chunkAccumulator.allPresent(chunk.getId())) {\r\n            NDArrayMessage message = chunkAccumulator.reassemble(chunk.getId());\r\n            ndArrayCallback.onNDArrayMessage(message);\r\n        }\r\n    } else {\r\n        NDArrayMessage message = NDArrayMessage.fromBuffer(buffer, offset);\r\n        ndArrayCallback.onNDArrayMessage(message);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.getFieldMapping",
	"Comment": "retrieves the field mappings on an index or indices using the get field mapping api.see get field mapping api on elastic.co",
	"Method": "GetFieldMappingsResponse getFieldMapping(GetFieldMappingsRequest getFieldMappingsRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(getFieldMappingsRequest, IndicesRequestConverters::getFieldMapping, options, GetFieldMappingsResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getDatafeed",
	"Comment": "gets one or more machine learning datafeed configuration info.for additional infosee ml get datafeed documentation",
	"Method": "GetDatafeedResponse getDatafeed(GetDatafeedRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::getDatafeed, options, GetDatafeedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.RandomFactory.getNewRandomInstance",
	"Comment": "this method returns new onject implementing random interface, initialized with seed value, with size of elements in buffer",
	"Method": "Random getNewRandomInstance(Random getNewRandomInstance,long seed,Random getNewRandomInstance,long seed,long size){\r\n    try {\r\n        Class<?> c = randomClass;\r\n        Constructor<?> constructor = c.getConstructor(long.class, long.class);\r\n        Random t = (Random) constructor.newInstance(seed, size);\r\n        return t;\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4jBackend.loadLibrary",
	"Comment": "adds the supplied java archive library to java.class.path. this is benignif the library is already loaded.",
	"Method": "void loadLibrary(File jar){\r\n    try {\r\n        java.net.URLClassLoader loader = (java.net.URLClassLoader) ClassLoader.getSystemClassLoader();\r\n        java.net.URL url = jar.toURI().toURL();\r\n        for (java.net.URL it : java.util.Arrays.asList(loader.getURLs())) {\r\n            if (it.equals(url)) {\r\n                return;\r\n            }\r\n        }\r\n        java.lang.reflect.Method method = java.net.URLClassLoader.class.getDeclaredMethod(\"addURL\", new Class[] { java.net.URL.class });\r\n        method.setAccessible(true);\r\n        method.invoke(loader, new Object[] { url });\r\n    } catch (final java.lang.NoSuchMethodException | java.lang.IllegalAccessException | java.net.MalformedURLException | java.lang.reflect.InvocationTargetException e) {\r\n        throw new NoAvailableBackendException(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.exists",
	"Comment": "checks for the existence of a document. returns true if it exists, false otherwise.see get api on elastic.co",
	"Method": "boolean exists(GetRequest getRequest,RequestOptions options){\r\n    return performRequest(getRequest, RequestConverters::exists, options, RestHighLevelClient::convertExistsResponse, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.sizeOfDataType",
	"Comment": "this method returns size of element for specified datatype, in bytes",
	"Method": "int sizeOfDataType(int sizeOfDataType,DataBuffer.Type dtype){\r\n    switch(dtype) {\r\n        case FLOAT:\r\n            return 4;\r\n        case HALF:\r\n            return 2;\r\n        case INT:\r\n            return 4;\r\n        default:\r\n        case DOUBLE:\r\n            return 8;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getJobAsync",
	"Comment": "gets one or more machine learning job configuration info, asynchronously.for additional infosee ml get job documentation",
	"Method": "void getJobAsync(GetJobRequest request,RequestOptions options,ActionListener<GetJobResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getJob, options, GetJobResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.BooleanIndexing.and",
	"Comment": "and over the whole ndarray given some condition, with respect to dimensions",
	"Method": "boolean and(INDArray n,Condition cond,boolean[] and,INDArray n,Condition condition,int dimension){\r\n    if (!(condition instanceof BaseCondition))\r\n        throw new UnsupportedOperationException(\"Only static Conditions are supported\");\r\n    MatchCondition op = new MatchCondition(n, condition);\r\n    INDArray arr = Nd4j.getExecutioner().exec(op, dimension);\r\n    boolean[] result = new boolean[(int) arr.length()];\r\n    long tadLength = Shape.getTADLength(n.shape(), dimension);\r\n    for (int i = 0; i < arr.length(); i++) {\r\n        if (arr.getDouble(i) == tadLength)\r\n            result[i] = true;\r\n        else\r\n            result[i] = false;\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.nio.BytesChannelContext.singleFlush",
	"Comment": "returns a boolean indicating if the operation was fully flushed.",
	"Method": "boolean singleFlush(FlushOperation flushOperation){\r\n    int written = flushToChannel(flushOperation.getBuffersToWrite());\r\n    flushOperation.incrementIndex(written);\r\n    return flushOperation.isFullyFlushed();\r\n}"
}, {
	"Path": "org.elasticsearch.client.RollupClient.putRollupJobAsync",
	"Comment": "asynchronously put a rollup job into the clustersee the docs for more.",
	"Method": "void putRollupJobAsync(PutRollupJobRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, RollupRequestConverters::putJob, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.datafeed.DatafeedUpdate.equals",
	"Comment": "the lists of indices and types are compared for equality but they are notsorted first so this test could fail simply because the indices and typeslists are in different orders.also note this could be a heavy operation when a query or aggregationsare set as we need to convert the bytes references into maps to correctlycompare them.",
	"Method": "boolean equals(Object other){\r\n    if (this == other) {\r\n        return true;\r\n    }\r\n    if (other == null || getClass() != other.getClass()) {\r\n        return false;\r\n    }\r\n    DatafeedUpdate that = (DatafeedUpdate) other;\r\n    return Objects.equals(this.id, that.id) && Objects.equals(this.jobId, that.jobId) && Objects.equals(this.frequency, that.frequency) && Objects.equals(this.queryDelay, that.queryDelay) && Objects.equals(this.indices, that.indices) && Objects.equals(this.types, that.types) && Objects.equals(asMap(this.query), asMap(that.query)) && Objects.equals(this.scrollSize, that.scrollSize) && Objects.equals(asMap(this.aggregations), asMap(that.aggregations)) && Objects.equals(this.delayedDataCheckConfig, that.delayedDataCheckConfig) && Objects.equals(this.scriptFields, that.scriptFields) && Objects.equals(this.chunkingConfig, that.chunkingConfig);\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettingsTests.randomCredential",
	"Comment": "generates a random googlecredential along with its corresponding service account file provided as a byte array",
	"Method": "Tuple<ServiceAccountCredentials, byte[]> randomCredential(String clientName){\r\n    final KeyPair keyPair = KeyPairGenerator.getInstance(\"RSA\").generateKeyPair();\r\n    final ServiceAccountCredentials.Builder credentialBuilder = ServiceAccountCredentials.newBuilder();\r\n    credentialBuilder.setClientId(\"id_\" + clientName);\r\n    credentialBuilder.setClientEmail(clientName);\r\n    credentialBuilder.setProjectId(\"project_id_\" + clientName);\r\n    credentialBuilder.setPrivateKey(keyPair.getPrivate());\r\n    credentialBuilder.setPrivateKeyId(\"private_key_id_\" + clientName);\r\n    credentialBuilder.setScopes(Collections.singleton(StorageScopes.DEVSTORAGE_FULL_CONTROL));\r\n    final String encodedPrivateKey = Base64.getEncoder().encodeToString(keyPair.getPrivate().getEncoded());\r\n    final String serviceAccount = \"{\\\"type\\\":\\\"service_account\\\",\" + \"\\\"project_id\\\":\\\"project_id_\" + clientName + \"\\\",\" + \"\\\"private_key_id\\\":\\\"private_key_id_\" + clientName + \"\\\",\" + \"\\\"private_key\\\":\\\"-----BEGIN PRIVATE KEY-----\\\\n\" + encodedPrivateKey + \"\\\\n-----END PRIVATE KEY-----\\\\n\\\",\" + \"\\\"client_email\\\":\\\"\" + clientName + \"\\\",\" + \"\\\"client_id\\\":\\\"id_\" + clientName + \"\\\"\" + \"}\";\r\n    return Tuple.tuple(credentialBuilder.build(), serviceAccount.getBytes(StandardCharsets.UTF_8));\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.buffer.BaseDataBuffer.isConstant",
	"Comment": "this method returns whether this databuffer is constant, or not.constant buffer means that it modified only during creation time, and then it stays the same for all lifecycle. i.e. used in shape info databuffers.",
	"Method": "boolean isConstant(){\r\n    return constant;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.getTemplate",
	"Comment": "gets index templates using the index templates apiseeindex templates apion elastic.co",
	"Method": "GetIndexTemplatesResponse getTemplate(GetIndexTemplatesRequest getIndexTemplatesRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(getIndexTemplatesRequest, IndicesRequestConverters::getTemplates, options, GetIndexTemplatesResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.api.Augmentation.groupBy",
	"Comment": "sorts all map members into groups determined by the supplied mapping function.",
	"Method": "Map<U, List<T>> groupBy(Iterable<T> receiver,Function<T, U> mapper,Map<T, Map<K, V>> groupBy,Map<K, V> receiver,BiFunction<K, V, T> mapper){\r\n    Map<T, Map<K, V>> map = new LinkedHashMap();\r\n    for (Map.Entry<K, V> kvPair : receiver.entrySet()) {\r\n        T mapped = mapper.apply(kvPair.getKey(), kvPair.getValue());\r\n        Map<K, V> results = map.get(mapped);\r\n        if (results == null) {\r\n            if (receiver instanceof TreeMap) {\r\n                results = new TreeMap();\r\n            } else {\r\n                results = new LinkedHashMap();\r\n            }\r\n            map.put(mapped, results);\r\n        }\r\n        results.put(kvPair.getKey(), kvPair.getValue());\r\n    }\r\n    return map;\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.getEnsuredShape",
	"Comment": "get ensured shapes that wind up being scalar end up with the write shape",
	"Method": "int[] getEnsuredShape(int[] shape,long[] getEnsuredShape,long[] shape,int[] getEnsuredShape,int rows,int columns){\r\n    return getEnsuredShape(new int[] { rows, columns });\r\n}"
}, {
	"Path": "org.elasticsearch.client.CcrClient.pauseFollow",
	"Comment": "instructs a follower index to pause the following of a leader index.see the docs for more.",
	"Method": "AcknowledgedResponse pauseFollow(PauseFollowRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, CcrRequestConverters::pauseFollow, options, AcknowledgedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.index.rankeval.RankEvalRequestIT.testIndicesOptions",
	"Comment": "test that multiple indices work, setting indices options is possible and works as expected",
	"Method": "void testIndicesOptions(){\r\n    SearchSourceBuilder amsterdamQuery = new SearchSourceBuilder().query(new MatchAllQueryBuilder());\r\n    List<RatedDocument> relevantDocs = createRelevant(\"2\", \"3\", \"4\", \"5\", \"6\");\r\n    relevantDocs.add(new RatedDocument(\"test2\", \"7\", RELEVANT_RATING_1));\r\n    List<RatedRequest> specifications = new ArrayList();\r\n    specifications.add(new RatedRequest(\"amsterdam_query\", relevantDocs, amsterdamQuery));\r\n    RankEvalSpec task = new RankEvalSpec(specifications, new PrecisionAtK());\r\n    RankEvalRequest request = new RankEvalRequest(task, new String[] { TEST_INDEX, \"test2\" });\r\n    request.setRankEvalSpec(task);\r\n    RankEvalResponse response = client().execute(RankEvalAction.INSTANCE, request).actionGet();\r\n    Detail details = (PrecisionAtK.Detail) response.getPartialResults().get(\"amsterdam_query\").getMetricDetails();\r\n    assertEquals(7, details.getRetrieved());\r\n    assertEquals(6, details.getRelevantRetrieved());\r\n    assertTrue(client().admin().indices().prepareClose(\"test2\").get().isAcknowledged());\r\n    request.indicesOptions(IndicesOptions.fromParameters(null, \"true\", null, \"false\", SearchRequest.DEFAULT_INDICES_OPTIONS));\r\n    response = client().execute(RankEvalAction.INSTANCE, request).actionGet();\r\n    details = (PrecisionAtK.Detail) response.getPartialResults().get(\"amsterdam_query\").getMetricDetails();\r\n    assertEquals(6, details.getRetrieved());\r\n    assertEquals(5, details.getRelevantRetrieved());\r\n    assertTrue(client().admin().indices().prepareClose(\"test2\").get().isAcknowledged());\r\n    request.indicesOptions(IndicesOptions.fromParameters(null, \"false\", null, \"false\", SearchRequest.DEFAULT_INDICES_OPTIONS));\r\n    response = client().execute(RankEvalAction.INSTANCE, request).actionGet();\r\n    assertEquals(1, response.getFailures().size());\r\n    assertThat(response.getFailures().get(\"amsterdam_query\"), instanceOf(IndexClosedException.class));\r\n    request = new RankEvalRequest(task, new String[] { \"tes*\" });\r\n    request.indicesOptions(IndicesOptions.fromParameters(\"none\", null, null, \"false\", SearchRequest.DEFAULT_INDICES_OPTIONS));\r\n    response = client().execute(RankEvalAction.INSTANCE, request).actionGet();\r\n    details = (PrecisionAtK.Detail) response.getPartialResults().get(\"amsterdam_query\").getMetricDetails();\r\n    assertEquals(0, details.getRetrieved());\r\n    request.indicesOptions(IndicesOptions.fromParameters(\"open\", null, null, \"false\", SearchRequest.DEFAULT_INDICES_OPTIONS));\r\n    response = client().execute(RankEvalAction.INSTANCE, request).actionGet();\r\n    details = (PrecisionAtK.Detail) response.getPartialResults().get(\"amsterdam_query\").getMetricDetails();\r\n    assertEquals(6, details.getRetrieved());\r\n    assertEquals(5, details.getRelevantRetrieved());\r\n    request.indicesOptions(IndicesOptions.fromParameters(\"closed\", null, null, \"false\", SearchRequest.DEFAULT_INDICES_OPTIONS));\r\n    response = client().execute(RankEvalAction.INSTANCE, request).actionGet();\r\n    assertEquals(1, response.getFailures().size());\r\n    assertThat(response.getFailures().get(\"amsterdam_query\"), instanceOf(IndexClosedException.class));\r\n    request = new RankEvalRequest(task, new String[] { \"bad*\" });\r\n    request.indicesOptions(IndicesOptions.fromParameters(null, null, \"true\", \"false\", SearchRequest.DEFAULT_INDICES_OPTIONS));\r\n    response = client().execute(RankEvalAction.INSTANCE, request).actionGet();\r\n    details = (PrecisionAtK.Detail) response.getPartialResults().get(\"amsterdam_query\").getMetricDetails();\r\n    assertEquals(0, details.getRetrieved());\r\n    request.indicesOptions(IndicesOptions.fromParameters(null, null, \"false\", \"false\", SearchRequest.DEFAULT_INDICES_OPTIONS));\r\n    response = client().execute(RankEvalAction.INSTANCE, request).actionGet();\r\n    assertEquals(1, response.getFailures().size());\r\n    assertThat(response.getFailures().get(\"amsterdam_query\"), instanceOf(IndexNotFoundException.class));\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.BaseNDArrayFactory.pullRows",
	"Comment": "this method produces concatenated array, that consist from tensors, fetched from source array, against some dimension and specified indexes",
	"Method": "INDArray pullRows(INDArray source,int sourceDimension,int[] indexes,char order,INDArray pullRows,INDArray source,int sourceDimension,int[] indexes){\r\n    return pullRows(source, sourceDimension, indexes, Nd4j.order());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.writeException",
	"Comment": "this method is called when an attempt to write to a channel throws an exception.",
	"Method": "void writeException(SocketChannelContext context,Exception exception){\r\n    context.handleException(exception);\r\n}"
}, {
	"Path": "org.elasticsearch.plugin.noop.action.search.NoopSearchRequestBuilder.addScriptField",
	"Comment": "adds a script based field to load and return. the field does not have to be stored,but its recommended to use non analyzed or numeric fields.",
	"Method": "NoopSearchRequestBuilder addScriptField(String name,Script script){\r\n    sourceBuilder().scriptField(name, script);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.performRequestAsyncAndParseOptionalEntity",
	"Comment": "async request which returns empty optionals in the case of 404s or parses entity into an optional",
	"Method": "void performRequestAsyncAndParseOptionalEntity(Req request,CheckedFunction<Req, Request, IOException> requestConverter,RequestOptions options,CheckedFunction<XContentParser, Resp, IOException> entityParser,ActionListener<Optional<Resp>> listener){\r\n    Optional<ValidationException> validationException = request.validate();\r\n    if (validationException != null && validationException.isPresent()) {\r\n        listener.onFailure(validationException.get());\r\n        return;\r\n    }\r\n    Request req;\r\n    try {\r\n        req = requestConverter.apply(request);\r\n    } catch (Exception e) {\r\n        listener.onFailure(e);\r\n        return;\r\n    }\r\n    req.setOptions(options);\r\n    ResponseListener responseListener = wrapResponseListener404sOptional(response -> parseEntity(response.getEntity(), entityParser), listener);\r\n    client.performRequestAsync(req, responseListener);\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.NDArrayMath.mapIndexOntoVector",
	"Comment": "this maps an index of a vectoron to a vector in the matrix that can be usedfor indexing in to a tensor",
	"Method": "long mapIndexOntoVector(int index,INDArray arr){\r\n    long ret = index * arr.size(-1);\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.messages.aggregations.DotAggregation.processMessage",
	"Comment": "this method will be started in context of executor, either shard, client or backup node",
	"Method": "void processMessage(){\r\n    if (chunks == null) {\r\n        chunks = new TreeMap();\r\n        chunksCounter = new AtomicInteger(1);\r\n        addToChunks(payload);\r\n    }\r\n    clipboard.pin(this);\r\n    if (clipboard.isReady(this.getOriginatorId(), this.getTaskId())) {\r\n        trainer.aggregationFinished(clipboard.unpin(this.getOriginatorId(), this.taskId));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.plugin.noop.action.search.NoopSearchRequestBuilder.addDocValueField",
	"Comment": "adds a docvalue based field to load and return. the field does not have to be stored,but its recommended to use non analyzed or numeric fields.",
	"Method": "NoopSearchRequestBuilder addDocValueField(String name){\r\n    sourceBuilder().docValueField(name);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.ArrayUtil.nonOneStride",
	"Comment": "for use with row vectors to ensure consistent strideswith varying offsets",
	"Method": "int nonOneStride(int[] arr){\r\n    for (int i = 0; i < arr.length; i++) if (arr[i] != 1)\r\n        return arr[i];\r\n    return 1;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.flushAsync",
	"Comment": "asynchronously flush one or more indices using the flush api.seeflush api on elastic.co",
	"Method": "void flushAsync(FlushRequest flushRequest,RequestOptions options,ActionListener<FlushResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(flushRequest, IndicesRequestConverters::flush, options, FlushResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.allocation.ClusterAllocationExplainRequest.setCurrentNode",
	"Comment": "requests the explain api to explain an already assigned replica shard currently allocated tothe given node.",
	"Method": "ClusterAllocationExplainRequest setCurrentNode(String currentNodeId){\r\n    this.currentNode = currentNodeId;\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.buffer.BaseDataBuffer.originalOffset",
	"Comment": "returns the offset of the buffer relative to originaldatabuffer",
	"Method": "long originalOffset(){\r\n    return originalOffset;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.lifecycleManagementStatusAsync",
	"Comment": "asynchronously get the status of index lifecycle managementsee the docs for more.",
	"Method": "void lifecycleManagementStatusAsync(LifecycleManagementStatusRequest request,RequestOptions options,ActionListener<LifecycleManagementStatusResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IndexLifecycleRequestConverters::lifecycleManagementStatus, options, LifecycleManagementStatusResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.InstallPluginCommand.setFileAttributes",
	"Comment": "sets the attributes for a path iff posix attributes are supported",
	"Method": "void setFileAttributes(Path path,Set<PosixFilePermission> permissions){\r\n    PosixFileAttributeView fileAttributeView = Files.getFileAttributeView(path, PosixFileAttributeView.class);\r\n    if (fileAttributeView != null) {\r\n        Files.setPosixFilePermissions(path, permissions);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.watcher.WatcherStatsResponse.getHeader",
	"Comment": "gets information about the number of total, successful and failed nodes the request was run on.also includes exceptions if relevant.",
	"Method": "NodesResponseHeader getHeader(){\r\n    return header;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.getAsync",
	"Comment": "asynchronously retrieves a document by id using the get api.see get api on elastic.co",
	"Method": "void getAsync(GetRequest getRequest,RequestOptions options,ActionListener<GetResponse> listener){\r\n    performRequestAsyncAndParseEntity(getRequest, RequestConverters::get, options, GetResponse::fromXContent, listener, singleton(404));\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.putLifecyclePolicy",
	"Comment": "create or modify a lifecycle definition. see the docs for more.",
	"Method": "AcknowledgedResponse putLifecyclePolicy(PutLifecyclePolicyRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IndexLifecycleRequestConverters::putLifecyclePolicy, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.util.StringUtils.byteToHexString",
	"Comment": "given an array of bytes it will convert the bytes to a hex stringrepresentation of the bytes",
	"Method": "String byteToHexString(byte[] bytes,int start,int end,String byteToHexString,byte bytes,String byteToHexString,byte b){\r\n    return byteToHexString(new byte[] { b });\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteCalendar",
	"Comment": "deletes the given machine learning calendarfor additional info seeml delete calendar documentation",
	"Method": "AcknowledgedResponse deleteCalendar(DeleteCalendarRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, MLRequestConverters::deleteCalendar, options, AcknowledgedResponse::fromXContent, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.ops.transforms.Transforms.ceiling",
	"Comment": "binary matrix of whether the number at a given index is greater than",
	"Method": "INDArray ceiling(INDArray ndArray,INDArray ceiling,INDArray ndArray,boolean copyOnOps){\r\n    return exec(copyOnOps ? new Ceil(ndArray, ndArray.dup()) : new Ceil(ndArray, ndArray));\r\n}"
}, {
	"Path": "org.elasticsearch.client.IngestClient.getPipeline",
	"Comment": "get an existing pipeline.see get pipeline api on elastic.co",
	"Method": "GetPipelineResponse getPipeline(GetPipelineRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IngestRequestConverters::getPipeline, options, GetPipelineResponse::fromXContent, Collections.singleton(404));\r\n}"
}, {
	"Path": "org.nd4j.linalg.primitives.Counter.keySetSorted",
	"Comment": "this method returns list of elements, sorted by their counts",
	"Method": "List<T> keySetSorted(){\r\n    List<T> result = new ArrayList();\r\n    PriorityQueue<Pair<T, Double>> pq = asPriorityQueue();\r\n    while (!pq.isEmpty()) {\r\n        result.add(pq.poll().getFirst());\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.shape.Shape.ind2sub",
	"Comment": "convert a linear index tothe equivalent nd index based on the shape of the specified ndarray.infers the number of indices from the specified shape.",
	"Method": "int[] ind2sub(int[] shape,long index,long numIndices,long[] ind2sub,long[] shape,long index,long numIndices,int[] ind2sub,int[] shape,long index,long[] ind2sub,long[] shape,long index,long[] ind2sub,INDArray arr,long index){\r\n    if (arr.rank() == 1)\r\n        return new long[] { (int) index };\r\n    return ind2sub(arr.shape(), index, ArrayUtil.prodLong(arr.shape()));\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.stopILM",
	"Comment": "stop the index lifecycle management feature.see the docs for more.",
	"Method": "AcknowledgedResponse stopILM(StopILMRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IndexLifecycleRequestConverters::stopILM, options, AcknowledgedResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.nio.utils.ExceptionsHelper.rethrowAndSuppress",
	"Comment": "rethrows the first exception in the list and adds all remaining to the suppressed list.if the given list is empty no exception is thrown",
	"Method": "void rethrowAndSuppress(List<T> exceptions){\r\n    T main = null;\r\n    for (T ex : exceptions) {\r\n        main = useOrSuppress(main, ex);\r\n    }\r\n    if (main != null) {\r\n        throw main;\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.updater.storage.NoUpdateStorage.doGetUpdate",
	"Comment": "a method for actually performing the implementationof retrieving the ndarray",
	"Method": "NDArrayMessage doGetUpdate(int index){\r\n    throw new UnsupportedOperationException(\"Nothing is being stored in this implementation\");\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.reindexRethrottleAsync",
	"Comment": "executes a reindex rethrottling request.see the reindex rethrottling api on elastic.co",
	"Method": "void reindexRethrottleAsync(RethrottleRequest rethrottleRequest,RequestOptions options,ActionListener<ListTasksResponse> listener){\r\n    performRequestAsyncAndParseEntity(rethrottleRequest, RequestConverters::rethrottleReindex, options, ListTasksResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.script.mustache.SearchTemplateResponseTests.createSearchResponse",
	"Comment": "for simplicity we create a minimal response, as there is already a dedicatedtest class for search response parsing and serialization.",
	"Method": "SearchResponse createSearchResponse(){\r\n    long tookInMillis = randomNonNegativeLong();\r\n    int totalShards = randomIntBetween(1, Integer.MAX_VALUE);\r\n    int successfulShards = randomIntBetween(0, totalShards);\r\n    int skippedShards = randomIntBetween(0, totalShards);\r\n    InternalSearchResponse internalSearchResponse = InternalSearchResponse.empty();\r\n    return new SearchResponse(internalSearchResponse, null, totalShards, successfulShards, skippedShards, tookInMillis, ShardSearchFailure.EMPTY_ARRAY, SearchResponse.Clusters.EMPTY);\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.DataSet.scale",
	"Comment": "divides the input data transformby the max number in each row",
	"Method": "void scale(){\r\n    FeatureUtil.scaleByMax(getFeatures());\r\n}"
}, {
	"Path": "com.jakewharton.disklrucache.DiskLruCache.setMaxSize",
	"Comment": "changes the maximum number of bytes the cache can store and queues a jobto trim the existing store, if necessary.",
	"Method": "void setMaxSize(long maxSize){\r\n    this.maxSize = maxSize;\r\n    executorService.submit(cleanupCallable);\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.deleteDatafeedAsync",
	"Comment": "deletes the given machine learning datafeed asynchronously and notifies the listener on completionfor additional infosee ml delete datafeed documentation",
	"Method": "void deleteDatafeedAsync(DeleteDatafeedRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::deleteDatafeed, options, AcknowledgedResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.painless.LambdaBootstrap.generateLambdaConstructor",
	"Comment": "generates a constructor that will take in capturedarguments if any and store them in their respectivemember fields.",
	"Method": "void generateLambdaConstructor(ClassWriter cw,Type lambdaClassType,MethodType factoryMethodType,Capture[] captures){\r\n    String conDesc = factoryMethodType.changeReturnType(void.class).toMethodDescriptorString();\r\n    Method conMeth = new Method(CTOR_METHOD_NAME, conDesc);\r\n    Type baseConType = Type.getType(Object.class);\r\n    Method baseConMeth = new Method(CTOR_METHOD_NAME, MethodType.methodType(void.class).toMethodDescriptorString());\r\n    int modifiers = (captures.length > 0) ? ACC_PRIVATE : ACC_PUBLIC;\r\n    GeneratorAdapter constructor = new GeneratorAdapter(modifiers, conMeth, cw.visitMethod(modifiers, CTOR_METHOD_NAME, conDesc, null, null));\r\n    constructor.visitCode();\r\n    constructor.loadThis();\r\n    constructor.invokeConstructor(baseConType, baseConMeth);\r\n    for (int captureCount = 0; captureCount < captures.length; ++captureCount) {\r\n        constructor.loadThis();\r\n        constructor.loadArg(captureCount);\r\n        constructor.putField(lambdaClassType, captures[captureCount].name, captures[captureCount].type);\r\n    }\r\n    constructor.returnValue();\r\n    constructor.endMethod();\r\n    if (captures.length > 0) {\r\n        generateStaticCtorDelegator(cw, ACC_PUBLIC, LAMBDA_FACTORY_METHOD_NAME, lambdaClassType, factoryMethodType);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.v2.ModelParameterServer.isInitialized",
	"Comment": "this method checks if modelparameterserver was initialized",
	"Method": "boolean isInitialized(){\r\n    return launchLock.get();\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.buffer.factory.CudaDataBufferFactory.createSame",
	"Comment": "this method will create new databuffer of the same datatype & same length",
	"Method": "DataBuffer createSame(DataBuffer buffer,boolean init,DataBuffer createSame,DataBuffer buffer,boolean init,MemoryWorkspace workspace){\r\n    switch(buffer.dataType()) {\r\n        case INT:\r\n            return createInt(buffer.length(), init, workspace);\r\n        case FLOAT:\r\n            return createFloat(buffer.length(), init, workspace);\r\n        case DOUBLE:\r\n            return createDouble(buffer.length(), init, workspace);\r\n        case HALF:\r\n            return createHalf(buffer.length(), init, workspace);\r\n        default:\r\n            throw new UnsupportedOperationException(\"Unknown dataType: \" + buffer.dataType());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndexLifecycleClient.explainLifecycle",
	"Comment": "explain the lifecycle state for an indexsee the docs for more.",
	"Method": "ExplainLifecycleResponse explainLifecycle(ExplainLifecycleRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, IndexLifecycleRequestConverters::explainLifecycle, options, ExplainLifecycleResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.join.query.HasParentQueryBuilder.score",
	"Comment": "returns true if the parent score is mapped into the child documents",
	"Method": "boolean score(){\r\n    return score;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.Rational.normalize",
	"Comment": "normalize to coprime numerator and denominator.also copy a negative sign of the denominator to the numerator.",
	"Method": "void normalize(){\r\n    final BigInteger g = a.gcd(b);\r\n    if (g.compareTo(BigInteger.ONE) > 0) {\r\n        a = a.divide(g);\r\n        b = b.divide(g);\r\n    }\r\n    if (b.compareTo(BigInteger.ZERO) == -1) {\r\n        a = a.negate();\r\n        b = b.negate();\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.preprocessor.classimbalance.BaseUnderSamplingPreProcessor.donotMaskMinorityWindows",
	"Comment": "if set will not mask timesteps if they fall in a tbptt segment with at least one minority class label",
	"Method": "void donotMaskMinorityWindows(){\r\n    this.donotMaskMinorityWindows = true;\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AllocationPoint.attachBuffer",
	"Comment": "this method stores weakreference to original basecudadatabuffer",
	"Method": "void attachBuffer(BaseDataBuffer buffer){\r\n    originalDataBufferReference = new WeakReference<BaseDataBuffer>(buffer);\r\n}"
}, {
	"Path": "io.dropwizard.jersey.params.AbstractParam.errorMessage",
	"Comment": "given a string representation which was unable to be parsed and the exception thrown, producean entity to be sent to the client.",
	"Method": "String errorMessage(Exception e){\r\n    return String.format(\"%s is invalid: %s\", parameterName, e.getMessage());\r\n}"
}, {
	"Path": "org.elasticsearch.client.TasksClient.listAsync",
	"Comment": "asynchronously get current tasks using the task management api.see task management api on elastic.co",
	"Method": "void listAsync(ListTasksRequest request,RequestOptions options,ActionListener<ListTasksResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, TasksRequestConverters::listTasks, options, ListTasksResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.indices.FreezeIndexRequest.getWaitForActiveShards",
	"Comment": "returns the wait for active shard cound or null if the default should be used",
	"Method": "ActiveShardCount getWaitForActiveShards(){\r\n    return waitForActiveShards;\r\n}"
}, {
	"Path": "org.elasticsearch.join.query.JoinQueryBuilders.hasParentQuery",
	"Comment": "constructs a new parent query, with the parent type and the query to run on the parent documents. theresults of this query are the children docs that those parent docs matched.",
	"Method": "HasParentQueryBuilder hasParentQuery(String type,QueryBuilder query,boolean score){\r\n    return new HasParentQueryBuilder(type, query, score);\r\n}"
}, {
	"Path": "org.elasticsearch.painless.LambdaBootstrap.generateCaptureFields",
	"Comment": "generates member fields for captured variablesbased on the parameters for the factory method.",
	"Method": "Capture[] generateCaptureFields(ClassWriter cw,MethodType factoryMethodType){\r\n    int captureTotal = factoryMethodType.parameterCount();\r\n    Capture[] captures = new Capture[captureTotal];\r\n    for (int captureCount = 0; captureCount < captureTotal; ++captureCount) {\r\n        captures[captureCount] = new Capture(captureCount, factoryMethodType.parameterType(captureCount));\r\n        int modifiers = ACC_PRIVATE | ACC_FINAL;\r\n        FieldVisitor fv = cw.visitField(modifiers, captures[captureCount].name, captures[captureCount].desc, null, null);\r\n        fv.visitEnd();\r\n    }\r\n    return captures;\r\n}"
}, {
	"Path": "com.jakewharton.disklrucache.DiskLruCache.getMaxSize",
	"Comment": "returns the maximum number of bytes that this cache should use to storeits data.",
	"Method": "long getMaxSize(){\r\n    return maxSize;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.health.ClusterHealthRequestBuilder.setWaitForNoInitializingShards",
	"Comment": "sets whether the request should wait for there to be no initializing shards beforeretrieving the cluster health status.defaults to false, meaning theoperation does not wait on there being no more initializing shards.set to trueto wait until the number of initializing shards in the cluster is 0.",
	"Method": "ClusterHealthRequestBuilder setWaitForNoInitializingShards(boolean waitForNoInitializingShards){\r\n    request.waitForNoInitializingShards(waitForNoInitializingShards);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getDatafeedStatsAsync",
	"Comment": "gets statistics for one or more machine learning datafeeds, asynchronously.for additional infosee get datafeed stats docs",
	"Method": "void getDatafeedStatsAsync(GetDatafeedStatsRequest request,RequestOptions options,ActionListener<GetDatafeedStatsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getDatafeedStats, options, GetDatafeedStatsResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.Uint8.getDescriptor",
	"Comment": "this method returns compression descriptor. it should be unique for any compressor implementation",
	"Method": "String getDescriptor(){\r\n    return \"UINT8\";\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.transport.BaseTransport.clientMessageHandler",
	"Comment": "this message handler is responsible for receiving messages on client side",
	"Method": "void clientMessageHandler(DirectBuffer buffer,int offset,int length,Header header){\r\n    byte[] data = new byte[length];\r\n    buffer.getBytes(offset, data);\r\n    MeaningfulMessage message = (MeaningfulMessage) VoidMessage.fromBytes(data);\r\n    completed.put(message.getTaskId(), message);\r\n}"
}, {
	"Path": "org.nd4j.linalg.api.shape.Shape.getOffset",
	"Comment": "get the offset of the specified indices from the shape info buffer",
	"Method": "long getOffset(long baseOffset,int[] shape,int[] stride,int indices,long getOffset,IntBuffer shapeInformation,int[] indices,long getOffset,LongBuffer shapeInformation,int[] indices,long getOffset,LongBuffer shapeInformation,long indices,long getOffset,IntBuffer shapeInformation,long indices,long getOffset,DataBuffer shapeInformation,int[] indices,long getOffset,DataBuffer shapeInformation,long indices,long getOffset,int[] shapeInformation,int indices,long getOffset,long[] shapeInformation,int indices,long getOffset,long[] shapeInformation,long indices,long getOffset,DataBuffer shapeInformation,int row,int col,long getOffset,IntBuffer shapeInformation,int row,int col,long getOffset,IntBuffer shapeInformation,int dim0,int dim1,int dim2,long getOffset,DataBuffer shapeInformation,int dim0,int dim1,int dim2,long getOffset,IntBuffer shapeInformation,int dim0,int dim1,int dim2,int dim3,long getOffset,DataBuffer shapeInformation,int dim0,int dim1,int dim2,int dim3){\r\n    int rank = rank(shapeInformation);\r\n    if (rank != 4)\r\n        throw new IllegalArgumentException(\"Cannot use this getOffset method on arrays of rank != 4 (rank is: \" + rank + \")\");\r\n    return getOffsetUnsafe(shapeInformation, dim0, dim1, dim2, dim3);\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.update",
	"Comment": "updates a document using the update api.see update api on elastic.co",
	"Method": "UpdateResponse update(UpdateRequest updateRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(updateRequest, RequestConverters::update, options, UpdateResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.updateByQueryAsync",
	"Comment": "asynchronously executes an update by query request.see update by query api on elastic.co",
	"Method": "void updateByQueryAsync(UpdateByQueryRequest updateByQueryRequest,RequestOptions options,ActionListener<BulkByScrollResponse> listener){\r\n    performRequestAsyncAndParseEntity(updateByQueryRequest, RequestConverters::updateByQuery, options, BulkByScrollResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getMlInfoAsync",
	"Comment": "gets machine learning information about default values and limits, asynchronously.for additional infosee machine learning info",
	"Method": "void getMlInfoAsync(MlInfoRequest request,RequestOptions options,ActionListener<MlInfoResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::mlInfo, options, MlInfoResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.getLastCycleAllocations",
	"Comment": "this method returns number of bytes allocated during last full cycle",
	"Method": "long getLastCycleAllocations(){\r\n    return lastCycleAllocations.get();\r\n}"
}, {
	"Path": "org.nd4j.tools.PropertyParser.toInt",
	"Comment": "get property. the method returns the default value if the property is not parsed.",
	"Method": "int toInt(String name,int toInt,String name,int defaultValue){\r\n    try {\r\n        return parseInt(name);\r\n    } catch (Exception e) {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.IngestClient.deletePipelineAsync",
	"Comment": "asynchronously delete an existing pipeline.seedelete pipeline api on elastic.co",
	"Method": "void deletePipelineAsync(DeletePipelineRequest request,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, IngestRequestConverters::deletePipeline, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.job.config.Job.getEstablishedModelMemory",
	"Comment": "the established model memory of the job, or null if modelmemory has not reached equilibrium yet.",
	"Method": "Long getEstablishedModelMemory(){\r\n    return establishedModelMemory;\r\n}"
}, {
	"Path": "org.nd4j.jackson.objectmapper.holder.ObjectMapperHolder.getJsonMapper",
	"Comment": "get a single object mapper for usewith reading and writing json",
	"Method": "ObjectMapper getJsonMapper(){\r\n    return objectMapper;\r\n}"
}, {
	"Path": "org.elasticsearch.client.core.TermVectorsResponse.getId",
	"Comment": "returns the id of the requestcan be null if there is no document id",
	"Method": "String getId(){\r\n    return id;\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.existsAlias",
	"Comment": "checks if one or more aliases exist using the aliases exist api.see indices aliases api on elastic.co",
	"Method": "boolean existsAlias(GetAliasesRequest getAliasesRequest,RequestOptions options){\r\n    return restHighLevelClient.performRequest(getAliasesRequest, IndicesRequestConverters::existsAlias, options, RestHighLevelClient::convertExistsResponse, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractAsyncBulkByScrollAction.addDestinationIndices",
	"Comment": "add to the list of indices that were modified by this request. this is the list of indices refreshed at the end of the request if therequest asks for a refresh.",
	"Method": "void addDestinationIndices(Collection<String> indices){\r\n    destinationIndices.addAll(indices);\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.DataSet.batchBy",
	"Comment": "partitions a dataset in to mini batches whereeach dataset in each list is of the specified number of examples",
	"Method": "List<DataSet> batchBy(int num){\r\n    List<DataSet> batched = Lists.newArrayList();\r\n    for (List<DataSet> splitBatch : Lists.partition(asList(), num)) {\r\n        batched.add(DataSet.merge(splitBatch));\r\n    }\r\n    return batched;\r\n}"
}, {
	"Path": "org.nd4j.jita.handler.impl.CudaZeroHandler.getAllocatedHostMemory",
	"Comment": "this method returns total amount of host memory allocated within this memoryhandler",
	"Method": "long getAllocatedHostMemory(){\r\n    return zeroUseCounter.get();\r\n}"
}, {
	"Path": "org.elasticsearch.join.query.HasChildQueryBuilder.minMaxChildren",
	"Comment": "defines the minimum number of children that are required to match for the parent to be considered a match andthe maximum number of children that are required to match for the parent to be considered a match.",
	"Method": "HasChildQueryBuilder minMaxChildren(int minChildren,int maxChildren){\r\n    if (minChildren < 0) {\r\n        throw new IllegalArgumentException(\"[\" + NAME + \"] requires non-negative 'min_children' field\");\r\n    }\r\n    if (maxChildren < 0) {\r\n        throw new IllegalArgumentException(\"[\" + NAME + \"] requires non-negative 'max_children' field\");\r\n    }\r\n    if (maxChildren < minChildren) {\r\n        throw new IllegalArgumentException(\"[\" + NAME + \"] 'max_children' is less than 'min_children'\");\r\n    }\r\n    this.minChildren = minChildren;\r\n    this.maxChildren = maxChildren;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.EvilSecurityTests.testSymlinkPermissions",
	"Comment": "when a configured dir is a symlink, test that permissions work on link target",
	"Method": "void testSymlinkPermissions(){\r\n    assumeFalse(\"windows does not automatically grant permission to the target of symlinks\", Constants.WINDOWS);\r\n    Path dir = createTempDir();\r\n    Path target = dir.resolve(\"target\");\r\n    Files.createDirectory(target);\r\n    Path link = dir.resolve(\"link\");\r\n    try {\r\n        Files.createSymbolicLink(link, target);\r\n    } catch (UnsupportedOperationException | IOException e) {\r\n        assumeNoException(\"test requires filesystem that supports symbolic links\", e);\r\n    } catch (SecurityException e) {\r\n        assumeNoException(\"test cannot create symbolic links with security manager enabled\", e);\r\n    }\r\n    Permissions permissions = new Permissions();\r\n    FilePermissionUtils.addDirectoryPath(permissions, \"testing\", link, \"read\");\r\n    assertExactPermissions(new FilePermission(link.toString(), \"read\"), permissions);\r\n    assertExactPermissions(new FilePermission(link.resolve(\"foo\").toString(), \"read\"), permissions);\r\n    assertExactPermissions(new FilePermission(target.toString(), \"read\"), permissions);\r\n    assertExactPermissions(new FilePermission(target.resolve(\"foo\").toString(), \"read\"), permissions);\r\n}"
}, {
	"Path": "org.nd4j.jita.conf.Configuration.useDevice",
	"Comment": "this method forces specific device to be used. all other devices present in system will be ignored.",
	"Method": "Configuration useDevice(Integer deviceId){\r\n    return useDevices(deviceId);\r\n}"
}, {
	"Path": "io.dropwizard.validation.selfvalidating.ViolationCollector.setViolationOccurred",
	"Comment": "manually sets if a violation occurred. this is automatically set if addviolation is called.",
	"Method": "void setViolationOccurred(boolean violationOccurred){\r\n    this.violationOccurred = violationOccurred;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.correlation",
	"Comment": "returns the correlation coefficient of two double vectors.",
	"Method": "double correlation(double[] residuals,double targetAttribute){\r\n    double[] predictedValues = new double[residuals.length];\r\n    for (int i = 0; i < predictedValues.length; i++) {\r\n        predictedValues[i] = targetAttribute[i] - residuals[i];\r\n    }\r\n    double ssErr = ssError(predictedValues, targetAttribute);\r\n    double total = ssTotal(residuals, targetAttribute);\r\n    return 1 - (ssErr / total);\r\n}"
}, {
	"Path": "org.elasticsearch.client.SecurityClient.getSslCertificates",
	"Comment": "synchronously retrieve the x.509 certificates that are used to encrypt communications in an elasticsearch cluster.see the docs for more.",
	"Method": "GetSslCertificatesResponse getSslCertificates(RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(GetSslCertificatesRequest.INSTANCE, GetSslCertificatesRequest::getRequest, options, GetSslCertificatesResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.GetDatafeedRequest.getDatafeedIds",
	"Comment": "all the datafeedids for which to get configuration information",
	"Method": "List<String> getDatafeedIds(){\r\n    return datafeedIds;\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.Uint8.getCompressionType",
	"Comment": "this method returns compression optype provided by specific ndarraycompressor implementation",
	"Method": "CompressionType getCompressionType(){\r\n    return CompressionType.LOSSY;\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.license",
	"Comment": "provides methods for accessing the elastic licensed licensing apis thatare shipped with the default distribution of elasticsearch. all ofthese apis will 404 if run against the oss distribution of elasticsearch.see the licensing apis on elastic.co for more information.",
	"Method": "LicenseClient license(){\r\n    return licenseClient;\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.updateFilterAsync",
	"Comment": "updates a machine learning filter asynchronously and notifies listener on completionfor additional infosee ml update filter documentation",
	"Method": "void updateFilterAsync(UpdateFilterRequest request,RequestOptions options,ActionListener<PutFilterResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::updateFilter, options, PutFilterResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.information",
	"Comment": "this returns the entropy for a given vector of probabilities.",
	"Method": "double information(double[] probabilities){\r\n    double total = 0.0;\r\n    for (double d : probabilities) {\r\n        total += (-1.0 * log2(d) * d);\r\n    }\r\n    return total;\r\n}"
}, {
	"Path": "org.nd4j.linalg.learning.AdaDeltaUpdater.applyUpdater",
	"Comment": "get the updated gradient for the given gradientand also update the state of ada delta.",
	"Method": "void applyUpdater(INDArray gradient,int iteration,int epoch){\r\n    if (msg == null || msdx == null)\r\n        throw new IllegalStateException(\"Updater has not been initialized with view state\");\r\n    double rho = config.getRho();\r\n    double epsilon = config.getEpsilon();\r\n    msg.muli(rho).addi(gradient.mul(gradient).muli(1 - rho));\r\n    INDArray rmsdx_t1 = Transforms.sqrt(msdx.add(epsilon), false);\r\n    INDArray rmsg_t = Transforms.sqrt(msg.add(epsilon), false);\r\n    INDArray update = gradient.muli(rmsdx_t1.divi(rmsg_t));\r\n    msdx.muli(rho).addi(update.mul(update).muli(1 - rho));\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AtomicAllocator.getTotalAllocatedDeviceMemory",
	"Comment": "this method returns total amount of memory allocated on specified device",
	"Method": "long getTotalAllocatedDeviceMemory(Integer deviceId){\r\n    return 0L;\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.NoOp.getCompressionType",
	"Comment": "this method returns compression optype provided by specific ndarraycompressor implementation",
	"Method": "CompressionType getCompressionType(){\r\n    return CompressionType.LOSSLESS;\r\n}"
}, {
	"Path": "com.googlecode.d2j.dex.writer.CodeWriter.visitConstStmt",
	"Comment": "kfmt21c,kfmt31c,kfmt11n,kfmt21h,kfmt21s,kfmt31i,kfmt51l",
	"Method": "void visitConstStmt(Op op,int ra,Object value){\r\n    switch(op.format) {\r\n        case kFmt21c:\r\n        case kFmt31c:\r\n            value = cp.wrapEncodedItem(value);\r\n            ops.add(new CodeWriter.IndexedInsn(op, ra, 0, (BaseItem) value));\r\n            break;\r\n        case kFmt11n:\r\n            ops.add(new PreBuildInsn(build11n(op, ra, ((Number) value).intValue())));\r\n            break;\r\n        case kFmt21h:\r\n            ops.add(new PreBuildInsn(build21h(op, ra, ((Number) value))));\r\n            break;\r\n        case kFmt21s:\r\n            ops.add(new PreBuildInsn(build21s(op, ra, ((Number) value))));\r\n            break;\r\n        case kFmt31i:\r\n            ops.add(new PreBuildInsn(build31i(op, ra, ((Number) value))));\r\n            break;\r\n        case kFmt51l:\r\n            ops.add(new PreBuildInsn(build51l(op, ra, ((Number) value))));\r\n            break;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.gcs.GoogleCloudStorageService.toTimeout",
	"Comment": "converts timeout values from the settings to a timeout value for the googlecloud sdk",
	"Method": "Integer toTimeout(TimeValue timeout){\r\n    if (timeout == null || TimeValue.ZERO.equals(timeout)) {\r\n        return -1;\r\n    }\r\n    if (TimeValue.MINUS_ONE.equals(timeout)) {\r\n        return 0;\r\n    }\r\n    return Math.toIntExact(timeout.getMillis());\r\n}"
}, {
	"Path": "org.nd4j.linalg.dimensionalityreduction.PCA.convertBackToFeatures",
	"Comment": "take the data that has been transformed to the principal components about the mean andtransform it back into the original feature set.make sure to fill in zeroes in columnswhere components were dropped!",
	"Method": "INDArray convertBackToFeatures(INDArray data){\r\n    return Nd4j.tensorMmul(eigenvectors, data, new int[][] { { 1 }, { 1 } }).transposei().addiRowVector(mean);\r\n}"
}, {
	"Path": "org.nd4j.linalg.jcublas.util.CudaArgs.getModuleNameFor",
	"Comment": "for invoking a cuda kernelthis returns the module opname for the given op",
	"Method": "String getModuleNameFor(Op op){\r\n    String moduleName = null;\r\n    if (op instanceof Accumulation) {\r\n        moduleName = \"reduce\";\r\n        if (op.opName().equals(\"cosinesimilarity\")) {\r\n            moduleName = \"reduce3\";\r\n        } else if (op.opName().equals(\"euclidean\")) {\r\n            moduleName = \"reduce3\";\r\n        } else if (op.opName().equals(\"manhattan\")) {\r\n            moduleName = \"reduce3\";\r\n        }\r\n    } else if (op instanceof TransformOp) {\r\n        if (op.opName().equals(\"add\")) {\r\n            moduleName = \"pairWiseTransform\";\r\n        } else if (op.opName().equals(\"copy\")) {\r\n            moduleName = \"pairWiseTransform\";\r\n        } else if (op.opName().equals(\"div\")) {\r\n            moduleName = \"pairWiseTransform\";\r\n        } else if (op.opName().equals(\"mul\")) {\r\n            moduleName = \"pairWiseTransform\";\r\n        } else if (op.opName().equals(\"rdiv\")) {\r\n            moduleName = \"pairWiseTransform\";\r\n        } else if (op.opName().equals(\"rsub\")) {\r\n            moduleName = \"pairWiseTransform\";\r\n        } else if (op.opName().equals(\"sub\")) {\r\n            moduleName = \"pairWiseTransform\";\r\n        } else {\r\n            moduleName = \"transform\";\r\n        }\r\n    } else if (op instanceof ScalarOp) {\r\n        moduleName = \"scalar\";\r\n    } else if (op instanceof BroadcastOp) {\r\n        moduleName = \"broadcast\";\r\n    } else if (op instanceof IndexAccumulation) {\r\n        moduleName = \"indexReduce\";\r\n    }\r\n    return moduleName;\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.InstallPluginCommand.getPublicKeyId",
	"Comment": "return the public key id of the signing key that is expected to have signed the official plugin.",
	"Method": "String getPublicKeyId(){\r\n    return \"D27D666CD88E42B4\";\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.DataSet.filterAndStrip",
	"Comment": "strips the dataset down to the specified labelsand remaps them",
	"Method": "void filterAndStrip(int[] labels){\r\n    DataSet filtered = filterBy(labels);\r\n    List<Integer> newLabels = new ArrayList();\r\n    Map<Integer, Integer> labelMap = new HashMap();\r\n    for (int i = 0; i < labels.length; i++) labelMap.put(labels[i], i);\r\n    for (int i = 0; i < filtered.numExamples(); i++) {\r\n        DataSet example = filtered.get(i);\r\n        int o2 = example.outcome();\r\n        Integer outcome = labelMap.get(o2);\r\n        newLabels.add(outcome);\r\n    }\r\n    INDArray newLabelMatrix = Nd4j.create(filtered.numExamples(), labels.length);\r\n    if (newLabelMatrix.rows() != newLabels.size())\r\n        throw new IllegalStateException(\"Inconsistent label sizes\");\r\n    for (int i = 0; i < newLabelMatrix.rows(); i++) {\r\n        Integer i2 = newLabels.get(i);\r\n        if (i2 == null)\r\n            throw new IllegalStateException(\"Label not found on row \" + i);\r\n        INDArray newRow = FeatureUtil.toOutcomeVector(i2, labels.length);\r\n        newLabelMatrix.putRow(i, newRow);\r\n    }\r\n    setFeatures(filtered.getFeatures());\r\n    setLabels(newLabelMatrix);\r\n}"
}, {
	"Path": "org.elasticsearch.client.TasksClient.list",
	"Comment": "get current tasks using the task management api.see task management api on elastic.co",
	"Method": "ListTasksResponse list(ListTasksRequest request,RequestOptions options){\r\n    return restHighLevelClient.performRequestAndParseEntity(request, TasksRequestConverters::listTasks, options, ListTasksResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.provider.BasicWorkspaceManager.destroyAllWorkspacesForCurrentThread",
	"Comment": "this method destroys all workspaces allocated in current thread",
	"Method": "void destroyAllWorkspacesForCurrentThread(){\r\n    ensureThreadExistense();\r\n    List<MemoryWorkspace> workspaces = new ArrayList();\r\n    workspaces.addAll(backingMap.get().values());\r\n    for (MemoryWorkspace workspace : workspaces) {\r\n        destroyWorkspace(workspace);\r\n    }\r\n    System.gc();\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.searchTemplate",
	"Comment": "executes a request using the search template api.see search template apion elastic.co.",
	"Method": "SearchTemplateResponse searchTemplate(SearchTemplateRequest searchTemplateRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(searchTemplateRequest, RequestConverters::searchTemplate, options, SearchTemplateResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.concurrency.RRWLock.detachObject",
	"Comment": "this method notifies locker that specific object was removed from tracking list",
	"Method": "void detachObject(Object object){\r\n    objectLocks.remove(object);\r\n}"
}, {
	"Path": "io.dropwizard.jersey.validation.ConstraintMessage.getMessage",
	"Comment": "gets the human friendly location of where the violation was raised.",
	"Method": "String getMessage(ConstraintViolation<?> v,Invocable invocable){\r\n    final Pair<Path, ? extends ConstraintDescriptor<?>> of = Pair.of(v.getPropertyPath(), v.getConstraintDescriptor());\r\n    final String cachePrefix = PREFIX_CACHE.getIfPresent(of);\r\n    if (cachePrefix == null) {\r\n        final String prefix = calculatePrefix(v, invocable);\r\n        PREFIX_CACHE.put(of, prefix);\r\n        return prefix + v.getMessage();\r\n    }\r\n    return cachePrefix + v.getMessage();\r\n}"
}, {
	"Path": "org.elasticsearch.http.nio.cors.NioCorsConfig.preflightResponseHeaders",
	"Comment": "returns http response headers that should be added to a cors preflight response.",
	"Method": "HttpHeaders preflightResponseHeaders(){\r\n    if (preflightHeaders.isEmpty()) {\r\n        return EmptyHttpHeaders.INSTANCE;\r\n    }\r\n    final HttpHeaders preflightHeaders = new DefaultHttpHeaders();\r\n    for (Map.Entry<CharSequence, Callable<?>> entry : this.preflightHeaders.entrySet()) {\r\n        final Object value = getValue(entry.getValue());\r\n        if (value instanceof Iterable) {\r\n            preflightHeaders.add(entry.getKey().toString(), (Iterable<?>) value);\r\n        } else {\r\n            preflightHeaders.add(entry.getKey().toString(), value);\r\n        }\r\n    }\r\n    return preflightHeaders;\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.partitionVariable",
	"Comment": "this will partition the given whole variable data applytransformtodestination in to the specified chunk number.",
	"Method": "List<List<Double>> partitionVariable(List<Double> arr,int chunk){\r\n    int count = 0;\r\n    List<List<Double>> ret = new ArrayList<List<Double>>();\r\n    while (count < arr.size()) {\r\n        List<Double> sublist = arr.subList(count, count + chunk);\r\n        count += chunk;\r\n        ret.add(sublist);\r\n    }\r\n    for (List<Double> lists : ret) {\r\n        if (lists.size() < chunk)\r\n            ret.remove(lists);\r\n    }\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.getNumberOfExternalAllocations",
	"Comment": "this method returns number of spilled allocations, that can be purged at the end of block",
	"Method": "int getNumberOfExternalAllocations(){\r\n    return externalCount.get();\r\n}"
}, {
	"Path": "org.elasticsearch.painless.LambdaBootstrap.validateTypes",
	"Comment": "validates some conversions at link time.currently, only ensures that the lambda methodwith a return value cannot delegate to a delegate method with no return type.",
	"Method": "void validateTypes(MethodType interfaceMethodType,MethodType delegateMethodType){\r\n    if (interfaceMethodType.returnType() != void.class && delegateMethodType.returnType() == void.class) {\r\n        throw new LambdaConversionException(\"lambda expects return type [\" + interfaceMethodType.returnType() + \"], but found return type [void]\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.health.ClusterHealthResponse.setStatus",
	"Comment": "allows to explicitly override the derived cluster health status.",
	"Method": "void setStatus(ClusterHealthStatus status){\r\n    if (status == null) {\r\n        throw new IllegalArgumentException(\"'status' must not be null\");\r\n    }\r\n    this.clusterHealthStatus = status;\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.Indices.createFromStartAndEnd",
	"Comment": "create indices representing intervalsalong each dimension",
	"Method": "INDArrayIndex[] createFromStartAndEnd(INDArray start,INDArray end,INDArrayIndex[] createFromStartAndEnd,INDArray start,INDArray end,boolean inclusive){\r\n    if (start.length() != end.length())\r\n        throw new IllegalArgumentException(\"Start length must be equal to end length\");\r\n    else {\r\n        if (start.length() > Integer.MAX_VALUE)\r\n            throw new ND4JIllegalStateException(\"Can't proceed with INDArray with length > Integer.MAX_VALUE\");\r\n        INDArrayIndex[] indexes = new INDArrayIndex[(int) start.length()];\r\n        for (int i = 0; i < indexes.length; i++) {\r\n            indexes[i] = NDArrayIndex.interval(start.getInt(i), end.getInt(i), inclusive);\r\n        }\r\n        return indexes;\r\n    }\r\n}"
}, {
	"Path": "io.dropwizard.auth.CachingAuthorizer.size",
	"Comment": "returns the number of principals for which there are cachedrole associations.",
	"Method": "long size(){\r\n    return cache.estimatedSize();\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.putFilterAsync",
	"Comment": "creates a new machine learning filter asynchronously and notifies listener on completionfor additional infosee ml put filter documentation",
	"Method": "void putFilterAsync(PutFilterRequest request,RequestOptions options,ActionListener<PutFilterResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::putFilter, options, PutFilterResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.count",
	"Comment": "executes a count request using the count api.see count api on elastic.co",
	"Method": "CountResponse count(CountRequest countRequest,RequestOptions options){\r\n    return performRequestAndParseEntity(countRequest, RequestConverters::count, options, CountResponse::fromXContent, emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.SnapshotClient.deleteRepositoryAsync",
	"Comment": "asynchronously deletes a snapshot repository.seesnapshot and restoreapi on elastic.co",
	"Method": "void deleteRepositoryAsync(DeleteRepositoryRequest deleteRepositoryRequest,RequestOptions options,ActionListener<AcknowledgedResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(deleteRepositoryRequest, SnapshotRequestConverters::deleteRepository, options, AcknowledgedResponse::fromXContent, listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.aeron.ipc.AeronUtil.subscriberLoop",
	"Comment": "return a reusable, parameterized eventloop that calls and idlerwhen no messages are received",
	"Method": "Consumer<Subscription> subscriberLoop(FragmentHandler fragmentHandler,int limit,AtomicBoolean running,AtomicBoolean launched,Consumer<Subscription> subscriberLoop,FragmentHandler fragmentHandler,int limit,AtomicBoolean running,IdleStrategy idleStrategy,AtomicBoolean launched){\r\n    return (subscription) -> {\r\n        try {\r\n            while (running.get()) {\r\n                idleStrategy.idle(subscription.poll(fragmentHandler, limit));\r\n                launched.set(true);\r\n            }\r\n        } catch (final Exception ex) {\r\n            LangUtil.rethrowUnchecked(ex);\r\n        }\r\n    };\r\n}"
}, {
	"Path": "org.elasticsearch.client.ml.CloseJobRequest.setForce",
	"Comment": "should the closing be forced.use to close a failed job, or to forcefully close a job which has not responded to its initial close request.",
	"Method": "void setForce(boolean force){\r\n    this.force = force;\r\n}"
}, {
	"Path": "org.elasticsearch.script.mustache.MultiSearchTemplateRequest.maxConcurrentSearchRequests",
	"Comment": "sets how many search requests specified in this multi search requests are allowed to be ran concurrently.",
	"Method": "int maxConcurrentSearchRequests(MultiSearchTemplateRequest maxConcurrentSearchRequests,int maxConcurrentSearchRequests){\r\n    if (maxConcurrentSearchRequests < 1) {\r\n        throw new IllegalArgumentException(\"maxConcurrentSearchRequests must be positive\");\r\n    }\r\n    this.maxConcurrentSearchRequests = maxConcurrentSearchRequests;\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.linalg.primitives.Optional.get",
	"Comment": "if a value is present in this optional, returns the value, otherwise throws nosuchelementexception.",
	"Method": "T get(){\r\n    if (!isPresent()) {\r\n        throw new NoSuchElementException(\"Optional is empty\");\r\n    }\r\n    return value;\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.AbstractCompressor.compress",
	"Comment": "this method creates compressed indarray from java double array, skipping usual indarray instantiation routines",
	"Method": "INDArray compress(INDArray array,DataBuffer compress,DataBuffer buffer,INDArray compress,float[] data,INDArray compress,double[] data,INDArray compress,float[] data,int[] shape,char order,INDArray compress,double[] data,int[] shape,char order){\r\n    DoublePointer pointer = new DoublePointer(data);\r\n    DataBuffer shapeInfo = Nd4j.getShapeInfoProvider().createShapeInformation(shape, order).getFirst();\r\n    DataBuffer buffer = compressPointer(DataBuffer.TypeEx.DOUBLE, pointer, data.length, 8);\r\n    return Nd4j.createArrayFromShapeBuffer(buffer, shapeInfo);\r\n}"
}, {
	"Path": "org.elasticsearch.painless.Globals.addSyntheticMethod",
	"Comment": "adds a new synthetic method to be written. it must be analyzed!",
	"Method": "void addSyntheticMethod(SFunction function){\r\n    if (!function.synthetic) {\r\n        throw new IllegalStateException(\"method: \" + function.name + \" is not synthetic\");\r\n    }\r\n    if (syntheticMethods.put(function.name, function) != null) {\r\n        throw new IllegalStateException(\"synthetic method: \" + function.name + \" already exists\");\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.dataset.api.iterator.MultipleEpochsIterator.next",
	"Comment": "like the standard next method but allows acustomizable number of examples returned",
	"Method": "DataSet next(int num,DataSet next){\r\n    if (!iter.hasNext() && passes < numPasses) {\r\n        passes++;\r\n        batch = 0;\r\n        log.info(\"Epoch \" + passes + \" batch \" + batch);\r\n        iter.reset();\r\n    }\r\n    batch++;\r\n    DataSet next = iter.next();\r\n    if (preProcessor != null)\r\n        preProcessor.preProcess(next);\r\n    return next;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.ReindexTestCase.expectedSliceStatuses",
	"Comment": "figures out how many slice statuses to expect in the response",
	"Method": "int expectedSliceStatuses(int requestSlices,Collection<String> indices,int expectedSliceStatuses,int slicesConfigured,String index){\r\n    return expectedSliceStatuses(slicesConfigured, singleton(index));\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.SerializationUtils.writeObject",
	"Comment": "writes the object to the output streamthis does not flush the stream",
	"Method": "void writeObject(Serializable toSave,OutputStream writeTo){\r\n    try {\r\n        ObjectOutputStream os = new ObjectOutputStream(writeTo);\r\n        os.writeObject(toSave);\r\n    } catch (Exception e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestClientMultipleHostsIntegTests.testNodeSelector",
	"Comment": "test host selector against a real server andtest what happens after calling",
	"Method": "void testNodeSelector(){\r\n    try (RestClient restClient = buildRestClient(firstPositionNodeSelector())) {\r\n        Request request = new Request(\"GET\", \"/200\");\r\n        int rounds = between(1, 10);\r\n        for (int i = 0; i < rounds; i++) {\r\n            if (stoppedFirstHost) {\r\n                try {\r\n                    restClient.performRequest(request);\r\n                    fail(\"expected to fail to connect\");\r\n                } catch (ConnectException e) {\r\n                    if (false == System.getProperty(\"os.name\").startsWith(\"Windows\")) {\r\n                        assertEquals(\"Connection refused\", e.getMessage());\r\n                    }\r\n                }\r\n            } else {\r\n                Response response = restClient.performRequest(request);\r\n                assertEquals(httpHosts[0], response.getHost());\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cloud.azure.classic.AbstractAzureComputeServiceTestCase.registerAzureNode",
	"Comment": "register an existing node as a azure node, exposing its address and details htrough",
	"Method": "void registerAzureNode(String nodeName){\r\n    TransportService transportService = internalCluster().getInstance(TransportService.class, nodeName);\r\n    assertNotNull(transportService);\r\n    DiscoveryNode discoveryNode = transportService.getLocalNode();\r\n    assertNotNull(discoveryNode);\r\n    if (nodes.put(discoveryNode.getName(), discoveryNode) != null) {\r\n        throw new IllegalArgumentException(\"Node [\" + discoveryNode.getName() + \"] cannot be registered twice in Azure\");\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.jita.memory.impl.CudaDirectProvider.malloc",
	"Comment": "this method provides pointerspair to memory chunk specified by allocationshape",
	"Method": "PointersPair malloc(AllocationShape shape,AllocationPoint point,AllocationStatus location){\r\n    switch(location) {\r\n        case HOST:\r\n            {\r\n                Pointer devicePointer = new Pointer();\r\n                long reqMem = AllocationUtils.getRequiredMemory(shape);\r\n                if (reqMem < 1)\r\n                    reqMem = 1;\r\n                Pointer pointer = nativeOps.mallocHost(reqMem, 0);\r\n                if (pointer == null)\r\n                    throw new RuntimeException(\"Can't allocate [HOST] memory: \" + reqMem + \"; threadId: \" + Thread.currentThread().getId());\r\n                Pointer hostPointer = new CudaPointer(pointer);\r\n                PointersPair devicePointerInfo = new PointersPair();\r\n                devicePointerInfo.setDevicePointer(new CudaPointer(hostPointer, reqMem));\r\n                devicePointerInfo.setHostPointer(new CudaPointer(hostPointer, reqMem));\r\n                point.setPointers(devicePointerInfo);\r\n                point.setAllocationStatus(AllocationStatus.HOST);\r\n                return devicePointerInfo;\r\n            }\r\n        case DEVICE:\r\n            {\r\n                int deviceId = AtomicAllocator.getInstance().getDeviceId();\r\n                long reqMem = AllocationUtils.getRequiredMemory(shape);\r\n                if (reqMem < 1)\r\n                    reqMem = 1;\r\n                Pointer pointer = nativeOps.mallocDevice(reqMem, null, 0);\r\n                if (pointer == null)\r\n                    return null;\r\n                Pointer devicePointer = new CudaPointer(pointer);\r\n                PointersPair devicePointerInfo = point.getPointers();\r\n                if (devicePointerInfo == null)\r\n                    devicePointerInfo = new PointersPair();\r\n                devicePointerInfo.setDevicePointer(new CudaPointer(devicePointer, reqMem));\r\n                point.setAllocationStatus(AllocationStatus.DEVICE);\r\n                point.setDeviceId(deviceId);\r\n                return devicePointerInfo;\r\n            }\r\n        default:\r\n            throw new IllegalStateException(\"Unsupported location for malloc: [\" + location + \"]\");\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.linalg.compression.CompressionSerDeTests.testAutoDecompression1",
	"Comment": "this test checks for automatic decompression after deserialization",
	"Method": "void testAutoDecompression1(){\r\n    INDArray array = Nd4j.linspace(1, 250, 250);\r\n    INDArray compressed = Nd4j.getCompressor().compress(array, \"UINT8\");\r\n    ByteArrayOutputStream bos = new ByteArrayOutputStream();\r\n    Nd4j.write(bos, compressed);\r\n    ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray());\r\n    INDArray result = Nd4j.read(bis);\r\n    assertEquals(array, result);\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT.testBasicUsage",
	"Comment": "turkish has some funny casing. this test shows how you can solve this kind of thing easily with collation. instead of using lowercasefilter, use a turkish collator with primary strength. then things will sort and match correctly.",
	"Method": "void testBasicUsage(){\r\n    String index = \"foo\";\r\n    String type = \"mytype\";\r\n    String[] equivalent = { \"I WİLL USE TURKİSH CASING\", \"ı will use turkish casıng\" };\r\n    XContentBuilder builder = jsonBuilder().startObject().startObject(\"properties\").startObject(\"collate\").field(\"type\", \"icu_collation_keyword\").field(\"language\", \"tr\").field(\"strength\", \"primary\").endObject().endObject().endObject();\r\n    assertAcked(client().admin().indices().prepareCreate(index).addMapping(type, builder));\r\n    indexRandom(true, client().prepareIndex(index, type, \"1\").setSource(\"{\\\"collate\\\":\\\"\" + equivalent[0] + \"\\\"}\", XContentType.JSON), client().prepareIndex(index, type, \"2\").setSource(\"{\\\"collate\\\":\\\"\" + equivalent[1] + \"\\\"}\", XContentType.JSON));\r\n    SearchRequest request = new SearchRequest().indices(index).types(type).source(// secondary sort should kick in because both will collate to same value\r\n    new SearchSourceBuilder().fetchSource(false).query(QueryBuilders.termQuery(\"collate\", randomBoolean() ? equivalent[0] : equivalent[1])).sort(\"collate\").sort(\"_id\", SortOrder.DESC));\r\n    SearchResponse response = client().search(request).actionGet();\r\n    assertNoFailures(response);\r\n    assertHitCount(response, 2L);\r\n    assertOrderedSearchHits(response, \"2\", \"1\");\r\n}"
}, {
	"Path": "org.elasticsearch.client.MachineLearningClient.getCalendarEventsAsync",
	"Comment": "gets the events for a a machine learning calendar asynchronously, notifies the listener on completionfor additional infoseeget calendar events api",
	"Method": "void getCalendarEventsAsync(GetCalendarEventsRequest request,RequestOptions options,ActionListener<GetCalendarEventsResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(request, MLRequestConverters::getCalendarEvents, options, GetCalendarEventsResponse::fromXContent, listener, Collections.emptySet());\r\n}"
}, {
	"Path": "org.elasticsearch.client.LicenseClient.getLicenseAsync",
	"Comment": "asynchronously returns the current license for the cluster cluster.",
	"Method": "void getLicenseAsync(GetLicenseRequest request,RequestOptions options,ActionListener<GetLicenseResponse> listener){\r\n    restHighLevelClient.performRequestAsync(request, LicenseRequestConverters::getLicense, options, response -> new GetLicenseResponse(convertResponseToJson(response)), listener, emptySet());\r\n}"
}, {
	"Path": "org.nd4j.linalg.memory.abstracts.Nd4jWorkspace.notifyScopeBorrowed",
	"Comment": "this method temporary enters this workspace, without reset applied",
	"Method": "MemoryWorkspace notifyScopeBorrowed(){\r\n    if (isBorrowed.get())\r\n        throw new ND4JIllegalStateException(\"Workspace [\" + id + \"]: Can't borrow from borrowed workspace\");\r\n    borrowingWorkspace = Nd4j.getMemoryManager().getCurrentWorkspace();\r\n    isBorrowed.set(true);\r\n    Nd4j.getMemoryManager().setCurrentWorkspace(this);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.painless.PainlessScript.convertToScriptException",
	"Comment": "adds stack trace and other useful information to exceptions thrownfrom a painless script.",
	"Method": "ScriptException convertToScriptException(Throwable t,Map<String, List<String>> extraMetadata){\r\n    List<String> scriptStack = new ArrayList();\r\n    for (StackTraceElement element : t.getStackTrace()) {\r\n        if (WriterConstants.CLASS_NAME.equals(element.getClassName())) {\r\n            int offset = element.getLineNumber();\r\n            if (offset == -1) {\r\n                scriptStack.add(\"<<< unknown portion of script >>>\");\r\n            } else {\r\n                offset--;\r\n                int startOffset = getPreviousStatement(offset);\r\n                if (startOffset == -1) {\r\n                    assert false;\r\n                    startOffset = 0;\r\n                }\r\n                int endOffset = getNextStatement(startOffset);\r\n                if (endOffset == -1) {\r\n                    endOffset = getSource().length();\r\n                }\r\n                String snippet = getSource().substring(startOffset, endOffset);\r\n                scriptStack.add(snippet);\r\n                StringBuilder pointer = new StringBuilder();\r\n                for (int i = startOffset; i < offset; i++) {\r\n                    pointer.append(' ');\r\n                }\r\n                pointer.append(\"^---- HERE\");\r\n                scriptStack.add(pointer.toString());\r\n            }\r\n            break;\r\n        } else if (!shouldFilter(element)) {\r\n            scriptStack.add(element.toString());\r\n        }\r\n    }\r\n    ScriptException scriptException = new ScriptException(\"runtime error\", t, scriptStack, getName(), PainlessScriptEngine.NAME);\r\n    for (Map.Entry<String, List<String>> entry : extraMetadata.entrySet()) {\r\n        scriptException.addMetadata(entry.getKey(), entry.getValue());\r\n    }\r\n    return scriptException;\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.bilinearProducts",
	"Comment": "returns a column vector where each entry is the nth bilinearproduct of the nth slices of the two tensors.",
	"Method": "INDArray bilinearProducts(INDArray curr,INDArray in){\r\n    return INSTANCE.bilinearProducts(curr, in);\r\n}"
}, {
	"Path": "org.elasticsearch.painless.lookup.PainlessCast.boxOriginalType",
	"Comment": "create a cast where the original type will be boxed, and then the cast will be performed.",
	"Method": "PainlessCast boxOriginalType(Class<?> originalType,Class<?> targetType,boolean explicitCast,Class<?> boxOriginalType){\r\n    Objects.requireNonNull(originalType);\r\n    Objects.requireNonNull(targetType);\r\n    Objects.requireNonNull(boxOriginalType);\r\n    return new PainlessCast(originalType, targetType, explicitCast, null, null, boxOriginalType, null);\r\n}"
}, {
	"Path": "org.nd4j.linalg.ops.transforms.Transforms.normalizeZeroMeanAndUnitVariance",
	"Comment": "normalize data to zero mean and unit variancesubstract by the mean and divide by the standard deviation",
	"Method": "INDArray normalizeZeroMeanAndUnitVariance(INDArray toNormalize){\r\n    INDArray columnMeans = toNormalize.mean(0);\r\n    INDArray columnStds = toNormalize.std(0);\r\n    toNormalize.subiRowVector(columnMeans);\r\n    columnStds.addi(Nd4j.EPS_THRESHOLD);\r\n    toNormalize.diviRowVector(columnStds);\r\n    return toNormalize;\r\n}"
}, {
	"Path": "org.nd4j.aeron.ipc.AeronNDArraySubscriber.startSubscriber",
	"Comment": "start a subscriber in another threadbased on the given parameters",
	"Method": "AeronNDArraySubscriber startSubscriber(Aeron aeron,String host,int port,NDArrayCallback callback,int streamId,AtomicBoolean running,AeronNDArraySubscriber startSubscriber,Aeron.Context context,String host,int port,NDArrayCallback callback,int streamId,AtomicBoolean running){\r\n    AeronNDArraySubscriber subscriber = AeronNDArraySubscriber.builder().streamId(streamId).ctx(context).channel(AeronUtil.aeronChannel(host, port)).running(running).ndArrayCallback(callback).build();\r\n    Thread t = new Thread(() -> {\r\n        try {\r\n            subscriber.launch();\r\n        } catch (Exception e) {\r\n            e.printStackTrace();\r\n        }\r\n    });\r\n    t.start();\r\n    return subscriber;\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.hstack",
	"Comment": "concatenates two matrices horizontally. matrices must have identicalnumbers of rows.",
	"Method": "INDArray hstack(INDArray arrs,INDArray hstack,Collection<INDArray> arrs){\r\n    INDArray[] arrays = arrs.toArray(new INDArray[0]);\r\n    INDArray ret = INSTANCE.hstack(arrays);\r\n    logCreationIfNecessary(ret);\r\n    return ret;\r\n}"
}, {
	"Path": "org.elasticsearch.script.mustache.MultiSearchTemplateRequest.add",
	"Comment": "add a search template request to execute. note, the order is important, the search response will be returned in thesame order as the search requests.",
	"Method": "MultiSearchTemplateRequest add(SearchTemplateRequestBuilder request,MultiSearchTemplateRequest add,SearchTemplateRequest request){\r\n    requests.add(request);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.jita.handler.impl.CudaZeroHandler.getAllocationStatistics",
	"Comment": "this method returns total amount of memory allocated within system",
	"Method": "Table<AllocationStatus, Integer, Long> getAllocationStatistics(){\r\n    Table<AllocationStatus, Integer, Long> table = HashBasedTable.create();\r\n    table.put(AllocationStatus.HOST, 0, zeroUseCounter.get());\r\n    for (Integer deviceId : configuration.getAvailableDevices()) {\r\n        table.put(AllocationStatus.DEVICE, deviceId, getAllocatedDeviceMemory(deviceId));\r\n    }\r\n    return table;\r\n}"
}, {
	"Path": "org.nd4j.linalg.indexing.Indices.isScalar",
	"Comment": "check if the given indexesover the specified arrayare searching for a scalar",
	"Method": "boolean isScalar(INDArray indexOver,INDArrayIndex indexes){\r\n    boolean allOneLength = true;\r\n    for (int i = 0; i < indexes.length; i++) {\r\n        allOneLength = allOneLength && indexes[i].length() == 1;\r\n    }\r\n    int numNewAxes = NDArrayIndex.numNewAxis(indexes);\r\n    if (allOneLength && numNewAxes == 0 && indexes.length == indexOver.rank())\r\n        return true;\r\n    else if (allOneLength && indexes.length == indexOver.rank() - numNewAxes) {\r\n        return allOneLength;\r\n    }\r\n    return allOneLength;\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AtomicAllocator.initHostCollectors",
	"Comment": "this method executes preconfigured number of host memory garbage collectors",
	"Method": "void initHostCollectors(){\r\n    for (int i = 0; i < configuration.getNumberOfGcThreads(); i++) {\r\n        ReferenceQueue<BaseDataBuffer> queue = new ReferenceQueue();\r\n        UnifiedGarbageCollectorThread uThread = new UnifiedGarbageCollectorThread(i, queue);\r\n        Nd4j.getAffinityManager().attachThreadToDevice(uThread, getDeviceId());\r\n        queueMap.put(i, queue);\r\n        uThread.start();\r\n        collectorsUnified.put(i, uThread);\r\n    }\r\n}"
}, {
	"Path": "org.nd4j.jita.handler.impl.CudaZeroHandler.getAllocatedHostObjects",
	"Comment": "this method returns total number of allocated objects in host memory",
	"Method": "long getAllocatedHostObjects(Long bucketId,long getAllocatedHostObjects){\r\n    AtomicLong counter = new AtomicLong(0);\r\n    for (Long threadId : zeroAllocations.keySet()) {\r\n        counter.addAndGet(zeroAllocations.get(threadId).size());\r\n    }\r\n    return counter.get();\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractAsyncBulkByScrollAction.setScroll",
	"Comment": "set the last returned scrollid. exists entirely for testing.",
	"Method": "void setScroll(String scroll){\r\n    scrollSource.setScroll(scroll);\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.vstack",
	"Comment": "concatenates two matrices vertically. matrices must have identicalnumbers of columns.",
	"Method": "INDArray vstack(INDArray arrs,INDArray vstack,Collection<INDArray> arrs){\r\n    INDArray[] arrays = arrs.toArray(new INDArray[0]);\r\n    INDArray ret = INSTANCE.vstack(arrays);\r\n    logCreationIfNecessary(ret);\r\n    return ret;\r\n}"
}, {
	"Path": "org.nd4j.aeron.ipc.chunk.InMemoryChunkAccumulator.reassemble",
	"Comment": "reassemble an ndarray messagefrom a set of chunksnote that once reassemble is called,the associated chunk lists will automaticallybe removed from storage.",
	"Method": "NDArrayMessage reassemble(String id){\r\n    List<NDArrayMessageChunk> chunkList = chunks.get(id);\r\n    if (chunkList.size() != chunkList.get(0).getNumChunks())\r\n        throw new IllegalStateException(\"Unable to reassemble message chunk \" + id + \" missing \" + (chunkList.get(0).getNumChunks() - chunkList.size()) + \"chunks\");\r\n    NDArrayMessageChunk[] inOrder = new NDArrayMessageChunk[chunkList.size()];\r\n    for (NDArrayMessageChunk chunk : chunkList) {\r\n        inOrder[chunk.getChunkIndex()] = chunk;\r\n    }\r\n    NDArrayMessage message = NDArrayMessage.fromChunks(inOrder);\r\n    chunkList.clear();\r\n    chunks.remove(id);\r\n    return message;\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.connectException",
	"Comment": "this method is called when an attempt to connect a channel throws an exception.",
	"Method": "void connectException(SocketChannelContext context,Exception exception){\r\n    context.handleException(exception);\r\n}"
}, {
	"Path": "org.nd4j.linalg.ops.transforms.Transforms.floor",
	"Comment": "binary matrix of whether the number at a given index is greater than",
	"Method": "INDArray floor(INDArray ndArray,INDArray floor,INDArray ndArray,boolean dup){\r\n    return exec(dup ? new Floor(ndArray.dup()) : new Floor(ndArray));\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.impl.AtomicAllocator.freeMemory",
	"Comment": "this method releases memory allocated for this allocation point",
	"Method": "void freeMemory(AllocationPoint point){\r\n    if (point.getAllocationStatus() == AllocationStatus.DEVICE) {\r\n        this.getMemoryHandler().getMemoryProvider().free(point);\r\n        point.setAllocationStatus(AllocationStatus.HOST);\r\n        this.getMemoryHandler().getMemoryProvider().free(point);\r\n        this.getMemoryHandler().forget(point, AllocationStatus.DEVICE);\r\n    } else {\r\n        this.getMemoryHandler().getMemoryProvider().free(point);\r\n        this.getMemoryHandler().forget(point, AllocationStatus.HOST);\r\n    }\r\n    allocationsMap.remove(point.getObjectId());\r\n}"
}, {
	"Path": "io.dropwizard.migrations.DbDumpCommandTest.assertInsertData",
	"Comment": "assert a correctness of a change set with insertion data into a table",
	"Method": "void assertInsertData(Element changeSet){\r\n    final Element insert = getFirstElement(changeSet, \"insert\");\r\n    assertThat(insert.getAttribute(\"catalogName\")).isEqualTo(\"TEST-DB\");\r\n    assertThat(insert.getAttribute(\"schemaName\")).isEqualTo(\"PUBLIC\");\r\n    assertThat(insert.getAttribute(\"tableName\")).isEqualTo(\"PERSONS\");\r\n    final NodeList columns = insert.getElementsByTagName(\"column\");\r\n    final Element idColumn = (Element) columns.item(0);\r\n    assertThat(idColumn.getAttribute(\"name\")).isEqualTo(\"ID\");\r\n    assertThat(idColumn.getAttribute(\"valueNumeric\")).isEqualTo(\"1\");\r\n    final Element nameColumn = (Element) columns.item(1);\r\n    assertThat(nameColumn.getAttribute(\"name\")).isEqualTo(\"NAME\");\r\n    assertThat(nameColumn.getAttribute(\"value\")).isEqualTo(\"Bill Smith\");\r\n    final Element emailColumn = (Element) columns.item(2);\r\n    assertThat(emailColumn.getAttribute(\"name\")).isEqualTo(\"EMAIL\");\r\n    assertThat(emailColumn.getAttribute(\"value\")).isEqualTo(\"bill@smith.me\");\r\n}"
}, {
	"Path": "org.nd4j.linalg.factory.Nd4j.randomBinomial",
	"Comment": "fill the target array with random values generated according to a binomial distribution with the specifiednumber of trials and probability",
	"Method": "INDArray randomBinomial(int nTrials,double p,long shape,INDArray randomBinomial,int nTrials,double p,INDArray target){\r\n    Preconditions.checkArgument(p >= 0 && p <= 1.0, \"Invalid probability: must be in range 0 to 1, got %s\", p);\r\n    Preconditions.checkArgument(nTrials >= 0, \"Number of trials must be positive: got %s\", nTrials);\r\n    return Nd4j.getExecutioner().exec(new BinomialDistribution(target, nTrials, p));\r\n}"
}, {
	"Path": "io.dropwizard.cli.Command.onError",
	"Comment": "method is called if there is an issue parsing configuration, setting up theenvironment, or running the command itself. the default is printing the stacktraceto facilitate debugging, but can be customized per command.",
	"Method": "void onError(Cli cli,Namespace namespace,Throwable e){\r\n    e.printStackTrace(cli.getStdErr());\r\n}"
}, {
	"Path": "org.elasticsearch.client.RestHighLevelClient.explain",
	"Comment": "executes a request using the explain api.see explain api on elastic.co",
	"Method": "ExplainResponse explain(ExplainRequest explainRequest,RequestOptions options){\r\n    return performRequest(explainRequest, RequestConverters::explain, options, response -> {\r\n        CheckedFunction<XContentParser, ExplainResponse, IOException> entityParser = parser -> ExplainResponse.fromXContent(parser, convertExistsResponse(response));\r\n        return parseEntity(response.getEntity(), entityParser);\r\n    }, singleton(404));\r\n}"
}, {
	"Path": "org.elasticsearch.client.IndicesClient.getAliasAsync",
	"Comment": "asynchronously gets one or more aliases using the get index aliases api.seeindices aliases api onelastic.co",
	"Method": "void getAliasAsync(GetAliasesRequest getAliasesRequest,RequestOptions options,ActionListener<GetAliasesResponse> listener){\r\n    restHighLevelClient.performRequestAsyncAndParseEntity(getAliasesRequest, IndicesRequestConverters::getAlias, options, GetAliasesResponse::fromXContent, listener, singleton(RestStatus.NOT_FOUND.getStatus()));\r\n}"
}, {
	"Path": "org.nd4j.linalg.compression.BasicNDArrayCompressor.compress",
	"Comment": "this method returns compressed indarray instance which contains jvm array passed in",
	"Method": "DataBuffer compress(DataBuffer buffer,DataBuffer compress,DataBuffer buffer,String algorithm,INDArray compress,INDArray array,INDArray compress,INDArray array,String algorithm,INDArray compress,float[] array,INDArray compress,double[] array){\r\n    return codecs.get(defaultCompression).compress(array);\r\n}"
}, {
	"Path": "org.nd4j.parameterserver.distributed.logic.routing.InterleavedRouterTest.assignTarget1",
	"Comment": "testing default assignment for everything, but training requests",
	"Method": "void assignTarget1(){\r\n    InterleavedRouter router = new InterleavedRouter();\r\n    router.init(configuration, transport);\r\n    for (int i = 0; i < 100; i++) {\r\n        VoidMessage message = new InitializationRequestMessage(100, 10, 123, false, false, 10);\r\n        int target = router.assignTarget(message);\r\n        assertTrue(target >= 0 && target <= 3);\r\n        assertEquals(originator, message.getOriginatorId());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.nio.EventHandler.readException",
	"Comment": "this method is called when an attempt to read from a channel throws an exception.",
	"Method": "void readException(SocketChannelContext context,Exception exception){\r\n    context.handleException(exception);\r\n}"
}, {
	"Path": "org.nd4j.compression.impl.Int8.getDescriptor",
	"Comment": "this method returns compression descriptor. it should be unique for any compressor implementation",
	"Method": "String getDescriptor(){\r\n    return \"INT8\";\r\n}"
}, {
	"Path": "org.elasticsearch.painless.CompilerSettings.isPicky",
	"Comment": "returns true if the compiler should be picky. this means it runs slower and enables additionalruntime checks, throwing an exception if there are ambiguities in the grammar or other low levelparsing problems.",
	"Method": "boolean isPicky(){\r\n    return picky;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.settings.ClusterUpdateSettingsRequestBuilder.setTransientSettings",
	"Comment": "sets the transient settings to be updated. they will not survive a full cluster restart",
	"Method": "ClusterUpdateSettingsRequestBuilder setTransientSettings(Settings settings,ClusterUpdateSettingsRequestBuilder setTransientSettings,Settings.Builder settings,ClusterUpdateSettingsRequestBuilder setTransientSettings,String settings,XContentType xContentType,ClusterUpdateSettingsRequestBuilder setTransientSettings,Map<String, ?> settings){\r\n    request.transientSettings(settings);\r\n    return this;\r\n}"
}, {
	"Path": "org.nd4j.jita.allocator.time.impl.SimpleTimer.getFrequencyOfEvents",
	"Comment": "this method returns average frequency of events happened within predefined timeframe",
	"Method": "double getFrequencyOfEvents(){\r\n    return getNumberOfEvents() / (double) TimeUnit.SECONDS.convert(timeframe, TimeUnit.MILLISECONDS);\r\n}"
}, {
	"Path": "org.nd4j.linalg.util.MathUtils.determinationCoefficient",
	"Comment": "this returns the determination coefficient of two vectors given a length",
	"Method": "double determinationCoefficient(double[] y1,double[] y2,int n){\r\n    return Math.pow(correlation(y1, y2), 2);\r\n}"
}]