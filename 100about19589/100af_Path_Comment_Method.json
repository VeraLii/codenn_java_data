[{
	"Path": "org.elasticsearch.index.translog.TestTranslog.getCurrentTerm",
	"Comment": "returns the primary term associated with the current translog writer of the given translog.",
	"Method": "long getCurrentTerm(Translog translog){\r\n    return translog.getCurrent().getPrimaryTerm();\r\n}"
}, {
	"Path": "org.elasticsearch.transport.ConnectionProfile.getConnectTimeout",
	"Comment": "returns the connect timeout or null if no explicit timeout is set on this profile.",
	"Method": "TimeValue getConnectTimeout(){\r\n    return connectTimeout;\r\n}"
}, {
	"Path": "org.elasticsearch.nodesinfo.NodeInfoStreamingTests.assertExpectedUnchanged",
	"Comment": "once we start changing them between versions this method has to be changed as well",
	"Method": "void assertExpectedUnchanged(NodeInfo nodeInfo,NodeInfo readNodeInfo){\r\n    assertThat(nodeInfo.getBuild().toString(), equalTo(readNodeInfo.getBuild().toString()));\r\n    assertThat(nodeInfo.getHostname(), equalTo(readNodeInfo.getHostname()));\r\n    assertThat(nodeInfo.getVersion(), equalTo(readNodeInfo.getVersion()));\r\n    compareJsonOutput(nodeInfo.getHttp(), readNodeInfo.getHttp());\r\n    compareJsonOutput(nodeInfo.getJvm(), readNodeInfo.getJvm());\r\n    compareJsonOutput(nodeInfo.getProcess(), readNodeInfo.getProcess());\r\n    compareJsonOutput(nodeInfo.getSettings(), readNodeInfo.getSettings());\r\n    compareJsonOutput(nodeInfo.getThreadPool(), readNodeInfo.getThreadPool());\r\n    compareJsonOutput(nodeInfo.getTransport(), readNodeInfo.getTransport());\r\n    compareJsonOutput(nodeInfo.getNode(), readNodeInfo.getNode());\r\n    compareJsonOutput(nodeInfo.getOs(), readNodeInfo.getOs());\r\n    compareJsonOutput(nodeInfo.getPlugins(), readNodeInfo.getPlugins());\r\n    compareJsonOutput(nodeInfo.getIngest(), readNodeInfo.getIngest());\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.ReplicaShardAllocatorTests.testSyncIdMatch",
	"Comment": "verifies that when there is a sync id match but no files match, we allocate it to matching node.",
	"Method": "void testSyncIdMatch(){\r\n    RoutingAllocation allocation = onePrimaryOnNode1And1Replica(yesAllocationDeciders());\r\n    DiscoveryNode nodeToMatch = randomBoolean() ? node2 : node3;\r\n    testAllocator.addData(node1, \"MATCH\", new StoreFileMetaData(\"file1\", 10, \"MATCH_CHECKSUM\", MIN_SUPPORTED_LUCENE_VERSION)).addData(nodeToMatch, \"MATCH\", new StoreFileMetaData(\"file1\", 10, \"NO_MATCH_CHECKSUM\", MIN_SUPPORTED_LUCENE_VERSION));\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo(nodeToMatch.getId()));\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.DelayedAllocationIT.testDelayedAllocationChangeWithSettingTo100ms",
	"Comment": "verify that when explicitly changing the value of the index setting for the delayedallocation to a very small value, it kicks the allocation of the unassigned shardeven though the node it was hosted on will not come back.",
	"Method": "void testDelayedAllocationChangeWithSettingTo100ms(){\r\n    internalCluster().startNodes(3);\r\n    prepareCreate(\"test\").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1).put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), TimeValue.timeValueHours(1))).get();\r\n    ensureGreen(\"test\");\r\n    indexRandomData();\r\n    internalCluster().stopRandomNode(InternalTestCluster.nameFilter(findNodeWithShard()));\r\n    assertBusy(() -> assertThat(client().admin().cluster().prepareState().all().get().getState().getRoutingNodes().unassigned().size() > 0, equalTo(true)));\r\n    assertThat(client().admin().cluster().prepareHealth().get().getDelayedUnassignedShards(), equalTo(1));\r\n    assertAcked(client().admin().indices().prepareUpdateSettings(\"test\").setSettings(Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), TimeValue.timeValueMillis(100))).get());\r\n    ensureGreen(\"test\");\r\n    assertThat(client().admin().cluster().prepareHealth().get().getDelayedUnassignedShards(), equalTo(0));\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.GatewayIndexStateIT.testRecoverMissingAnalyzer",
	"Comment": "this test really tests worst case scenario where we have a missing analyzer setting.in that case we now have the ability to check the index on local recovery from diskif it is sane and if we can successfully create an indexservice.this also includes plugins etc.",
	"Method": "void testRecoverMissingAnalyzer(){\r\n    logger.info(\"--> starting one node\");\r\n    internalCluster().startNode();\r\n    prepareCreate(\"test\").setSettings(Settings.builder().put(\"index.analysis.analyzer.test.tokenizer\", \"standard\").put(\"index.number_of_shards\", \"1\")).addMapping(\"type1\", \"{\\n\" + \"    \\\"type1\\\": {\\n\" + \"      \\\"properties\\\": {\\n\" + \"        \\\"field1\\\": {\\n\" + \"          \\\"type\\\": \\\"text\\\",\\n\" + \"          \\\"analyzer\\\": \\\"test\\\"\\n\" + \"        }\\n\" + \"      }\\n\" + \"    }\\n\" + \"  }}\", XContentType.JSON).get();\r\n    logger.info(\"--> indexing a simple document\");\r\n    client().prepareIndex(\"test\", \"type1\", \"1\").setSource(\"field1\", \"value one\").setRefreshPolicy(IMMEDIATE).get();\r\n    logger.info(\"--> waiting for green status\");\r\n    if (usually()) {\r\n        ensureYellow();\r\n    } else {\r\n        internalCluster().startNode();\r\n        client().admin().cluster().health(Requests.clusterHealthRequest().waitForGreenStatus().waitForEvents(Priority.LANGUID).waitForNoRelocatingShards(true).waitForNodes(\"2\")).actionGet();\r\n    }\r\n    ClusterState state = client().admin().cluster().prepareState().get().getState();\r\n    IndexMetaData metaData = state.getMetaData().index(\"test\");\r\n    for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {\r\n        IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(metaData.getSettings().filter((s) -> \"index.analysis.analyzer.test.tokenizer\".equals(s) == false)).build();\r\n        IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()));\r\n    }\r\n    internalCluster().fullRestart();\r\n    ensureGreen(metaData.getIndex().getName());\r\n    state = client().admin().cluster().prepareState().get().getState();\r\n    assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState());\r\n    ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen(\"test\").get());\r\n    assertEquals(ex.getMessage(), \"Failed to verify index \" + metaData.getIndex());\r\n    assertNotNull(ex.getCause());\r\n    assertEquals(MapperParsingException.class, ex.getCause().getClass());\r\n    assertThat(ex.getCause().getMessage(), containsString(\"analyzer [test] not found for field [field1]\"));\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShardTests.indexOnReplicaWithGaps",
	"Comment": "index on the specified shard while introducing sequence number gaps.",
	"Method": "Result indexOnReplicaWithGaps(IndexShard indexShard,int operations,int offset){\r\n    int localCheckpoint = offset;\r\n    int max = offset;\r\n    boolean gap = false;\r\n    for (int i = offset + 1; i < operations; i++) {\r\n        if (!rarely() || i == operations - 1) {\r\n            final String id = Integer.toString(i);\r\n            SourceToParse sourceToParse = SourceToParse.source(indexShard.shardId().getIndexName(), \"_doc\", id, new BytesArray(\"{}\"), XContentType.JSON);\r\n            indexShard.applyIndexOperationOnReplica(i, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse);\r\n            if (!gap && i == localCheckpoint + 1) {\r\n                localCheckpoint++;\r\n            }\r\n            max = i;\r\n        } else {\r\n            gap = true;\r\n        }\r\n        if (rarely()) {\r\n            indexShard.flush(new FlushRequest());\r\n        }\r\n    }\r\n    assert localCheckpoint == indexShard.getLocalCheckpoint();\r\n    assert !gap || (localCheckpoint != max);\r\n    return new Result(localCheckpoint, max);\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.aggregations",
	"Comment": "gets the bytes representing the aggregation builders for this request.",
	"Method": "AggregatorFactories.Builder aggregations(){\r\n    return aggregations;\r\n}"
}, {
	"Path": "org.elasticsearch.license.XPackLicenseState.isDeprecationAllowed",
	"Comment": "deprecation apis are always allowed as long as there is an active license",
	"Method": "boolean isDeprecationAllowed(){\r\n    return status.active;\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractWireTestCase.testSerialization",
	"Comment": "test serialization and deserialization of the test instance.",
	"Method": "void testSerialization(){\r\n    for (int runs = 0; runs < NUMBER_OF_TEST_RUNS; runs++) {\r\n        T testInstance = createTestInstance();\r\n        assertSerialization(testInstance);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.rounding.TimeZoneRoundingTests.testDST_America_St_Johns",
	"Comment": "test for a time zone whose days overlap because the clocks are set back across midnight at the end of dst.",
	"Method": "void testDST_America_St_Johns(){\r\n    DateTimeUnit timeUnit = DateTimeUnit.DAY_OF_MONTH;\r\n    DateTimeZone tz = DateTimeZone.forID(\"America/St_Johns\");\r\n    Rounding rounding = new TimeUnitRounding(timeUnit, tz);\r\n    {\r\n        long timeBeforeFirstMidnight = time(\"2006-10-28T23:30:00.000-02:30\");\r\n        long floor = rounding.round(timeBeforeFirstMidnight);\r\n        assertThat(floor, isDate(time(\"2006-10-28T00:00:00.000-02:30\"), tz));\r\n        long ceiling = rounding.nextRoundingValue(timeBeforeFirstMidnight);\r\n        assertThat(ceiling, isDate(time(\"2006-10-29T00:00:00.000-02:30\"), tz));\r\n        assertInterval(floor, timeBeforeFirstMidnight, ceiling, rounding, tz);\r\n    }\r\n    {\r\n        long timeBetweenMidnights = time(\"2006-10-29T00:00:30.000-02:30\");\r\n        long floor = rounding.round(timeBetweenMidnights);\r\n        assertThat(floor, isDate(time(\"2006-10-29T00:00:00.000-02:30\"), tz));\r\n        long ceiling = rounding.nextRoundingValue(timeBetweenMidnights);\r\n        assertThat(ceiling, isDate(time(\"2006-10-30T00:00:00.000-03:30\"), tz));\r\n        assertInterval(floor, timeBetweenMidnights, ceiling, rounding, tz);\r\n    }\r\n    {\r\n        long timeBetweenMidnights = time(\"2006-10-28T23:30:00.000-03:30\");\r\n        long floor = rounding.round(timeBetweenMidnights);\r\n        assertThat(floor, isDate(time(\"2006-10-28T00:00:00.000-02:30\"), tz));\r\n        long ceiling = rounding.nextRoundingValue(timeBetweenMidnights);\r\n        assertThat(ceiling, isDate(time(\"2006-10-29T00:00:00.000-02:30\"), tz));\r\n        assertInterval(floor, timeBetweenMidnights, ceiling, rounding, tz);\r\n    }\r\n    {\r\n        long timeAfterSecondMidnight = time(\"2006-10-29T06:00:00.000-03:30\");\r\n        long floor = rounding.round(timeAfterSecondMidnight);\r\n        assertThat(floor, isDate(time(\"2006-10-29T00:00:00.000-02:30\"), tz));\r\n        long ceiling = rounding.nextRoundingValue(timeAfterSecondMidnight);\r\n        assertThat(ceiling, isDate(time(\"2006-10-30T00:00:00.000-03:30\"), tz));\r\n        assertInterval(floor, timeAfterSecondMidnight, ceiling, rounding, tz);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.profile.query.QueryProfilerIT.testBool",
	"Comment": "this test verifies that the output is reasonable for a nested query",
	"Method": "void testBool(){\r\n    createIndex(\"test\");\r\n    ensureGreen();\r\n    int numDocs = randomIntBetween(100, 150);\r\n    IndexRequestBuilder[] docs = new IndexRequestBuilder[numDocs];\r\n    for (int i = 0; i < numDocs; i++) {\r\n        docs[i] = client().prepareIndex(\"test\", \"type1\", String.valueOf(i)).setSource(\"field1\", English.intToEnglish(i), \"field2\", i);\r\n    }\r\n    indexRandom(true, docs);\r\n    QueryBuilder q = QueryBuilders.boolQuery().must(QueryBuilders.matchQuery(\"field1\", \"one\")).must(QueryBuilders.matchQuery(\"field1\", \"two\"));\r\n    SearchResponse resp = client().prepareSearch().setQuery(q).setProfile(true).setSearchType(SearchType.QUERY_THEN_FETCH).execute().actionGet();\r\n    Map<String, ProfileShardResult> p = resp.getProfileResults();\r\n    assertNotNull(p);\r\n    assertThat(\"Profile response should not be an empty array\", resp.getProfileResults().size(), not(0));\r\n    for (Map.Entry<String, ProfileShardResult> shardResult : resp.getProfileResults().entrySet()) {\r\n        for (QueryProfileShardResult searchProfiles : shardResult.getValue().getQueryProfileResults()) {\r\n            for (ProfileResult result : searchProfiles.getQueryResults()) {\r\n                assertEquals(result.getQueryName(), \"BooleanQuery\");\r\n                assertEquals(result.getLuceneDescription(), \"+field1:one +field1:two\");\r\n                assertThat(result.getTime(), greaterThan(0L));\r\n                assertNotNull(result.getTimeBreakdown());\r\n                assertEquals(result.getProfiledChildren().size(), 2);\r\n                List<ProfileResult> children = result.getProfiledChildren();\r\n                assertEquals(children.size(), 2);\r\n                ProfileResult childProfile = children.get(0);\r\n                assertEquals(childProfile.getQueryName(), \"TermQuery\");\r\n                assertEquals(childProfile.getLuceneDescription(), \"field1:one\");\r\n                assertThat(childProfile.getTime(), greaterThan(0L));\r\n                assertNotNull(childProfile.getTimeBreakdown());\r\n                assertEquals(childProfile.getProfiledChildren().size(), 0);\r\n                childProfile = children.get(1);\r\n                assertEquals(childProfile.getQueryName(), \"TermQuery\");\r\n                assertEquals(childProfile.getLuceneDescription(), \"field1:two\");\r\n                assertThat(childProfile.getTime(), greaterThan(0L));\r\n                assertNotNull(childProfile.getTimeBreakdown());\r\n            }\r\n            CollectorResult result = searchProfiles.getCollectorResult();\r\n            assertThat(result.getName(), not(isEmptyOrNullString()));\r\n            assertThat(result.getTime(), greaterThan(0L));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.internal.ScrollContext.getFromContext",
	"Comment": "returns the object or null if the given key does not have avalue in the context",
	"Method": "T getFromContext(String key){\r\n    return context != null ? (T) context.get(key) : null;\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.phrase.LaplaceModelTests.createMutation",
	"Comment": "mutate the given model so the returned smoothing model is different",
	"Method": "Laplace createMutation(SmoothingModel input){\r\n    Laplace original = (Laplace) input;\r\n    return new Laplace(original.getAlpha() + 0.1);\r\n}"
}, {
	"Path": "org.elasticsearch.common.cache.CacheTests.testInvalidate",
	"Comment": "randomly invalidate some cached entries, then check that a lookup for each of those and only those keys is null",
	"Method": "void testInvalidate(){\r\n    Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();\r\n    for (int i = 0; i < numberOfEntries; i++) {\r\n        cache.put(i, Integer.toString(i));\r\n    }\r\n    Set<Integer> keys = new HashSet();\r\n    for (Integer key : cache.keys()) {\r\n        if (rarely()) {\r\n            cache.invalidate(key);\r\n            keys.add(key);\r\n        }\r\n    }\r\n    for (int i = 0; i < numberOfEntries; i++) {\r\n        if (keys.contains(i)) {\r\n            assertNull(cache.get(i));\r\n        } else {\r\n            assertNotNull(cache.get(i));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.CardinalityIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(cardinality(\"foo\").field(\"d\").script(new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, \"_value\", emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(cardinality(\"foo\").field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.license.CryptUtils.writeEncryptedPrivateKey",
	"Comment": "returns encrypted private key file content with provided passphrase",
	"Method": "byte[] writeEncryptedPrivateKey(PrivateKey privateKey,byte[] writeEncryptedPrivateKey,PrivateKey privateKey,char[] passPhrase){\r\n    PKCS8EncodedKeySpec encodedKeySpec = new PKCS8EncodedKeySpec(privateKey.getEncoded());\r\n    return encrypt(encodedKeySpec.getEncoded(), passPhrase);\r\n}"
}, {
	"Path": "org.elasticsearch.search.fetch.StoredFieldsContext.fetchFields",
	"Comment": "returns true if the stored fields should be fetched, false otherwise.",
	"Method": "boolean fetchFields(){\r\n    return fetchFields;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.InternalAggregation.isMapped",
	"Comment": "return true if this aggregation is mapped, and can lead a reduction.if this agg returnsfalse, it should return itself if asked to lead a reduction",
	"Method": "boolean isMapped(){\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.SizeBlockingQueueTests.testQueueSize",
	"Comment": "tests that the size of a queue remains at most the capacity while offers are made to a queue when at capacity. this test would havepreviously failed when the size of the queue was incremented and exposed externally even though the item offered to the queue wasnever actually added to the queue.",
	"Method": "void testQueueSize(){\r\n    final int capacity = randomIntBetween(1, 32);\r\n    final BlockingQueue<Integer> blockingQueue = new ArrayBlockingQueue(capacity);\r\n    final SizeBlockingQueue<Integer> sizeBlockingQueue = new SizeBlockingQueue(blockingQueue, capacity);\r\n    for (int i = 0; i < capacity; i++) {\r\n        sizeBlockingQueue.offer(i);\r\n    }\r\n    final int iterations = 1 << 16;\r\n    final CyclicBarrier barrier = new CyclicBarrier(2);\r\n    final Thread queueOfferThread = new Thread(() -> {\r\n        for (int i = 0; i < iterations; i++) {\r\n            try {\r\n                barrier.await();\r\n            } catch (final BrokenBarrierException | InterruptedException e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n            sizeBlockingQueue.offer(capacity + i);\r\n        }\r\n    });\r\n    queueOfferThread.start();\r\n    final AtomicInteger maxSize = new AtomicInteger();\r\n    final Thread queueSizeThread = new Thread(() -> {\r\n        for (int i = 0; i < iterations; i++) {\r\n            try {\r\n                barrier.await();\r\n            } catch (final BrokenBarrierException | InterruptedException e) {\r\n                throw new RuntimeException(e);\r\n            }\r\n            maxSize.set(Math.max(maxSize.get(), sizeBlockingQueue.size()));\r\n        }\r\n    });\r\n    queueSizeThread.start();\r\n    queueOfferThread.join();\r\n    queueSizeThread.join();\r\n    assertThat(maxSize.get(), equalTo(capacity));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.DateDerivativeIT.testSingleValuedFieldNormalised_timeZone_CET_DstEnd",
	"Comment": "do a derivative on a date histogram with time zone cet at dst end",
	"Method": "void testSingleValuedFieldNormalised_timeZone_CET_DstEnd(){\r\n    createIndex(IDX_DST_END);\r\n    DateTimeZone timezone = DateTimeZone.forID(\"CET\");\r\n    List<IndexRequestBuilder> builders = new ArrayList();\r\n    addNTimes(1, IDX_DST_END, new DateTime(\"2012-10-27T01:00:00\", timezone), builders);\r\n    addNTimes(2, IDX_DST_END, new DateTime(\"2012-10-28T01:00:00\", timezone), builders);\r\n    addNTimes(3, IDX_DST_END, new DateTime(\"2012-10-29T01:00:00\", timezone), builders);\r\n    addNTimes(4, IDX_DST_END, new DateTime(\"2012-10-30T01:00:00\", timezone), builders);\r\n    indexRandom(true, builders);\r\n    ensureSearchable();\r\n    SearchResponse response = client().prepareSearch(IDX_DST_END).addAggregation(dateHistogram(\"histo\").field(\"date\").dateHistogramInterval(DateHistogramInterval.DAY).timeZone(timezone).minDocCount(0).subAggregation(derivative(\"deriv\", \"_count\").unit(DateHistogramInterval.HOUR))).execute().actionGet();\r\n    assertSearchResponse(response);\r\n    Histogram deriv = response.getAggregations().get(\"histo\");\r\n    assertThat(deriv, notNullValue());\r\n    assertThat(deriv.getName(), equalTo(\"histo\"));\r\n    List<? extends Bucket> buckets = deriv.getBuckets();\r\n    assertThat(buckets.size(), equalTo(4));\r\n    assertBucket(buckets.get(0), new DateTime(\"2012-10-27\", timezone).toDateTime(DateTimeZone.UTC), 1L, nullValue(), null, null);\r\n    assertBucket(buckets.get(1), new DateTime(\"2012-10-28\", timezone).toDateTime(DateTimeZone.UTC), 2L, notNullValue(), 1d, 1d / 24d);\r\n    assertBucket(buckets.get(2), new DateTime(\"2012-10-29\", timezone).toDateTime(DateTimeZone.UTC), 3L, notNullValue(), 1d, 1d / 25d);\r\n    assertBucket(buckets.get(3), new DateTime(\"2012-10-30\", timezone).toDateTime(DateTimeZone.UTC), 4L, notNullValue(), 1d, 1d / 24d);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.TasksIT.resetTaskManageListeners",
	"Comment": "resets all recording task event listeners with the given action mask on all nodes",
	"Method": "void resetTaskManageListeners(String actionMasks){\r\n    for (Map.Entry<Tuple<String, String>, RecordingTaskManagerListener> entry : listeners.entrySet()) {\r\n        if (actionMasks == null || entry.getKey().v2().equals(actionMasks)) {\r\n            entry.getValue().reset();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.RandomObjects.randomSource",
	"Comment": "returns a random source in a given xcontenttype containing a random number of fields, objects and array, with maximum depth 5.the minimum number of fields per object is provided as an argument.",
	"Method": "BytesReference randomSource(Random random,BytesReference randomSource,Random random,XContentType xContentType,BytesReference randomSource,Random random,XContentType xContentType,int minNumFields){\r\n    try (XContentBuilder builder = XContentFactory.contentBuilder(xContentType)) {\r\n        builder.startObject();\r\n        addFields(random, builder, minNumFields, 0);\r\n        builder.endObject();\r\n        return BytesReference.bytes(builder);\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.transport.MockTransportService.addNodeConnectedBehavior",
	"Comment": "adds a node connected behavior that is the default node connected behavior.",
	"Method": "boolean addNodeConnectedBehavior(TransportService transportService,StubbableConnectionManager.NodeConnectedBehavior behavior,boolean addNodeConnectedBehavior,TransportAddress transportAddress,StubbableConnectionManager.NodeConnectedBehavior behavior,boolean addNodeConnectedBehavior,StubbableConnectionManager.NodeConnectedBehavior behavior){\r\n    return connectionManager().setDefaultNodeConnectedBehavior(behavior);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomClusterStateParts",
	"Comment": "takes an existing cluster state and randomly adds, removes or updates a cluster state part using randompart generator.if a new part is added the prefix value is used as a prefix of randomly generated part name.",
	"Method": "ClusterState randomClusterStateParts(ClusterState clusterState,String prefix,RandomClusterPart<T> randomPart){\r\n    ClusterState.Builder builder = ClusterState.builder(clusterState);\r\n    ImmutableOpenMap<String, T> parts = randomPart.parts(clusterState);\r\n    int partCount = parts.size();\r\n    if (partCount > 0) {\r\n        List<String> randomParts = randomSubsetOf(randomInt(partCount - 1), randomPart.parts(clusterState).keys().toArray(String.class));\r\n        for (String part : randomParts) {\r\n            if (randomBoolean()) {\r\n                randomPart.remove(builder, part);\r\n            } else {\r\n                randomPart.put(builder, randomPart.randomChange(parts.get(part)));\r\n            }\r\n        }\r\n    }\r\n    int additionalPartCount = randomIntBetween(1, 20);\r\n    for (int i = 0; i < additionalPartCount; i++) {\r\n        String name = randomName(prefix);\r\n        randomPart.put(builder, randomPart.randomCreate(name));\r\n    }\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.common.io.stream.AbstractWriteableEnumTestCase.assertReadFromStream",
	"Comment": "a convenience method for testing the read of a writeable enum",
	"Method": "void assertReadFromStream(int ordinal,Writeable expected){\r\n    try (BytesStreamOutput out = new BytesStreamOutput()) {\r\n        out.writeVInt(ordinal);\r\n        try (StreamInput in = out.bytes().streamInput()) {\r\n            assertThat(reader.read(in), equalTo(expected));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.process.autodetect.state.ModelSizeStats.getLogTime",
	"Comment": "the wall clock time at the point when this instance was created.",
	"Method": "Date getLogTime(){\r\n    return logTime;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.ensureYellowAndNoInitializingShards",
	"Comment": "ensures the cluster has a yellow state via the cluster health api and ensures the that cluster has no initializing shardsfor the given indices",
	"Method": "ClusterHealthStatus ensureYellowAndNoInitializingShards(String indices){\r\n    return ensureColor(ClusterHealthStatus.YELLOW, TimeValue.timeValueSeconds(30), true, indices);\r\n}"
}, {
	"Path": "org.elasticsearch.script.ScriptServiceTests.testCompilationCircuitBreaking",
	"Comment": "simply by multiplying by five, so even setting it to one, requires five compilations to break",
	"Method": "void testCompilationCircuitBreaking(){\r\n    buildScriptService(Settings.EMPTY);\r\n    scriptService.setMaxCompilationRate(Tuple.tuple(1, TimeValue.timeValueMinutes(1)));\r\n    scriptService.checkCompilationLimit();\r\n    expectThrows(CircuitBreakingException.class, () -> scriptService.checkCompilationLimit());\r\n    scriptService.setMaxCompilationRate(Tuple.tuple(2, TimeValue.timeValueMinutes(1)));\r\n    scriptService.checkCompilationLimit();\r\n    scriptService.checkCompilationLimit();\r\n    expectThrows(CircuitBreakingException.class, () -> scriptService.checkCompilationLimit());\r\n    int count = randomIntBetween(5, 50);\r\n    scriptService.setMaxCompilationRate(Tuple.tuple(count, TimeValue.timeValueMinutes(1)));\r\n    for (int i = 0; i < count; i++) {\r\n        scriptService.checkCompilationLimit();\r\n    }\r\n    expectThrows(CircuitBreakingException.class, () -> scriptService.checkCompilationLimit());\r\n    scriptService.setMaxCompilationRate(Tuple.tuple(0, TimeValue.timeValueMinutes(1)));\r\n    expectThrows(CircuitBreakingException.class, () -> scriptService.checkCompilationLimit());\r\n    scriptService.setMaxCompilationRate(Tuple.tuple(Integer.MAX_VALUE, TimeValue.timeValueMinutes(1)));\r\n    int largeLimit = randomIntBetween(1000, 10000);\r\n    for (int i = 0; i < largeLimit; i++) {\r\n        scriptService.checkCompilationLimit();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.MovAvgPipelineAggregationBuilder.modelBuilder",
	"Comment": "sets a movavgmodel for the moving average. the model is used todefine what type of moving average you want to use on the series",
	"Method": "MovAvgPipelineAggregationBuilder modelBuilder(MovAvgModelBuilder model){\r\n    if (model == null) {\r\n        throw new IllegalArgumentException(\"[model] must not be null: [\" + name + \"]\");\r\n    }\r\n    this.model = model.build();\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.license.RemoteClusterLicenseChecker.containsRemoteIndex",
	"Comment": "predicate to test if the collection of index names contains any that represent the name of a remote index.",
	"Method": "boolean containsRemoteIndex(List<String> indices){\r\n    return indices.stream().anyMatch(RemoteClusterLicenseChecker::isRemoteIndex);\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.ScriptSortBuilderTests.testBuildCorrectComparatorType",
	"Comment": "test that the correct comparator sort is returned, based on the script type",
	"Method": "void testBuildCorrectComparatorType(){\r\n    ScriptSortBuilder sortBuilder = new ScriptSortBuilder(mockScript(MOCK_SCRIPT_NAME), ScriptSortType.STRING);\r\n    SortField sortField = sortBuilder.build(createMockShardContext()).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(BytesRefFieldComparatorSource.class));\r\n    sortBuilder = new ScriptSortBuilder(mockScript(MOCK_SCRIPT_NAME), ScriptSortType.NUMBER);\r\n    sortField = sortBuilder.build(createMockShardContext()).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(DoubleValuesComparatorSource.class));\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.process.autodetect.state.DataCounts.getInvalidDateCount",
	"Comment": "the number of records with an invalid date field that couldnot be parsed or converted to epoch time.",
	"Method": "long getInvalidDateCount(){\r\n    return invalidDateCount;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilderTests.createRandomDecayFunction",
	"Comment": "create a random decay function setting all of its constructor parameters randomly. the caller is responsible for randomizing otherfields.",
	"Method": "DecayFunctionBuilder<?> createRandomDecayFunction(){\r\n    String field = randomFrom(INT_FIELD_NAME, DOUBLE_FIELD_NAME, DATE_FIELD_NAME, GEO_POINT_FIELD_NAME);\r\n    Object origin;\r\n    Object scale;\r\n    Object offset;\r\n    switch(field) {\r\n        case GEO_POINT_FIELD_NAME:\r\n            origin = new GeoPoint(randomDouble(), randomDouble()).geohash();\r\n            scale = randomFrom(DistanceUnit.values()).toString(randomDouble());\r\n            offset = randomFrom(DistanceUnit.values()).toString(randomDouble());\r\n            break;\r\n        case DATE_FIELD_NAME:\r\n            origin = new DateTime(System.currentTimeMillis() - randomIntBetween(0, 1000000), DateTimeZone.UTC).toString();\r\n            scale = randomTimeValue(1, 1000, \"d\", \"h\", \"ms\", \"s\", \"m\");\r\n            offset = randomPositiveTimeValue();\r\n            break;\r\n        default:\r\n            origin = randomBoolean() ? randomInt() : randomFloat();\r\n            scale = randomBoolean() ? between(1, Integer.MAX_VALUE) : randomFloat() + Float.MIN_NORMAL;\r\n            offset = randomBoolean() ? between(1, Integer.MAX_VALUE) : randomFloat() + Float.MIN_NORMAL;\r\n            break;\r\n    }\r\n    offset = randomBoolean() ? null : offset;\r\n    double decay = randomDouble();\r\n    switch(randomIntBetween(0, 2)) {\r\n        case 0:\r\n            return new GaussDecayFunctionBuilder(field, origin, scale, offset, decay);\r\n        case 1:\r\n            return new ExponentialDecayFunctionBuilder(field, origin, scale, offset, decay);\r\n        case 2:\r\n            return new LinearDecayFunctionBuilder(field, origin, scale, offset, decay);\r\n        default:\r\n            throw new UnsupportedOperationException();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.nameFilter",
	"Comment": "returns a predicate that only accepts settings of nodes with one of the given names.",
	"Method": "Predicate<Settings> nameFilter(String nodeName){\r\n    return new NodeNamePredicate(new HashSet(Arrays.asList(nodeName)));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramAggregationBuilder.interval",
	"Comment": "get the current interval in milliseconds that is set on this builder.",
	"Method": "long interval(DateHistogramAggregationBuilder interval,long interval){\r\n    if (interval < 1) {\r\n        throw new IllegalArgumentException(\"[interval] must be 1 or greater for histogram aggregation [\" + name + \"]\");\r\n    }\r\n    this.interval = interval;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.ClusterDisruptionIT.testRejoinDocumentExistsInAllShardCopies",
	"Comment": "test that a document which is indexed on the majority side of a partition, is available from the minority side,once the partition is healed",
	"Method": "void testRejoinDocumentExistsInAllShardCopies(){\r\n    List<String> nodes = startCluster(3);\r\n    assertAcked(prepareCreate(\"test\").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 2)).get());\r\n    ensureGreen(\"test\");\r\n    nodes = new ArrayList(nodes);\r\n    Collections.shuffle(nodes, random());\r\n    String isolatedNode = nodes.get(0);\r\n    String notIsolatedNode = nodes.get(1);\r\n    TwoPartitions partitions = isolateNode(isolatedNode);\r\n    NetworkDisruption scheme = addRandomDisruptionType(partitions);\r\n    scheme.startDisrupting();\r\n    ensureStableCluster(2, notIsolatedNode);\r\n    assertFalse(client(notIsolatedNode).admin().cluster().prepareHealth(\"test\").setWaitForYellowStatus().get().isTimedOut());\r\n    IndexResponse indexResponse = internalCluster().client(notIsolatedNode).prepareIndex(\"test\", \"type\").setSource(\"field\", \"value\").get();\r\n    assertThat(indexResponse.getVersion(), equalTo(1L));\r\n    logger.info(\"Verifying if document exists via node[{}]\", notIsolatedNode);\r\n    GetResponse getResponse = internalCluster().client(notIsolatedNode).prepareGet(\"test\", \"type\", indexResponse.getId()).setPreference(\"_local\").get();\r\n    assertThat(getResponse.isExists(), is(true));\r\n    assertThat(getResponse.getVersion(), equalTo(1L));\r\n    assertThat(getResponse.getId(), equalTo(indexResponse.getId()));\r\n    scheme.stopDisrupting();\r\n    ensureStableCluster(3);\r\n    ensureGreen(\"test\");\r\n    for (String node : nodes) {\r\n        logger.info(\"Verifying if document exists after isolating node[{}] via node[{}]\", isolatedNode, node);\r\n        getResponse = internalCluster().client(node).prepareGet(\"test\", \"type\", indexResponse.getId()).setPreference(\"_local\").get();\r\n        assertThat(getResponse.isExists(), is(true));\r\n        assertThat(getResponse.getVersion(), equalTo(1L));\r\n        assertThat(getResponse.getId(), equalTo(indexResponse.getId()));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractQueryTestCase.testMustRewrite",
	"Comment": "this test ensures that queries that need to be rewritten have dedicated tests.these queries must override this method accordingly.",
	"Method": "void testMustRewrite(){\r\n    QueryShardContext context = createShardContext();\r\n    context.setAllowUnmappedFields(true);\r\n    QB queryBuilder = createTestQueryBuilder();\r\n    queryBuilder.toQuery(context);\r\n}"
}, {
	"Path": "org.elasticsearch.Version.fromString",
	"Comment": "returns the version given its string representation, current version if the argument is null or empty",
	"Method": "Version fromString(String version){\r\n    if (!Strings.hasLength(version)) {\r\n        return Version.CURRENT;\r\n    }\r\n    final boolean snapshot;\r\n    if (snapshot = version.endsWith(\"-SNAPSHOT\")) {\r\n        version = version.substring(0, version.length() - 9);\r\n    }\r\n    String[] parts = version.split(\"[.-]\");\r\n    if (parts.length < 3 || parts.length > 4) {\r\n        throw new IllegalArgumentException(\"the version needs to contain major, minor, and revision, and optionally the build: \" + version);\r\n    }\r\n    try {\r\n        final int rawMajor = Integer.parseInt(parts[0]);\r\n        if (rawMajor >= 5 && snapshot) {\r\n            throw new IllegalArgumentException(\"illegal version format - snapshots are only supported until version 2.x\");\r\n        }\r\n        if (rawMajor >= 7 && parts.length == 4) {\r\n            throw new IllegalArgumentException(\"illegal version format - qualifiers are only supported until version 6.x\");\r\n        }\r\n        final int betaOffset = rawMajor < 5 ? 0 : 25;\r\n        final int major = rawMajor * 1000000;\r\n        final int minor = Integer.parseInt(parts[1]) * 10000;\r\n        final int revision = Integer.parseInt(parts[2]) * 100;\r\n        int build = 99;\r\n        if (parts.length == 4) {\r\n            String buildStr = parts[3];\r\n            if (buildStr.startsWith(\"alpha\")) {\r\n                assert rawMajor >= 5 : \"major must be >= 5 but was \" + major;\r\n                build = Integer.parseInt(buildStr.substring(5));\r\n                assert build < 25 : \"expected a beta build but \" + build + \" >= 25\";\r\n            } else if (buildStr.startsWith(\"Beta\") || buildStr.startsWith(\"beta\")) {\r\n                build = betaOffset + Integer.parseInt(buildStr.substring(4));\r\n                assert build < 50 : \"expected a beta build but \" + build + \" >= 50\";\r\n            } else if (buildStr.startsWith(\"RC\") || buildStr.startsWith(\"rc\")) {\r\n                build = Integer.parseInt(buildStr.substring(2)) + 50;\r\n            } else {\r\n                throw new IllegalArgumentException(\"unable to parse version \" + version);\r\n            }\r\n        }\r\n        return fromId(major + minor + revision + build);\r\n    } catch (NumberFormatException e) {\r\n        throw new IllegalArgumentException(\"unable to parse version \" + version, e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.LoggingTaskListener.instance",
	"Comment": "get the instance of noopactionlistener cast appropriately.",
	"Method": "TaskListener<Response> instance(){\r\n    return (TaskListener<Response>) INSTANCE;\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotShardsService.notifyFailedSnapshotShard",
	"Comment": "notify the master node that the given shard failed to be snapshotted",
	"Method": "void notifyFailedSnapshotShard(Snapshot snapshot,ShardId shardId,String localNodeId,String failure){\r\n    sendSnapshotShardUpdate(snapshot, shardId, new ShardSnapshotStatus(localNodeId, State.FAILED, failure));\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.graph.action.GraphExploreRequestBuilder.maxDocsPerDiversityValue",
	"Comment": "optional number of permitted docs with same value in sampled searchresults. must also declare which field using samplediversityfield",
	"Method": "GraphExploreRequestBuilder maxDocsPerDiversityValue(int max,int maxDocsPerDiversityValue){\r\n    return request.maxDocsPerDiversityValue();\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.composite.DateHistogramValuesSourceBuilder.interval",
	"Comment": "returns the interval in milliseconds that is set on this source",
	"Method": "long interval(DateHistogramValuesSourceBuilder interval,long interval){\r\n    if (interval < 1) {\r\n        throw new IllegalArgumentException(\"[interval] must be 1 or greater for [date_histogram] source\");\r\n    }\r\n    this.interval = interval;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.config.Job.getEstablishedModelMemory",
	"Comment": "the established model memory of the job, or null if modelmemory has not reached equilibrium yet.",
	"Method": "Long getEstablishedModelMemory(){\r\n    return establishedModelMemory;\r\n}"
}, {
	"Path": "org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase",
	"Comment": "try to load the query results from the cache or execute the query phase directly if the cache cannot be used.",
	"Method": "void loadOrExecuteQueryPhase(ShardSearchRequest request,SearchContext context){\r\n    final boolean canCache = indicesService.canCache(request, context);\r\n    context.getQueryShardContext().freezeContext();\r\n    if (canCache) {\r\n        indicesService.loadIntoContext(request, context, queryPhase);\r\n    } else {\r\n        queryPhase.execute(context);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.InternalOrder.isKeyAsc",
	"Comment": "determine if the ordering strategy is sorting on bucket key ascending.",
	"Method": "boolean isKeyAsc(BucketOrder order){\r\n    return isOrder(order, KEY_ASC);\r\n}"
}, {
	"Path": "org.elasticsearch.script.ScriptContext.findMethod",
	"Comment": "returns a method with the given name, or throws an exception if multiple are found.",
	"Method": "Method findMethod(String type,Class<?> clazz,String methodName){\r\n    Method foundMethod = null;\r\n    for (Method method : clazz.getMethods()) {\r\n        if (method.getName().equals(methodName)) {\r\n            if (foundMethod != null) {\r\n                throw new IllegalArgumentException(\"Cannot have multiple \" + methodName + \" methods on \" + type + \" class [\" + clazz.getName() + \"] for script context [\" + name + \"]\");\r\n            }\r\n            foundMethod = method;\r\n        }\r\n    }\r\n    return foundMethod;\r\n}"
}, {
	"Path": "org.elasticsearch.search.SearchHit.getSourceRef",
	"Comment": "returns bytes reference, also uncompress the source if needed.",
	"Method": "BytesReference getSourceRef(){\r\n    if (this.source == null) {\r\n        return null;\r\n    }\r\n    try {\r\n        this.source = CompressorFactory.uncompressIfNeeded(this.source);\r\n        return this.source;\r\n    } catch (IOException e) {\r\n        throw new ElasticsearchParseException(\"failed to decompress source\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregationBuilder.minDocCount",
	"Comment": "set the minimum count of matching documents that buckets need to have and return this builder so that calls can be chained.",
	"Method": "long minDocCount(HistogramAggregationBuilder minDocCount,long minDocCount){\r\n    if (minDocCount < 0) {\r\n        throw new IllegalArgumentException(\"[minDocCount] must be greater than or equal to 0. Found [\" + minDocCount + \"] in [\" + name + \"]\");\r\n    }\r\n    this.minDocCount = minDocCount;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.CorruptedFileIT.testCorruptFileAndRecover",
	"Comment": "tests that we can actually recover from a corruption on the primary given that we have replica shards around.",
	"Method": "void testCorruptFileAndRecover(){\r\n    int numDocs = scaledRandomIntBetween(100, 1000);\r\n    internalCluster().ensureAtLeastNumDataNodes(3);\r\n    if (cluster().numDataNodes() == 3) {\r\n        logger.info(\"--> cluster has [3] data nodes, corrupted primary will be overwritten\");\r\n    }\r\n    assertThat(cluster().numDataNodes(), greaterThanOrEqualTo(3));\r\n    assertAcked(prepareCreate(\"test\").setSettings(// no checkindex - we corrupt shards on purpose\r\n    Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, \"1\").put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, \"1\").put(MergePolicyConfig.INDEX_MERGE_ENABLED, false).put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), // no translog based flush - it might change the .liv / segments.N files\r\n    false).put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB))));\r\n    ensureGreen();\r\n    disableAllocation(\"test\");\r\n    IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs];\r\n    for (int i = 0; i < builders.length; i++) {\r\n        builders[i] = client().prepareIndex(\"test\", \"type\").setSource(\"field\", \"value\");\r\n    }\r\n    indexRandom(true, builders);\r\n    ensureGreen();\r\n    assertAllSuccessful(client().admin().indices().prepareFlush().setForce(true).execute().actionGet());\r\n    SearchResponse countResponse = client().prepareSearch().setSize(0).get();\r\n    assertHitCount(countResponse, numDocs);\r\n    final int numShards = numShards(\"test\");\r\n    ShardRouting corruptedShardRouting = corruptRandomPrimaryFile();\r\n    logger.info(\"--> {} corrupted\", corruptedShardRouting);\r\n    enableAllocation(\"test\");\r\n    Settings build = Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, \"2\").build();\r\n    client().admin().indices().prepareUpdateSettings(\"test\").setSettings(build).get();\r\n    ClusterHealthResponse health = client().admin().cluster().health(// sometimes due to cluster rebalacing and random settings default timeout is just not enough.\r\n    Requests.clusterHealthRequest(\"test\").waitForGreenStatus().timeout(\"5m\").waitForNoRelocatingShards(true)).actionGet();\r\n    if (health.isTimedOut()) {\r\n        logger.info(\"cluster state:\\n{}\\n{}\", client().admin().cluster().prepareState().get().getState(), client().admin().cluster().preparePendingClusterTasks().get());\r\n        assertThat(\"timed out waiting for green state\", health.isTimedOut(), equalTo(false));\r\n    }\r\n    assertThat(health.getStatus(), equalTo(ClusterHealthStatus.GREEN));\r\n    final int numIterations = scaledRandomIntBetween(5, 20);\r\n    for (int i = 0; i < numIterations; i++) {\r\n        SearchResponse response = client().prepareSearch().setSize(numDocs).get();\r\n        assertHitCount(response, numDocs);\r\n    }\r\n    final CountDownLatch latch = new CountDownLatch(numShards * 3);\r\n    final CopyOnWriteArrayList<Exception> exception = new CopyOnWriteArrayList();\r\n    final IndexEventListener listener = new IndexEventListener() {\r\n        @Override\r\n        public void afterIndexShardClosed(ShardId sid, @Nullable IndexShard indexShard, Settings indexSettings) {\r\n            if (indexShard != null) {\r\n                Store store = indexShard.store();\r\n                store.incRef();\r\n                try {\r\n                    if (!Lucene.indexExists(store.directory()) && indexShard.state() == IndexShardState.STARTED) {\r\n                        return;\r\n                    }\r\n                    BytesStreamOutput os = new BytesStreamOutput();\r\n                    PrintStream out = new PrintStream(os, false, StandardCharsets.UTF_8.name());\r\n                    CheckIndex.Status status = store.checkIndex(out);\r\n                    out.flush();\r\n                    if (!status.clean) {\r\n                        logger.warn(\"check index [failure]\\n{}\", os.bytes().utf8ToString());\r\n                        throw new IOException(\"index check failure\");\r\n                    }\r\n                } catch (Exception e) {\r\n                    exception.add(e);\r\n                } finally {\r\n                    store.decRef();\r\n                    latch.countDown();\r\n                }\r\n            }\r\n        }\r\n    };\r\n    for (MockIndexEventListener.TestEventListener eventListener : internalCluster().getDataNodeInstances(MockIndexEventListener.TestEventListener.class)) {\r\n        eventListener.setNewDelegate(listener);\r\n    }\r\n    try {\r\n        client().admin().indices().prepareDelete(\"test\").get();\r\n        latch.await();\r\n        assertThat(exception, empty());\r\n    } finally {\r\n        for (MockIndexEventListener.TestEventListener eventListener : internalCluster().getDataNodeInstances(MockIndexEventListener.TestEventListener.class)) {\r\n            eventListener.setNewDelegate(null);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.CorruptedFileIT.testCorruptFileAndRecover",
	"Comment": "tests that we can actually recover from a corruption on the primary given that we have replica shards around.",
	"Method": "void testCorruptFileAndRecover(){\r\n    if (indexShard != null) {\r\n        Store store = indexShard.store();\r\n        store.incRef();\r\n        try {\r\n            if (!Lucene.indexExists(store.directory()) && indexShard.state() == IndexShardState.STARTED) {\r\n                return;\r\n            }\r\n            BytesStreamOutput os = new BytesStreamOutput();\r\n            PrintStream out = new PrintStream(os, false, StandardCharsets.UTF_8.name());\r\n            CheckIndex.Status status = store.checkIndex(out);\r\n            out.flush();\r\n            if (!status.clean) {\r\n                logger.warn(\"check index [failure]\\n{}\", os.bytes().utf8ToString());\r\n                throw new IOException(\"index check failure\");\r\n            }\r\n        } catch (Exception e) {\r\n            exception.add(e);\r\n        } finally {\r\n            store.decRef();\r\n            latch.countDown();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TransportService.acceptIncomingRequests",
	"Comment": "start accepting incoming requests.when the transport layer starts up it will block any incoming requests untilthis method is called",
	"Method": "void acceptIncomingRequests(){\r\n    blockIncomingRequestsLatch.countDown();\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.FiltersTests.testFiltersSortedByKey",
	"Comment": "test that when passing in keyed filters as list or array, the list stored internally is sorted by keyalso check the list passed in is not modified by this but rather copied",
	"Method": "void testFiltersSortedByKey(){\r\n    KeyedFilter[] original = new KeyedFilter[] { new KeyedFilter(\"bbb\", new MatchNoneQueryBuilder()), new KeyedFilter(\"aaa\", new MatchNoneQueryBuilder()) };\r\n    FiltersAggregationBuilder builder;\r\n    builder = new FiltersAggregationBuilder(\"my-agg\", original);\r\n    assertEquals(\"aaa\", builder.filters().get(0).key());\r\n    assertEquals(\"bbb\", builder.filters().get(1).key());\r\n    assertEquals(\"bbb\", original[0].key());\r\n    assertEquals(\"aaa\", original[1].key());\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.create.ShrinkIndexIT.testCreateShrinkIndexFails",
	"Comment": "tests that we can manually recover from a failed allocation due to shards being moved away etc.",
	"Method": "void testCreateShrinkIndexFails(){\r\n    internalCluster().ensureAtLeastNumDataNodes(2);\r\n    prepareCreate(\"source\").setSettings(Settings.builder().put(indexSettings()).put(\"number_of_shards\", randomIntBetween(2, 7)).put(\"number_of_replicas\", 0)).get();\r\n    for (int i = 0; i < 20; i++) {\r\n        client().prepareIndex(\"source\", \"type\").setSource(\"{\\\"foo\\\" : \\\"bar\\\", \\\"i\\\" : \" + i + \"}\", XContentType.JSON).get();\r\n    }\r\n    ImmutableOpenMap<String, DiscoveryNode> dataNodes = client().admin().cluster().prepareState().get().getState().nodes().getDataNodes();\r\n    assertTrue(\"at least 2 nodes but was: \" + dataNodes.size(), dataNodes.size() >= 2);\r\n    DiscoveryNode[] discoveryNodes = dataNodes.values().toArray(DiscoveryNode.class);\r\n    String spareNode = discoveryNodes[0].getName();\r\n    String mergeNode = discoveryNodes[1].getName();\r\n    ensureGreen();\r\n    client().admin().indices().prepareUpdateSettings(\"source\").setSettings(Settings.builder().put(\"index.routing.allocation.require._name\", mergeNode).put(\"index.blocks.write\", true)).get();\r\n    ensureGreen();\r\n    client().admin().indices().prepareResizeIndex(\"source\", \"target\").setWaitForActiveShards(ActiveShardCount.NONE).setSettings(// we manually exclude the merge node to forcefully fuck it up\r\n    Settings.builder().put(\"index.routing.allocation.exclude._name\", mergeNode).put(\"index.number_of_replicas\", 0).put(\"index.allocation.max_retries\", 1).build()).get();\r\n    client().admin().cluster().prepareHealth(\"target\").setWaitForEvents(Priority.LANGUID).get();\r\n    client().admin().indices().prepareUpdateSettings(\"source\").setSettings(Settings.builder().put(\"index.routing.allocation.require._name\", spareNode).put(\"index.blocks.write\", true)).get();\r\n    ensureGreen(\"source\");\r\n    client().admin().indices().prepareUpdateSettings(\"target\").setSettings(Settings.builder().putNull(\"index.routing.allocation.exclude._name\")).get();\r\n    assertBusy(() -> {\r\n        ClusterStateResponse clusterStateResponse = client().admin().cluster().prepareState().get();\r\n        RoutingTable routingTables = clusterStateResponse.getState().routingTable();\r\n        assertTrue(routingTables.index(\"target\").shard(0).getShards().get(0).unassigned());\r\n        assertEquals(UnassignedInfo.Reason.ALLOCATION_FAILED, routingTables.index(\"target\").shard(0).getShards().get(0).unassignedInfo().getReason());\r\n        assertEquals(1, routingTables.index(\"target\").shard(0).getShards().get(0).unassignedInfo().getNumFailedAllocations());\r\n    });\r\n    client().admin().indices().prepareUpdateSettings(\"source\").setSettings(Settings.builder().put(\"index.routing.allocation.require._name\", mergeNode)).get();\r\n    ensureGreen(\"source\");\r\n    final InternalClusterInfoService infoService = (InternalClusterInfoService) internalCluster().getInstance(ClusterInfoService.class, internalCluster().getMasterName());\r\n    infoService.refresh();\r\n    ClusterRerouteResponse clusterRerouteResponse = client().admin().cluster().prepareReroute().setRetryFailed(true).get();\r\n    long expectedShardSize = clusterRerouteResponse.getState().routingTable().index(\"target\").shard(0).getShards().get(0).getExpectedShardSize();\r\n    assertTrue(\"expected shard size must be set but wasn't: \" + expectedShardSize, expectedShardSize > 0);\r\n    ensureGreen();\r\n    assertHitCount(client().prepareSearch(\"target\").setSize(100).setQuery(new TermsQueryBuilder(\"foo\", \"bar\")).get(), 20);\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.process.autodetect.state.DataCounts.getMissingFieldCount",
	"Comment": "the number of missing fields that had beenconfigured for analysis.",
	"Method": "long getMissingFieldCount(){\r\n    return missingFieldCount;\r\n}"
}, {
	"Path": "org.elasticsearch.rest.action.RestActions.parseTopLevelQueryBuilder",
	"Comment": "parses a top level query including the query element that wraps it",
	"Method": "QueryBuilder parseTopLevelQueryBuilder(XContentParser parser){\r\n    try {\r\n        QueryBuilder queryBuilder = null;\r\n        XContentParser.Token first = parser.nextToken();\r\n        if (first == null) {\r\n            return null;\r\n        } else if (first != XContentParser.Token.START_OBJECT) {\r\n            throw new ParsingException(parser.getTokenLocation(), \"Expected [\" + XContentParser.Token.START_OBJECT + \"] but found [\" + first + \"]\", parser.getTokenLocation());\r\n        }\r\n        for (XContentParser.Token token = parser.nextToken(); token != XContentParser.Token.END_OBJECT; token = parser.nextToken()) {\r\n            if (token == XContentParser.Token.FIELD_NAME) {\r\n                String fieldName = parser.currentName();\r\n                if (\"query\".equals(fieldName)) {\r\n                    queryBuilder = parseInnerQueryBuilder(parser);\r\n                } else {\r\n                    throw new ParsingException(parser.getTokenLocation(), \"request does not support [\" + parser.currentName() + \"]\");\r\n                }\r\n            }\r\n        }\r\n        return queryBuilder;\r\n    } catch (ParsingException e) {\r\n        throw e;\r\n    } catch (Exception e) {\r\n        throw new ParsingException(parser == null ? null : parser.getTokenLocation(), \"Failed to parse\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.rest.RestRequest.unconsumedParams",
	"Comment": "returns a list of parameters that have not yet been consumed. this method returns a copy,callers are free to modify the returned list.",
	"Method": "List<String> unconsumedParams(){\r\n    return params.keySet().stream().filter(p -> !consumedParams.contains(p)).collect(Collectors.toList());\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocatorTests.testNoMatchingAllocationIdFound",
	"Comment": "tests when the node returns data with a shard allocation id that does not match active allocation ids, it will be moved to ignoreunassigned.",
	"Method": "void testNoMatchingAllocationIdFound(){\r\n    RoutingAllocation allocation = routingAllocationWithOnePrimaryNoReplicas(yesAllocationDeciders(), CLUSTER_RECOVERED, \"id2\");\r\n    testAllocator.addData(node1, \"id1\", randomBoolean());\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodesChanged(), equalTo(true));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().get(0).shardId(), equalTo(shardId));\r\n    assertClusterHealthStatus(allocation, ClusterHealthStatus.YELLOW);\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.graph.action.GraphExploreRequestBuilder.createNextHop",
	"Comment": "add a stage in the graph exploration. each hop represents a stage of querying elasticsearch to identify terms which can then be connnectedto other terms in a subsequent hop.",
	"Method": "Hop createNextHop(QueryBuilder guidingQuery){\r\n    return request.createNextHop(guidingQuery);\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.builders.AbstractShapeBuilderTestCase.testFromXContent",
	"Comment": "test that creates new shape from a random test shape and checks both for equality",
	"Method": "void testFromXContent(){\r\n    for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {\r\n        SB testShape = createTestShapeBuilder();\r\n        XContentBuilder contentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));\r\n        if (randomBoolean()) {\r\n            contentBuilder.prettyPrint();\r\n        }\r\n        XContentBuilder builder = testShape.toXContent(contentBuilder, ToXContent.EMPTY_PARAMS);\r\n        XContentBuilder shuffled = shuffleXContent(builder);\r\n        try (XContentParser shapeContentParser = createParser(shuffled)) {\r\n            shapeContentParser.nextToken();\r\n            ShapeBuilder<?, ?> parsedShape = ShapeParser.parse(shapeContentParser);\r\n            assertNotSame(testShape, parsedShape);\r\n            assertEquals(testShape, parsedShape);\r\n            assertEquals(testShape.hashCode(), parsedShape.hashCode());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.segments.IndicesSegmentsRequestTests.testRequestOnClosedIndex",
	"Comment": "with the default indicesoptions inherited from broadcastoperationrequest this will raise an exception",
	"Method": "void testRequestOnClosedIndex(){\r\n    client().admin().indices().prepareClose(\"test\").get();\r\n    try {\r\n        client().admin().indices().prepareSegments(\"test\").get();\r\n        fail(\"Expected IndexClosedException\");\r\n    } catch (IndexClosedException e) {\r\n        assertThat(e.getMessage(), is(\"closed\"));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.joda.JodaDateMathParserTests.testImplicitRounding",
	"Comment": "implicit rounding happening when parts of the date are not specified",
	"Method": "void testImplicitRounding(){\r\n    assertDateMathEquals(\"2014-11-18\", \"2014-11-18\", 0, false, null);\r\n    assertDateMathEquals(\"2014-11-18\", \"2014-11-18T23:59:59.999Z\", 0, true, null);\r\n    assertDateMathEquals(\"2014-11-18T09:20\", \"2014-11-18T09:20\", 0, false, null);\r\n    assertDateMathEquals(\"2014-11-18T09:20\", \"2014-11-18T09:20:59.999Z\", 0, true, null);\r\n    assertDateMathEquals(\"2014-11-18\", \"2014-11-17T23:00:00.000Z\", 0, false, DateTimeZone.forID(\"CET\"));\r\n    assertDateMathEquals(\"2014-11-18\", \"2014-11-18T22:59:59.999Z\", 0, true, DateTimeZone.forID(\"CET\"));\r\n    assertDateMathEquals(\"2014-11-18T09:20\", \"2014-11-18T08:20:00.000Z\", 0, false, DateTimeZone.forID(\"CET\"));\r\n    assertDateMathEquals(\"2014-11-18T09:20\", \"2014-11-18T08:20:59.999Z\", 0, true, DateTimeZone.forID(\"CET\"));\r\n    FormatDateTimeFormatter formatter = Joda.forPattern(\"YYYY-MM-ddZ\");\r\n    JodaDateMathParser parser = new JodaDateMathParser(formatter);\r\n    long time = parser.parse(\"2011-10-09+01:00\", () -> 0, false, (ZoneId) null);\r\n    assertEquals(this.parser.parse(\"2011-10-09T00:00:00.000+01:00\", () -> 0), time);\r\n    time = parser.parse(\"2011-10-09+01:00\", () -> 0, true, (ZoneId) null);\r\n    assertEquals(this.parser.parse(\"2011-10-09T23:59:59.999+01:00\", () -> 0), time);\r\n}"
}, {
	"Path": "org.elasticsearch.common.time.DateFormattersTests.testEpochSecondParser",
	"Comment": "as this feature is supported it also makes sense to make it exact",
	"Method": "void testEpochSecondParser(){\r\n    DateFormatter formatter = DateFormatters.forPattern(\"epoch_second\");\r\n    assertThat(Instant.from(formatter.parse(\"1234.567\")).toEpochMilli(), is(1234567L));\r\n    assertThat(Instant.from(formatter.parse(\"1234.\")).getNano(), is(0));\r\n    assertThat(Instant.from(formatter.parse(\"1234.\")).getEpochSecond(), is(1234L));\r\n    assertThat(Instant.from(formatter.parse(\"1234.1\")).getNano(), is(100_000_000));\r\n    assertThat(Instant.from(formatter.parse(\"1234.12\")).getNano(), is(120_000_000));\r\n    assertThat(Instant.from(formatter.parse(\"1234.123\")).getNano(), is(123_000_000));\r\n    assertThat(Instant.from(formatter.parse(\"1234.1234\")).getNano(), is(123_400_000));\r\n    assertThat(Instant.from(formatter.parse(\"1234.12345\")).getNano(), is(123_450_000));\r\n    assertThat(Instant.from(formatter.parse(\"1234.123456\")).getNano(), is(123_456_000));\r\n    assertThat(Instant.from(formatter.parse(\"1234.1234567\")).getNano(), is(123_456_700));\r\n    assertThat(Instant.from(formatter.parse(\"1234.12345678\")).getNano(), is(123_456_780));\r\n    assertThat(Instant.from(formatter.parse(\"1234.123456789\")).getNano(), is(123_456_789));\r\n    assertThat(Instant.from(formatter.parse(\"-1234.567\")).toEpochMilli(), is(-1234567L));\r\n    assertThat(Instant.from(formatter.parse(\"-1234\")).getNano(), is(0));\r\n    DateTimeParseException e = expectThrows(DateTimeParseException.class, () -> formatter.parse(\"1234.1234567890\"));\r\n    assertThat(e.getMessage(), is(\"too much granularity after dot [1234.1234567890]\"));\r\n    e = expectThrows(DateTimeParseException.class, () -> formatter.parse(\"1234.123456789013221\"));\r\n    assertThat(e.getMessage(), is(\"too much granularity after dot [1234.123456789013221]\"));\r\n    e = expectThrows(DateTimeParseException.class, () -> formatter.parse(\"abc\"));\r\n    assertThat(e.getMessage(), is(\"invalid number [abc]\"));\r\n    e = expectThrows(DateTimeParseException.class, () -> formatter.parse(\"1234.abc\"));\r\n    assertThat(e.getMessage(), is(\"invalid number [1234.abc]\"));\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksDecidersTestCase.assertPersistentTasks",
	"Comment": "asserts that the cluster state contains nbtasks tasks that verify the given predicate",
	"Method": "void assertPersistentTasks(long nbTasks,ClusterState clusterState,Predicate<PersistentTasksCustomMetaData.PersistentTask> predicate){\r\n    PersistentTasksCustomMetaData tasks = clusterState.metaData().custom(PersistentTasksCustomMetaData.TYPE);\r\n    assertNotNull(\"Persistent tasks must be not null\", tasks);\r\n    assertEquals(nbTasks, tasks.tasks().stream().filter(predicate).count());\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.BulkByScrollTaskStatusTests.assertTaskStatusEquals",
	"Comment": "assert that two task statuses are equal after serialization.",
	"Method": "void assertTaskStatusEquals(Version version,BulkByScrollTask.Status expected,BulkByScrollTask.Status actual){\r\n    assertEquals(expected.getTotal(), actual.getTotal());\r\n    assertEquals(expected.getUpdated(), actual.getUpdated());\r\n    assertEquals(expected.getCreated(), actual.getCreated());\r\n    assertEquals(expected.getDeleted(), actual.getDeleted());\r\n    assertEquals(expected.getBatches(), actual.getBatches());\r\n    assertEquals(expected.getVersionConflicts(), actual.getVersionConflicts());\r\n    assertEquals(expected.getNoops(), actual.getNoops());\r\n    assertEquals(expected.getBulkRetries(), actual.getBulkRetries());\r\n    assertEquals(expected.getSearchRetries(), actual.getSearchRetries());\r\n    assertEquals(expected.getThrottled(), actual.getThrottled());\r\n    assertEquals(expected.getRequestsPerSecond(), actual.getRequestsPerSecond(), 0f);\r\n    assertEquals(expected.getReasonCancelled(), actual.getReasonCancelled());\r\n    assertEquals(expected.getThrottledUntil(), actual.getThrottledUntil());\r\n    assertThat(actual.getSliceStatuses(), Matchers.hasSize(expected.getSliceStatuses().size()));\r\n    for (int i = 0; i < expected.getSliceStatuses().size(); i++) {\r\n        BulkByScrollTask.StatusOrException sliceStatus = expected.getSliceStatuses().get(i);\r\n        if (sliceStatus == null) {\r\n            assertNull(actual.getSliceStatuses().get(i));\r\n        } else if (sliceStatus.getException() == null) {\r\n            assertNull(actual.getSliceStatuses().get(i).getException());\r\n            assertTaskStatusEquals(version, sliceStatus.getStatus(), actual.getSliceStatuses().get(i).getStatus());\r\n        } else {\r\n            assertNull(actual.getSliceStatuses().get(i).getStatus());\r\n            assertEquals(sliceStatus.getException().getMessage(), actual.getSliceStatuses().get(i).getException().getMessage());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.getMinMasterNodes",
	"Comment": "calculates a min master nodes value based on the given number of master nodes",
	"Method": "int getMinMasterNodes(int eligibleMasterNodes){\r\n    return eligibleMasterNodes / 2 + 1;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.SimulatedAnealingMinimizer.minimize",
	"Comment": "runs the simulated annealing algorithm and produces a model with new coefficients that, theoreticallyfit the data better and generalizes to future forecasts without overfitting.",
	"Method": "MovAvgModel minimize(MovAvgModel model,EvictingQueue<Double> train,double[] test){\r\n    double temp = 1;\r\n    double minTemp = 0.0001;\r\n    int iterations = 100;\r\n    double alpha = 0.9;\r\n    MovAvgModel bestModel = model;\r\n    MovAvgModel oldModel = model;\r\n    double oldCost = cost(model, train, test);\r\n    double bestCost = oldCost;\r\n    while (temp > minTemp) {\r\n        for (int i = 0; i < iterations; i++) {\r\n            MovAvgModel newModel = oldModel.neighboringModel();\r\n            double newCost = cost(newModel, train, test);\r\n            double ap = acceptanceProbability(oldCost, newCost, temp);\r\n            if (ap > Math.random()) {\r\n                oldModel = newModel;\r\n                oldCost = newCost;\r\n                if (newCost < bestCost) {\r\n                    bestCost = newCost;\r\n                    bestModel = newModel;\r\n                }\r\n            }\r\n        }\r\n        temp *= alpha;\r\n    }\r\n    return bestModel;\r\n}"
}, {
	"Path": "org.elasticsearch.test.transport.MockTransportService.addGetConnectionBehavior",
	"Comment": "adds a get connection behavior that is the default get connection behavior.",
	"Method": "boolean addGetConnectionBehavior(TransportService transportService,StubbableConnectionManager.GetConnectionBehavior behavior,boolean addGetConnectionBehavior,TransportAddress transportAddress,StubbableConnectionManager.GetConnectionBehavior behavior,boolean addGetConnectionBehavior,StubbableConnectionManager.GetConnectionBehavior behavior){\r\n    return connectionManager().setDefaultGetConnectionBehavior(behavior);\r\n}"
}, {
	"Path": "org.elasticsearch.license.LicenseUtils.signatureNeedsUpdate",
	"Comment": "checks if the signature of a self generated license with older version needs to berecreated with the new key",
	"Method": "boolean signatureNeedsUpdate(License license,DiscoveryNodes currentNodes){\r\n    assert License.VERSION_CRYPTO_ALGORITHMS == License.VERSION_CURRENT : \"update this method when adding a new version\";\r\n    return (\"basic\".equals(license.type()) || \"trial\".equals(license.type())) && (license.version() < License.VERSION_CRYPTO_ALGORITHMS && compatibleLicenseVersion(currentNodes) == License.VERSION_CRYPTO_ALGORITHMS);\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT.testUnrestorableIndexDuringRestore",
	"Comment": "test that restoring an index with shard allocation filtering settings that preventsits allocation does not hang indefinitely.",
	"Method": "void testUnrestorableIndexDuringRestore(){\r\n    final String indexName = \"unrestorable-index\";\r\n    Settings restoreIndexSettings = Settings.builder().put(\"index.routing.allocation.include._name\", randomAlphaOfLength(5)).build();\r\n    Consumer<UnassignedInfo> checkUnassignedInfo = unassignedInfo -> {\r\n        assertThat(unassignedInfo.getReason(), equalTo(UnassignedInfo.Reason.NEW_INDEX_RESTORED));\r\n    };\r\n    Runnable fixupAction = () -> {\r\n        assertAcked(client().admin().indices().prepareUpdateSettings(indexName).setSettings(Settings.builder().putNull(\"index.routing.allocation.include._name\").build()));\r\n        assertAcked(client().admin().cluster().prepareReroute().setRetryFailed(true));\r\n    };\r\n    unrestorableUseCase(indexName, Settings.EMPTY, Settings.EMPTY, restoreIndexSettings, checkUnassignedInfo, fixupAction);\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.MasterDisruptionIT.testStaleMasterNotHijackingMajority",
	"Comment": "tests that emulates a frozen elected master node that unfreezes and pushes his cluster state to other nodesthat already are following another elected master node. these nodes should reject this cluster state and preventthem from following the stale master.",
	"Method": "void testStaleMasterNotHijackingMajority(){\r\n    final List<String> nodes = startCluster(3);\r\n    final String oldMasterNode = internalCluster().getMasterName();\r\n    for (String node : nodes) {\r\n        ensureStableCluster(3, node);\r\n    }\r\n    assertMaster(oldMasterNode, nodes);\r\n    SingleNodeDisruption masterNodeDisruption = new LongGCDisruption(random(), oldMasterNode);\r\n    final List<String> majoritySide = new ArrayList(nodes);\r\n    majoritySide.remove(oldMasterNode);\r\n    final Map<String, List<Tuple<String, String>>> masters = Collections.synchronizedMap(new HashMap<String, List<Tuple<String, String>>>());\r\n    for (final String node : majoritySide) {\r\n        masters.put(node, new ArrayList<Tuple<String, String>>());\r\n        internalCluster().getInstance(ClusterService.class, node).addListener(event -> {\r\n            DiscoveryNode previousMaster = event.previousState().nodes().getMasterNode();\r\n            DiscoveryNode currentMaster = event.state().nodes().getMasterNode();\r\n            if (!Objects.equals(previousMaster, currentMaster)) {\r\n                logger.info(\"node {} received new cluster state: {} \\n and had previous cluster state: {}\", node, event.state(), event.previousState());\r\n                String previousMasterNodeName = previousMaster != null ? previousMaster.getName() : null;\r\n                String currentMasterNodeName = currentMaster != null ? currentMaster.getName() : null;\r\n                masters.get(node).add(new Tuple(previousMasterNodeName, currentMasterNodeName));\r\n            }\r\n        });\r\n    }\r\n    final CountDownLatch oldMasterNodeSteppedDown = new CountDownLatch(1);\r\n    internalCluster().getInstance(ClusterService.class, oldMasterNode).addListener(event -> {\r\n        if (event.state().nodes().getMasterNodeId() == null) {\r\n            oldMasterNodeSteppedDown.countDown();\r\n        }\r\n    });\r\n    internalCluster().setDisruptionScheme(masterNodeDisruption);\r\n    logger.info(\"freezing node [{}]\", oldMasterNode);\r\n    masterNodeDisruption.startDisrupting();\r\n    assertDifferentMaster(majoritySide.get(0), oldMasterNode);\r\n    assertDifferentMaster(majoritySide.get(1), oldMasterNode);\r\n    boolean failed = true;\r\n    try {\r\n        assertDiscoveryCompleted(majoritySide);\r\n        failed = false;\r\n    } finally {\r\n        if (failed) {\r\n            logger.error(\"discovery failed to complete, probably caused by a blocked thread: {}\", new HotThreads().busiestThreads(Integer.MAX_VALUE).ignoreIdleThreads(false).detect());\r\n        }\r\n    }\r\n    internalCluster().getInstance(ClusterService.class, oldMasterNode).submitStateUpdateTask(\"sneaky-update\", new ClusterStateUpdateTask(Priority.IMMEDIATE) {\r\n        @Override\r\n        public ClusterState execute(ClusterState currentState) throws Exception {\r\n            return ClusterState.builder(currentState).build();\r\n        }\r\n        @Override\r\n        public void onFailure(String source, Exception e) {\r\n            logger.warn(() -> new ParameterizedMessage(\"failure [{}]\", source), e);\r\n        }\r\n    });\r\n    final String newMasterNode = internalCluster().getMasterName(majoritySide.get(0));\r\n    logger.info(\"new detected master node [{}]\", newMasterNode);\r\n    logger.info(\"Unfreeze node [{}]\", oldMasterNode);\r\n    masterNodeDisruption.stopDisrupting();\r\n    oldMasterNodeSteppedDown.await(30, TimeUnit.SECONDS);\r\n    assertDiscoveryCompleted(nodes);\r\n    assertMaster(newMasterNode, nodes);\r\n    assertThat(masters.size(), equalTo(2));\r\n    for (Map.Entry<String, List<Tuple<String, String>>> entry : masters.entrySet()) {\r\n        String nodeName = entry.getKey();\r\n        List<Tuple<String, String>> recordedMasterTransition = entry.getValue();\r\n        assertThat(\"[\" + nodeName + \"] Each node should only record two master node transitions\", recordedMasterTransition.size(), equalTo(2));\r\n        assertThat(\"[\" + nodeName + \"] First transition's previous master should be [null]\", recordedMasterTransition.get(0).v1(), equalTo(oldMasterNode));\r\n        assertThat(\"[\" + nodeName + \"] First transition's current master should be [\" + newMasterNode + \"]\", recordedMasterTransition.get(0).v2(), nullValue());\r\n        assertThat(\"[\" + nodeName + \"] Second transition's previous master should be [null]\", recordedMasterTransition.get(1).v1(), nullValue());\r\n        assertThat(\"[\" + nodeName + \"] Second transition's current master should be [\" + newMasterNode + \"]\", recordedMasterTransition.get(1).v2(), equalTo(newMasterNode));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.MasterDisruptionIT.testStaleMasterNotHijackingMajority",
	"Comment": "tests that emulates a frozen elected master node that unfreezes and pushes his cluster state to other nodesthat already are following another elected master node. these nodes should reject this cluster state and preventthem from following the stale master.",
	"Method": "void testStaleMasterNotHijackingMajority(){\r\n    return ClusterState.builder(currentState).build();\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.MasterDisruptionIT.testStaleMasterNotHijackingMajority",
	"Comment": "tests that emulates a frozen elected master node that unfreezes and pushes his cluster state to other nodesthat already are following another elected master node. these nodes should reject this cluster state and preventthem from following the stale master.",
	"Method": "void testStaleMasterNotHijackingMajority(){\r\n    logger.warn(() -> new ParameterizedMessage(\"failure [{}]\", source), e);\r\n}"
}, {
	"Path": "org.elasticsearch.search.SearchHit.getNestedIdentity",
	"Comment": "if this is a nested hit then nested reference information is returned otherwise null is returned.",
	"Method": "NestedIdentity getNestedIdentity(){\r\n    return nestedIdentity;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.assertResultsAndLogOnFailure",
	"Comment": "ensures the result counts are as expected, and logs the results if different",
	"Method": "void assertResultsAndLogOnFailure(long expectedResults,SearchResponse searchResponse){\r\n    if (searchResponse.getHits().getTotalHits() != expectedResults) {\r\n        StringBuilder sb = new StringBuilder(\"search result contains [\");\r\n        sb.append(searchResponse.getHits().getTotalHits()).append(\"] results. expected [\").append(expectedResults).append(\"]\");\r\n        String failMsg = sb.toString();\r\n        for (SearchHit hit : searchResponse.getHits().getHits()) {\r\n            sb.append(\"\\n-> _index: [\").append(hit.getIndex()).append(\"] type [\").append(hit.getType()).append(\"] id [\").append(hit.getId()).append(\"]\");\r\n        }\r\n        logger.warn(\"{}\", sb);\r\n        fail(failMsg);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.restspec.ClientYamlSuiteRestApi.getParams",
	"Comment": "gets all parameters supported by the api. for every parameter defines if itis required or optional.",
	"Method": "Map<String, Boolean> getParams(){\r\n    return params;\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.scriptField",
	"Comment": "adds a script field under the given name with the provided script.",
	"Method": "SearchSourceBuilder scriptField(String name,Script script,SearchSourceBuilder scriptField,String name,Script script,boolean ignoreFailure){\r\n    if (scriptFields == null) {\r\n        scriptFields = new ArrayList();\r\n    }\r\n    scriptFields.add(new ScriptField(name, script, ignoreFailure));\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.createTestAnalysis",
	"Comment": "creates an testanalysis with all the default analyzers configured.",
	"Method": "TestAnalysis createTestAnalysis(Index index,Settings settings,AnalysisPlugin analysisPlugins,TestAnalysis createTestAnalysis,Index index,Settings nodeSettings,Settings settings,AnalysisPlugin analysisPlugins,TestAnalysis createTestAnalysis,IndexSettings indexSettings,Settings nodeSettings,AnalysisPlugin analysisPlugins){\r\n    Environment env = TestEnvironment.newEnvironment(nodeSettings);\r\n    AnalysisModule analysisModule = new AnalysisModule(env, Arrays.asList(analysisPlugins));\r\n    AnalysisRegistry analysisRegistry = analysisModule.getAnalysisRegistry();\r\n    return new TestAnalysis(analysisRegistry.build(indexSettings), analysisRegistry.buildTokenFilterFactories(indexSettings), analysisRegistry.buildTokenizerFactories(indexSettings), analysisRegistry.buildCharFilterFactories(indexSettings));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificantTextAggregationBuilder.minDocCount",
	"Comment": "set the minimum document count terms should have in order to appear inthe response.",
	"Method": "SignificantTextAggregationBuilder minDocCount(long minDocCount){\r\n    if (minDocCount < 0) {\r\n        throw new IllegalArgumentException(\"[minDocCount] must be greater than or equal to 0. Found [\" + minDocCount + \"] in [\" + name + \"]\");\r\n    }\r\n    bucketCountThresholds.setMinDocCount(minDocCount);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.TasksIT.waitForTestTaskStartOnAllNodes",
	"Comment": "wait for the test task to be running on all nodes and return the taskid of the primary task.",
	"Method": "TaskId waitForTestTaskStartOnAllNodes(){\r\n    assertBusy(() -> {\r\n        List<TaskInfo> tasks = client().admin().cluster().prepareListTasks().setActions(TestTaskPlugin.TestTaskAction.NAME + \"[n]\").get().getTasks();\r\n        assertEquals(internalCluster().size(), tasks.size());\r\n    });\r\n    List<TaskInfo> task = client().admin().cluster().prepareListTasks().setActions(TestTaskPlugin.TestTaskAction.NAME).get().getTasks();\r\n    assertThat(task, hasSize(1));\r\n    return task.get(0).getTaskId();\r\n}"
}, {
	"Path": "org.elasticsearch.search.geo.GeoShapeIntegrationIT.testOrientationPersistence",
	"Comment": "test that orientation parameter correctly persists across cluster restart",
	"Method": "void testOrientationPersistence(){\r\n    String idxName = \"orientation\";\r\n    String mapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject(\"shape\").startObject(\"properties\").startObject(\"location\").field(\"type\", \"geo_shape\").field(\"orientation\", \"left\").endObject().endObject().endObject().endObject());\r\n    assertAcked(prepareCreate(idxName).addMapping(\"shape\", mapping, XContentType.JSON));\r\n    mapping = Strings.toString(XContentFactory.jsonBuilder().startObject().startObject(\"shape\").startObject(\"properties\").startObject(\"location\").field(\"type\", \"geo_shape\").field(\"orientation\", \"right\").endObject().endObject().endObject().endObject());\r\n    assertAcked(prepareCreate(idxName + \"2\").addMapping(\"shape\", mapping, XContentType.JSON));\r\n    ensureGreen(idxName, idxName + \"2\");\r\n    internalCluster().fullRestart();\r\n    ensureGreen(idxName, idxName + \"2\");\r\n    IndicesService indicesService = internalCluster().getInstance(IndicesService.class, findNodeName(idxName));\r\n    IndexService indexService = indicesService.indexService(resolveIndex(idxName));\r\n    MappedFieldType fieldType = indexService.mapperService().fullName(\"location\");\r\n    assertThat(fieldType, instanceOf(GeoShapeFieldMapper.GeoShapeFieldType.class));\r\n    GeoShapeFieldMapper.GeoShapeFieldType gsfm = (GeoShapeFieldMapper.GeoShapeFieldType) fieldType;\r\n    ShapeBuilder.Orientation orientation = gsfm.orientation();\r\n    assertThat(orientation, equalTo(ShapeBuilder.Orientation.CLOCKWISE));\r\n    assertThat(orientation, equalTo(ShapeBuilder.Orientation.LEFT));\r\n    assertThat(orientation, equalTo(ShapeBuilder.Orientation.CW));\r\n    indicesService = internalCluster().getInstance(IndicesService.class, findNodeName(idxName + \"2\"));\r\n    indexService = indicesService.indexService(resolveIndex((idxName + \"2\")));\r\n    fieldType = indexService.mapperService().fullName(\"location\");\r\n    assertThat(fieldType, instanceOf(GeoShapeFieldMapper.GeoShapeFieldType.class));\r\n    gsfm = (GeoShapeFieldMapper.GeoShapeFieldType) fieldType;\r\n    orientation = gsfm.orientation();\r\n    assertThat(orientation, equalTo(ShapeBuilder.Orientation.COUNTER_CLOCKWISE));\r\n    assertThat(orientation, equalTo(ShapeBuilder.Orientation.RIGHT));\r\n    assertThat(orientation, equalTo(ShapeBuilder.Orientation.CCW));\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.SuggestBuilders.completionSuggestion",
	"Comment": "creates a completion suggestion lookup query with the provided field",
	"Method": "CompletionSuggestionBuilder completionSuggestion(String fieldname){\r\n    return new CompletionSuggestionBuilder(fieldname);\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.settings",
	"Comment": "return consistent index settings for the provided index version.",
	"Method": "Settings.Builder settings(Version version){\r\n    Settings.Builder builder = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, version);\r\n    return builder;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TransportActionProxy.wrapRequest",
	"Comment": "wraps the actual request in a proxy request object that encodes the target node.",
	"Method": "TransportRequest wrapRequest(DiscoveryNode node,TransportRequest request){\r\n    return new ProxyRequest(request, node);\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.completion.RegexOptions.getMaxDeterminizedStates",
	"Comment": "returns the maximum automaton states allowed for fuzzy expansion",
	"Method": "int getMaxDeterminizedStates(){\r\n    return maxDeterminizedStates;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.TopHitsAggregationBuilder.sorts",
	"Comment": "gets the bytes representing the sort builders for this request.",
	"Method": "TopHitsAggregationBuilder sorts(List<SortBuilder<?>> sorts,List<SortBuilder<?>> sorts){\r\n    return sorts;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.DiscoveryDisruptionIT.testNodeNotReachableFromMaster",
	"Comment": "adds an asymmetric break between a master and one of the nodes and makessure that the node is removed form the cluster, that the node start pinging and thatthe cluster reforms when healed.",
	"Method": "void testNodeNotReachableFromMaster(){\r\n    startCluster(3);\r\n    String masterNode = internalCluster().getMasterName();\r\n    String nonMasterNode = null;\r\n    while (nonMasterNode == null) {\r\n        nonMasterNode = randomFrom(internalCluster().getNodeNames());\r\n        if (nonMasterNode.equals(masterNode)) {\r\n            nonMasterNode = null;\r\n        }\r\n    }\r\n    logger.info(\"blocking request from master [{}] to [{}]\", masterNode, nonMasterNode);\r\n    MockTransportService masterTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, masterNode);\r\n    if (randomBoolean()) {\r\n        masterTransportService.addUnresponsiveRule(internalCluster().getInstance(TransportService.class, nonMasterNode));\r\n    } else {\r\n        masterTransportService.addFailToSendNoConnectRule(internalCluster().getInstance(TransportService.class, nonMasterNode));\r\n    }\r\n    logger.info(\"waiting for [{}] to be removed from cluster\", nonMasterNode);\r\n    ensureStableCluster(2, masterNode);\r\n    logger.info(\"waiting for [{}] to have no master\", nonMasterNode);\r\n    assertNoMaster(nonMasterNode);\r\n    logger.info(\"healing partition and checking cluster reforms\");\r\n    masterTransportService.clearAllRules();\r\n    ensureStableCluster(3);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.support.AggregationPath.resolveValue",
	"Comment": "resolves the value pointed by this path given an aggregations root",
	"Method": "double resolveValue(HasAggregations root){\r\n    HasAggregations parent = root;\r\n    double value = Double.NaN;\r\n    for (int i = 0; i < pathElements.size(); i++) {\r\n        AggregationPath.PathElement token = pathElements.get(i);\r\n        Aggregation agg = parent.getAggregations().get(token.name);\r\n        if (agg == null) {\r\n            throw new IllegalArgumentException(\"Invalid order path [\" + this + \"]. Cannot find aggregation named [\" + token.name + \"]\");\r\n        }\r\n        if (agg instanceof SingleBucketAggregation) {\r\n            if (token.key != null && !token.key.equals(\"doc_count\")) {\r\n                throw new IllegalArgumentException(\"Invalid order path [\" + this + \"]. Unknown value key [\" + token.key + \"] for single-bucket aggregation [\" + token.name + \"]. Either use [doc_count] as key or drop the key all together\");\r\n            }\r\n            parent = (SingleBucketAggregation) agg;\r\n            value = ((SingleBucketAggregation) agg).getDocCount();\r\n            continue;\r\n        }\r\n        if (i != pathElements.size() - 1) {\r\n            throw new IllegalArgumentException(\"Invalid order path [\" + this + \"]. Metrics aggregations cannot have sub-aggregations (at [\" + token + \">\" + pathElements.get(i + 1) + \"]\");\r\n        }\r\n        if (agg instanceof InternalNumericMetricsAggregation.SingleValue) {\r\n            if (token.key != null && !token.key.equals(\"value\")) {\r\n                throw new IllegalArgumentException(\"Invalid order path [\" + this + \"]. Unknown value key [\" + token.key + \"] for single-value metric aggregation [\" + token.name + \"]. Either use [value] as key or drop the key all together\");\r\n            }\r\n            parent = null;\r\n            value = ((InternalNumericMetricsAggregation.SingleValue) agg).value();\r\n            continue;\r\n        }\r\n        if (token.key == null) {\r\n            throw new IllegalArgumentException(\"Invalid order path [\" + this + \"]. Missing value key in [\" + token + \"] which refers to a multi-value metric aggregation\");\r\n        }\r\n        parent = null;\r\n        value = ((InternalNumericMetricsAggregation.MultiValue) agg).value(token.key);\r\n    }\r\n    return value;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.FieldSortBuilderTests.testBuildSortFieldOrder",
	"Comment": "test that the sort builder order gets transferred correctly to the sortfield",
	"Method": "void testBuildSortFieldOrder(){\r\n    QueryShardContext shardContextMock = createMockShardContext();\r\n    FieldSortBuilder fieldSortBuilder = new FieldSortBuilder(\"value\");\r\n    SortField sortField = fieldSortBuilder.build(shardContextMock).field;\r\n    SortedNumericSortField expectedSortField = new SortedNumericSortField(\"value\", SortField.Type.DOUBLE, false);\r\n    expectedSortField.setMissingValue(Double.POSITIVE_INFINITY);\r\n    assertEquals(expectedSortField, sortField);\r\n    fieldSortBuilder = new FieldSortBuilder(\"value\").order(SortOrder.ASC);\r\n    sortField = fieldSortBuilder.build(shardContextMock).field;\r\n    expectedSortField = new SortedNumericSortField(\"value\", SortField.Type.DOUBLE, false);\r\n    expectedSortField.setMissingValue(Double.POSITIVE_INFINITY);\r\n    assertEquals(expectedSortField, sortField);\r\n    fieldSortBuilder = new FieldSortBuilder(\"value\").order(SortOrder.DESC);\r\n    sortField = fieldSortBuilder.build(shardContextMock).field;\r\n    expectedSortField = new SortedNumericSortField(\"value\", SortField.Type.DOUBLE, true, SortedNumericSelector.Type.MAX);\r\n    expectedSortField.setMissingValue(Double.NEGATIVE_INFINITY);\r\n    assertEquals(expectedSortField, sortField);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore.getScore",
	"Comment": "calculates the significance of a term in a sample against a background ofnormal distributions by comparing the changes in frequency. this is the heartof the significant terms feature.",
	"Method": "double getScore(long subsetFreq,long subsetSize,long supersetFreq,long supersetSize){\r\n    checkFrequencyValidity(subsetFreq, subsetSize, supersetFreq, supersetSize, \"JLHScore\");\r\n    if ((subsetSize == 0) || (supersetSize == 0)) {\r\n        return 0;\r\n    }\r\n    if (supersetFreq == 0) {\r\n        supersetFreq = 1;\r\n    }\r\n    double subsetProbability = (double) subsetFreq / (double) subsetSize;\r\n    double supersetProbability = (double) supersetFreq / (double) supersetSize;\r\n    double absoluteProbabilityChange = subsetProbability - supersetProbability;\r\n    if (absoluteProbabilityChange <= 0) {\r\n        return 0;\r\n    }\r\n    double relativeProbabilityChange = (subsetProbability / supersetProbability);\r\n    return absoluteProbabilityChange * relativeProbabilityChange;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.ScriptSortBuilder.setNestedFilter",
	"Comment": "sets the nested filter that the nested objects should match with in order to be taken into accountfor sorting.",
	"Method": "ScriptSortBuilder setNestedFilter(QueryBuilder nestedFilter){\r\n    if (this.nestedSort != null) {\r\n        throw new IllegalArgumentException(\"Setting both nested_path/nested_filter and nested not allowed\");\r\n    }\r\n    this.nestedFilter = nestedFilter;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocatorTests.testRestore",
	"Comment": "tests that when restoring from a snapshot and we find a node with a shard copy and allocationdeciders say yes, we allocate to that node.",
	"Method": "void testRestore(){\r\n    RoutingAllocation allocation = getRestoreRoutingAllocation(yesAllocationDeciders(), \"allocId\");\r\n    testAllocator.addData(node1, \"some allocId\", randomBoolean());\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodesChanged(), equalTo(true));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().isEmpty(), equalTo(true));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));\r\n    assertClusterHealthStatus(allocation, ClusterHealthStatus.YELLOW);\r\n}"
}, {
	"Path": "org.elasticsearch.test.transport.MockTransportService.addSendBehavior",
	"Comment": "adds a new send behavior that is used for communication with the given delegate address.",
	"Method": "boolean addSendBehavior(TransportService transportService,StubbableTransport.SendRequestBehavior sendBehavior,boolean addSendBehavior,TransportAddress transportAddress,StubbableTransport.SendRequestBehavior sendBehavior,boolean addSendBehavior,StubbableTransport.SendRequestBehavior behavior){\r\n    return transport().setDefaultSendBehavior(behavior);\r\n}"
}, {
	"Path": "org.elasticsearch.rest.action.cat.RestRecoveryAction.buildRecoveryTable",
	"Comment": "buildrecoverytable will build a table of recovery information suitablefor displaying at the command line.",
	"Method": "Table buildRecoveryTable(RestRequest request,RecoveryResponse response){\r\n    Table t = getTableWithHeader(request);\r\n    for (String index : response.shardRecoveryStates().keySet()) {\r\n        List<RecoveryState> shardRecoveryStates = response.shardRecoveryStates().get(index);\r\n        if (shardRecoveryStates.size() == 0) {\r\n            continue;\r\n        }\r\n        CollectionUtil.introSort(shardRecoveryStates, new Comparator<RecoveryState>() {\r\n            @Override\r\n            public int compare(RecoveryState o1, RecoveryState o2) {\r\n                int id1 = o1.getShardId().id();\r\n                int id2 = o2.getShardId().id();\r\n                if (id1 < id2) {\r\n                    return -1;\r\n                } else if (id1 > id2) {\r\n                    return 1;\r\n                } else {\r\n                    return 0;\r\n                }\r\n            }\r\n        });\r\n        for (RecoveryState state : shardRecoveryStates) {\r\n            t.startRow();\r\n            t.addCell(index);\r\n            t.addCell(state.getShardId().id());\r\n            t.addCell(new TimeValue(state.getTimer().time()));\r\n            t.addCell(state.getRecoverySource().getType().toString().toLowerCase(Locale.ROOT));\r\n            t.addCell(state.getStage().toString().toLowerCase(Locale.ROOT));\r\n            t.addCell(state.getSourceNode() == null ? \"n/a\" : state.getSourceNode().getHostName());\r\n            t.addCell(state.getSourceNode() == null ? \"n/a\" : state.getSourceNode().getName());\r\n            t.addCell(state.getTargetNode().getHostName());\r\n            t.addCell(state.getTargetNode().getName());\r\n            t.addCell(state.getRecoverySource() == null || state.getRecoverySource().getType() != RecoverySource.Type.SNAPSHOT ? \"n/a\" : ((SnapshotRecoverySource) state.getRecoverySource()).snapshot().getRepository());\r\n            t.addCell(state.getRecoverySource() == null || state.getRecoverySource().getType() != RecoverySource.Type.SNAPSHOT ? \"n/a\" : ((SnapshotRecoverySource) state.getRecoverySource()).snapshot().getSnapshotId().getName());\r\n            t.addCell(state.getIndex().totalRecoverFiles());\r\n            t.addCell(state.getIndex().recoveredFileCount());\r\n            t.addCell(String.format(Locale.ROOT, \"%1.1f%%\", state.getIndex().recoveredFilesPercent()));\r\n            t.addCell(state.getIndex().totalFileCount());\r\n            t.addCell(state.getIndex().totalRecoverBytes());\r\n            t.addCell(state.getIndex().recoveredBytes());\r\n            t.addCell(String.format(Locale.ROOT, \"%1.1f%%\", state.getIndex().recoveredBytesPercent()));\r\n            t.addCell(state.getIndex().totalBytes());\r\n            t.addCell(state.getTranslog().totalOperations());\r\n            t.addCell(state.getTranslog().recoveredOperations());\r\n            t.addCell(String.format(Locale.ROOT, \"%1.1f%%\", state.getTranslog().recoveredPercent()));\r\n            t.endRow();\r\n        }\r\n    }\r\n    return t;\r\n}"
}, {
	"Path": "org.elasticsearch.rest.action.cat.RestRecoveryAction.buildRecoveryTable",
	"Comment": "buildrecoverytable will build a table of recovery information suitablefor displaying at the command line.",
	"Method": "Table buildRecoveryTable(RestRequest request,RecoveryResponse response){\r\n    int id1 = o1.getShardId().id();\r\n    int id2 = o2.getShardId().id();\r\n    if (id1 < id2) {\r\n        return -1;\r\n    } else if (id1 > id2) {\r\n        return 1;\r\n    } else {\r\n        return 0;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.gateway.TestGatewayAllocator.addKnownAllocation",
	"Comment": "manually add a specific shard to the allocations the gateway keeps track of",
	"Method": "void addKnownAllocation(ShardRouting shard){\r\n    knownAllocations.computeIfAbsent(shard.currentNodeId(), id -> new HashMap()).put(shard.shardId(), shard);\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexResponseTests.testFromXContentWithRandomFields",
	"Comment": "this test adds random fields and objects to the xcontent rendered out toensure we can parse it back to be forward compatible with additions tothe xcontent",
	"Method": "void testFromXContentWithRandomFields(){\r\n    doFromXContentTestWithRandomFields(true);\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.section.DoSection.checkWarningHeaders",
	"Comment": "check that the response contains only the warning headers that we expect.",
	"Method": "void checkWarningHeaders(List<String> warningHeaders,Version masterVersion){\r\n    final List<String> unexpected = new ArrayList();\r\n    final List<String> unmatched = new ArrayList();\r\n    final List<String> missing = new ArrayList();\r\n    final Set<String> expected = new LinkedHashSet(expectedWarningHeaders.stream().map(DeprecationLogger::escapeAndEncode).collect(Collectors.toList()));\r\n    for (final String header : warningHeaders) {\r\n        final Matcher matcher = WARNING_HEADER_PATTERN.matcher(header);\r\n        final boolean matches = matcher.matches();\r\n        if (matches) {\r\n            final String message = matcher.group(1);\r\n            if (masterVersion.before(Version.V_7_0_0) && message.equals(\"the default number of shards will change from [5] to [1] in 7.0.0; \" + \"if you wish to continue using the default of [5] shards, \" + \"you must manage this on the create index request or with an index template\")) {\r\n            } else if (message.startsWith(\"[types removal]\")) {\r\n            } else if (expected.remove(message) == false) {\r\n                unexpected.add(header);\r\n            }\r\n        } else {\r\n            unmatched.add(header);\r\n        }\r\n    }\r\n    if (expected.isEmpty() == false) {\r\n        for (final String header : expected) {\r\n            missing.add(header);\r\n        }\r\n    }\r\n    if (unexpected.isEmpty() == false || unmatched.isEmpty() == false || missing.isEmpty() == false) {\r\n        final StringBuilder failureMessage = new StringBuilder();\r\n        appendBadHeaders(failureMessage, unexpected, \"got unexpected warning header\" + (unexpected.size() > 1 ? \"s\" : \"\"));\r\n        appendBadHeaders(failureMessage, unmatched, \"got unmatched warning header\" + (unmatched.size() > 1 ? \"s\" : \"\"));\r\n        appendBadHeaders(failureMessage, missing, \"did not get expected warning header\" + (missing.size() > 1 ? \"s\" : \"\"));\r\n        fail(failureMessage.toString());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.threadpool.Scheduler.scheduleWithFixedDelay",
	"Comment": "schedules a periodic action that runs on scheduler thread. do not run blocking calls on the scheduler thread. subclasses may allowto execute on a different executor, in which case blocking calls are allowed.",
	"Method": "Cancellable scheduleWithFixedDelay(Runnable command,TimeValue interval,String executor){\r\n    return new ReschedulingRunnable(command, interval, executor, this, (e) -> {\r\n    }, (e) -> {\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.GatewayIndexStateIT.testIndexDeletionWhenNodeRejoins",
	"Comment": "this test ensures that when an index deletion takes place while a node is offline, when thatnode rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.",
	"Method": "void testIndexDeletionWhenNodeRejoins(){\r\n    final String indexName = \"test-index-del-on-node-rejoin-idx\";\r\n    final int numNodes = 2;\r\n    final List<String> nodes;\r\n    logger.info(\"--> starting a cluster with \" + numNodes + \" nodes\");\r\n    nodes = internalCluster().startNodes(numNodes, Settings.builder().put(IndexGraveyard.SETTING_MAX_TOMBSTONES.getKey(), randomIntBetween(10, 100)).build());\r\n    logger.info(\"--> create an index\");\r\n    createIndex(indexName);\r\n    logger.info(\"--> waiting for green status\");\r\n    ensureGreen();\r\n    final String indexUUID = resolveIndex(indexName).getUUID();\r\n    logger.info(\"--> restart a random date node, deleting the index in between stopping and restarting\");\r\n    internalCluster().restartRandomDataNode(new RestartCallback() {\r\n        @Override\r\n        public Settings onNodeStopped(final String nodeName) throws Exception {\r\n            nodes.remove(nodeName);\r\n            logger.info(\"--> stopped node[{}], remaining nodes {}\", nodeName, nodes);\r\n            assert nodes.size() > 0;\r\n            final String otherNode = nodes.get(0);\r\n            logger.info(\"--> delete index and verify it is deleted\");\r\n            final Client client = client(otherNode);\r\n            client.admin().indices().prepareDelete(indexName).execute().actionGet();\r\n            assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists());\r\n            return super.onNodeStopped(nodeName);\r\n        }\r\n    });\r\n    logger.info(\"--> wait until all nodes are back online\");\r\n    client().admin().cluster().health(Requests.clusterHealthRequest().waitForEvents(Priority.LANGUID).waitForNodes(Integer.toString(numNodes))).actionGet();\r\n    logger.info(\"--> waiting for green status\");\r\n    ensureGreen();\r\n    logger.info(\"--> verify that the deleted index is removed from the cluster and not reimported as dangling by the restarted node\");\r\n    assertFalse(client().admin().indices().prepareExists(indexName).execute().actionGet().isExists());\r\n    assertBusy(() -> {\r\n        final NodeEnvironment nodeEnv = internalCluster().getInstance(NodeEnvironment.class);\r\n        try {\r\n            assertFalse(\"index folder \" + indexUUID + \" should be deleted\", nodeEnv.availableIndexFolders().contains(indexUUID));\r\n        } catch (IOException e) {\r\n            logger.error(\"Unable to retrieve available index folders from the node\", e);\r\n            fail(\"Unable to retrieve available index folders from the node\");\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.GatewayIndexStateIT.testIndexDeletionWhenNodeRejoins",
	"Comment": "this test ensures that when an index deletion takes place while a node is offline, when thatnode rejoins the cluster, it deletes the index locally instead of importing it as a dangling index.",
	"Method": "void testIndexDeletionWhenNodeRejoins(){\r\n    nodes.remove(nodeName);\r\n    logger.info(\"--> stopped node[{}], remaining nodes {}\", nodeName, nodes);\r\n    assert nodes.size() > 0;\r\n    final String otherNode = nodes.get(0);\r\n    logger.info(\"--> delete index and verify it is deleted\");\r\n    final Client client = client(otherNode);\r\n    client.admin().indices().prepareDelete(indexName).execute().actionGet();\r\n    assertFalse(client.admin().indices().prepareExists(indexName).execute().actionGet().isExists());\r\n    return super.onNodeStopped(nodeName);\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.SuggestionBuilder.size",
	"Comment": "sets the maximum suggestions to be returned per suggest text term.",
	"Method": "T size(int size,Integer size){\r\n    return this.size;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.TopHitsIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    try {\r\n        assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n        indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n        assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n        assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n        SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(topHits(\"foo\").scriptField(\"bar\", new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, \"5\", Collections.emptyMap()))).get();\r\n        assertSearchResponse(r);\r\n        assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n        assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n        r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(topHits(\"foo\").sort(SortBuilders.scriptSort(new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, \"5\", Collections.emptyMap()), ScriptSortType.STRING))).get();\r\n        assertSearchResponse(r);\r\n        assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n        assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n        r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(topHits(\"foo\")).get();\r\n        assertSearchResponse(r);\r\n        assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n        assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n    } finally {\r\n        assertAcked(client().admin().indices().prepareDelete(\"cache_test_idx\"));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocatorTests.testNoAsyncFetchData",
	"Comment": "tests that when async fetch returns that there is no data, the shard will not be allocated.",
	"Method": "void testNoAsyncFetchData(){\r\n    final RoutingAllocation allocation = routingAllocationWithOnePrimaryNoReplicas(yesAllocationDeciders(), CLUSTER_RECOVERED, \"allocId\");\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodesChanged(), equalTo(true));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().get(0).shardId(), equalTo(shardId));\r\n    assertClusterHealthStatus(allocation, ClusterHealthStatus.YELLOW);\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.FieldSortBuilder.unmappedType",
	"Comment": "returns the type to use in case the current field is not mapped in anindex.",
	"Method": "FieldSortBuilder unmappedType(String type,String unmappedType){\r\n    return this.unmappedType;\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.term.TermSuggestionBuilder.maxInspections",
	"Comment": "get the factor for inspecting more candidate suggestions setting.",
	"Method": "TermSuggestionBuilder maxInspections(int maxInspections,int maxInspections){\r\n    return maxInspections;\r\n}"
}, {
	"Path": "org.elasticsearch.search.rescore.QueryRescorerBuilderTests.createParser",
	"Comment": "create a new parser from the rescorer string representation and reset context with it",
	"Method": "XContentParser createParser(String rescoreElement){\r\n    XContentParser parser = createParser(JsonXContent.jsonXContent, rescoreElement);\r\n    assertTrue(parser.nextToken() == XContentParser.Token.START_OBJECT);\r\n    return parser;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESSingleNodeTestCase.createIndex",
	"Comment": "create a new index on the singleton node with the provided index settings.",
	"Method": "IndexService createIndex(String index,IndexService createIndex,String index,Settings settings,IndexService createIndex,String index,Settings settings,String type,XContentBuilder mappings,IndexService createIndex,String index,Settings settings,String type,Object mappings,IndexService createIndex,String index,CreateIndexRequestBuilder createIndexRequestBuilder){\r\n    assertAcked(createIndexRequestBuilder.get());\r\n    ClusterHealthResponse health = client().admin().cluster().health(Requests.clusterHealthRequest(index).waitForYellowStatus().waitForEvents(Priority.LANGUID).waitForNoRelocatingShards(true)).actionGet();\r\n    assertThat(health.getStatus(), lessThanOrEqualTo(ClusterHealthStatus.YELLOW));\r\n    assertThat(\"Cluster must be a single node cluster\", health.getNumberOfDataNodes(), equalTo(1));\r\n    IndicesService instanceFromNode = getInstanceFromNode(IndicesService.class);\r\n    return instanceFromNode.indexServiceSafe(resolveIndex(index));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.MaxIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(max(\"foo\").field(\"d\").script(new Script(ScriptType.INLINE, AggregationTestScriptsPlugin.NAME, \"_value + 1\", emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(max(\"foo\").field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.search.SearchPhaseResult.getRequestId",
	"Comment": "returns the results request id that is used to reference the search context on the executingnode",
	"Method": "long getRequestId(){\r\n    return requestId;\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.completion.context.GeoQueryContext.getNeighbours",
	"Comment": "returns the precision levels at which geohash cells neighbours are considered",
	"Method": "List<Integer> getNeighbours(){\r\n    return neighbours;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MatchQueryBuilderTests.createGiantGraph",
	"Comment": "creates a graph token stream with 2 side paths at each position.",
	"Method": "CannedBinaryTokenStream.BinaryToken[] createGiantGraph(int numPos){\r\n    List<CannedBinaryTokenStream.BinaryToken> tokens = new ArrayList();\r\n    BytesRef term1 = new BytesRef(\"foo\");\r\n    BytesRef term2 = new BytesRef(\"bar\");\r\n    for (int i = 0; i < numPos; ) {\r\n        if (i % 2 == 0) {\r\n            tokens.add(new CannedBinaryTokenStream.BinaryToken(term2, 1, 1));\r\n            tokens.add(new CannedBinaryTokenStream.BinaryToken(term1, 0, 2));\r\n            i += 2;\r\n        } else {\r\n            tokens.add(new CannedBinaryTokenStream.BinaryToken(term2, 1, 1));\r\n            i++;\r\n        }\r\n    }\r\n    return tokens.toArray(new CannedBinaryTokenStream.BinaryToken[0]);\r\n}"
}, {
	"Path": "org.elasticsearch.test.DiffableTestUtils.assertDiffApplication",
	"Comment": "asserts that changes are applied correctly, i.e. that applying diffs to localinstance produces that objectequal but not the same as the remotechanges instance.",
	"Method": "T assertDiffApplication(T remoteChanges,T localInstance,Diff<T> diffs){\r\n    T localChanges = diffs.apply(localInstance);\r\n    assertEquals(remoteChanges, localChanges);\r\n    assertEquals(remoteChanges.hashCode(), localChanges.hashCode());\r\n    assertNotSame(remoteChanges, localChanges);\r\n    return localChanges;\r\n}"
}, {
	"Path": "org.elasticsearch.protocol.xpack.graph.GraphExploreRequest.createNextHop",
	"Comment": "add a stage in the graph exploration. each hop represents a stage ofquerying elasticsearch to identify terms which can then be connnected toother terms in a subsequent hop.",
	"Method": "Hop createNextHop(QueryBuilder guidingQuery){\r\n    Hop parent = null;\r\n    if (hops.size() > 0) {\r\n        parent = hops.get(hops.size() - 1);\r\n    }\r\n    Hop newHop = new Hop(parent);\r\n    newHop.guidingQuery = guidingQuery;\r\n    hops.add(newHop);\r\n    return newHop;\r\n}"
}, {
	"Path": "org.elasticsearch.search.profile.query.QueryProfiler.startRewriteTime",
	"Comment": "begin timing the rewrite phase of a request.all rewrites are accumulated together into asingle metric",
	"Method": "void startRewriteTime(){\r\n    ((InternalQueryProfileTree) profileTree).startRewriteTime();\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.GeoDistanceSortBuilderTests.testBuildCoerce",
	"Comment": "test that if coercion is used, a point gets normalized but the original values in the builder are unchanged",
	"Method": "void testBuildCoerce(){\r\n    QueryShardContext shardContextMock = createMockShardContext();\r\n    GeoDistanceSortBuilder sortBuilder = new GeoDistanceSortBuilder(\"fieldName\", -180.0, -360.0);\r\n    sortBuilder.validation(GeoValidationMethod.COERCE);\r\n    assertEquals(-180.0, sortBuilder.points()[0].getLat(), 0.0);\r\n    assertEquals(-360.0, sortBuilder.points()[0].getLon(), 0.0);\r\n    SortField sortField = sortBuilder.build(shardContextMock).field;\r\n    assertEquals(LatLonDocValuesField.newDistanceSort(\"fieldName\", 0.0, 180.0), sortField);\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.startNode",
	"Comment": "starts a node with the given settings builder and returns its name.",
	"Method": "void startNode(String startNode,String startNode,Settings.Builder settings,String startNode,Settings settings){\r\n    return startNodes(settings).get(0);\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TransportService.getConnection",
	"Comment": "returns either a real transport connection or a local node connection if we are using the local node optimization.",
	"Method": "Transport.Connection getConnection(DiscoveryNode node){\r\n    if (isLocalNode(node)) {\r\n        return localNodeConnection;\r\n    } else {\r\n        return connectionManager.getConnection(node);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.config.AnalysisConfig.analysisFields",
	"Comment": "return the set of fields required by the analysis.these are the influencer fields, metric field, partition field,by field and over field of each detector, plus the summary countfield and the categorization field name of the job.null and empty strings are filtered from theconfig.",
	"Method": "Set<String> analysisFields(){\r\n    Set<String> analysisFields = termFields();\r\n    addIfNotNull(analysisFields, categorizationFieldName);\r\n    addIfNotNull(analysisFields, summaryCountFieldName);\r\n    for (Detector d : getDetectors()) {\r\n        addIfNotNull(analysisFields, d.getFieldName());\r\n    }\r\n    analysisFields.remove(\"\");\r\n    return analysisFields;\r\n}"
}, {
	"Path": "org.elasticsearch.script.FieldScript.getDoc",
	"Comment": "the doc lookup for the lucene segment this script was created for.",
	"Method": "Map<String, ScriptDocValues<?>> getDoc(){\r\n    return leafLookup.doc();\r\n}"
}, {
	"Path": "org.elasticsearch.VersionTests.testLuceneVersionIsSameOnMinorRelease",
	"Comment": "this test ensures we never bump the lucene version in a bugfix release",
	"Method": "void testLuceneVersionIsSameOnMinorRelease(){\r\n    for (Version version : VersionUtils.allReleasedVersions()) {\r\n        for (Version other : VersionUtils.allReleasedVersions()) {\r\n            if (other.onOrAfter(version)) {\r\n                assertTrue(\"lucene versions must be \" + other + \" >= \" + version, other.luceneVersion.onOrAfter(version.luceneVersion));\r\n            }\r\n            if (other.isAlpha() == false && version.isAlpha() == false && other.major == version.major && other.minor == version.minor) {\r\n                assertEquals(version + \" vs. \" + other, other.luceneVersion.major, version.luceneVersion.major);\r\n                assertEquals(version + \" vs. \" + other, other.luceneVersion.minor, version.luceneVersion.minor);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.UnassignedInfoTests.testNodeLeave",
	"Comment": "tests that during reroute when a node is detected as leaving the cluster, the right unassigned meta is set",
	"Method": "void testNodeLeave(){\r\n    AllocationService allocation = createAllocationService();\r\n    MetaData metaData = MetaData.builder().put(IndexMetaData.builder(\"test\").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1)).build();\r\n    ClusterState clusterState = ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)).metaData(metaData).routingTable(RoutingTable.builder().addAsNew(metaData.index(\"test\")).build()).build();\r\n    clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().add(newNode(\"node1\")).add(newNode(\"node2\"))).build();\r\n    clusterState = allocation.reroute(clusterState, \"reroute\");\r\n    clusterState = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));\r\n    clusterState = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));\r\n    assertThat(clusterState.getRoutingNodes().unassigned().size() > 0, equalTo(false));\r\n    clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove(\"node2\")).build();\r\n    clusterState = allocation.deassociateDeadNodes(clusterState, true, \"reroute\");\r\n    assertThat(clusterState.getRoutingNodes().unassigned().size() > 0, equalTo(true));\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1));\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo(), notNullValue());\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.NODE_LEFT));\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getUnassignedTimeInMillis(), greaterThan(0L));\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.stopRandomNode",
	"Comment": "stops a random node in the cluster that applies to the given filter or non if the non of the nodes applies to thefilter.",
	"Method": "void stopRandomNode(Predicate<Settings> filter){\r\n    ensureOpen();\r\n    NodeAndClient nodeAndClient = getRandomNodeAndClient(nc -> filter.test(nc.node.settings()));\r\n    if (nodeAndClient != null) {\r\n        logger.info(\"Closing filtered random node [{}] \", nodeAndClient.name);\r\n        stopNodesAndClient(nodeAndClient);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.RoutingTableTests.updateActiveAllocations",
	"Comment": "reverse engineer the in sync aid based on the given indexroutingtable",
	"Method": "IndexMetaData updateActiveAllocations(IndexRoutingTable indexRoutingTable,IndexMetaData indexMetaData){\r\n    IndexMetaData.Builder imdBuilder = IndexMetaData.builder(indexMetaData);\r\n    for (IndexShardRoutingTable shardTable : indexRoutingTable) {\r\n        for (ShardRouting shardRouting : shardTable) {\r\n            Set<String> insyncAids = shardTable.activeShards().stream().map(shr -> shr.allocationId().getId()).collect(Collectors.toSet());\r\n            final ShardRouting primaryShard = shardTable.primaryShard();\r\n            if (primaryShard.initializing() && primaryShard.recoverySource().getType() == RecoverySource.Type.EXISTING_STORE) {\r\n                insyncAids.add(primaryShard.allocationId().getId());\r\n            }\r\n            imdBuilder.putInSyncAllocationIds(shardRouting.id(), insyncAids);\r\n        }\r\n    }\r\n    return imdBuilder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.node.ResponseCollectorServiceTests.testConcurrentAddingAndRemoving",
	"Comment": "test that concurrently adding values and removing nodes does not cause exceptions",
	"Method": "void testConcurrentAddingAndRemoving(){\r\n    String[] nodes = new String[] { \"a\", \"b\", \"c\", \"d\" };\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    Runnable f = () -> {\r\n        try {\r\n            latch.await();\r\n        } catch (InterruptedException e) {\r\n            fail(\"should not be interrupted\");\r\n        }\r\n        for (int i = 0; i < randomIntBetween(100, 200); i++) {\r\n            if (randomBoolean()) {\r\n                collector.removeNode(randomFrom(nodes));\r\n            }\r\n            collector.addNodeStatistics(randomFrom(nodes), randomIntBetween(1, 100), randomIntBetween(1, 100), randomIntBetween(1, 100));\r\n        }\r\n    };\r\n    Thread t1 = new Thread(f);\r\n    Thread t2 = new Thread(f);\r\n    Thread t3 = new Thread(f);\r\n    Thread t4 = new Thread(f);\r\n    t1.start();\r\n    t2.start();\r\n    t3.start();\r\n    t4.start();\r\n    latch.countDown();\r\n    t1.join();\r\n    t2.join();\r\n    t3.join();\r\n    t4.join();\r\n    final Map<String, ResponseCollectorService.ComputedNodeStats> nodeStats = collector.getAllNodeStatistics();\r\n    logger.info(\"--> got stats: {}\", nodeStats);\r\n    for (String nodeId : nodes) {\r\n        if (nodeStats.containsKey(nodeId)) {\r\n            assertThat(nodeStats.get(nodeId).queueSize, greaterThan(0));\r\n            assertThat(nodeStats.get(nodeId).responseTime, greaterThan(0.0));\r\n            assertThat(nodeStats.get(nodeId).serviceTime, greaterThan(0.0));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.indices.store.IndicesStoreIntegrationIT.testShardCleanupIfShardDeletionAfterRelocationFailedAndIndexDeleted",
	"Comment": "test that shard is deleted in case shardactiverequest after relocation and next incoming cluster state is an index delete.",
	"Method": "void testShardCleanupIfShardDeletionAfterRelocationFailedAndIndexDeleted(){\r\n    final String node_1 = internalCluster().startNode();\r\n    logger.info(\"--> creating index [test] with one shard and on replica\");\r\n    assertAcked(prepareCreate(\"test\").setSettings(Settings.builder().put(indexSettings()).put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)));\r\n    ensureGreen(\"test\");\r\n    ClusterState state = client().admin().cluster().prepareState().get().getState();\r\n    Index index = state.metaData().index(\"test\").getIndex();\r\n    assertThat(Files.exists(shardDirectory(node_1, index, 0)), equalTo(true));\r\n    assertThat(Files.exists(indexDirectory(node_1, index)), equalTo(true));\r\n    final String node_2 = internalCluster().startDataOnlyNode(Settings.builder().build());\r\n    assertFalse(client().admin().cluster().prepareHealth().setWaitForNodes(\"2\").get().isTimedOut());\r\n    assertThat(Files.exists(shardDirectory(node_1, index, 0)), equalTo(true));\r\n    assertThat(Files.exists(indexDirectory(node_1, index)), equalTo(true));\r\n    assertThat(Files.exists(shardDirectory(node_2, index, 0)), equalTo(false));\r\n    assertThat(Files.exists(indexDirectory(node_2, index)), equalTo(false));\r\n    MockTransportService transportServiceNode_1 = (MockTransportService) internalCluster().getInstance(TransportService.class, node_1);\r\n    TransportService transportServiceNode_2 = internalCluster().getInstance(TransportService.class, node_2);\r\n    final CountDownLatch shardActiveRequestSent = new CountDownLatch(1);\r\n    transportServiceNode_1.addSendBehavior(transportServiceNode_2, (connection, requestId, action, request, options) -> {\r\n        if (action.equals(\"internal:index/shard/exists\") && shardActiveRequestSent.getCount() > 0) {\r\n            shardActiveRequestSent.countDown();\r\n            logger.info(\"prevent shard active request from being sent\");\r\n            throw new ConnectTransportException(connection.getNode(), \"DISCONNECT: simulated\");\r\n        }\r\n        connection.sendRequest(requestId, action, request, options);\r\n    });\r\n    logger.info(\"--> move shard from {} to {}, and wait for relocation to finish\", node_1, node_2);\r\n    internalCluster().client().admin().cluster().prepareReroute().add(new MoveAllocationCommand(\"test\", 0, node_1, node_2)).get();\r\n    shardActiveRequestSent.await();\r\n    ClusterHealthResponse clusterHealth = client().admin().cluster().prepareHealth().setWaitForNoRelocatingShards(true).get();\r\n    assertThat(clusterHealth.isTimedOut(), equalTo(false));\r\n    logClusterState();\r\n    client().admin().indices().prepareDelete(\"test\").get();\r\n    assertThat(waitForShardDeletion(node_1, index, 0), equalTo(false));\r\n    assertThat(waitForIndexDeletion(node_1, index), equalTo(false));\r\n    assertThat(Files.exists(shardDirectory(node_1, index, 0)), equalTo(false));\r\n    assertThat(Files.exists(indexDirectory(node_1, index)), equalTo(false));\r\n    assertThat(waitForShardDeletion(node_2, index, 0), equalTo(false));\r\n    assertThat(waitForIndexDeletion(node_2, index), equalTo(false));\r\n    assertThat(Files.exists(shardDirectory(node_2, index, 0)), equalTo(false));\r\n    assertThat(Files.exists(indexDirectory(node_2, index)), equalTo(false));\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.graph.action.GraphExploreRequestBuilder.setTypes",
	"Comment": "the types of documents the graph exploration will run against. defaultsto all types.",
	"Method": "GraphExploreRequestBuilder setTypes(String types){\r\n    request.types(types);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.ESRestTestCase.logIfThereAreRunningTasks",
	"Comment": "logs a message if there are still running tasks. the reasoning is that any tasks still running are state the is trying to bleed intoother tests.",
	"Method": "void logIfThereAreRunningTasks(){\r\n    Set<String> runningTasks = runningTasks(adminClient().performRequest(new Request(\"GET\", \"/_tasks\")));\r\n    runningTasks.remove(ListTasksAction.NAME);\r\n    runningTasks.remove(ListTasksAction.NAME + \"[n]\");\r\n    if (runningTasks.isEmpty()) {\r\n        return;\r\n    }\r\n    List<String> stillRunning = new ArrayList(runningTasks);\r\n    sort(stillRunning);\r\n    logger.info(\"There are still tasks running after this test that might break subsequent tests {}.\", stillRunning);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.DelayedAllocationServiceTests.testDelayedUnassignedScheduleRerouteAfterDelayedReroute",
	"Comment": "this tests that a new delayed reroute is scheduled right after a delayed reroute was run",
	"Method": "void testDelayedUnassignedScheduleRerouteAfterDelayedReroute(){\r\n    TimeValue shortDelaySetting = timeValueMillis(100);\r\n    TimeValue longDelaySetting = TimeValue.timeValueSeconds(1);\r\n    MetaData metaData = MetaData.builder().put(IndexMetaData.builder(\"short_delay\").settings(settings(Version.CURRENT).put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), shortDelaySetting)).numberOfShards(1).numberOfReplicas(1)).put(IndexMetaData.builder(\"long_delay\").settings(settings(Version.CURRENT).put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), longDelaySetting)).numberOfShards(1).numberOfReplicas(1)).build();\r\n    ClusterState clusterState = ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)).metaData(metaData).routingTable(RoutingTable.builder().addAsNew(metaData.index(\"short_delay\")).addAsNew(metaData.index(\"long_delay\")).build()).nodes(DiscoveryNodes.builder().add(newNode(\"node0\", singleton(DiscoveryNode.Role.MASTER))).localNodeId(\"node0\").masterNodeId(\"node0\").add(newNode(\"node1\")).add(newNode(\"node2\")).add(newNode(\"node3\")).add(newNode(\"node4\"))).build();\r\n    clusterState = allocationService.reroute(clusterState, \"reroute\");\r\n    clusterState = allocationService.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));\r\n    clusterState = allocationService.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));\r\n    assertThat(\"all shards should be started\", clusterState.getRoutingNodes().shardsWithState(STARTED).size(), equalTo(4));\r\n    ShardRouting shortDelayReplica = null;\r\n    for (ShardRouting shardRouting : clusterState.getRoutingTable().allShards(\"short_delay\")) {\r\n        if (shardRouting.primary() == false) {\r\n            shortDelayReplica = shardRouting;\r\n            break;\r\n        }\r\n    }\r\n    assertNotNull(shortDelayReplica);\r\n    ShardRouting longDelayReplica = null;\r\n    for (ShardRouting shardRouting : clusterState.getRoutingTable().allShards(\"long_delay\")) {\r\n        if (shardRouting.primary() == false) {\r\n            longDelayReplica = shardRouting;\r\n            break;\r\n        }\r\n    }\r\n    assertNotNull(longDelayReplica);\r\n    final long baseTimestampNanos = System.nanoTime();\r\n    ClusterState clusterStateBeforeNodeLeft = clusterState;\r\n    clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder(clusterState.nodes()).remove(shortDelayReplica.currentNodeId()).remove(longDelayReplica.currentNodeId())).build();\r\n    allocationService.setNanoTimeOverride(baseTimestampNanos);\r\n    clusterState = allocationService.deassociateDeadNodes(clusterState, true, \"reroute\");\r\n    final ClusterState stateWithDelayedShards = clusterState;\r\n    assertEquals(2, UnassignedInfo.getNumberOfDelayedUnassigned(stateWithDelayedShards));\r\n    RoutingNodes.UnassignedShards.UnassignedIterator iter = stateWithDelayedShards.getRoutingNodes().unassigned().iterator();\r\n    assertEquals(baseTimestampNanos, iter.next().unassignedInfo().getUnassignedTimeInNanos());\r\n    assertEquals(baseTimestampNanos, iter.next().unassignedInfo().getUnassignedTimeInNanos());\r\n    CountDownLatch latch1 = new CountDownLatch(1);\r\n    AtomicReference<ClusterStateUpdateTask> clusterStateUpdateTask1 = new AtomicReference();\r\n    doAnswer(invocationOnMock -> {\r\n        clusterStateUpdateTask1.set((ClusterStateUpdateTask) invocationOnMock.getArguments()[1]);\r\n        latch1.countDown();\r\n        return null;\r\n    }).when(clusterService).submitStateUpdateTask(eq(CLUSTER_UPDATE_TASK_SOURCE), any(ClusterStateUpdateTask.class));\r\n    assertNull(delayedAllocationService.delayedRerouteTask.get());\r\n    long delayUntilClusterChangeEvent = TimeValue.timeValueNanos(randomInt((int) shortDelaySetting.nanos() - 1)).nanos();\r\n    long clusterChangeEventTimestampNanos = baseTimestampNanos + delayUntilClusterChangeEvent;\r\n    delayedAllocationService.setNanoTimeOverride(clusterChangeEventTimestampNanos);\r\n    delayedAllocationService.clusterChanged(new ClusterChangedEvent(\"fake node left\", stateWithDelayedShards, clusterStateBeforeNodeLeft));\r\n    DelayedAllocationService.DelayedRerouteTask firstDelayedRerouteTask = delayedAllocationService.delayedRerouteTask.get();\r\n    assertNotNull(firstDelayedRerouteTask);\r\n    assertFalse(firstDelayedRerouteTask.cancelScheduling.get());\r\n    assertThat(firstDelayedRerouteTask.baseTimestampNanos, equalTo(clusterChangeEventTimestampNanos));\r\n    assertThat(firstDelayedRerouteTask.nextDelay.nanos(), equalTo(UnassignedInfo.findNextDelayedAllocation(clusterChangeEventTimestampNanos, stateWithDelayedShards)));\r\n    assertThat(firstDelayedRerouteTask.nextDelay.nanos(), equalTo(shortDelaySetting.nanos() - (clusterChangeEventTimestampNanos - baseTimestampNanos)));\r\n    assertTrue(latch1.await(30, TimeUnit.SECONDS));\r\n    verify(clusterService).submitStateUpdateTask(eq(CLUSTER_UPDATE_TASK_SOURCE), eq(clusterStateUpdateTask1.get()));\r\n    long nanoTimeForReroute = clusterChangeEventTimestampNanos + shortDelaySetting.nanos() + timeValueMillis(randomInt(50)).nanos();\r\n    allocationService.setNanoTimeOverride(nanoTimeForReroute);\r\n    ClusterState stateWithOnlyOneDelayedShard = clusterStateUpdateTask1.get().execute(stateWithDelayedShards);\r\n    assertEquals(1, UnassignedInfo.getNumberOfDelayedUnassigned(stateWithOnlyOneDelayedShard));\r\n    assertNull(delayedAllocationService.delayedRerouteTask.get());\r\n    CountDownLatch latch2 = new CountDownLatch(1);\r\n    AtomicReference<ClusterStateUpdateTask> clusterStateUpdateTask2 = new AtomicReference();\r\n    doAnswer(invocationOnMock -> {\r\n        clusterStateUpdateTask2.set((ClusterStateUpdateTask) invocationOnMock.getArguments()[1]);\r\n        latch2.countDown();\r\n        return null;\r\n    }).when(clusterService).submitStateUpdateTask(eq(CLUSTER_UPDATE_TASK_SOURCE), any(ClusterStateUpdateTask.class));\r\n    delayUntilClusterChangeEvent = timeValueMillis(randomInt(50)).nanos();\r\n    clusterChangeEventTimestampNanos = nanoTimeForReroute + delayUntilClusterChangeEvent;\r\n    delayedAllocationService.setNanoTimeOverride(clusterChangeEventTimestampNanos);\r\n    delayedAllocationService.clusterChanged(new ClusterChangedEvent(CLUSTER_UPDATE_TASK_SOURCE, stateWithOnlyOneDelayedShard, stateWithDelayedShards));\r\n    DelayedAllocationService.DelayedRerouteTask secondDelayedRerouteTask = delayedAllocationService.delayedRerouteTask.get();\r\n    assertNotNull(secondDelayedRerouteTask);\r\n    assertFalse(secondDelayedRerouteTask.cancelScheduling.get());\r\n    assertThat(secondDelayedRerouteTask.baseTimestampNanos, equalTo(clusterChangeEventTimestampNanos));\r\n    assertThat(secondDelayedRerouteTask.nextDelay.nanos(), equalTo(UnassignedInfo.findNextDelayedAllocation(clusterChangeEventTimestampNanos, stateWithOnlyOneDelayedShard)));\r\n    assertThat(secondDelayedRerouteTask.nextDelay.nanos(), equalTo(longDelaySetting.nanos() - (clusterChangeEventTimestampNanos - baseTimestampNanos)));\r\n    assertTrue(latch2.await(30, TimeUnit.SECONDS));\r\n    verify(clusterService).submitStateUpdateTask(eq(CLUSTER_UPDATE_TASK_SOURCE), eq(clusterStateUpdateTask2.get()));\r\n    nanoTimeForReroute = clusterChangeEventTimestampNanos + longDelaySetting.nanos() + timeValueMillis(randomInt(50)).nanos();\r\n    allocationService.setNanoTimeOverride(nanoTimeForReroute);\r\n    ClusterState stateWithNoDelayedShards = clusterStateUpdateTask2.get().execute(stateWithOnlyOneDelayedShard);\r\n    assertEquals(0, UnassignedInfo.getNumberOfDelayedUnassigned(stateWithNoDelayedShards));\r\n    assertNull(delayedAllocationService.delayedRerouteTask.get());\r\n    delayedAllocationService.setNanoTimeOverride(nanoTimeForReroute + timeValueMillis(randomInt(50)).nanos());\r\n    delayedAllocationService.clusterChanged(new ClusterChangedEvent(CLUSTER_UPDATE_TASK_SOURCE, stateWithNoDelayedShards, stateWithOnlyOneDelayedShard));\r\n    assertNull(delayedAllocationService.delayedRerouteTask.get());\r\n    verifyNoMoreInteractions(clusterService);\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.graph.action.GraphExploreRequestBuilder.setTimeout",
	"Comment": "an optional timeout to control how long the graph exploration is allowedto take.",
	"Method": "GraphExploreRequestBuilder setTimeout(TimeValue timeout,GraphExploreRequestBuilder setTimeout,String timeout){\r\n    request.timeout(timeout);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.process.autodetect.state.CategorizerState.extractJobId",
	"Comment": "given the id of a categorizer state document it extracts the job id",
	"Method": "String extractJobId(String docId){\r\n    int suffixIndex = docId.lastIndexOf(\"_\" + TYPE);\r\n    return suffixIndex <= 0 ? null : docId.substring(0, suffixIndex);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregationBuilder.shardMinDocCount",
	"Comment": "set the minimum document count terms should have on the shard in order toappear in the response.",
	"Method": "SignificantTermsAggregationBuilder shardMinDocCount(long shardMinDocCount){\r\n    if (shardMinDocCount < 0) {\r\n        throw new IllegalArgumentException(\"[shardMinDocCount] must be greater than or equal to 0. Found [\" + shardMinDocCount + \"] in [\" + name + \"]\");\r\n    }\r\n    bucketCountThresholds.setShardMinDocCount(shardMinDocCount);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractQueryTestCase.builderGeneratesCacheableQueries",
	"Comment": "whether the queries produced by this builder are expected to be cacheable.",
	"Method": "boolean builderGeneratesCacheableQueries(){\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.protocol.xpack.graph.GraphExploreRequest.maxDocsPerDiversityValue",
	"Comment": "optional number of permitted docs with same value in sampled searchresults. must also declare which field using samplediversityfield",
	"Method": "void maxDocsPerDiversityValue(int maxDocs,int maxDocsPerDiversityValue){\r\n    return maxDocsPerDiversityValue;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.NodeJoinControllerTests.testElectionBasedOnConflictingNodes",
	"Comment": "tests tha node can become a master, even though the last cluster state it knows containsnodes that conflict with the joins it got and needs to become a master",
	"Method": "void testElectionBasedOnConflictingNodes(){\r\n    ClusterState initialState = initialState(true);\r\n    final DiscoveryNode masterNode = initialState.nodes().getLocalNode();\r\n    final DiscoveryNode otherNode = new DiscoveryNode(\"other_node\", buildNewFakeTransportAddress(), emptyMap(), EnumSet.allOf(DiscoveryNode.Role.class), Version.CURRENT);\r\n    DiscoveryNodes.Builder discoBuilder = DiscoveryNodes.builder(initialState.nodes());\r\n    discoBuilder.masterNodeId(null);\r\n    discoBuilder.add(otherNode);\r\n    ClusterState.Builder stateBuilder = ClusterState.builder(initialState).nodes(discoBuilder);\r\n    if (randomBoolean()) {\r\n        IndexMetaData indexMetaData = IndexMetaData.builder(\"test\").settings(Settings.builder().put(SETTING_VERSION_CREATED, Version.CURRENT).put(SETTING_NUMBER_OF_SHARDS, 1).put(SETTING_NUMBER_OF_REPLICAS, 1).put(SETTING_CREATION_DATE, System.currentTimeMillis())).build();\r\n        IndexRoutingTable.Builder indexRoutingTableBuilder = IndexRoutingTable.builder(indexMetaData.getIndex());\r\n        RoutingTable.Builder routing = new RoutingTable.Builder();\r\n        routing.addAsNew(indexMetaData);\r\n        final ShardId shardId = new ShardId(\"test\", \"_na_\", 0);\r\n        IndexShardRoutingTable.Builder indexShardRoutingBuilder = new IndexShardRoutingTable.Builder(shardId);\r\n        final DiscoveryNode primaryNode = randomBoolean() ? masterNode : otherNode;\r\n        final DiscoveryNode replicaNode = primaryNode.equals(masterNode) ? otherNode : masterNode;\r\n        final boolean primaryStarted = randomBoolean();\r\n        indexShardRoutingBuilder.addShard(TestShardRouting.newShardRouting(\"test\", 0, primaryNode.getId(), null, true, primaryStarted ? ShardRoutingState.STARTED : ShardRoutingState.INITIALIZING, primaryStarted ? null : new UnassignedInfo(UnassignedInfo.Reason.INDEX_REOPENED, \"getting there\")));\r\n        if (primaryStarted) {\r\n            boolean replicaStared = randomBoolean();\r\n            indexShardRoutingBuilder.addShard(TestShardRouting.newShardRouting(\"test\", 0, replicaNode.getId(), null, false, replicaStared ? ShardRoutingState.STARTED : ShardRoutingState.INITIALIZING, replicaStared ? null : new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, \"getting there\")));\r\n        } else {\r\n            indexShardRoutingBuilder.addShard(TestShardRouting.newShardRouting(\"test\", 0, null, null, false, ShardRoutingState.UNASSIGNED, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, \"life sucks\")));\r\n        }\r\n        indexRoutingTableBuilder.addIndexShard(indexShardRoutingBuilder.build());\r\n        IndexRoutingTable indexRoutingTable = indexRoutingTableBuilder.build();\r\n        IndexMetaData updatedIndexMetaData = updateActiveAllocations(indexRoutingTable, indexMetaData);\r\n        stateBuilder.metaData(MetaData.builder().put(updatedIndexMetaData, false).generateClusterUuidIfNeeded()).routingTable(RoutingTable.builder().add(indexRoutingTable).build());\r\n    }\r\n    setupMasterServiceAndNodeJoinController(stateBuilder.build());\r\n    final DiscoveryNode conflictingNode = randomBoolean() ? new DiscoveryNode(otherNode.getId(), randomBoolean() ? otherNode.getAddress() : buildNewFakeTransportAddress(), otherNode.getAttributes(), otherNode.getRoles(), Version.CURRENT) : new DiscoveryNode(\"conflicting_address_node\", otherNode.getAddress(), otherNode.getAttributes(), otherNode.getRoles(), Version.CURRENT);\r\n    nodeJoinController.startElectionContext();\r\n    final SimpleFuture joinFuture = joinNodeAsync(conflictingNode);\r\n    final CountDownLatch elected = new CountDownLatch(1);\r\n    nodeJoinController.waitToBeElectedAsMaster(1, TimeValue.timeValueHours(5), new NodeJoinController.ElectionCallback() {\r\n        @Override\r\n        public void onElectedAsMaster(ClusterState state) {\r\n            elected.countDown();\r\n        }\r\n        @Override\r\n        public void onFailure(Throwable t) {\r\n            logger.error(\"failed to be elected as master\", t);\r\n            throw new AssertionError(\"failed to be elected as master\", t);\r\n        }\r\n    });\r\n    elected.await();\r\n    joinFuture.get();\r\n    final ClusterState finalState = discoveryState(masterService);\r\n    final DiscoveryNodes finalNodes = finalState.nodes();\r\n    assertTrue(finalNodes.isLocalNodeElectedMaster());\r\n    assertThat(finalNodes.getLocalNode(), equalTo(masterNode));\r\n    assertThat(finalNodes.getSize(), equalTo(2));\r\n    assertThat(finalNodes.get(conflictingNode.getId()), equalTo(conflictingNode));\r\n    List<ShardRouting> activeShardsOnRestartedNode = StreamSupport.stream(finalState.getRoutingNodes().node(conflictingNode.getId()).spliterator(), false).filter(ShardRouting::active).collect(Collectors.toList());\r\n    assertThat(activeShardsOnRestartedNode, empty());\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.NodeJoinControllerTests.testElectionBasedOnConflictingNodes",
	"Comment": "tests tha node can become a master, even though the last cluster state it knows containsnodes that conflict with the joins it got and needs to become a master",
	"Method": "void testElectionBasedOnConflictingNodes(){\r\n    elected.countDown();\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.NodeJoinControllerTests.testElectionBasedOnConflictingNodes",
	"Comment": "tests tha node can become a master, even though the last cluster state it knows containsnodes that conflict with the joins it got and needs to become a master",
	"Method": "void testElectionBasedOnConflictingNodes(){\r\n    logger.error(\"failed to be elected as master\", t);\r\n    throw new AssertionError(\"failed to be elected as master\", t);\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TcpTransport.internalSendMessage",
	"Comment": "sends a message to the given channel, using the given callbacks.",
	"Method": "void internalSendMessage(TcpChannel channel,BytesReference message,ActionListener<Void> listener){\r\n    channel.getChannelStats().markAccessed(threadPool.relativeTimeInMillis());\r\n    transportLogger.logOutboundMessage(channel, message);\r\n    try {\r\n        channel.sendMessage(message, new SendListener(channel, message.length(), listener));\r\n    } catch (Exception ex) {\r\n        listener.onFailure(ex);\r\n        onException(channel, ex);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.PercentilesBucketPipelineAggregationBuilder.percents",
	"Comment": "set the percentages to calculate percentiles for in this aggregation",
	"Method": "double[] percents(PercentilesBucketPipelineAggregationBuilder percents,double[] percents){\r\n    if (percents == null) {\r\n        throw new IllegalArgumentException(\"[percents] must not be null: [\" + name + \"]\");\r\n    }\r\n    for (Double p : percents) {\r\n        if (p == null || p < 0.0 || p > 100.0) {\r\n            throw new IllegalArgumentException(PERCENTS_FIELD.getPreferredName() + \" must only contain non-null doubles from 0.0-100.0 inclusive\");\r\n        }\r\n    }\r\n    this.percents = percents;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.cache.CacheTests.testCacheStats",
	"Comment": "cache some entries, then randomly lookup keys that do not exist, then check the stats",
	"Method": "void testCacheStats(){\r\n    AtomicLong evictions = new AtomicLong();\r\n    Set<Integer> keys = new HashSet();\r\n    Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().setMaximumWeight(numberOfEntries / 2).removalListener(notification -> {\r\n        keys.remove(notification.getKey());\r\n        evictions.incrementAndGet();\r\n    }).build();\r\n    for (int i = 0; i < numberOfEntries; i++) {\r\n        keys.add(i);\r\n        cache.put(i, Integer.toString(i));\r\n    }\r\n    long hits = 0;\r\n    long misses = 0;\r\n    Integer missingKey = 0;\r\n    for (Integer key : keys) {\r\n        --missingKey;\r\n        if (rarely()) {\r\n            misses++;\r\n            cache.get(missingKey);\r\n        } else {\r\n            hits++;\r\n            cache.get(key);\r\n        }\r\n    }\r\n    assertEquals(hits, cache.stats().getHits());\r\n    assertEquals(misses, cache.stats().getMisses());\r\n    assertEquals((long) Math.ceil(numberOfEntries / 2.0), evictions.get());\r\n    assertEquals(evictions.get(), cache.stats().getEvictions());\r\n}"
}, {
	"Path": "org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilderTests.testFromXContent",
	"Comment": "creates random highlighter, renders it to xcontent and back to new instance that should be equal to original",
	"Method": "void testFromXContent(){\r\n    for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {\r\n        HighlightBuilder highlightBuilder = randomHighlighterBuilder();\r\n        XContentBuilder builder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));\r\n        if (randomBoolean()) {\r\n            builder.prettyPrint();\r\n        }\r\n        XContentBuilder shuffled;\r\n        if (randomBoolean()) {\r\n            highlightBuilder.useExplicitFieldOrder(true);\r\n            highlightBuilder.toXContent(builder, ToXContent.EMPTY_PARAMS);\r\n            shuffled = shuffleXContent(builder);\r\n        } else {\r\n            highlightBuilder.toXContent(builder, ToXContent.EMPTY_PARAMS);\r\n            shuffled = shuffleXContent(builder, \"fields\");\r\n        }\r\n        try (XContentParser parser = createParser(shuffled)) {\r\n            parser.nextToken();\r\n            HighlightBuilder secondHighlightBuilder;\r\n            try {\r\n                secondHighlightBuilder = HighlightBuilder.fromXContent(parser);\r\n            } catch (RuntimeException e) {\r\n                throw new RuntimeException(\"Error parsing \" + highlightBuilder, e);\r\n            }\r\n            assertNotSame(highlightBuilder, secondHighlightBuilder);\r\n            assertEquals(highlightBuilder, secondHighlightBuilder);\r\n            assertEquals(highlightBuilder.hashCode(), secondHighlightBuilder.hashCode());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.SignificantTermsSignificanceScoreIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    ScriptHeuristic scriptHeuristic = getScriptSignificanceHeuristic();\r\n    boolean useSigText = randomBoolean();\r\n    SearchResponse r;\r\n    if (useSigText) {\r\n        r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(significantText(\"foo\", \"s\").significanceHeuristic(scriptHeuristic)).get();\r\n    } else {\r\n        r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(significantTerms(\"foo\").field(\"s\").significanceHeuristic(scriptHeuristic)).get();\r\n    }\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    if (useSigText) {\r\n        r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(significantText(\"foo\", \"s\")).get();\r\n    } else {\r\n        r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(significantTerms(\"foo\").field(\"s\")).get();\r\n    }\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.GeoDistanceSortBuilder.sortMode",
	"Comment": "returns which distance to use for sorting in the case a document contains multiple geo points.",
	"Method": "GeoDistanceSortBuilder sortMode(SortMode sortMode,SortMode sortMode){\r\n    return this.sortMode;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.PrimaryAllocationIT.testForceAllocatePrimaryOnNoDecision",
	"Comment": "this test ensures that for an unassigned primary shard that has a valid shard copy on at least one node,we will force allocate the primary shard to one of those nodes, even if the allocation deciders all returna no decision to allocate.",
	"Method": "void testForceAllocatePrimaryOnNoDecision(){\r\n    logger.info(\"--> starting 1 node\");\r\n    final String node = internalCluster().startNode();\r\n    logger.info(\"--> creating index with 1 primary and 0 replicas\");\r\n    final String indexName = \"test-idx\";\r\n    assertAcked(client().admin().indices().prepareCreate(indexName).setSettings(Settings.builder().put(IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.getKey(), 1).put(IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.getKey(), 0)).get());\r\n    logger.info(\"--> update the settings to prevent allocation to the data node\");\r\n    assertTrue(client().admin().indices().prepareUpdateSettings(indexName).setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_EXCLUDE_GROUP_SETTING.getKey() + \"_name\", node)).get().isAcknowledged());\r\n    logger.info(\"--> full cluster restart\");\r\n    internalCluster().fullRestart();\r\n    logger.info(\"--> checking that the primary shard is force allocated to the data node despite being blocked by the exclude filter\");\r\n    ensureGreen(indexName);\r\n    assertEquals(1, client().admin().cluster().prepareState().get().getState().routingTable().index(indexName).shardsWithState(ShardRoutingState.STARTED).size());\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.SuggestBuilderTests.testFromXContent",
	"Comment": "creates random suggestion builder, renders it to xcontent and back to new instance that should be equal to original",
	"Method": "void testFromXContent(){\r\n    for (int runs = 0; runs < NUMBER_OF_RUNS; runs++) {\r\n        SuggestBuilder suggestBuilder = randomSuggestBuilder();\r\n        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));\r\n        if (randomBoolean()) {\r\n            xContentBuilder.prettyPrint();\r\n        }\r\n        suggestBuilder.toXContent(xContentBuilder, ToXContent.EMPTY_PARAMS);\r\n        try (XContentParser parser = createParser(xContentBuilder)) {\r\n            SuggestBuilder secondSuggestBuilder = SuggestBuilder.fromXContent(parser);\r\n            assertNotSame(suggestBuilder, secondSuggestBuilder);\r\n            assertEquals(suggestBuilder, secondSuggestBuilder);\r\n            assertEquals(suggestBuilder.hashCode(), secondSuggestBuilder.hashCode());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.process.autodetect.state.DataCounts.getInputBytes",
	"Comment": "the total number of bytes sent to this job.this value includes the bytes from anyrecordsthat have been discarded for anyreasone.g. because the date cannot be read",
	"Method": "long getInputBytes(){\r\n    return inputBytes;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.FieldSortBuilder.setNestedFilter",
	"Comment": "sets the nested filter that the nested objects should match with in orderto be taken into account for sorting.",
	"Method": "FieldSortBuilder setNestedFilter(QueryBuilder nestedFilter){\r\n    if (this.nestedSort != null) {\r\n        throw new IllegalArgumentException(\"Setting both nested_path/nested_filter and nested not allowed\");\r\n    }\r\n    this.nestedFilter = nestedFilter;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.breaker.HierarchyCircuitBreakerServiceTests.testBorrowingSiblingBreakerMemory",
	"Comment": "test that a breaker correctly redistributes to a different breaker, inthis case, the request breaker borrows space from the fielddata breaker",
	"Method": "void testBorrowingSiblingBreakerMemory(){\r\n    Settings clusterSettings = Settings.builder().put(HierarchyCircuitBreakerService.USE_REAL_MEMORY_USAGE_SETTING.getKey(), false).put(HierarchyCircuitBreakerService.TOTAL_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), \"200mb\").put(HierarchyCircuitBreakerService.REQUEST_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), \"150mb\").put(HierarchyCircuitBreakerService.FIELDDATA_CIRCUIT_BREAKER_LIMIT_SETTING.getKey(), \"150mb\").build();\r\n    try (CircuitBreakerService service = new HierarchyCircuitBreakerService(clusterSettings, new ClusterSettings(clusterSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS))) {\r\n        CircuitBreaker requestCircuitBreaker = service.getBreaker(CircuitBreaker.REQUEST);\r\n        CircuitBreaker fieldDataCircuitBreaker = service.getBreaker(CircuitBreaker.FIELDDATA);\r\n        assertEquals(new ByteSizeValue(200, ByteSizeUnit.MB).getBytes(), service.stats().getStats(CircuitBreaker.PARENT).getLimit());\r\n        assertEquals(new ByteSizeValue(150, ByteSizeUnit.MB).getBytes(), requestCircuitBreaker.getLimit());\r\n        assertEquals(new ByteSizeValue(150, ByteSizeUnit.MB).getBytes(), fieldDataCircuitBreaker.getLimit());\r\n        double fieldDataUsedBytes = fieldDataCircuitBreaker.addEstimateBytesAndMaybeBreak(new ByteSizeValue(50, ByteSizeUnit.MB).getBytes(), \"should not break\");\r\n        assertEquals(new ByteSizeValue(50, ByteSizeUnit.MB).getBytes(), fieldDataUsedBytes, 0.0);\r\n        double requestUsedBytes = requestCircuitBreaker.addEstimateBytesAndMaybeBreak(new ByteSizeValue(50, ByteSizeUnit.MB).getBytes(), \"should not break\");\r\n        assertEquals(new ByteSizeValue(50, ByteSizeUnit.MB).getBytes(), requestUsedBytes, 0.0);\r\n        requestUsedBytes = requestCircuitBreaker.addEstimateBytesAndMaybeBreak(new ByteSizeValue(50, ByteSizeUnit.MB).getBytes(), \"should not break\");\r\n        assertEquals(new ByteSizeValue(100, ByteSizeUnit.MB).getBytes(), requestUsedBytes, 0.0);\r\n        CircuitBreakingException exception = expectThrows(CircuitBreakingException.class, () -> requestCircuitBreaker.addEstimateBytesAndMaybeBreak(new ByteSizeValue(50, ByteSizeUnit.MB).getBytes(), \"should break\"));\r\n        assertThat(exception.getMessage(), containsString(\"[parent] Data too large, data for [should break] would be\"));\r\n        assertThat(exception.getMessage(), containsString(\"which is larger than the limit of [209715200/200mb]\"));\r\n        assertThat(exception.getMessage(), containsString(\"usages [request=157286400/150mb, fielddata=54001664/51.5mb, in_flight_requests=0/0b, accounting=0/0b]\"));\r\n        assertThat(exception.getDurability(), equalTo(CircuitBreaker.Durability.TRANSIENT));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.restspec.ClientYamlSuiteRestApi.getSupportedMethods",
	"Comment": "returns the supported http methods given the rest parameters provided",
	"Method": "List<String> getSupportedMethods(Set<String> restParams){\r\n    if (\"index\".equals(name) || \"create\".equals(name)) {\r\n        List<String> indexMethods = new ArrayList();\r\n        for (String method : methods) {\r\n            if (restParams.contains(\"id\")) {\r\n                if (HttpPut.METHOD_NAME.equals(method)) {\r\n                    indexMethods.add(method);\r\n                }\r\n            } else {\r\n                if (HttpPost.METHOD_NAME.equals(method)) {\r\n                    indexMethods.add(method);\r\n                }\r\n            }\r\n        }\r\n        return indexMethods;\r\n    }\r\n    return methods;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TransportActionProxy.getProxyAction",
	"Comment": "returns the corresponding proxy action for the given action",
	"Method": "String getProxyAction(String action){\r\n    return PROXY_ACTION_PREFIX + action;\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksDecidersTestCase.assertNbUnassignedTasks",
	"Comment": "asserts that the given cluster state contains nbtasks tasks that are not assigned",
	"Method": "void assertNbUnassignedTasks(long nbTasks,ClusterState clusterState){\r\n    assertPersistentTasks(nbTasks, clusterState, task -> task.isAssigned() == false);\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShardTestCase.newStartedShard",
	"Comment": "creates a new empty shard with the specified settings and engine factory and starts it.",
	"Method": "IndexShard newStartedShard(IndexShard newStartedShard,Settings settings,IndexShard newStartedShard,boolean primary,IndexShard newStartedShard,boolean primary,Settings settings,EngineFactory engineFactory,IndexShard newStartedShard,CheckedFunction<Boolean, IndexShard, IOException> shardFunction,boolean primary){\r\n    IndexShard shard = shardFunction.apply(primary);\r\n    if (primary) {\r\n        recoverShardFromStore(shard);\r\n        assertThat(shard.getMaxSeqNoOfUpdatesOrDeletes(), equalTo(shard.seqNoStats().getMaxSeqNo()));\r\n    } else {\r\n        recoveryEmptyReplica(shard, true);\r\n    }\r\n    return shard;\r\n}"
}, {
	"Path": "org.elasticsearch.common.time.DateFormattersTests.testEpochMillisParser",
	"Comment": "as this feature is supported it also makes sense to make it exact",
	"Method": "void testEpochMillisParser(){\r\n    DateFormatter formatter = DateFormatters.forPattern(\"epoch_millis\");\r\n    {\r\n        Instant instant = Instant.from(formatter.parse(\"12345.6789\"));\r\n        assertThat(instant.getEpochSecond(), is(12L));\r\n        assertThat(instant.getNano(), is(345_678_900));\r\n    }\r\n    {\r\n        Instant instant = Instant.from(formatter.parse(\"12345\"));\r\n        assertThat(instant.getEpochSecond(), is(12L));\r\n        assertThat(instant.getNano(), is(345_000_000));\r\n    }\r\n    {\r\n        Instant instant = Instant.from(formatter.parse(\"12345.\"));\r\n        assertThat(instant.getEpochSecond(), is(12L));\r\n        assertThat(instant.getNano(), is(345_000_000));\r\n    }\r\n    {\r\n        Instant instant = Instant.from(formatter.parse(\"-12345.6789\"));\r\n        assertThat(instant.getEpochSecond(), is(-13L));\r\n        assertThat(instant.getNano(), is(1_000_000_000 - 345_678_900));\r\n    }\r\n    {\r\n        Instant instant = Instant.from(formatter.parse(\"-436134.241272\"));\r\n        assertThat(instant.getEpochSecond(), is(-437L));\r\n        assertThat(instant.getNano(), is(1_000_000_000 - 134_241_272));\r\n    }\r\n    {\r\n        Instant instant = Instant.from(formatter.parse(\"-12345\"));\r\n        assertThat(instant.getEpochSecond(), is(-13L));\r\n        assertThat(instant.getNano(), is(1_000_000_000 - 345_000_000));\r\n    }\r\n    {\r\n        Instant instant = Instant.from(formatter.parse(\"0\"));\r\n        assertThat(instant.getEpochSecond(), is(0L));\r\n        assertThat(instant.getNano(), is(0));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.CorruptedFileIT.pruneOldDeleteGenerations",
	"Comment": "prunes the list of index files such that only the latest del generation files are contained.",
	"Method": "void pruneOldDeleteGenerations(Set<Path> files){\r\n    final TreeSet<Path> delFiles = new TreeSet();\r\n    for (Path file : files) {\r\n        if (file.getFileName().toString().endsWith(\".liv\")) {\r\n            delFiles.add(file);\r\n        }\r\n    }\r\n    Path last = null;\r\n    for (Path current : delFiles) {\r\n        if (last != null) {\r\n            final String newSegmentName = IndexFileNames.parseSegmentName(current.getFileName().toString());\r\n            final String oldSegmentName = IndexFileNames.parseSegmentName(last.getFileName().toString());\r\n            if (newSegmentName.equals(oldSegmentName)) {\r\n                int oldGen = Integer.parseInt(IndexFileNames.stripExtension(IndexFileNames.stripSegmentName(last.getFileName().toString())).replace(\"_\", \"\"), Character.MAX_RADIX);\r\n                int newGen = Integer.parseInt(IndexFileNames.stripExtension(IndexFileNames.stripSegmentName(current.getFileName().toString())).replace(\"_\", \"\"), Character.MAX_RADIX);\r\n                if (newGen > oldGen) {\r\n                    files.remove(last);\r\n                } else {\r\n                    files.remove(current);\r\n                    continue;\r\n                }\r\n            }\r\n        }\r\n        last = current;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.ReplicaShardAllocatorTests.testNoDataForReplicaOnAnyNode",
	"Comment": "verifies that when there is primary data, but no data at all on other nodes, the shard keepsunassigned to be allocated later on.",
	"Method": "void testNoDataForReplicaOnAnyNode(){\r\n    RoutingAllocation allocation = onePrimaryOnNode1And1Replica(yesAllocationDeciders());\r\n    testAllocator.addData(node1, \"MATCH\", new StoreFileMetaData(\"file1\", 10, \"MATCH_CHECKSUM\", MIN_SUPPORTED_LUCENE_VERSION));\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).get(0).shardId(), equalTo(shardId));\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.ensureAtLeastNumDataNodes",
	"Comment": "ensures that at least n data nodes are present in the cluster.if more nodes than n are present this method will notstop any of the running nodes.",
	"Method": "void ensureAtLeastNumDataNodes(int n){\r\n    int size = numDataNodes();\r\n    if (size < n) {\r\n        logger.info(\"increasing cluster size from {} to {}\", size, n);\r\n        if (numSharedDedicatedMasterNodes > 0) {\r\n            startDataOnlyNodes(n - size);\r\n        } else {\r\n            startNodes(n - size);\r\n        }\r\n        validateClusterFormed();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.terms.TermsAggregationBuilder.includeExclude",
	"Comment": "get terms to include and exclude from the aggregation results",
	"Method": "TermsAggregationBuilder includeExclude(IncludeExclude includeExclude,IncludeExclude includeExclude){\r\n    return includeExclude;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.results.ForecastRequestStats.getProgress",
	"Comment": "progress information of the forecastrequest in the range 0 to 1,while 1 means finished",
	"Method": "double getProgress(){\r\n    return progress;\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.Features.areAllSupported",
	"Comment": "tells whether all the features provided as argument are supported",
	"Method": "boolean areAllSupported(List<String> features){\r\n    try {\r\n        for (String feature : features) {\r\n            if (feature.equals(\"xpack\")) {\r\n                if (false == ESRestTestCase.hasXPack()) {\r\n                    return false;\r\n                }\r\n            } else if (feature.equals(\"no_xpack\")) {\r\n                if (ESRestTestCase.hasXPack()) {\r\n                    return false;\r\n                }\r\n            } else if (false == SUPPORTED.contains(feature)) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(\"error checking if xpack is available\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.script.AbstractSortScript.getDoc",
	"Comment": "the doc lookup for the lucene segment this script was created for.",
	"Method": "Map<String, ScriptDocValues<?>> getDoc(){\r\n    return leafLookup.doc();\r\n}"
}, {
	"Path": "org.elasticsearch.index.replication.ESIndexLevelReplicationTestCase.deleteOnPrimary",
	"Comment": "executes the delete request on the primary, and modifies it for replicas.",
	"Method": "BulkShardRequest deleteOnPrimary(DeleteRequest request,IndexShard primary){\r\n    return executeReplicationRequestOnPrimary(primary, request);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.composite.CompositeValuesCollectorQueue.compareCurrent",
	"Comment": "compares the current candidate with the values in the queue and returnsthe slot if the candidate is already in the queue or null if the candidate is not present.",
	"Method": "Integer compareCurrent(){\r\n    return keys.get(CANDIDATE_SLOT);\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.config.Job.allInputFields",
	"Comment": "get all input data fields mentioned in the job configuration,namely analysis fields and the time field.",
	"Method": "Collection<String> allInputFields(){\r\n    Set<String> allFields = new TreeSet();\r\n    if (analysisConfig != null) {\r\n        allFields.addAll(analysisConfig.analysisFields());\r\n    }\r\n    if (dataDescription != null) {\r\n        String timeField = dataDescription.getTimeField();\r\n        if (timeField != null) {\r\n            allFields.add(timeField);\r\n        }\r\n    }\r\n    allFields.remove(\"\");\r\n    allFields.remove(AnalysisConfig.ML_CATEGORY_FIELD);\r\n    return allFields;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.GeoDistanceSortBuilder.points",
	"Comment": "returns the points to create the range distance facets from.",
	"Method": "GeoDistanceSortBuilder points(GeoPoint points,GeoPoint[] points){\r\n    return this.points.toArray(new GeoPoint[this.points.size()]);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.EngineTestCase.generateNewSeqNo",
	"Comment": "generate a new sequence number and return it. only works on internalengines",
	"Method": "long generateNewSeqNo(Engine engine){\r\n    assert engine instanceof InternalEngine : \"expected InternalEngine, got: \" + engine.getClass();\r\n    InternalEngine internalEngine = (InternalEngine) engine;\r\n    return internalEngine.getLocalCheckpointTracker().generateSeqNo();\r\n}"
}, {
	"Path": "org.elasticsearch.test.engine.MockEngineSupport.flushOrClose",
	"Comment": "returns the closeaction to execute on the actual engine. note this method changes the state onthe first call and treats subsequent calls as if the engine passed is already closed.",
	"Method": "CloseAction flushOrClose(CloseAction originalAction){\r\n    if (closing.compareAndSet(false, true)) {\r\n        if (mockContext.random.nextBoolean()) {\r\n            return CloseAction.FLUSH_AND_CLOSE;\r\n        } else {\r\n            return CloseAction.CLOSE;\r\n        }\r\n    } else {\r\n        return originalAction;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.MovFnPipelineAggregationBuilder.parse",
	"Comment": "used for serialization testing, since pipeline aggs serialize themselves as a named object but are parsedas a regular object with the name passed in.",
	"Method": "MovFnPipelineAggregationBuilder parse(String aggName,XContentParser parser,MovFnPipelineAggregationBuilder parse,XContentParser parser){\r\n    parser.nextToken();\r\n    if (parser.currentToken().equals(XContentParser.Token.START_OBJECT)) {\r\n        parser.nextToken();\r\n        if (parser.currentToken().equals(XContentParser.Token.FIELD_NAME)) {\r\n            String aggName = parser.currentName();\r\n            parser.nextToken();\r\n            parser.nextToken();\r\n            return PARSER.apply(aggName).apply(parser, null);\r\n        }\r\n    }\r\n    throw new IllegalStateException(\"Expected aggregation name but none found\");\r\n}"
}, {
	"Path": "org.elasticsearch.test.TestCluster.wipeAllTemplates",
	"Comment": "removes all templates, except the templates defined in the exclude",
	"Method": "void wipeAllTemplates(Set<String> exclude){\r\n    if (size() > 0) {\r\n        GetIndexTemplatesResponse response = client().admin().indices().prepareGetTemplates().get();\r\n        for (IndexTemplateMetaData indexTemplate : response.getIndexTemplates()) {\r\n            if (exclude.contains(indexTemplate.getName())) {\r\n                continue;\r\n            }\r\n            try {\r\n                client().admin().indices().prepareDeleteTemplate(indexTemplate.getName()).execute().actionGet();\r\n            } catch (IndexTemplateMissingException e) {\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.TopHitsAggregationBuilder.sort",
	"Comment": "adds a sort against the given field name and the sort ordering.",
	"Method": "TopHitsAggregationBuilder sort(String name,SortOrder order,TopHitsAggregationBuilder sort,String name,TopHitsAggregationBuilder sort,SortBuilder<?> sort){\r\n    if (sort == null) {\r\n        throw new IllegalArgumentException(\"[sort] must not be null: [\" + name + \"]\");\r\n    }\r\n    if (sorts == null) {\r\n        sorts = new ArrayList();\r\n    }\r\n    sorts.add(sort);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.HDRPercentileRanksIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(percentileRanks(\"foo\", new double[] { 50.0 }).method(PercentilesMethod.HDR).field(\"d\").script(new Script(ScriptType.INLINE, AggregationTestScriptsPlugin.NAME, \"_value - 1\", emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(percentileRanks(\"foo\", new double[] { 50.0 }).method(PercentilesMethod.HDR).field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractQueryTestCase.assertSerialization",
	"Comment": "serialize the given query builder and asserts that both are equal",
	"Method": "QueryBuilder assertSerialization(QueryBuilder testQuery,QueryBuilder assertSerialization,QueryBuilder testQuery,Version version){\r\n    try (BytesStreamOutput output = new BytesStreamOutput()) {\r\n        output.setVersion(version);\r\n        output.writeNamedWriteable(testQuery);\r\n        try (StreamInput in = new NamedWriteableAwareStreamInput(output.bytes().streamInput(), namedWriteableRegistry())) {\r\n            in.setVersion(version);\r\n            QueryBuilder deserializedQuery = in.readNamedWriteable(QueryBuilder.class);\r\n            assertEquals(testQuery, deserializedQuery);\r\n            assertEquals(testQuery.hashCode(), deserializedQuery.hashCode());\r\n            assertNotSame(testQuery, deserializedQuery);\r\n            return deserializedQuery;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogTests.testConcurrentWriteViewsAndSnapshot",
	"Comment": "tests that concurrent readers and writes maintain view and snapshot semantics",
	"Method": "void testConcurrentWriteViewsAndSnapshot(){\r\n    final Thread[] writers = new Thread[randomIntBetween(1, 3)];\r\n    final Thread[] readers = new Thread[randomIntBetween(1, 3)];\r\n    final int flushEveryOps = randomIntBetween(5, 100);\r\n    final int maxOps = randomIntBetween(200, 1000);\r\n    final Object signalReaderSomeDataWasIndexed = new Object();\r\n    final AtomicLong idGenerator = new AtomicLong();\r\n    final CyclicBarrier barrier = new CyclicBarrier(writers.length + readers.length + 1);\r\n    final Map<Translog.Operation, Translog.Location> writtenOps = ConcurrentCollections.newConcurrentMap();\r\n    final AtomicBoolean run = new AtomicBoolean(true);\r\n    final Object flushMutex = new Object();\r\n    final AtomicLong lastCommittedLocalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\r\n    final LocalCheckpointTracker tracker = LocalCheckpointTrackerTests.createEmptyTracker();\r\n    final TranslogDeletionPolicy deletionPolicy = translog.getDeletionPolicy();\r\n    final List<Exception> errors = new CopyOnWriteArrayList();\r\n    logger.info(\"using [{}] readers. [{}] writers. flushing every ~[{}] ops.\", readers.length, writers.length, flushEveryOps);\r\n    for (int i = 0; i < writers.length; i++) {\r\n        final String threadName = \"writer_\" + i;\r\n        final int threadId = i;\r\n        writers[i] = new Thread(new AbstractRunnable() {\r\n            @Override\r\n            public void doRun() throws BrokenBarrierException, InterruptedException, IOException {\r\n                barrier.await();\r\n                int counter = 0;\r\n                while (run.get() && idGenerator.get() < maxOps) {\r\n                    long id = idGenerator.getAndIncrement();\r\n                    final Translog.Operation op;\r\n                    final Translog.Operation.Type type = Translog.Operation.Type.values()[((int) (id % Translog.Operation.Type.values().length))];\r\n                    switch(type) {\r\n                        case CREATE:\r\n                        case INDEX:\r\n                            op = new Translog.Index(\"type\", \"\" + id, id, primaryTerm.get(), new byte[] { (byte) id });\r\n                            break;\r\n                        case DELETE:\r\n                            op = new Translog.Delete(\"test\", Long.toString(id), id, primaryTerm.get(), newUid(Long.toString(id)));\r\n                            break;\r\n                        case NO_OP:\r\n                            op = new Translog.NoOp(id, 1, Long.toString(id));\r\n                            break;\r\n                        default:\r\n                            throw new AssertionError(\"unsupported operation type [\" + type + \"]\");\r\n                    }\r\n                    Translog.Location location = translog.add(op);\r\n                    tracker.markSeqNoAsCompleted(id);\r\n                    Translog.Location existing = writtenOps.put(op, location);\r\n                    if (existing != null) {\r\n                        fail(\"duplicate op [\" + op + \"], old entry at \" + location);\r\n                    }\r\n                    if (id % writers.length == threadId) {\r\n                        translog.ensureSynced(location);\r\n                    }\r\n                    if (id % flushEveryOps == 0) {\r\n                        synchronized (flushMutex) {\r\n                            long localCheckpoint = tracker.getCheckpoint();\r\n                            translog.rollGeneration();\r\n                            lastCommittedLocalCheckpoint.set(localCheckpoint);\r\n                            deletionPolicy.setTranslogGenerationOfLastCommit(translog.currentFileGeneration());\r\n                            deletionPolicy.setMinTranslogGenerationForRecovery(translog.getMinGenerationForSeqNo(localCheckpoint + 1).translogFileGeneration);\r\n                            translog.trimUnreferencedReaders();\r\n                        }\r\n                    }\r\n                    if (id % 7 == 0) {\r\n                        synchronized (signalReaderSomeDataWasIndexed) {\r\n                            signalReaderSomeDataWasIndexed.notifyAll();\r\n                        }\r\n                    }\r\n                    counter++;\r\n                }\r\n                logger.info(\"--> [{}] done. wrote [{}] ops.\", threadName, counter);\r\n            }\r\n            @Override\r\n            public void onFailure(Exception e) {\r\n                logger.error(() -> new ParameterizedMessage(\"--> writer [{}] had an error\", threadName), e);\r\n                errors.add(e);\r\n            }\r\n        }, threadName);\r\n        writers[i].start();\r\n    }\r\n    for (int i = 0; i < readers.length; i++) {\r\n        final String threadId = \"reader_\" + i;\r\n        readers[i] = new Thread(new AbstractRunnable() {\r\n            Closeable retentionLock = null;\r\n            long committedLocalCheckpointAtView;\r\n            @Override\r\n            public void onFailure(Exception e) {\r\n                logger.error(() -> new ParameterizedMessage(\"--> reader [{}] had an error\", threadId), e);\r\n                errors.add(e);\r\n                try {\r\n                    closeRetentionLock();\r\n                } catch (IOException inner) {\r\n                    inner.addSuppressed(e);\r\n                    logger.error(\"unexpected error while closing view, after failure\", inner);\r\n                }\r\n            }\r\n            void closeRetentionLock() throws IOException {\r\n                if (retentionLock != null) {\r\n                    retentionLock.close();\r\n                }\r\n            }\r\n            void acquireRetentionLock() throws IOException {\r\n                closeRetentionLock();\r\n                retentionLock = translog.acquireRetentionLock();\r\n                committedLocalCheckpointAtView = lastCommittedLocalCheckpoint.get();\r\n                logger.info(\"--> [{}] min gen after acquiring lock [{}]\", threadId, translog.getMinFileGeneration());\r\n            }\r\n            @Override\r\n            protected void doRun() throws Exception {\r\n                barrier.await();\r\n                int iter = 0;\r\n                while (idGenerator.get() < maxOps) {\r\n                    if (iter++ % 10 == 0) {\r\n                        acquireRetentionLock();\r\n                    }\r\n                    Set<Translog.Operation> expectedOps = new HashSet(writtenOps.keySet());\r\n                    expectedOps.removeIf(op -> op.seqNo() <= committedLocalCheckpointAtView);\r\n                    try (Translog.Snapshot snapshot = translog.newSnapshotFromMinSeqNo(committedLocalCheckpointAtView + 1L)) {\r\n                        Translog.Operation op;\r\n                        while ((op = snapshot.next()) != null) {\r\n                            expectedOps.remove(op);\r\n                        }\r\n                    }\r\n                    if (expectedOps.isEmpty() == false) {\r\n                        StringBuilder missed = new StringBuilder(\"missed \").append(expectedOps.size()).append(\" operations from [\").append(committedLocalCheckpointAtView + 1L).append(\"]\");\r\n                        boolean failed = false;\r\n                        for (Translog.Operation expectedOp : expectedOps) {\r\n                            final Translog.Location loc = writtenOps.get(expectedOp);\r\n                            failed = true;\r\n                            missed.append(\"\\n --> [\").append(expectedOp).append(\"] written at \").append(loc);\r\n                        }\r\n                        if (failed) {\r\n                            fail(missed.toString());\r\n                        }\r\n                    }\r\n                    synchronized (signalReaderSomeDataWasIndexed) {\r\n                        if (idGenerator.get() < maxOps) {\r\n                            signalReaderSomeDataWasIndexed.wait();\r\n                        }\r\n                    }\r\n                }\r\n                closeRetentionLock();\r\n                logger.info(\"--> [{}] done. tested [{}] snapshots\", threadId, iter);\r\n            }\r\n        }, threadId);\r\n        readers[i].start();\r\n    }\r\n    barrier.await();\r\n    logger.debug(\"--> waiting for threads to stop\");\r\n    for (Thread thread : writers) {\r\n        thread.join();\r\n    }\r\n    logger.debug(\"--> waiting for readers to stop\");\r\n    synchronized (signalReaderSomeDataWasIndexed) {\r\n        idGenerator.set(Long.MAX_VALUE);\r\n        signalReaderSomeDataWasIndexed.notifyAll();\r\n    }\r\n    for (Thread thread : readers) {\r\n        thread.join();\r\n    }\r\n    if (errors.size() > 0) {\r\n        Throwable e = errors.get(0);\r\n        for (Throwable suppress : errors.subList(1, errors.size())) {\r\n            e.addSuppressed(suppress);\r\n        }\r\n        throw e;\r\n    }\r\n    logger.info(\"--> test done. total ops written [{}]\", writtenOps.size());\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogTests.testConcurrentWriteViewsAndSnapshot",
	"Comment": "tests that concurrent readers and writes maintain view and snapshot semantics",
	"Method": "void testConcurrentWriteViewsAndSnapshot(){\r\n    barrier.await();\r\n    int counter = 0;\r\n    while (run.get() && idGenerator.get() < maxOps) {\r\n        long id = idGenerator.getAndIncrement();\r\n        final Translog.Operation op;\r\n        final Translog.Operation.Type type = Translog.Operation.Type.values()[((int) (id % Translog.Operation.Type.values().length))];\r\n        switch(type) {\r\n            case CREATE:\r\n            case INDEX:\r\n                op = new Translog.Index(\"type\", \"\" + id, id, primaryTerm.get(), new byte[] { (byte) id });\r\n                break;\r\n            case DELETE:\r\n                op = new Translog.Delete(\"test\", Long.toString(id), id, primaryTerm.get(), newUid(Long.toString(id)));\r\n                break;\r\n            case NO_OP:\r\n                op = new Translog.NoOp(id, 1, Long.toString(id));\r\n                break;\r\n            default:\r\n                throw new AssertionError(\"unsupported operation type [\" + type + \"]\");\r\n        }\r\n        Translog.Location location = translog.add(op);\r\n        tracker.markSeqNoAsCompleted(id);\r\n        Translog.Location existing = writtenOps.put(op, location);\r\n        if (existing != null) {\r\n            fail(\"duplicate op [\" + op + \"], old entry at \" + location);\r\n        }\r\n        if (id % writers.length == threadId) {\r\n            translog.ensureSynced(location);\r\n        }\r\n        if (id % flushEveryOps == 0) {\r\n            synchronized (flushMutex) {\r\n                long localCheckpoint = tracker.getCheckpoint();\r\n                translog.rollGeneration();\r\n                lastCommittedLocalCheckpoint.set(localCheckpoint);\r\n                deletionPolicy.setTranslogGenerationOfLastCommit(translog.currentFileGeneration());\r\n                deletionPolicy.setMinTranslogGenerationForRecovery(translog.getMinGenerationForSeqNo(localCheckpoint + 1).translogFileGeneration);\r\n                translog.trimUnreferencedReaders();\r\n            }\r\n        }\r\n        if (id % 7 == 0) {\r\n            synchronized (signalReaderSomeDataWasIndexed) {\r\n                signalReaderSomeDataWasIndexed.notifyAll();\r\n            }\r\n        }\r\n        counter++;\r\n    }\r\n    logger.info(\"--> [{}] done. wrote [{}] ops.\", threadName, counter);\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogTests.testConcurrentWriteViewsAndSnapshot",
	"Comment": "tests that concurrent readers and writes maintain view and snapshot semantics",
	"Method": "void testConcurrentWriteViewsAndSnapshot(){\r\n    logger.error(() -> new ParameterizedMessage(\"--> writer [{}] had an error\", threadName), e);\r\n    errors.add(e);\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogTests.testConcurrentWriteViewsAndSnapshot",
	"Comment": "tests that concurrent readers and writes maintain view and snapshot semantics",
	"Method": "void testConcurrentWriteViewsAndSnapshot(){\r\n    logger.error(() -> new ParameterizedMessage(\"--> reader [{}] had an error\", threadId), e);\r\n    errors.add(e);\r\n    try {\r\n        closeRetentionLock();\r\n    } catch (IOException inner) {\r\n        inner.addSuppressed(e);\r\n        logger.error(\"unexpected error while closing view, after failure\", inner);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogTests.testConcurrentWriteViewsAndSnapshot",
	"Comment": "tests that concurrent readers and writes maintain view and snapshot semantics",
	"Method": "void testConcurrentWriteViewsAndSnapshot(){\r\n    if (retentionLock != null) {\r\n        retentionLock.close();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogTests.testConcurrentWriteViewsAndSnapshot",
	"Comment": "tests that concurrent readers and writes maintain view and snapshot semantics",
	"Method": "void testConcurrentWriteViewsAndSnapshot(){\r\n    closeRetentionLock();\r\n    retentionLock = translog.acquireRetentionLock();\r\n    committedLocalCheckpointAtView = lastCommittedLocalCheckpoint.get();\r\n    logger.info(\"--> [{}] min gen after acquiring lock [{}]\", threadId, translog.getMinFileGeneration());\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogTests.testConcurrentWriteViewsAndSnapshot",
	"Comment": "tests that concurrent readers and writes maintain view and snapshot semantics",
	"Method": "void testConcurrentWriteViewsAndSnapshot(){\r\n    barrier.await();\r\n    int iter = 0;\r\n    while (idGenerator.get() < maxOps) {\r\n        if (iter++ % 10 == 0) {\r\n            acquireRetentionLock();\r\n        }\r\n        Set<Translog.Operation> expectedOps = new HashSet(writtenOps.keySet());\r\n        expectedOps.removeIf(op -> op.seqNo() <= committedLocalCheckpointAtView);\r\n        try (Translog.Snapshot snapshot = translog.newSnapshotFromMinSeqNo(committedLocalCheckpointAtView + 1L)) {\r\n            Translog.Operation op;\r\n            while ((op = snapshot.next()) != null) {\r\n                expectedOps.remove(op);\r\n            }\r\n        }\r\n        if (expectedOps.isEmpty() == false) {\r\n            StringBuilder missed = new StringBuilder(\"missed \").append(expectedOps.size()).append(\" operations from [\").append(committedLocalCheckpointAtView + 1L).append(\"]\");\r\n            boolean failed = false;\r\n            for (Translog.Operation expectedOp : expectedOps) {\r\n                final Translog.Location loc = writtenOps.get(expectedOp);\r\n                failed = true;\r\n                missed.append(\"\\n --> [\").append(expectedOp).append(\"] written at \").append(loc);\r\n            }\r\n            if (failed) {\r\n                fail(missed.toString());\r\n            }\r\n        }\r\n        synchronized (signalReaderSomeDataWasIndexed) {\r\n            if (idGenerator.get() < maxOps) {\r\n                signalReaderSomeDataWasIndexed.wait();\r\n            }\r\n        }\r\n    }\r\n    closeRetentionLock();\r\n    logger.info(\"--> [{}] done. tested [{}] snapshots\", threadId, iter);\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.ClientYamlTestExecutionContext.callApi",
	"Comment": "calls an elasticsearch api with the parameters and request body provided as arguments.saves the obtained response in the execution context.",
	"Method": "ClientYamlTestResponse callApi(String apiName,Map<String, String> params,List<Map<String, Object>> bodies,Map<String, String> headers,ClientYamlTestResponse callApi,String apiName,Map<String, String> params,List<Map<String, Object>> bodies,Map<String, String> headers,NodeSelector nodeSelector){\r\n    Map<String, String> requestParams = new HashMap(params);\r\n    requestParams.putIfAbsent(\"error_trace\", \"true\");\r\n    for (Map.Entry<String, String> entry : requestParams.entrySet()) {\r\n        if (stash.containsStashedValue(entry.getValue())) {\r\n            entry.setValue(stash.getValue(entry.getValue()).toString());\r\n        }\r\n    }\r\n    Map<String, String> requestHeaders = new HashMap(headers);\r\n    for (Map.Entry<String, String> entry : requestHeaders.entrySet()) {\r\n        if (stash.containsStashedValue(entry.getValue())) {\r\n            entry.setValue(stash.getValue(entry.getValue()).toString());\r\n        }\r\n    }\r\n    HttpEntity entity = createEntity(bodies, requestHeaders);\r\n    try {\r\n        response = callApiInternal(apiName, requestParams, entity, requestHeaders, nodeSelector);\r\n        return response;\r\n    } catch (ClientYamlTestResponseException e) {\r\n        response = e.getRestTestResponse();\r\n        throw e;\r\n    } finally {\r\n        Object responseBody = response != null ? response.getBody() : null;\r\n        stash.stashValue(\"body\", responseBody);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.PrimaryAllocationIT.testPrimaryReplicaResyncFailed",
	"Comment": "this test asserts that replicas failed to execute resync operations will be failed but not marked as stale.",
	"Method": "void testPrimaryReplicaResyncFailed(){\r\n    String master = internalCluster().startMasterOnlyNode(Settings.EMPTY);\r\n    final int numberOfReplicas = between(2, 3);\r\n    final String oldPrimary = internalCluster().startDataOnlyNode();\r\n    assertAcked(prepareCreate(\"test\", Settings.builder().put(indexSettings()).put(SETTING_NUMBER_OF_SHARDS, 1).put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas)));\r\n    final ShardId shardId = new ShardId(clusterService().state().metaData().index(\"test\").getIndex(), 0);\r\n    final Set<String> replicaNodes = new HashSet(internalCluster().startDataOnlyNodes(numberOfReplicas));\r\n    ensureGreen();\r\n    assertAcked(client(master).admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(\"cluster.routing.allocation.enable\", \"none\")).get());\r\n    logger.info(\"--> Indexing with gap in seqno to ensure that some operations will be replayed in resync\");\r\n    long numDocs = scaledRandomIntBetween(5, 50);\r\n    for (int i = 0; i < numDocs; i++) {\r\n        IndexResponse indexResult = index(\"test\", \"doc\", Long.toString(i));\r\n        assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1));\r\n    }\r\n    final IndexShard oldPrimaryShard = internalCluster().getInstance(IndicesService.class, oldPrimary).getShardOrNull(shardId);\r\n    EngineTestCase.generateNewSeqNo(IndexShardTestCase.getEngine(oldPrimaryShard));\r\n    long moreDocs = scaledRandomIntBetween(1, 10);\r\n    for (int i = 0; i < moreDocs; i++) {\r\n        IndexResponse indexResult = index(\"test\", \"doc\", Long.toString(numDocs + i));\r\n        assertThat(indexResult.getShardInfo().getSuccessful(), equalTo(numberOfReplicas + 1));\r\n    }\r\n    final Set<String> replicasSide1 = Sets.newHashSet(randomSubsetOf(between(1, numberOfReplicas - 1), replicaNodes));\r\n    final Set<String> replicasSide2 = Sets.difference(replicaNodes, replicasSide1);\r\n    NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(replicasSide1, replicasSide2), new NetworkDisconnect());\r\n    internalCluster().setDisruptionScheme(partition);\r\n    logger.info(\"--> isolating some replicas during primary-replica resync\");\r\n    partition.startDisrupting();\r\n    internalCluster().stopRandomNode(InternalTestCluster.nameFilter(oldPrimary));\r\n    assertBusy(() -> {\r\n        ClusterState state = client(master).admin().cluster().prepareState().get().getState();\r\n        final IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shardId);\r\n        final String newPrimaryNode = state.getRoutingNodes().node(shardRoutingTable.primary.currentNodeId()).node().getName();\r\n        assertThat(newPrimaryNode, not(equalTo(oldPrimary)));\r\n        Set<String> selectedPartition = replicasSide1.contains(newPrimaryNode) ? replicasSide1 : replicasSide2;\r\n        assertThat(shardRoutingTable.activeShards(), hasSize(selectedPartition.size()));\r\n        for (ShardRouting activeShard : shardRoutingTable.activeShards()) {\r\n            assertThat(state.getRoutingNodes().node(activeShard.currentNodeId()).node().getName(), isIn(selectedPartition));\r\n        }\r\n        assertThat(state.metaData().index(\"test\").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1));\r\n    }, 1, TimeUnit.MINUTES);\r\n    assertAcked(client(master).admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(\"cluster.routing.allocation.enable\", \"all\")).get());\r\n    partition.stopDisrupting();\r\n    partition.ensureHealthy(internalCluster());\r\n    logger.info(\"--> stop disrupting network and re-enable allocation\");\r\n    assertBusy(() -> {\r\n        ClusterState state = client(master).admin().cluster().prepareState().get().getState();\r\n        assertThat(state.routingTable().shardRoutingTable(shardId).activeShards(), hasSize(numberOfReplicas));\r\n        assertThat(state.metaData().index(\"test\").inSyncAllocationIds(shardId.id()), hasSize(numberOfReplicas + 1));\r\n        for (String node : replicaNodes) {\r\n            IndexShard shard = internalCluster().getInstance(IndicesService.class, node).getShardOrNull(shardId);\r\n            assertThat(shard.getLocalCheckpoint(), equalTo(numDocs + moreDocs));\r\n        }\r\n    }, 30, TimeUnit.SECONDS);\r\n    internalCluster().assertConsistentHistoryBetweenTranslogAndLuceneIndex();\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificantTextAggregationBuilder.fieldName",
	"Comment": "sets the name of the text field that will be the subject of thisaggregation.",
	"Method": "SignificantTextAggregationBuilder fieldName(String fieldName){\r\n    this.fieldName = fieldName;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESSingleNodeTestCase.pluginList",
	"Comment": "helper method to create list of plugins without specifying generic types.",
	"Method": "Collection<Class<? extends Plugin>> pluginList(Class<? extends Plugin> plugins){\r\n    return Arrays.asList(plugins);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.RangeIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"i\", \"type=integer\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(jsonBuilder().startObject().field(\"i\", 1).endObject()), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(jsonBuilder().startObject().field(\"i\", 2).endObject()));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    Map<String, Object> params = new HashMap();\r\n    params.put(\"fieldname\", \"date\");\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(range(\"foo\").field(\"i\").script(new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, \"_value + 1\", Collections.emptyMap())).addRange(0, 10)).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(range(\"foo\").field(\"i\").addRange(0, 10)).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.support.AggregationPath.resolveTopmostAggregator",
	"Comment": "resolves the topmost aggregator pointed by this path using the given root as a point of reference.",
	"Method": "Aggregator resolveTopmostAggregator(Aggregator root){\r\n    AggregationPath.PathElement token = pathElements.get(0);\r\n    Aggregator aggregator = ProfilingAggregator.unwrap(root.subAggregator(token.name));\r\n    assert (aggregator instanceof SingleBucketAggregator) || (aggregator instanceof NumericMetricsAggregator) : \"this should be picked up before aggregation execution - on validate\";\r\n    return aggregator;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.TasksIT.numberOfEvents",
	"Comment": "returns the number of events that satisfy the criteria across all nodes",
	"Method": "int numberOfEvents(String actionMasks,Function<Tuple<Boolean, TaskInfo>, Boolean> criteria){\r\n    return findEvents(actionMasks, criteria).size();\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocatorTests.testFoundAllocationAndAllocating",
	"Comment": "tests that when there is a node to allocate the shard to, it will be allocated to it.",
	"Method": "void testFoundAllocationAndAllocating(){\r\n    final RoutingAllocation allocation = routingAllocationWithOnePrimaryNoReplicas(yesAllocationDeciders(), randomFrom(CLUSTER_RECOVERED, INDEX_REOPENED), \"allocId1\");\r\n    testAllocator.addData(node1, \"allocId1\", randomBoolean());\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodesChanged(), equalTo(true));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().isEmpty(), equalTo(true));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo(node1.getId()));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).allocationId().getId(), equalTo(\"allocId1\"));\r\n    assertClusterHealthStatus(allocation, ClusterHealthStatus.YELLOW);\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.copyWithNewSlice",
	"Comment": "create a shallow copy of this builder with a new slice configuration.",
	"Method": "SearchSourceBuilder copyWithNewSlice(SliceBuilder slice){\r\n    return shallowCopy(queryBuilder, postQueryBuilder, aggregations, slice, sorts, rescoreBuilders, highlightBuilder);\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.section.ApiCallSection.setNodeSelector",
	"Comment": "set the selector that decides which node can run this request.",
	"Method": "void setNodeSelector(NodeSelector nodeSelector){\r\n    this.nodeSelector = nodeSelector;\r\n}"
}, {
	"Path": "org.elasticsearch.test.junit.listeners.LoggingListener.processTestLogging",
	"Comment": "applies the test logging annotation and returns the existing logging levels.",
	"Method": "Map<String, String> processTestLogging(TestLogging testLogging){\r\n    final Map<String, String> map = getLoggersAndLevelsFromAnnotation(testLogging);\r\n    if (map == null) {\r\n        return Collections.emptyMap();\r\n    }\r\n    final Map<String, String> existing = new TreeMap();\r\n    for (final Map.Entry<String, String> entry : map.entrySet()) {\r\n        final Logger logger = resolveLogger(entry.getKey());\r\n        existing.put(entry.getKey(), logger.getLevel().toString());\r\n    }\r\n    for (final Map.Entry<String, String> entry : map.entrySet()) {\r\n        final Logger logger = resolveLogger(entry.getKey());\r\n        Loggers.setLevel(logger, entry.getValue());\r\n    }\r\n    return existing;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramAggregationBuilder.offset",
	"Comment": "set the offset on this builder, as a time value, and return the builder so that calls can be chained.",
	"Method": "long offset(DateHistogramAggregationBuilder offset,long offset,DateHistogramAggregationBuilder offset,String offset){\r\n    if (offset == null) {\r\n        throw new IllegalArgumentException(\"[offset] must not be null: [\" + name + \"]\");\r\n    }\r\n    return offset(parseStringOffset(offset));\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.RandomDocumentPicks.randomExistingFieldName",
	"Comment": "returns a randomly selected existing field name out of the fields that are containedin the document provided as an argument.",
	"Method": "String randomExistingFieldName(Random random,IngestDocument ingestDocument){\r\n    Map<String, Object> source = new TreeMap(ingestDocument.getSourceAndMetadata());\r\n    Map.Entry<String, Object> randomEntry = RandomPicks.randomFrom(random, source.entrySet());\r\n    String key = randomEntry.getKey();\r\n    while (randomEntry.getValue() instanceof Map) {\r\n        @SuppressWarnings(\"unchecked\")\r\n        Map<String, Object> map = (Map<String, Object>) randomEntry.getValue();\r\n        Map<String, Object> treeMap = new TreeMap(map);\r\n        randomEntry = RandomPicks.randomFrom(random, treeMap.entrySet());\r\n        key += \".\" + randomEntry.getKey();\r\n    }\r\n    assert ingestDocument.getFieldValue(key, Object.class) != null;\r\n    return key;\r\n}"
}, {
	"Path": "org.elasticsearch.license.XPackLicenseState.isActive",
	"Comment": "return true if the license is currently within its time boundaries, false otherwise.",
	"Method": "boolean isActive(){\r\n    return status.active;\r\n}"
}, {
	"Path": "org.elasticsearch.license.License.setOperationModeFileWatcher",
	"Comment": "sets the operation mode file watcher for the license and initializes thefile watcher when the license type allows to override operation mode from file",
	"Method": "void setOperationModeFileWatcher(OperationModeFileWatcher operationModeFileWatcher){\r\n    this.operationModeFileWatcher = operationModeFileWatcher;\r\n    if (canReadOperationModeFromFile()) {\r\n        this.operationModeFileWatcher.init();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.license.CryptUtils.readEncryptedPrivateKey",
	"Comment": "read encrypted private key file content with provided passphrase",
	"Method": "PrivateKey readEncryptedPrivateKey(byte[] fileContents,PrivateKey readEncryptedPrivateKey,byte[] fileContents,char[] passPhrase,boolean preV4){\r\n    byte[] keyBytes = preV4 ? decryptV3Format(fileContents) : decrypt(fileContents, passPhrase);\r\n    PKCS8EncodedKeySpec privateKeySpec = new PKCS8EncodedKeySpec(keyBytes);\r\n    try {\r\n        return KeyFactory.getInstance(KEY_ALGORITHM).generatePrivate(privateKeySpec);\r\n    } catch (NoSuchAlgorithmException | InvalidKeySpecException e) {\r\n        throw new IllegalStateException(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEventTests.createIndexMetadata",
	"Comment": "create the index metadata for a given index, with the specified version.",
	"Method": "IndexMetaData createIndexMetadata(Index index,IndexMetaData createIndexMetadata,Index index,long version){\r\n    final Settings settings = Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT).put(IndexMetaData.SETTING_INDEX_UUID, index.getUUID()).build();\r\n    return IndexMetaData.builder(index.getName()).settings(settings).numberOfShards(1).numberOfReplicas(0).creationDate(System.currentTimeMillis()).version(version).build();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    AtomicInteger preIndex = new AtomicInteger();\r\n    AtomicInteger postIndex = new AtomicInteger();\r\n    AtomicInteger postIndexException = new AtomicInteger();\r\n    AtomicInteger preDelete = new AtomicInteger();\r\n    AtomicInteger postDelete = new AtomicInteger();\r\n    AtomicInteger postDeleteException = new AtomicInteger();\r\n    ShardId randomShardId = new ShardId(new Index(randomAlphaOfLength(10), randomAlphaOfLength(10)), randomIntBetween(1, 10));\r\n    IndexingOperationListener listener = new IndexingOperationListener() {\r\n        @Override\r\n        public Engine.Index preIndex(ShardId shardId, Engine.Index operation) {\r\n            assertThat(shardId, is(randomShardId));\r\n            preIndex.incrementAndGet();\r\n            return operation;\r\n        }\r\n        @Override\r\n        public void postIndex(ShardId shardId, Engine.Index index, Engine.IndexResult result) {\r\n            assertThat(shardId, is(randomShardId));\r\n            switch(result.getResultType()) {\r\n                case SUCCESS:\r\n                    postIndex.incrementAndGet();\r\n                    break;\r\n                case FAILURE:\r\n                    postIndex(shardId, index, result.getFailure());\r\n                    break;\r\n                default:\r\n                    throw new IllegalArgumentException(\"unknown result type: \" + result.getResultType());\r\n            }\r\n        }\r\n        @Override\r\n        public void postIndex(ShardId shardId, Engine.Index index, Exception ex) {\r\n            assertThat(shardId, is(randomShardId));\r\n            postIndexException.incrementAndGet();\r\n        }\r\n        @Override\r\n        public Engine.Delete preDelete(ShardId shardId, Engine.Delete delete) {\r\n            assertThat(shardId, is(randomShardId));\r\n            preDelete.incrementAndGet();\r\n            return delete;\r\n        }\r\n        @Override\r\n        public void postDelete(ShardId shardId, Engine.Delete delete, Engine.DeleteResult result) {\r\n            assertThat(shardId, is(randomShardId));\r\n            switch(result.getResultType()) {\r\n                case SUCCESS:\r\n                    postDelete.incrementAndGet();\r\n                    break;\r\n                case FAILURE:\r\n                    postDelete(shardId, delete, result.getFailure());\r\n                    break;\r\n                default:\r\n                    throw new IllegalArgumentException(\"unknown result type: \" + result.getResultType());\r\n            }\r\n        }\r\n        @Override\r\n        public void postDelete(ShardId shardId, Engine.Delete delete, Exception ex) {\r\n            assertThat(shardId, is(randomShardId));\r\n            postDeleteException.incrementAndGet();\r\n        }\r\n    };\r\n    IndexingOperationListener throwingListener = new IndexingOperationListener() {\r\n        @Override\r\n        public Engine.Index preIndex(ShardId shardId, Engine.Index operation) {\r\n            throw new RuntimeException();\r\n        }\r\n        @Override\r\n        public void postIndex(ShardId shardId, Engine.Index index, Engine.IndexResult result) {\r\n            throw new RuntimeException();\r\n        }\r\n        @Override\r\n        public void postIndex(ShardId shardId, Engine.Index index, Exception ex) {\r\n            throw new RuntimeException();\r\n        }\r\n        @Override\r\n        public Engine.Delete preDelete(ShardId shardId, Engine.Delete delete) {\r\n            throw new RuntimeException();\r\n        }\r\n        @Override\r\n        public void postDelete(ShardId shardId, Engine.Delete delete, Engine.DeleteResult result) {\r\n            throw new RuntimeException();\r\n        }\r\n        @Override\r\n        public void postDelete(ShardId shardId, Engine.Delete delete, Exception ex) {\r\n            throw new RuntimeException();\r\n        }\r\n    };\r\n    final List<IndexingOperationListener> indexingOperationListeners = new ArrayList(Arrays.asList(listener, listener));\r\n    if (randomBoolean()) {\r\n        indexingOperationListeners.add(throwingListener);\r\n        if (randomBoolean()) {\r\n            indexingOperationListeners.add(throwingListener);\r\n        }\r\n    }\r\n    Collections.shuffle(indexingOperationListeners, random());\r\n    IndexingOperationListener.CompositeListener compositeListener = new IndexingOperationListener.CompositeListener(indexingOperationListeners, logger);\r\n    ParsedDocument doc = InternalEngineTests.createParsedDoc(\"1\", null);\r\n    Engine.Delete delete = new Engine.Delete(\"test\", \"1\", new Term(\"_id\", Uid.encodeId(doc.id())), randomNonNegativeLong());\r\n    Engine.Index index = new Engine.Index(new Term(\"_id\", Uid.encodeId(doc.id())), randomNonNegativeLong(), doc);\r\n    compositeListener.postDelete(randomShardId, delete, new Engine.DeleteResult(1, 0, SequenceNumbers.UNASSIGNED_SEQ_NO, true));\r\n    assertEquals(0, preIndex.get());\r\n    assertEquals(0, postIndex.get());\r\n    assertEquals(0, postIndexException.get());\r\n    assertEquals(0, preDelete.get());\r\n    assertEquals(2, postDelete.get());\r\n    assertEquals(0, postDeleteException.get());\r\n    compositeListener.postDelete(randomShardId, delete, new RuntimeException());\r\n    assertEquals(0, preIndex.get());\r\n    assertEquals(0, postIndex.get());\r\n    assertEquals(0, postIndexException.get());\r\n    assertEquals(0, preDelete.get());\r\n    assertEquals(2, postDelete.get());\r\n    assertEquals(2, postDeleteException.get());\r\n    compositeListener.preDelete(randomShardId, delete);\r\n    assertEquals(0, preIndex.get());\r\n    assertEquals(0, postIndex.get());\r\n    assertEquals(0, postIndexException.get());\r\n    assertEquals(2, preDelete.get());\r\n    assertEquals(2, postDelete.get());\r\n    assertEquals(2, postDeleteException.get());\r\n    compositeListener.postIndex(randomShardId, index, new Engine.IndexResult(0, 0, SequenceNumbers.UNASSIGNED_SEQ_NO, false));\r\n    assertEquals(0, preIndex.get());\r\n    assertEquals(2, postIndex.get());\r\n    assertEquals(0, postIndexException.get());\r\n    assertEquals(2, preDelete.get());\r\n    assertEquals(2, postDelete.get());\r\n    assertEquals(2, postDeleteException.get());\r\n    compositeListener.postIndex(randomShardId, index, new RuntimeException());\r\n    assertEquals(0, preIndex.get());\r\n    assertEquals(2, postIndex.get());\r\n    assertEquals(2, postIndexException.get());\r\n    assertEquals(2, preDelete.get());\r\n    assertEquals(2, postDelete.get());\r\n    assertEquals(2, postDeleteException.get());\r\n    compositeListener.preIndex(randomShardId, index);\r\n    assertEquals(2, preIndex.get());\r\n    assertEquals(2, postIndex.get());\r\n    assertEquals(2, postIndexException.get());\r\n    assertEquals(2, preDelete.get());\r\n    assertEquals(2, postDelete.get());\r\n    assertEquals(2, postDeleteException.get());\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    assertThat(shardId, is(randomShardId));\r\n    preIndex.incrementAndGet();\r\n    return operation;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    assertThat(shardId, is(randomShardId));\r\n    switch(result.getResultType()) {\r\n        case SUCCESS:\r\n            postIndex.incrementAndGet();\r\n            break;\r\n        case FAILURE:\r\n            postIndex(shardId, index, result.getFailure());\r\n            break;\r\n        default:\r\n            throw new IllegalArgumentException(\"unknown result type: \" + result.getResultType());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    assertThat(shardId, is(randomShardId));\r\n    postIndexException.incrementAndGet();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    assertThat(shardId, is(randomShardId));\r\n    preDelete.incrementAndGet();\r\n    return delete;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    assertThat(shardId, is(randomShardId));\r\n    switch(result.getResultType()) {\r\n        case SUCCESS:\r\n            postDelete.incrementAndGet();\r\n            break;\r\n        case FAILURE:\r\n            postDelete(shardId, delete, result.getFailure());\r\n            break;\r\n        default:\r\n            throw new IllegalArgumentException(\"unknown result type: \" + result.getResultType());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    assertThat(shardId, is(randomShardId));\r\n    postDeleteException.incrementAndGet();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    throw new RuntimeException();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    throw new RuntimeException();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    throw new RuntimeException();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    throw new RuntimeException();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    throw new RuntimeException();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexingOperationListenerTests.testListenersAreExecuted",
	"Comment": "this test also tests if calls are correct if one or more listeners throw exceptions",
	"Method": "void testListenersAreExecuted(){\r\n    throw new RuntimeException();\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.AvgIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(avg(\"foo\").field(\"d\").script(new Script(ScriptType.INLINE, METRIC_SCRIPT_ENGINE, VALUE_FIELD_SCRIPT, Collections.emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(avg(\"foo\").field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.RestoreService.validateSnapshotRestorable",
	"Comment": "checks that snapshots can be restored and have compatible version",
	"Method": "void validateSnapshotRestorable(String repository,SnapshotInfo snapshotInfo){\r\n    if (!snapshotInfo.state().restorable()) {\r\n        throw new SnapshotRestoreException(new Snapshot(repository, snapshotInfo.snapshotId()), \"unsupported snapshot state [\" + snapshotInfo.state() + \"]\");\r\n    }\r\n    if (Version.CURRENT.before(snapshotInfo.version())) {\r\n        throw new SnapshotRestoreException(new Snapshot(repository, snapshotInfo.snapshotId()), \"the snapshot was created with Elasticsearch version [\" + snapshotInfo.version() + \"] which is higher than the version of this node [\" + Version.CURRENT + \"]\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.support.ValuesSourceAggregationBuilder.missing",
	"Comment": "gets the value to use when the aggregation finds a missing value in adocument",
	"Method": "AB missing(Object missing,Object missing){\r\n    return missing;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.RandomAllocationDeciderTests.testRandomDecisions",
	"Comment": "this test will make random allocation decision on a growing and shrinkingcluster leading to a random distribution of the shards. after a certainamount of iterations the test allows allocation unless the same shard isalready allocated on a node and balances the cluster to gain optimalbalance.",
	"Method": "void testRandomDecisions(){\r\n    RandomAllocationDecider randomAllocationDecider = new RandomAllocationDecider(random());\r\n    AllocationService strategy = new AllocationService(new AllocationDeciders(new HashSet(Arrays.asList(new SameShardAllocationDecider(Settings.EMPTY, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS)), new ReplicaAfterPrimaryActiveAllocationDecider(), randomAllocationDecider))), new TestGatewayAllocator(), new BalancedShardsAllocator(Settings.EMPTY), EmptyClusterInfoService.INSTANCE);\r\n    int indices = scaledRandomIntBetween(1, 20);\r\n    Builder metaBuilder = MetaData.builder();\r\n    int maxNumReplicas = 1;\r\n    int totalNumShards = 0;\r\n    for (int i = 0; i < indices; i++) {\r\n        int replicas = scaledRandomIntBetween(0, 6);\r\n        maxNumReplicas = Math.max(maxNumReplicas, replicas + 1);\r\n        int numShards = scaledRandomIntBetween(1, 20);\r\n        totalNumShards += numShards * (replicas + 1);\r\n        metaBuilder.put(IndexMetaData.builder(\"INDEX_\" + i).settings(settings(Version.CURRENT)).numberOfShards(numShards).numberOfReplicas(replicas));\r\n    }\r\n    MetaData metaData = metaBuilder.build();\r\n    RoutingTable.Builder routingTableBuilder = RoutingTable.builder();\r\n    for (int i = 0; i < indices; i++) {\r\n        routingTableBuilder.addAsNew(metaData.index(\"INDEX_\" + i));\r\n    }\r\n    RoutingTable initialRoutingTable = routingTableBuilder.build();\r\n    ClusterState clusterState = ClusterState.builder(org.elasticsearch.cluster.ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)).metaData(metaData).routingTable(initialRoutingTable).build();\r\n    int numIters = scaledRandomIntBetween(5, 15);\r\n    int nodeIdCounter = 0;\r\n    int atMostNodes = scaledRandomIntBetween(Math.max(1, maxNumReplicas), 15);\r\n    final boolean frequentNodes = randomBoolean();\r\n    for (int i = 0; i < numIters; i++) {\r\n        logger.info(\"Start iteration [{}]\", i);\r\n        ClusterState.Builder stateBuilder = ClusterState.builder(clusterState);\r\n        DiscoveryNodes.Builder newNodesBuilder = DiscoveryNodes.builder(clusterState.nodes());\r\n        if (clusterState.nodes().getSize() <= atMostNodes && (nodeIdCounter == 0 || (frequentNodes ? frequently() : rarely()))) {\r\n            int numNodes = scaledRandomIntBetween(1, 3);\r\n            for (int j = 0; j < numNodes; j++) {\r\n                logger.info(\"adding node [{}]\", nodeIdCounter);\r\n                newNodesBuilder.add(newNode(\"NODE_\" + (nodeIdCounter++)));\r\n            }\r\n        }\r\n        boolean nodesRemoved = false;\r\n        if (nodeIdCounter > 1 && rarely()) {\r\n            int nodeId = scaledRandomIntBetween(0, nodeIdCounter - 2);\r\n            final String node = \"NODE_\" + nodeId;\r\n            boolean safeToRemove = true;\r\n            RoutingNode routingNode = clusterState.getRoutingNodes().node(node);\r\n            for (ShardRouting shard : routingNode != null ? routingNode : Collections.<ShardRouting>emptyList()) {\r\n                if (shard.active() && shard.primary()) {\r\n                    if (clusterState.routingTable().shardRoutingTable(shard.shardId()).activeShards().size() <= 1) {\r\n                        safeToRemove = false;\r\n                        break;\r\n                    }\r\n                }\r\n            }\r\n            if (safeToRemove) {\r\n                logger.info(\"removing node [{}]\", nodeId);\r\n                newNodesBuilder.remove(node);\r\n                nodesRemoved = true;\r\n            } else {\r\n                logger.debug(\"not removing node [{}] as it holds a primary with no replacement\", nodeId);\r\n            }\r\n        }\r\n        stateBuilder.nodes(newNodesBuilder.build());\r\n        clusterState = stateBuilder.build();\r\n        if (nodesRemoved) {\r\n            clusterState = strategy.deassociateDeadNodes(clusterState, true, \"reroute\");\r\n        } else {\r\n            clusterState = strategy.reroute(clusterState, \"reroute\");\r\n        }\r\n        if (clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size() > 0) {\r\n            clusterState = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));\r\n        }\r\n    }\r\n    logger.info(\"Fill up nodes such that every shard can be allocated\");\r\n    if (clusterState.nodes().getSize() < maxNumReplicas) {\r\n        ClusterState.Builder stateBuilder = ClusterState.builder(clusterState);\r\n        DiscoveryNodes.Builder newNodesBuilder = DiscoveryNodes.builder(clusterState.nodes());\r\n        for (int j = 0; j < (maxNumReplicas - clusterState.nodes().getSize()); j++) {\r\n            logger.info(\"adding node [{}]\", nodeIdCounter);\r\n            newNodesBuilder.add(newNode(\"NODE_\" + (nodeIdCounter++)));\r\n        }\r\n        stateBuilder.nodes(newNodesBuilder.build());\r\n        clusterState = stateBuilder.build();\r\n    }\r\n    randomAllocationDecider.alwaysSayYes = true;\r\n    logger.info(\"now say YES to everything\");\r\n    int iterations = 0;\r\n    do {\r\n        iterations++;\r\n        clusterState = strategy.reroute(clusterState, \"reroute\");\r\n        if (clusterState.getRoutingNodes().shardsWithState(INITIALIZING).size() > 0) {\r\n            clusterState = strategy.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));\r\n        }\r\n    } while (clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size() != 0 || clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size() != 0 && iterations < 200);\r\n    logger.info(\"Done Balancing after [{}] iterations. State:\\n{}\", iterations, clusterState);\r\n    assertThat(\"max num iteration exceeded\", iterations, Matchers.lessThan(200));\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(0));\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size(), equalTo(0));\r\n    int shards = clusterState.getRoutingNodes().shardsWithState(ShardRoutingState.STARTED).size();\r\n    assertThat(shards, equalTo(totalNumShards));\r\n    final int numNodes = clusterState.nodes().getSize();\r\n    final int upperBound = (int) Math.round(((shards / numNodes) * 1.10));\r\n    final int lowerBound = (int) Math.round(((shards / numNodes) * 0.90));\r\n    for (int i = 0; i < nodeIdCounter; i++) {\r\n        if (clusterState.getRoutingNodes().node(\"NODE_\" + i) == null) {\r\n            continue;\r\n        }\r\n        assertThat(clusterState.getRoutingNodes().node(\"NODE_\" + i).size(), Matchers.anyOf(Matchers.anyOf(equalTo((shards / numNodes) + 1), equalTo((shards / numNodes) - 1), equalTo((shards / numNodes))), Matchers.allOf(Matchers.greaterThanOrEqualTo(lowerBound), Matchers.lessThanOrEqualTo(upperBound))));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.ClusterStateCreationUtils.stateWithActivePrimary",
	"Comment": "creates cluster state with and index that has one shard and as many replicas as numberofreplicas.primary will be started in cluster state but replicas will be one of unassigned, initializing, started or relocating.",
	"Method": "ClusterState stateWithActivePrimary(String index,boolean activePrimaryLocal,int numberOfReplicas,ClusterState stateWithActivePrimary,String index,boolean activePrimaryLocal,int assignedReplicas,int unassignedReplicas){\r\n    ShardRoutingState[] replicaStates = new ShardRoutingState[assignedReplicas + unassignedReplicas];\r\n    for (int i = 0; i < assignedReplicas; i++) {\r\n        replicaStates[i] = randomFrom(ShardRoutingState.INITIALIZING, ShardRoutingState.STARTED, ShardRoutingState.RELOCATING);\r\n    }\r\n    for (int i = assignedReplicas; i < replicaStates.length; i++) {\r\n        replicaStates[i] = ShardRoutingState.UNASSIGNED;\r\n    }\r\n    return state(index, activePrimaryLocal, randomFrom(ShardRoutingState.STARTED, ShardRoutingState.RELOCATING), replicaStates);\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogTests.testRecoveryFromAFutureGenerationCleansUp",
	"Comment": "tests the situation where the node crashes after a translog gen was committed to lucene, but before the translog had the chanceto clean up its files.",
	"Method": "void testRecoveryFromAFutureGenerationCleansUp(){\r\n    int translogOperations = randomIntBetween(10, 100);\r\n    for (int op = 0; op < translogOperations / 2; op++) {\r\n        translog.add(new Translog.Index(\"test\", \"\" + op, op, primaryTerm.get(), Integer.toString(op).getBytes(Charset.forName(\"UTF-8\"))));\r\n        if (rarely()) {\r\n            translog.rollGeneration();\r\n        }\r\n    }\r\n    translog.rollGeneration();\r\n    long comittedGeneration = randomLongBetween(2, translog.currentFileGeneration());\r\n    for (int op = translogOperations / 2; op < translogOperations; op++) {\r\n        translog.add(new Translog.Index(\"test\", \"\" + op, op, primaryTerm.get(), Integer.toString(op).getBytes(Charset.forName(\"UTF-8\"))));\r\n        if (rarely()) {\r\n            translog.rollGeneration();\r\n        }\r\n    }\r\n    translog.close();\r\n    TranslogConfig config = translog.getConfig();\r\n    final TranslogDeletionPolicy deletionPolicy = new TranslogDeletionPolicy(-1, -1);\r\n    deletionPolicy.setTranslogGenerationOfLastCommit(randomLongBetween(comittedGeneration, Long.MAX_VALUE));\r\n    deletionPolicy.setMinTranslogGenerationForRecovery(comittedGeneration);\r\n    translog = new Translog(config, translog.getTranslogUUID(), deletionPolicy, () -> SequenceNumbers.NO_OPS_PERFORMED, primaryTerm::get);\r\n    assertThat(translog.getMinFileGeneration(), equalTo(1L));\r\n    for (long gen = 1; gen < translog.currentFileGeneration(); gen++) {\r\n        assertFileIsPresent(translog, gen);\r\n    }\r\n    translog.trimUnreferencedReaders();\r\n    for (long gen = 1; gen < comittedGeneration; gen++) {\r\n        assertFileDeleted(translog, gen);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.filter.FiltersAggregationBuilder.otherBucket",
	"Comment": "get whether to include a bucket for documents not matching any filter",
	"Method": "FiltersAggregationBuilder otherBucket(boolean otherBucket,boolean otherBucket){\r\n    return otherBucket;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.support.AggregationPath.resolveAggregator",
	"Comment": "resolves the aggregator pointed by this path using the given root as a point of reference.",
	"Method": "Aggregator resolveAggregator(Aggregator root){\r\n    Aggregator aggregator = root;\r\n    for (int i = 0; i < pathElements.size(); i++) {\r\n        AggregationPath.PathElement token = pathElements.get(i);\r\n        aggregator = ProfilingAggregator.unwrap(aggregator.subAggregator(token.name));\r\n        assert (aggregator instanceof SingleBucketAggregator && i <= pathElements.size() - 1) || (aggregator instanceof NumericMetricsAggregator && i == pathElements.size() - 1) : \"this should be picked up before aggregation execution - on validate\";\r\n    }\r\n    return aggregator;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.results.ForecastRequestStats.getId",
	"Comment": "return the document id used for indexing. as there is 1 and only 1 documentper forecast request, the id has no dynamic parts.",
	"Method": "String getId(){\r\n    return documentId(jobId, forecastId);\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.completion.FuzzyOptions.getMaxDeterminizedStates",
	"Comment": "returns the maximum automaton states allowed for fuzzy expansion",
	"Method": "int getMaxDeterminizedStates(){\r\n    return maxDeterminizedStates;\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequestBuilderTests.testSetSource",
	"Comment": "test setting the source for the request with different available setters",
	"Method": "void testSetSource(){\r\n    IndexRequestBuilder indexRequestBuilder = new IndexRequestBuilder(this.testClient, IndexAction.INSTANCE);\r\n    Map<String, String> source = new HashMap();\r\n    source.put(\"SomeKey\", \"SomeValue\");\r\n    indexRequestBuilder.setSource(source);\r\n    assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));\r\n    indexRequestBuilder.setSource(source, XContentType.JSON);\r\n    assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));\r\n    indexRequestBuilder.setSource(\"SomeKey\", \"SomeValue\");\r\n    assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));\r\n    indexRequestBuilder.setSource((Object) \"SomeKey\", \"SomeValue\");\r\n    assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));\r\n    ByteArrayOutputStream docOut = new ByteArrayOutputStream();\r\n    XContentBuilder doc = XContentFactory.jsonBuilder(docOut).startObject().field(\"SomeKey\", \"SomeValue\").endObject();\r\n    doc.close();\r\n    indexRequestBuilder.setSource(docOut.toByteArray(), XContentType.JSON);\r\n    assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true, indexRequestBuilder.request().getContentType()));\r\n    doc = XContentFactory.jsonBuilder().startObject().field(\"SomeKey\", \"SomeValue\").endObject();\r\n    doc.close();\r\n    indexRequestBuilder.setSource(doc);\r\n    assertEquals(EXPECTED_SOURCE, XContentHelper.convertToJson(indexRequestBuilder.request().source(), true));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.HDRPercentilesIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(percentiles(\"foo\").method(PercentilesMethod.HDR).field(\"d\").percentiles(50.0).script(new Script(ScriptType.INLINE, AggregationTestScriptsPlugin.NAME, \"_value - 1\", emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(percentiles(\"foo\").method(PercentilesMethod.HDR).field(\"d\").percentiles(50.0)).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.isTransportClient",
	"Comment": "tests if the client is a transport client or wraps a transport client.",
	"Method": "boolean isTransportClient(Client client){\r\n    if (TransportClient.class.isAssignableFrom(client.getClass())) {\r\n        return true;\r\n    } else if (client instanceof RandomizingClient) {\r\n        return isTransportClient(((RandomizingClient) client).in());\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorTests.testNumericSignificance",
	"Comment": "uses the significant terms aggregation to find the keywords in numericfields",
	"Method": "void testNumericSignificance(){\r\n    NumberFieldType longFieldType = new NumberFieldMapper.NumberFieldType(NumberFieldMapper.NumberType.LONG);\r\n    longFieldType.setName(\"long_field\");\r\n    TextFieldType textFieldType = new TextFieldType();\r\n    textFieldType.setName(\"text\");\r\n    textFieldType.setIndexAnalyzer(new NamedAnalyzer(\"my_analyzer\", AnalyzerScope.GLOBAL, new StandardAnalyzer()));\r\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\r\n    indexWriterConfig.setMaxBufferedDocs(100);\r\n    indexWriterConfig.setRAMBufferSizeMB(100);\r\n    final long ODD_VALUE = 3;\r\n    final long EVEN_VALUE = 6;\r\n    final long COMMON_VALUE = 2;\r\n    try (Directory dir = newDirectory();\r\n        IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {\r\n        for (int i = 0; i < 10; i++) {\r\n            Document doc = new Document();\r\n            if (i % 2 == 0) {\r\n                addFields(doc, NumberType.LONG.createFields(\"long_field\", ODD_VALUE, true, true, false));\r\n                doc.add(new Field(\"text\", \"odd\", textFieldType));\r\n            } else {\r\n                addFields(doc, NumberType.LONG.createFields(\"long_field\", EVEN_VALUE, true, true, false));\r\n                doc.add(new Field(\"text\", \"even\", textFieldType));\r\n            }\r\n            addFields(doc, NumberType.LONG.createFields(\"long_field\", COMMON_VALUE, true, true, false));\r\n            w.addDocument(doc);\r\n        }\r\n        SignificantTermsAggregationBuilder sigNumAgg = new SignificantTermsAggregationBuilder(\"sig_number\", null).field(\"long_field\");\r\n        sigNumAgg.executionHint(randomExecutionHint());\r\n        try (IndexReader reader = DirectoryReader.open(w)) {\r\n            assertEquals(\"test expects a single segment\", 1, reader.leaves().size());\r\n            IndexSearcher searcher = new IndexSearcher(reader);\r\n            SignificantLongTerms terms = searchAndReduce(searcher, new TermQuery(new Term(\"text\", \"odd\")), sigNumAgg, longFieldType);\r\n            assertEquals(1, terms.getBuckets().size());\r\n            assertNull(terms.getBucketByKey(Long.toString(EVEN_VALUE)));\r\n            assertNull(terms.getBucketByKey(Long.toString(COMMON_VALUE)));\r\n            assertNotNull(terms.getBucketByKey(Long.toString(ODD_VALUE)));\r\n            terms = searchAndReduce(searcher, new TermQuery(new Term(\"text\", \"even\")), sigNumAgg, longFieldType);\r\n            assertEquals(1, terms.getBuckets().size());\r\n            assertNull(terms.getBucketByKey(Long.toString(ODD_VALUE)));\r\n            assertNull(terms.getBucketByKey(Long.toString(COMMON_VALUE)));\r\n            assertNotNull(terms.getBucketByKey(Long.toString(EVEN_VALUE)));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.process.autodetect.state.ModelSizeStats.getTimestamp",
	"Comment": "the timestamp of the last processed record when this instance was created.",
	"Method": "Date getTimestamp(){\r\n    return timestamp;\r\n}"
}, {
	"Path": "org.elasticsearch.test.disruption.NetworkDisruption.applyToNodes",
	"Comment": "applies action to all disrupted links between two sets of nodes.",
	"Method": "void applyToNodes(String[] nodes1,String[] nodes2,BiConsumer<MockTransportService, MockTransportService> consumer){\r\n    for (String node1 : nodes1) {\r\n        if (disruptedLinks.nodes().contains(node1)) {\r\n            for (String node2 : nodes2) {\r\n                if (disruptedLinks.nodes().contains(node2)) {\r\n                    if (node1.equals(node2) == false) {\r\n                        if (disruptedLinks.disrupt(node1, node2)) {\r\n                            consumer.accept(transport(node1), transport(node2));\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.env.NodeEnvironmentTests.stringsToPaths",
	"Comment": "converts an array of strings to an array of paths, adding an additional child if specified",
	"Method": "Path[] stringsToPaths(String[] strings,String additional){\r\n    Path[] locations = new Path[strings.length];\r\n    for (int i = 0; i < strings.length; i++) {\r\n        locations[i] = PathUtils.get(strings[i], additional);\r\n    }\r\n    return locations;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.TransportTasksActionTests.testTaskNodeFiltering",
	"Comment": "this test starts nodes actions that blocks on all nodes. while node actions are blocked in the middle of executionit executes a tasks action that targets these blocked node actions. the test verifies that task actions are onlygetting executed on nodes that are not listed in the node filter.",
	"Method": "void testTaskNodeFiltering(){\r\n    setupTestNodes(Settings.EMPTY);\r\n    connectNodes(testNodes);\r\n    CountDownLatch checkLatch = new CountDownLatch(1);\r\n    ActionFuture<NodesResponse> future = startBlockingTestNodesAction(checkLatch);\r\n    String[] allNodes = new String[testNodes.length];\r\n    for (int i = 0; i < testNodes.length; i++) {\r\n        allNodes[i] = testNodes[i].getNodeId();\r\n    }\r\n    int filterNodesSize = randomInt(allNodes.length);\r\n    Set<String> filterNodes = new HashSet(randomSubsetOf(filterNodesSize, allNodes));\r\n    logger.info(\"Filtering out nodes {} size: {}\", filterNodes, filterNodesSize);\r\n    TestTasksAction[] tasksActions = new TestTasksAction[nodesCount];\r\n    for (int i = 0; i < testNodes.length; i++) {\r\n        final int node = i;\r\n        tasksActions[i] = new TestTasksAction(\"internal:testTasksAction\", testNodes[i].clusterService, testNodes[i].transportService) {\r\n            @Override\r\n            protected String[] filterNodeIds(DiscoveryNodes nodes, String[] nodesIds) {\r\n                String[] superNodes = super.filterNodeIds(nodes, nodesIds);\r\n                List<String> filteredNodes = new ArrayList();\r\n                for (String node : superNodes) {\r\n                    if (filterNodes.contains(node) == false) {\r\n                        filteredNodes.add(node);\r\n                    }\r\n                }\r\n                return filteredNodes.toArray(new String[filteredNodes.size()]);\r\n            }\r\n            @Override\r\n            protected void taskOperation(TestTasksRequest request, Task task, ActionListener<TestTaskResponse> listener) {\r\n                if (randomBoolean()) {\r\n                    listener.onResponse(new TestTaskResponse(testNodes[node].getNodeId()));\r\n                } else {\r\n                    threadPool.generic().execute(() -> listener.onResponse(new TestTaskResponse(testNodes[node].getNodeId())));\r\n                }\r\n            }\r\n        };\r\n    }\r\n    TestTasksRequest testTasksRequest = new TestTasksRequest();\r\n    testTasksRequest.setActions(\"internal:testAction[n]\");\r\n    TestTasksResponse response = ActionTestUtils.executeBlocking(tasksActions[randomIntBetween(0, nodesCount - 1)], testTasksRequest);\r\n    assertEquals(testNodes.length - filterNodes.size(), response.tasks.size());\r\n    assertEquals(0, response.getTaskFailures().size());\r\n    assertEquals(0, response.getNodeFailures().size());\r\n    for (TestTaskResponse taskResponse : response.tasks) {\r\n        String nodeId = taskResponse.getStatus();\r\n        assertFalse(\"Found response from filtered node \" + nodeId, filterNodes.contains(nodeId));\r\n    }\r\n    checkLatch.countDown();\r\n    NodesResponse responses = future.get();\r\n    assertEquals(0, responses.failureCount());\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.TransportTasksActionTests.testTaskNodeFiltering",
	"Comment": "this test starts nodes actions that blocks on all nodes. while node actions are blocked in the middle of executionit executes a tasks action that targets these blocked node actions. the test verifies that task actions are onlygetting executed on nodes that are not listed in the node filter.",
	"Method": "void testTaskNodeFiltering(){\r\n    String[] superNodes = super.filterNodeIds(nodes, nodesIds);\r\n    List<String> filteredNodes = new ArrayList();\r\n    for (String node : superNodes) {\r\n        if (filterNodes.contains(node) == false) {\r\n            filteredNodes.add(node);\r\n        }\r\n    }\r\n    return filteredNodes.toArray(new String[filteredNodes.size()]);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.TransportTasksActionTests.testTaskNodeFiltering",
	"Comment": "this test starts nodes actions that blocks on all nodes. while node actions are blocked in the middle of executionit executes a tasks action that targets these blocked node actions. the test verifies that task actions are onlygetting executed on nodes that are not listed in the node filter.",
	"Method": "void testTaskNodeFiltering(){\r\n    if (randomBoolean()) {\r\n        listener.onResponse(new TestTaskResponse(testNodes[node].getNodeId()));\r\n    } else {\r\n        threadPool.generic().execute(() -> listener.onResponse(new TestTaskResponse(testNodes[node].getNodeId())));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.uid.VersionsTests.testCacheFilterReader",
	"Comment": "test that version map cache behaves properly with a filtered reader",
	"Method": "void testCacheFilterReader(){\r\n    int size = VersionsAndSeqNoResolver.lookupStates.size();\r\n    Directory dir = newDirectory();\r\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Lucene.STANDARD_ANALYZER));\r\n    Document doc = new Document();\r\n    doc.add(new Field(IdFieldMapper.NAME, \"6\", IdFieldMapper.Defaults.FIELD_TYPE));\r\n    doc.add(new NumericDocValuesField(VersionFieldMapper.NAME, 87));\r\n    writer.addDocument(doc);\r\n    DirectoryReader reader = DirectoryReader.open(writer);\r\n    assertEquals(87, loadVersion(reader, new Term(IdFieldMapper.NAME, \"6\")));\r\n    assertEquals(size + 1, VersionsAndSeqNoResolver.lookupStates.size());\r\n    DirectoryReader wrapped = ElasticsearchDirectoryReader.wrap(reader, new ShardId(\"bogus\", \"_na_\", 5));\r\n    assertEquals(87, loadVersion(wrapped, new Term(IdFieldMapper.NAME, \"6\")));\r\n    assertEquals(size + 1, VersionsAndSeqNoResolver.lookupStates.size());\r\n    reader.close();\r\n    writer.close();\r\n    assertEquals(size, VersionsAndSeqNoResolver.lookupStates.size());\r\n    dir.close();\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationActionTests.testNoRerouteOnStaleClusterState",
	"Comment": "when relocating a primary shard, there is a cluster state update at the end of relocation where the active primary is switched fromthe relocation source to the relocation target. if relocation source receives and processes this cluster statebefore the relocation target, there is a time span where relocation source believes active primary to be onrelocation target and relocation target believes active primary to be on relocation source. this results in replicationrequests being sent back and forth.this test checks that replication request is not routed back from relocation target to relocation source in case ofstale index routing table on relocation target.",
	"Method": "void testNoRerouteOnStaleClusterState(){\r\n    final String index = \"test\";\r\n    final ShardId shardId = new ShardId(index, \"_na_\", 0);\r\n    ClusterState state = state(index, true, ShardRoutingState.RELOCATING);\r\n    String relocationTargetNode = state.getRoutingTable().shardRoutingTable(shardId).primaryShard().relocatingNodeId();\r\n    state = ClusterState.builder(state).nodes(DiscoveryNodes.builder(state.nodes()).localNodeId(relocationTargetNode)).build();\r\n    setState(clusterService, state);\r\n    logger.debug(\"--> relocation ongoing state:\\n{}\", clusterService.state());\r\n    Request request = new Request(shardId).timeout(\"1ms\").routedBasedOnClusterVersion(clusterService.state().version() + 1);\r\n    PlainActionFuture<TestResponse> listener = new PlainActionFuture();\r\n    TestAction.ReroutePhase reroutePhase = action.new ReroutePhase(null, request, listener);\r\n    reroutePhase.run();\r\n    assertListenerThrows(\"cluster state too old didn't cause a timeout\", listener, UnavailableShardsException.class);\r\n    assertTrue(request.isRetrySet.compareAndSet(true, false));\r\n    request = new Request(shardId).routedBasedOnClusterVersion(clusterService.state().version() + 1);\r\n    listener = new PlainActionFuture();\r\n    reroutePhase = action.new ReroutePhase(null, request, listener);\r\n    reroutePhase.run();\r\n    assertFalse(\"cluster state too old didn't cause a retry\", listener.isDone());\r\n    assertTrue(request.isRetrySet.get());\r\n    ShardRouting relocationTarget = clusterService.state().getRoutingTable().shardRoutingTable(shardId).shardsWithState(ShardRoutingState.INITIALIZING).get(0);\r\n    AllocationService allocationService = ESAllocationTestCase.createAllocationService();\r\n    ClusterState updatedState = allocationService.applyStartedShards(state, Collections.singletonList(relocationTarget));\r\n    setState(clusterService, updatedState);\r\n    logger.debug(\"--> relocation complete state:\\n{}\", clusterService.state());\r\n    IndexShardRoutingTable shardRoutingTable = clusterService.state().routingTable().index(index).shard(shardId.id());\r\n    final String primaryNodeId = shardRoutingTable.primaryShard().currentNodeId();\r\n    final List<CapturingTransport.CapturedRequest> capturedRequests = transport.getCapturedRequestsByTargetNodeAndClear().get(primaryNodeId);\r\n    assertThat(capturedRequests, notNullValue());\r\n    assertThat(capturedRequests.size(), equalTo(1));\r\n    assertThat(capturedRequests.get(0).action, equalTo(\"internal:testAction[p]\"));\r\n    assertIndexShardCounter(0);\r\n}"
}, {
	"Path": "org.elasticsearch.bwcompat.RecoveryWithUnsupportedIndicesIT.prepareBackwardsDataDir",
	"Comment": "return settings that could be used to start a node that has the given zipped home directory.",
	"Method": "Settings prepareBackwardsDataDir(Path backwardsIndex){\r\n    Path indexDir = createTempDir();\r\n    Path dataDir = indexDir.resolve(\"data\");\r\n    try (InputStream stream = Files.newInputStream(backwardsIndex)) {\r\n        TestUtil.unzip(stream, indexDir);\r\n    }\r\n    assertTrue(Files.exists(dataDir));\r\n    final Path[] list;\r\n    try (DirectoryStream<Path> stream = Files.newDirectoryStream(dataDir)) {\r\n        List<Path> dirs = new ArrayList();\r\n        for (Path p : stream) {\r\n            if (!p.getFileName().toString().startsWith(\"extra\")) {\r\n                dirs.add(p);\r\n            }\r\n        }\r\n        list = dirs.toArray(new Path[0]);\r\n    }\r\n    if (list.length != 1) {\r\n        StringBuilder builder = new StringBuilder(\"Backwards index must contain exactly one cluster\\n\");\r\n        for (Path line : list) {\r\n            builder.append(line.toString()).append('\\n');\r\n        }\r\n        throw new IllegalStateException(builder.toString());\r\n    }\r\n    Path src = list[0].resolve(NodeEnvironment.NODES_FOLDER);\r\n    Path dest = dataDir.resolve(NodeEnvironment.NODES_FOLDER);\r\n    assertTrue(Files.exists(src));\r\n    Files.move(src, dest);\r\n    assertFalse(Files.exists(src));\r\n    assertTrue(Files.exists(dest));\r\n    Settings.Builder builder = Settings.builder().put(Environment.PATH_DATA_SETTING.getKey(), dataDir.toAbsolutePath());\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.AbstractSuggestionBuilderTestCase.testFromXContent",
	"Comment": "creates random suggestion builder, renders it to xcontent and back to newinstance that should be equal to original",
	"Method": "void testFromXContent(){\r\n    for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {\r\n        SB suggestionBuilder = randomTestBuilder();\r\n        XContentBuilder xContentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));\r\n        if (randomBoolean()) {\r\n            xContentBuilder.prettyPrint();\r\n        }\r\n        xContentBuilder.startObject();\r\n        suggestionBuilder.toXContent(xContentBuilder, ToXContent.EMPTY_PARAMS);\r\n        xContentBuilder.endObject();\r\n        XContentBuilder shuffled = shuffleXContent(xContentBuilder, shuffleProtectedFields());\r\n        try (XContentParser parser = createParser(shuffled)) {\r\n            parser.nextToken();\r\n            SuggestionBuilder<?> secondSuggestionBuilder = SuggestionBuilder.fromXContent(parser);\r\n            assertNotSame(suggestionBuilder, secondSuggestionBuilder);\r\n            assertEquals(suggestionBuilder, secondSuggestionBuilder);\r\n            assertEquals(suggestionBuilder.hashCode(), secondSuggestionBuilder.hashCode());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.monitoring.client.MonitoringClient.bulk",
	"Comment": "executes a bulk of index operations that concern monitoring documents.",
	"Method": "void bulk(MonitoringBulkRequest request,ActionListener<MonitoringBulkResponse> listener,ActionFuture<MonitoringBulkResponse> bulk,MonitoringBulkRequest request){\r\n    return client.execute(MonitoringBulkAction.INSTANCE, request);\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.ClientYamlTestResponse.getBody",
	"Comment": "returns the body properly parsed depending on the content type.might be a string or a json object parsed as a map.",
	"Method": "Object getBody(){\r\n    if (parsedResponse != null) {\r\n        return parsedResponse.evaluate(\"\");\r\n    }\r\n    assert bodyContentType == null;\r\n    return getBodyAsString();\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TransportChannel.getVersion",
	"Comment": "returns the version of the other party that this channel will send a response to.",
	"Method": "Version getVersion(){\r\n    return Version.CURRENT;\r\n}"
}, {
	"Path": "org.elasticsearch.test.TestCluster.wipeTemplates",
	"Comment": "deletes index templates, support wildcard notation.if no template name is passed to this method all templates are removed.",
	"Method": "void wipeTemplates(String templates){\r\n    if (size() > 0) {\r\n        if (templates.length == 0) {\r\n            templates = new String[] { \"*\" };\r\n        }\r\n        for (String template : templates) {\r\n            try {\r\n                client().admin().indices().prepareDeleteTemplate(template).execute().actionGet();\r\n            } catch (IndexTemplateMissingException e) {\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.phrase.DirectCandidateGeneratorBuilder.size",
	"Comment": "sets the maximum suggestions to be returned per suggest text term.",
	"Method": "DirectCandidateGeneratorBuilder size(int size,Integer size){\r\n    return size;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.TaskStorageRetryIT.nodeSettings",
	"Comment": "lower the queue sizes to be small enough that both bulk and searches will time out and have to be retried.",
	"Method": "Settings nodeSettings(){\r\n    return Settings.builder().put(super.nodeSettings()).put(\"thread_pool.write.size\", 2).put(\"thread_pool.write.queue_size\", 0).build();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.IndexNameExpressionResolverTests.testConcreteIndicesWildcardNoMatch",
	"Comment": "test resolving wildcard pattern that matches no index of alias for random indicesoptions",
	"Method": "void testConcreteIndicesWildcardNoMatch(){\r\n    for (int i = 0; i < 10; i++) {\r\n        IndicesOptions indicesOptions = IndicesOptions.fromOptions(randomBoolean(), randomBoolean(), randomBoolean(), randomBoolean());\r\n        MetaData.Builder mdBuilder = MetaData.builder().put(indexBuilder(\"aaa\").state(State.OPEN).putAlias(AliasMetaData.builder(\"aaa_alias1\"))).put(indexBuilder(\"bbb\").state(State.OPEN).putAlias(AliasMetaData.builder(\"bbb_alias1\"))).put(indexBuilder(\"ccc\").state(State.CLOSE).putAlias(AliasMetaData.builder(\"ccc_alias1\")));\r\n        ClusterState state = ClusterState.builder(new ClusterName(\"_name\")).metaData(mdBuilder).build();\r\n        IndexNameExpressionResolver.Context context = new IndexNameExpressionResolver.Context(state, indicesOptions);\r\n        if (indicesOptions.allowNoIndices()) {\r\n            String[] concreteIndices = indexNameExpressionResolver.concreteIndexNames(context, \"Foo*\");\r\n            assertThat(concreteIndices, notNullValue());\r\n            assertThat(concreteIndices.length, equalTo(0));\r\n        } else {\r\n            expectThrows(IndexNotFoundException.class, () -> indexNameExpressionResolver.concreteIndexNames(context, \"Foo*\"));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.terms.StringTermsIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=keyword\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", \"foo\"), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", \"bar\"));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(terms(\"terms\").field(\"d\").script(new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, \"'foo_' + _value\", Collections.emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(terms(\"terms\").field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryStringQueryBuilderTests.testQuoteAnalyzer",
	"Comment": "the quote analyzer should overwrite any other forced analyzer in quoted parts of the query",
	"Method": "void testQuoteAnalyzer(){\r\n    Query query = new QueryStringQueryBuilder(\"ONE \\\"TWO THREE\\\"\").field(STRING_FIELD_NAME).analyzer(\"whitespace\").quoteAnalyzer(\"simple\").toQuery(createShardContext());\r\n    Query expectedQuery = new BooleanQuery.Builder().add(new BooleanClause(new TermQuery(new Term(STRING_FIELD_NAME, \"ONE\")), Occur.SHOULD)).add(new BooleanClause(new PhraseQuery.Builder().add(new Term(STRING_FIELD_NAME, \"two\"), 0).add(new Term(STRING_FIELD_NAME, \"three\"), 1).build(), Occur.SHOULD)).build();\r\n    assertEquals(expectedQuery, query);\r\n}"
}, {
	"Path": "org.elasticsearch.indices.store.IndicesStoreIntegrationIT.relocateAndBlockCompletion",
	"Comment": "relocate a shard and block cluster state processing on the relocation target node to activate the shard",
	"Method": "BlockClusterStateProcessing relocateAndBlockCompletion(Logger logger,String index,int shard,String nodeFrom,String nodeTo){\r\n    BlockClusterStateProcessing disruption = new BlockClusterStateProcessing(nodeTo, random());\r\n    internalCluster().setDisruptionScheme(disruption);\r\n    MockTransportService transportService = (MockTransportService) internalCluster().getInstance(TransportService.class, nodeTo);\r\n    ClusterService clusterService = internalCluster().getInstance(ClusterService.class, nodeTo);\r\n    CountDownLatch beginRelocationLatch = new CountDownLatch(1);\r\n    CountDownLatch receivedShardExistsRequestLatch = new CountDownLatch(1);\r\n    transportService.addTracer(new MockTransportService.Tracer() {\r\n        @Override\r\n        public void receivedRequest(long requestId, String action) {\r\n            if (action.equals(PeerRecoveryTargetService.Actions.FILES_INFO)) {\r\n                logger.info(\"received: {}, relocation starts\", action);\r\n                beginRelocationLatch.countDown();\r\n            } else if (action.equals(IndicesStore.ACTION_SHARD_EXISTS)) {\r\n                receivedShardExistsRequestLatch.countDown();\r\n                logger.info(\"received: {}, relocation done\", action);\r\n            } else if (action.equals(PeerRecoveryTargetService.Actions.WAIT_CLUSTERSTATE)) {\r\n                logger.info(\"received: {}, waiting on cluster state\", action);\r\n                try {\r\n                    assertBusy(() -> assertTrue(clusterService.state().routingTable().index(index).shard(shard).primaryShard().relocating()));\r\n                } catch (Exception e) {\r\n                    throw new RuntimeException(e);\r\n                }\r\n            }\r\n        }\r\n    });\r\n    internalCluster().client().admin().cluster().prepareReroute().add(new MoveAllocationCommand(index, shard, nodeFrom, nodeTo)).get();\r\n    logger.info(\"--> waiting for relocation to start\");\r\n    beginRelocationLatch.await();\r\n    logger.info(\"--> starting disruption\");\r\n    disruption.startDisrupting();\r\n    logger.info(\"--> waiting for relocation to finish\");\r\n    receivedShardExistsRequestLatch.await();\r\n    logger.info(\"--> relocation completed (but cluster state processing block still in place)\");\r\n    return disruption;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.store.IndicesStoreIntegrationIT.relocateAndBlockCompletion",
	"Comment": "relocate a shard and block cluster state processing on the relocation target node to activate the shard",
	"Method": "BlockClusterStateProcessing relocateAndBlockCompletion(Logger logger,String index,int shard,String nodeFrom,String nodeTo){\r\n    if (action.equals(PeerRecoveryTargetService.Actions.FILES_INFO)) {\r\n        logger.info(\"received: {}, relocation starts\", action);\r\n        beginRelocationLatch.countDown();\r\n    } else if (action.equals(IndicesStore.ACTION_SHARD_EXISTS)) {\r\n        receivedShardExistsRequestLatch.countDown();\r\n        logger.info(\"received: {}, relocation done\", action);\r\n    } else if (action.equals(PeerRecoveryTargetService.Actions.WAIT_CLUSTERSTATE)) {\r\n        logger.info(\"received: {}, waiting on cluster state\", action);\r\n        try {\r\n            assertBusy(() -> assertTrue(clusterService.state().routingTable().index(index).shard(shard).primaryShard().relocating()));\r\n        } catch (Exception e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.SimpleClusterStateIT.testFilteringByIndexWorks",
	"Comment": "retrieves the cluster state for the given indices and then checksthat the cluster state returns coherent data for both routing table and metadata.",
	"Method": "void testFilteringByIndexWorks(String[] indices,String[] expected){\r\n    ClusterStateResponse clusterState = client().admin().cluster().prepareState().clear().setMetaData(true).setRoutingTable(true).setIndices(indices).get();\r\n    ImmutableOpenMap<String, IndexMetaData> metaData = clusterState.getState().getMetaData().indices();\r\n    assertThat(metaData.size(), is(expected.length));\r\n    RoutingTable routingTable = clusterState.getState().getRoutingTable();\r\n    assertThat(routingTable.indicesRouting().size(), is(expected.length));\r\n    for (String expectedIndex : expected) {\r\n        assertThat(metaData, CollectionAssertions.hasKey(expectedIndex));\r\n        assertThat(routingTable.hasIndex(expectedIndex), is(true));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.transport.ConnectionManager.disconnectFromNode",
	"Comment": "disconnected from the given node, if not connected, will do nothing.",
	"Method": "void disconnectFromNode(DiscoveryNode node){\r\n    Transport.Connection nodeChannels = connectedNodes.remove(node);\r\n    if (nodeChannels != null) {\r\n        nodeChannels.close();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationActionTests.createTransportChannel",
	"Comment": "transport channel that is needed for replica operation testing.",
	"Method": "TransportChannel createTransportChannel(PlainActionFuture<TestResponse> listener){\r\n    return new TransportChannel() {\r\n        @Override\r\n        public String getProfileName() {\r\n            return \"\";\r\n        }\r\n        @Override\r\n        public void sendResponse(TransportResponse response) throws IOException {\r\n            listener.onResponse(((TestResponse) response));\r\n        }\r\n        @Override\r\n        public void sendResponse(Exception exception) throws IOException {\r\n            listener.onFailure(exception);\r\n        }\r\n        @Override\r\n        public String getChannelType() {\r\n            return \"replica_test\";\r\n        }\r\n    };\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationActionTests.createTransportChannel",
	"Comment": "transport channel that is needed for replica operation testing.",
	"Method": "TransportChannel createTransportChannel(PlainActionFuture<TestResponse> listener){\r\n    return \"\";\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationActionTests.createTransportChannel",
	"Comment": "transport channel that is needed for replica operation testing.",
	"Method": "TransportChannel createTransportChannel(PlainActionFuture<TestResponse> listener){\r\n    listener.onResponse(((TestResponse) response));\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationActionTests.createTransportChannel",
	"Comment": "transport channel that is needed for replica operation testing.",
	"Method": "TransportChannel createTransportChannel(PlainActionFuture<TestResponse> listener){\r\n    listener.onFailure(exception);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationActionTests.createTransportChannel",
	"Comment": "transport channel that is needed for replica operation testing.",
	"Method": "TransportChannel createTransportChannel(PlainActionFuture<TestResponse> listener){\r\n    return \"replica_test\";\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.BaseAggregationTestCase.testSerialization",
	"Comment": "test serialization and deserialization of the test aggregatorfactory.",
	"Method": "void testSerialization(){\r\n    AB testAgg = createTestAggregatorBuilder();\r\n    try (BytesStreamOutput output = new BytesStreamOutput()) {\r\n        output.writeNamedWriteable(testAgg);\r\n        try (StreamInput in = new NamedWriteableAwareStreamInput(output.bytes().streamInput(), namedWriteableRegistry())) {\r\n            AggregationBuilder deserialized = in.readNamedWriteable(AggregationBuilder.class);\r\n            assertEquals(testAgg, deserialized);\r\n            assertEquals(testAgg.hashCode(), deserialized.hashCode());\r\n            assertNotSame(testAgg, deserialized);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ClientHelper.stashWithOrigin",
	"Comment": "stashes the current context and sets the origin in the current context. the original context is returned as a stored context",
	"Method": "ThreadContext.StoredContext stashWithOrigin(ThreadContext threadContext,String origin){\r\n    return threadContext.stashWithOrigin(origin);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.master.IndexingMasterFailoverIT.testMasterFailoverDuringIndexingWithMappingChanges",
	"Comment": "indexing operations which entail mapping changes require a blocking request to the master node to update the mapping.if the master node is being disrupted or if it cannot commit cluster state changes, it needs to retry within timeout limits.this retry logic is implemented in transportmasternodeaction and tested by the following master failover scenario.",
	"Method": "void testMasterFailoverDuringIndexingWithMappingChanges(){\r\n    logger.info(\"--> start 4 nodes, 3 master, 1 data\");\r\n    final Settings sharedSettings = // for hitting simulated network failures quickly\r\n    Settings.builder().put(FaultDetection.PING_TIMEOUT_SETTING.getKey(), // for hitting simulated network failures quickly\r\n    \"1s\").put(FaultDetection.PING_RETRIES_SETTING.getKey(), // still long to induce failures but to long so test won't time out\r\n    \"1\").put(\"discovery.zen.join_timeout\", // <-- for hitting simulated network failures quickly\r\n    \"10s\").put(DiscoverySettings.PUBLISH_TIMEOUT_SETTING.getKey(), \"1s\").put(ElectMasterService.DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), 2).build();\r\n    internalCluster().startMasterOnlyNodes(3, sharedSettings);\r\n    String dataNode = internalCluster().startDataOnlyNode(sharedSettings);\r\n    logger.info(\"--> wait for all nodes to join the cluster\");\r\n    ensureStableCluster(4);\r\n    client().admin().indices().prepareCreate(\"myindex\").setSettings(Settings.builder().put(\"index.number_of_shards\", 1).put(\"index.number_of_replicas\", 0)).get();\r\n    ensureGreen(\"myindex\");\r\n    final CyclicBarrier barrier = new CyclicBarrier(2);\r\n    Thread indexingThread = new Thread(new Runnable() {\r\n        @Override\r\n        public void run() {\r\n            try {\r\n                barrier.await();\r\n            } catch (InterruptedException e) {\r\n                logger.warn(\"Barrier interrupted\", e);\r\n                return;\r\n            } catch (BrokenBarrierException e) {\r\n                logger.warn(\"Broken barrier\", e);\r\n                return;\r\n            }\r\n            for (int i = 0; i < 10; i++) {\r\n                IndexResponse response = client(dataNode).prepareIndex(\"myindex\", \"mytype\").setSource(\"field_\" + i, \"val\").get();\r\n                assertEquals(DocWriteResponse.Result.CREATED, response.getResult());\r\n            }\r\n        }\r\n    });\r\n    indexingThread.setName(\"indexingThread\");\r\n    indexingThread.start();\r\n    barrier.await();\r\n    String master = internalCluster().getMasterName();\r\n    Set<String> otherNodes = new HashSet(Arrays.asList(internalCluster().getNodeNames()));\r\n    otherNodes.remove(master);\r\n    NetworkDisruption partition = new NetworkDisruption(new TwoPartitions(Collections.singleton(master), otherNodes), new NetworkDisconnect());\r\n    internalCluster().setDisruptionScheme(partition);\r\n    logger.info(\"--> disrupting network\");\r\n    partition.startDisrupting();\r\n    logger.info(\"--> waiting for new master to be elected\");\r\n    ensureStableCluster(3, dataNode);\r\n    partition.stopDisrupting();\r\n    logger.info(\"--> waiting to heal\");\r\n    ensureStableCluster(4);\r\n    indexingThread.join();\r\n    ensureGreen(\"myindex\");\r\n    refresh();\r\n    assertThat(client().prepareSearch(\"myindex\").get().getHits().getTotalHits(), equalTo(10L));\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.master.IndexingMasterFailoverIT.testMasterFailoverDuringIndexingWithMappingChanges",
	"Comment": "indexing operations which entail mapping changes require a blocking request to the master node to update the mapping.if the master node is being disrupted or if it cannot commit cluster state changes, it needs to retry within timeout limits.this retry logic is implemented in transportmasternodeaction and tested by the following master failover scenario.",
	"Method": "void testMasterFailoverDuringIndexingWithMappingChanges(){\r\n    try {\r\n        barrier.await();\r\n    } catch (InterruptedException e) {\r\n        logger.warn(\"Barrier interrupted\", e);\r\n        return;\r\n    } catch (BrokenBarrierException e) {\r\n        logger.warn(\"Broken barrier\", e);\r\n        return;\r\n    }\r\n    for (int i = 0; i < 10; i++) {\r\n        IndexResponse response = client(dataNode).prepareIndex(\"myindex\", \"mytype\").setSource(\"field_\" + i, \"val\").get();\r\n        assertEquals(DocWriteResponse.Result.CREATED, response.getResult());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.ScriptSortBuilderTests.testBuildNested",
	"Comment": "test that the sort builder nested object gets created in the sortfield",
	"Method": "void testBuildNested(){\r\n    QueryShardContext shardContextMock = createMockShardContext();\r\n    ScriptSortBuilder sortBuilder = new ScriptSortBuilder(mockScript(MOCK_SCRIPT_NAME), ScriptSortType.NUMBER).setNestedSort(new NestedSortBuilder(\"path\").setFilter(QueryBuilders.matchAllQuery()));\r\n    SortField sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    XFieldComparatorSource comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    Nested nested = comparatorSource.nested();\r\n    assertNotNull(nested);\r\n    assertEquals(new MatchAllDocsQuery(), nested.getInnerQuery());\r\n    sortBuilder = new ScriptSortBuilder(mockScript(MOCK_SCRIPT_NAME), ScriptSortType.NUMBER).setNestedPath(\"path\");\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    nested = comparatorSource.nested();\r\n    assertNotNull(nested);\r\n    assertEquals(new TermQuery(new Term(TypeFieldMapper.NAME, \"__path\")), nested.getInnerQuery());\r\n    sortBuilder = new ScriptSortBuilder(mockScript(MOCK_SCRIPT_NAME), ScriptSortType.NUMBER).setNestedPath(\"path\").setNestedFilter(QueryBuilders.matchAllQuery());\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    nested = comparatorSource.nested();\r\n    assertNotNull(nested);\r\n    assertEquals(new MatchAllDocsQuery(), nested.getInnerQuery());\r\n    sortBuilder = new ScriptSortBuilder(mockScript(MOCK_SCRIPT_NAME), ScriptSortType.NUMBER).setNestedFilter(QueryBuilders.matchAllQuery());\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertNull(comparatorSource.nested());\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESSingleNodeTestCase.resetNodeAfterTest",
	"Comment": "this method returns true if the node that is used in the background should be resetafter each test. this is useful if the test changes the cluster state metadata etc. the default isfalse.",
	"Method": "boolean resetNodeAfterTest(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.ensureSearchable",
	"Comment": "ensures the cluster is in a searchable state for the given indices. this means a searchable copy of eachshard is available on the cluster.",
	"Method": "ClusterHealthStatus ensureSearchable(String indices){\r\n    return ensureGreen(indices);\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.ccr.action.ShardChangesTests.testGetOperationsBasedOnGlobalSequenceId",
	"Comment": "this emulates what the ccr persistent task will do for pulling",
	"Method": "void testGetOperationsBasedOnGlobalSequenceId(){\r\n    client().admin().indices().prepareCreate(\"index\").setSettings(Settings.builder().put(\"index.number_of_shards\", 1)).get();\r\n    client().prepareIndex(\"index\", \"doc\", \"1\").setSource(\"{}\", XContentType.JSON).get();\r\n    client().prepareIndex(\"index\", \"doc\", \"2\").setSource(\"{}\", XContentType.JSON).get();\r\n    client().prepareIndex(\"index\", \"doc\", \"3\").setSource(\"{}\", XContentType.JSON).get();\r\n    ShardStats shardStats = client().admin().indices().prepareStats(\"index\").get().getIndex(\"index\").getShards()[0];\r\n    long globalCheckPoint = shardStats.getSeqNoStats().getGlobalCheckpoint();\r\n    assertThat(globalCheckPoint, equalTo(2L));\r\n    String historyUUID = shardStats.getCommitStats().getUserData().get(Engine.HISTORY_UUID_KEY);\r\n    ShardChangesAction.Request request = new ShardChangesAction.Request(shardStats.getShardRouting().shardId(), historyUUID);\r\n    request.setFromSeqNo(0L);\r\n    request.setMaxOperationCount(3);\r\n    ShardChangesAction.Response response = client().execute(ShardChangesAction.INSTANCE, request).get();\r\n    assertThat(response.getOperations().length, equalTo(3));\r\n    Translog.Index operation = (Translog.Index) response.getOperations()[0];\r\n    assertThat(operation.seqNo(), equalTo(0L));\r\n    assertThat(operation.id(), equalTo(\"1\"));\r\n    operation = (Translog.Index) response.getOperations()[1];\r\n    assertThat(operation.seqNo(), equalTo(1L));\r\n    assertThat(operation.id(), equalTo(\"2\"));\r\n    operation = (Translog.Index) response.getOperations()[2];\r\n    assertThat(operation.seqNo(), equalTo(2L));\r\n    assertThat(operation.id(), equalTo(\"3\"));\r\n    client().prepareIndex(\"index\", \"doc\", \"3\").setSource(\"{}\", XContentType.JSON).get();\r\n    client().prepareIndex(\"index\", \"doc\", \"4\").setSource(\"{}\", XContentType.JSON).get();\r\n    client().prepareIndex(\"index\", \"doc\", \"5\").setSource(\"{}\", XContentType.JSON).get();\r\n    shardStats = client().admin().indices().prepareStats(\"index\").get().getIndex(\"index\").getShards()[0];\r\n    globalCheckPoint = shardStats.getSeqNoStats().getGlobalCheckpoint();\r\n    assertThat(globalCheckPoint, equalTo(5L));\r\n    request = new ShardChangesAction.Request(shardStats.getShardRouting().shardId(), historyUUID);\r\n    request.setFromSeqNo(3L);\r\n    request.setMaxOperationCount(3);\r\n    response = client().execute(ShardChangesAction.INSTANCE, request).get();\r\n    assertThat(response.getOperations().length, equalTo(3));\r\n    operation = (Translog.Index) response.getOperations()[0];\r\n    assertThat(operation.seqNo(), equalTo(3L));\r\n    assertThat(operation.id(), equalTo(\"3\"));\r\n    operation = (Translog.Index) response.getOperations()[1];\r\n    assertThat(operation.seqNo(), equalTo(4L));\r\n    assertThat(operation.id(), equalTo(\"4\"));\r\n    operation = (Translog.Index) response.getOperations()[2];\r\n    assertThat(operation.seqNo(), equalTo(5L));\r\n    assertThat(operation.id(), equalTo(\"5\"));\r\n}"
}, {
	"Path": "org.elasticsearch.search.MockSearchService.addActiveContext",
	"Comment": "add an active search context to the list of tracked contexts. package private for testing.",
	"Method": "void addActiveContext(SearchContext context){\r\n    ACTIVE_SEARCH_CONTEXTS.put(context, new RuntimeException(context.toString()));\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShardTests.testReadSnapshotAndCheckIndexConcurrently",
	"Comment": "simulates a scenario that happens when we are async fetching snapshot metadata from gatewayserviceand checking index concurrently. this should always be possible without any exception.",
	"Method": "void testReadSnapshotAndCheckIndexConcurrently(){\r\n    final boolean isPrimary = randomBoolean();\r\n    IndexShard indexShard = newStartedShard(isPrimary);\r\n    final long numDocs = between(10, 100);\r\n    for (long i = 0; i < numDocs; i++) {\r\n        indexDoc(indexShard, \"_doc\", Long.toString(i), \"{}\");\r\n        if (randomBoolean()) {\r\n            indexShard.refresh(\"test\");\r\n        }\r\n    }\r\n    indexShard.flush(new FlushRequest());\r\n    closeShards(indexShard);\r\n    final ShardRouting shardRouting = ShardRoutingHelper.initWithSameId(indexShard.routingEntry(), isPrimary ? RecoverySource.ExistingStoreRecoverySource.INSTANCE : RecoverySource.PeerRecoverySource.INSTANCE);\r\n    final IndexMetaData indexMetaData = IndexMetaData.builder(indexShard.indexSettings().getIndexMetaData()).settings(Settings.builder().put(indexShard.indexSettings.getSettings()).put(IndexSettings.INDEX_CHECK_ON_STARTUP.getKey(), randomFrom(\"false\", \"true\", \"checksum\"))).build();\r\n    final IndexShard newShard = newShard(shardRouting, indexShard.shardPath(), indexMetaData, null, null, indexShard.engineFactory, indexShard.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER);\r\n    Store.MetadataSnapshot storeFileMetaDatas = newShard.snapshotStoreMetadata();\r\n    assertTrue(\"at least 2 files, commit and data: \" + storeFileMetaDatas.toString(), storeFileMetaDatas.size() > 1);\r\n    AtomicBoolean stop = new AtomicBoolean(false);\r\n    CountDownLatch latch = new CountDownLatch(1);\r\n    Thread snapshotter = new Thread(() -> {\r\n        latch.countDown();\r\n        while (stop.get() == false) {\r\n            try {\r\n                Store.MetadataSnapshot readMeta = newShard.snapshotStoreMetadata();\r\n                assertThat(readMeta.getNumDocs(), equalTo(numDocs));\r\n                assertThat(storeFileMetaDatas.recoveryDiff(readMeta).different.size(), equalTo(0));\r\n                assertThat(storeFileMetaDatas.recoveryDiff(readMeta).missing.size(), equalTo(0));\r\n                assertThat(storeFileMetaDatas.recoveryDiff(readMeta).identical.size(), equalTo(storeFileMetaDatas.size()));\r\n            } catch (IOException e) {\r\n                throw new AssertionError(e);\r\n            }\r\n        }\r\n    });\r\n    snapshotter.start();\r\n    if (isPrimary) {\r\n        newShard.markAsRecovering(\"store\", new RecoveryState(newShard.routingEntry(), getFakeDiscoNode(newShard.routingEntry().currentNodeId()), null));\r\n    } else {\r\n        newShard.markAsRecovering(\"peer\", new RecoveryState(newShard.routingEntry(), getFakeDiscoNode(newShard.routingEntry().currentNodeId()), getFakeDiscoNode(newShard.routingEntry().currentNodeId())));\r\n    }\r\n    int iters = iterations(10, 100);\r\n    latch.await();\r\n    for (int i = 0; i < iters; i++) {\r\n        newShard.checkIndex();\r\n    }\r\n    assertTrue(stop.compareAndSet(false, true));\r\n    snapshotter.join();\r\n    closeShards(newShard);\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.ccr.action.bulk.BulkShardOperationsTests.testPrimaryTermFromFollower",
	"Comment": "test that we use the primary term on the follower when applying operations from the leader",
	"Method": "void testPrimaryTermFromFollower(){\r\n    final Settings settings = Settings.builder().put(CcrSettings.CCR_FOLLOWING_INDEX_SETTING.getKey(), true).put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true).build();\r\n    final IndexShard followerPrimary = newStartedShard(true, settings, new FollowingEngineFactory());\r\n    final long primaryTerm = randomLongBetween(1, Integer.MAX_VALUE);\r\n    int numOps = randomIntBetween(0, 127);\r\n    final List<Translog.Operation> operations = new ArrayList(randomIntBetween(0, 127));\r\n    for (int i = 0; i < numOps; i++) {\r\n        final String id = Integer.toString(i);\r\n        final long seqNo = i;\r\n        final Translog.Operation.Type type = randomValueOtherThan(Translog.Operation.Type.CREATE, () -> randomFrom(Translog.Operation.Type.values()));\r\n        switch(type) {\r\n            case INDEX:\r\n                operations.add(new Translog.Index(\"_doc\", id, seqNo, primaryTerm, 0, SOURCE, null, -1));\r\n                break;\r\n            case DELETE:\r\n                operations.add(new Translog.Delete(\"_doc\", id, new Term(\"_id\", Uid.encodeId(id)), seqNo, primaryTerm, 0));\r\n                break;\r\n            case NO_OP:\r\n                operations.add(new Translog.NoOp(seqNo, primaryTerm, \"test\"));\r\n                break;\r\n            default:\r\n                throw new IllegalStateException(\"unexpected operation type [\" + type + \"]\");\r\n        }\r\n    }\r\n    final TransportWriteAction.WritePrimaryResult<BulkShardOperationsRequest, BulkShardOperationsResponse> result = TransportBulkShardOperationsAction.shardOperationOnPrimary(followerPrimary.shardId(), followerPrimary.getHistoryUUID(), operations, numOps - 1, followerPrimary, logger);\r\n    try (Translog.Snapshot snapshot = followerPrimary.getHistoryOperations(\"test\", 0)) {\r\n        assertThat(snapshot.totalOperations(), equalTo(operations.size()));\r\n        Translog.Operation operation;\r\n        while ((operation = snapshot.next()) != null) {\r\n            assertThat(operation.primaryTerm(), equalTo(followerPrimary.getOperationPrimaryTerm()));\r\n        }\r\n    }\r\n    for (final Translog.Operation operation : result.replicaRequest().getOperations()) {\r\n        assertThat(operation.primaryTerm(), equalTo(followerPrimary.getOperationPrimaryTerm()));\r\n    }\r\n    closeShards(followerPrimary);\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.NetworkAddressTests.forgeScoped",
	"Comment": "creates scoped ipv6 address without any lookups. hostname can be null, for missing",
	"Method": "InetAddress forgeScoped(String hostname,String address,int scopeid){\r\n    byte[] bytes = InetAddress.getByName(address).getAddress();\r\n    return Inet6Address.getByAddress(hostname, bytes, scopeid);\r\n}"
}, {
	"Path": "org.elasticsearch.search.rescore.QueryRescorerBuilderTests.testSerialization",
	"Comment": "test serialization and deserialization of the rescore builder",
	"Method": "void testSerialization(){\r\n    for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {\r\n        RescorerBuilder<?> original = randomRescoreBuilder();\r\n        RescorerBuilder<?> deserialized = copy(original);\r\n        assertEquals(deserialized, original);\r\n        assertEquals(deserialized.hashCode(), original.hashCode());\r\n        assertNotSame(deserialized, original);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.completion.FuzzyOptions.getFuzzyMinLength",
	"Comment": "returns the length of input prefix after which edits are applied",
	"Method": "int getFuzzyMinLength(){\r\n    return fuzzyMinLength;\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.RandomDocumentPicks.randomFieldValue",
	"Comment": "generates a random field value, can be a string, a number, a list of an object itself.",
	"Method": "Object randomFieldValue(Random random,Object randomFieldValue,Random random,int currentDepth){\r\n    switch(RandomNumbers.randomIntBetween(random, 0, 9)) {\r\n        case 0:\r\n            return randomString(random);\r\n        case 1:\r\n            return random.nextInt();\r\n        case 2:\r\n            return random.nextBoolean();\r\n        case 3:\r\n            return random.nextDouble();\r\n        case 4:\r\n            List<String> stringList = new ArrayList();\r\n            int numStringItems = RandomNumbers.randomIntBetween(random, 1, 10);\r\n            for (int j = 0; j < numStringItems; j++) {\r\n                stringList.add(randomString(random));\r\n            }\r\n            return stringList;\r\n        case 5:\r\n            List<Integer> intList = new ArrayList();\r\n            int numIntItems = RandomNumbers.randomIntBetween(random, 1, 10);\r\n            for (int j = 0; j < numIntItems; j++) {\r\n                intList.add(random.nextInt());\r\n            }\r\n            return intList;\r\n        case 6:\r\n            List<Boolean> booleanList = new ArrayList();\r\n            int numBooleanItems = RandomNumbers.randomIntBetween(random, 1, 10);\r\n            for (int j = 0; j < numBooleanItems; j++) {\r\n                booleanList.add(random.nextBoolean());\r\n            }\r\n            return booleanList;\r\n        case 7:\r\n            List<Double> doubleList = new ArrayList();\r\n            int numDoubleItems = RandomNumbers.randomIntBetween(random, 1, 10);\r\n            for (int j = 0; j < numDoubleItems; j++) {\r\n                doubleList.add(random.nextDouble());\r\n            }\r\n            return doubleList;\r\n        case 8:\r\n            Map<String, Object> newNode = new HashMap();\r\n            addRandomFields(random, newNode, ++currentDepth);\r\n            return newNode;\r\n        case 9:\r\n            byte[] byteArray = new byte[RandomNumbers.randomIntBetween(random, 1, 1024)];\r\n            random.nextBytes(byteArray);\r\n            return byteArray;\r\n        default:\r\n            throw new UnsupportedOperationException();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.updateMinMasterNodes",
	"Comment": "updates the min master nodes setting in the current running cluster.",
	"Method": "int updateMinMasterNodes(int eligibleMasterNodeCount){\r\n    assert autoManageMinMasterNodes;\r\n    final int minMasterNodes = getMinMasterNodes(eligibleMasterNodeCount);\r\n    if (getMasterNodesCount() > 0) {\r\n        logger.debug(\"updating min_master_nodes to [{}]\", minMasterNodes);\r\n        try {\r\n            assertAcked(client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(DISCOVERY_ZEN_MINIMUM_MASTER_NODES_SETTING.getKey(), minMasterNodes)));\r\n        } catch (Exception e) {\r\n            throw new ElasticsearchException(\"failed to update minimum master node to [{}] (current masters [{}])\", e, minMasterNodes, getMasterNodesCount());\r\n        }\r\n    }\r\n    return minMasterNodes;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESSingleNodeTestCase.getInstanceFromNode",
	"Comment": "get an instance for a particular class using the injector of the singleton node.",
	"Method": "T getInstanceFromNode(Class<T> clazz){\r\n    return NODE.injector().getInstance(clazz);\r\n}"
}, {
	"Path": "org.elasticsearch.indices.memory.breaker.CircuitBreakerServiceIT.clearFieldData",
	"Comment": "issues a cache clear and waits 30 seconds for the field data breaker to be cleared",
	"Method": "void clearFieldData(){\r\n    client().admin().indices().prepareClearCache().setFieldDataCache(true).execute().actionGet();\r\n    assertBusy(() -> {\r\n        NodesStatsResponse resp = client().admin().cluster().prepareNodesStats().clear().setBreaker(true).get(new TimeValue(15, TimeUnit.SECONDS));\r\n        for (NodeStats nStats : resp.getNodes()) {\r\n            assertThat(\"fielddata breaker never reset back to 0\", nStats.getBreaker().getStats(CircuitBreaker.FIELDDATA).getEstimated(), equalTo(0L));\r\n        }\r\n    }, 30, TimeUnit.SECONDS);\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.GeoDistanceSortBuilderTests.testMultiValueMode",
	"Comment": "test that the sort builder mode gets transferred correctly to the sortfield",
	"Method": "void testMultiValueMode(){\r\n    QueryShardContext shardContextMock = createMockShardContext();\r\n    GeoDistanceSortBuilder geoDistanceSortBuilder = new GeoDistanceSortBuilder(\"fieldName\", 1.0, 1.0);\r\n    geoDistanceSortBuilder.sortMode(SortMode.MAX);\r\n    SortField sortField = geoDistanceSortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    XFieldComparatorSource comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertEquals(MultiValueMode.MAX, comparatorSource.sortMode());\r\n    geoDistanceSortBuilder = new GeoDistanceSortBuilder(\"fieldName\", 1.0, 1.0);\r\n    geoDistanceSortBuilder.order(SortOrder.DESC);\r\n    sortField = geoDistanceSortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertEquals(MultiValueMode.MAX, comparatorSource.sortMode());\r\n    geoDistanceSortBuilder = new GeoDistanceSortBuilder(\"fieldName\", 1.0, 1.0);\r\n    geoDistanceSortBuilder.order(SortOrder.ASC).unit(DistanceUnit.INCH);\r\n    sortField = geoDistanceSortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertEquals(MultiValueMode.MIN, comparatorSource.sortMode());\r\n    geoDistanceSortBuilder = new GeoDistanceSortBuilder(\"fieldName\", 1.0, 1.0);\r\n    geoDistanceSortBuilder.sortMode(SortMode.MIN).unit(DistanceUnit.INCH);\r\n    sortField = geoDistanceSortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertEquals(MultiValueMode.MIN, comparatorSource.sortMode());\r\n    geoDistanceSortBuilder.sortMode(SortMode.AVG);\r\n    sortField = geoDistanceSortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertEquals(MultiValueMode.AVG, comparatorSource.sortMode());\r\n    geoDistanceSortBuilder.sortMode(SortMode.MEDIAN);\r\n    sortField = geoDistanceSortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertEquals(MultiValueMode.MEDIAN, comparatorSource.sortMode());\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.MovAvgPipelineAggregationBuilder.minimize",
	"Comment": "gets whether the model should be fit to the data using a costminimizing algorithm.",
	"Method": "MovAvgPipelineAggregationBuilder minimize(boolean minimize,Boolean minimize){\r\n    return minimize;\r\n}"
}, {
	"Path": "org.elasticsearch.VersionTests.testUnreleasedVersion",
	"Comment": "it would never pass qa tests later on, but those come very far in the build and this is quick to check now.",
	"Method": "void testUnreleasedVersion(){\r\n    Version VERSION_5_1_0_UNRELEASED = Version.fromString(\"5.1.0\");\r\n    VersionTests.assertUnknownVersion(VERSION_5_1_0_UNRELEASED);\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocatorTests.testRestoreThrottle",
	"Comment": "tests that when restoring from a snapshot and we find a node with a shard copy and allocationdeciders say throttle, we add it to ignored shards.",
	"Method": "void testRestoreThrottle(){\r\n    RoutingAllocation allocation = getRestoreRoutingAllocation(throttleAllocationDeciders(), \"allocId\");\r\n    testAllocator.addData(node1, \"some allocId\", randomBoolean());\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodesChanged(), equalTo(true));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().isEmpty(), equalTo(false));\r\n    assertClusterHealthStatus(allocation, ClusterHealthStatus.YELLOW);\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.ensureGreen",
	"Comment": "ensures the cluster has a green state via the cluster health api. this method will also wait for relocations.it is useful to ensure that all action on the cluster have finished and all shards that were currently relocatingare now allocated and started.",
	"Method": "ClusterHealthStatus ensureGreen(String indices,ClusterHealthStatus ensureGreen,TimeValue timeout,String indices){\r\n    return ensureColor(ClusterHealthStatus.GREEN, timeout, false, indices);\r\n}"
}, {
	"Path": "org.elasticsearch.transport.RemoteClusterService.initializeRemoteClusters",
	"Comment": "connects to all remote clusters in a blocking fashion. this should be called on node startup to establish an initial connectionto all configured seed nodes.",
	"Method": "void initializeRemoteClusters(){\r\n    final TimeValue timeValue = REMOTE_INITIAL_CONNECTION_TIMEOUT_SETTING.get(settings);\r\n    final PlainActionFuture<Void> future = new PlainActionFuture();\r\n    Map<String, Tuple<String, List<Supplier<DiscoveryNode>>>> seeds = RemoteClusterAware.buildRemoteClustersDynamicConfig(settings);\r\n    updateRemoteClusters(seeds, future);\r\n    try {\r\n        future.get(timeValue.millis(), TimeUnit.MILLISECONDS);\r\n    } catch (InterruptedException e) {\r\n        Thread.currentThread().interrupt();\r\n    } catch (TimeoutException ex) {\r\n        logger.warn(\"failed to connect to remote clusters within {}\", timeValue.toString());\r\n    } catch (Exception e) {\r\n        throw new IllegalStateException(\"failed to connect to remote clusters\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.minScore",
	"Comment": "gets the minimum score below which docs will be filtered out.",
	"Method": "SearchSourceBuilder minScore(float minScore,Float minScore){\r\n    return minScore;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.PublishClusterStateActionTests.testTimeoutOrCommit",
	"Comment": "tests that cluster is committed or times out. it should never be the case that we failan update due to a commit timeout, but it ends up being committed anyway",
	"Method": "void testTimeoutOrCommit(){\r\n    Settings settings = // short but so we will sometime commit sometime timeout\r\n    Settings.builder().put(DiscoverySettings.COMMIT_TIMEOUT_SETTING.getKey(), \"1ms\").build();\r\n    MockNode master = createMockNode(\"master\", settings, null);\r\n    MockNode node = createMockNode(\"node\", settings, null);\r\n    ClusterState state = ClusterState.builder(master.clusterState).nodes(DiscoveryNodes.builder(master.clusterState.nodes()).add(node.discoveryNode).masterNodeId(master.discoveryNode.getId())).build();\r\n    for (int i = 0; i < 10; i++) {\r\n        state = ClusterState.builder(state).incrementVersion().build();\r\n        logger.debug(\"--> publishing version [{}], UUID [{}]\", state.version(), state.stateUUID());\r\n        boolean success;\r\n        try {\r\n            publishState(master.action, state, master.clusterState, 2).await(1, TimeUnit.HOURS);\r\n            success = true;\r\n        } catch (Discovery.FailedToCommitClusterStateException OK) {\r\n            success = false;\r\n        }\r\n        logger.debug(\"--> publishing [{}], verifying...\", success ? \"succeeded\" : \"failed\");\r\n        if (success) {\r\n            assertSameState(node.clusterState, state);\r\n        } else {\r\n            assertThat(node.clusterState.stateUUID(), not(equalTo(state.stateUUID())));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.transport.ConnectionProfile.getCompressionEnabled",
	"Comment": "returns boolean indicating if compression is enabled or null if no explicit compressionis set on this profile.",
	"Method": "Boolean getCompressionEnabled(){\r\n    return compressionEnabled;\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.ESRestTestCase.preserveILMPoliciesUponCompletion",
	"Comment": "returns whether to preserve ilm policies of this test. defaults to notpreserviing them. only runs at all if xpack is installed on the clusterbeing tested.",
	"Method": "boolean preserveILMPoliciesUponCompletion(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.range.AbstractRangeBuilder.processRanges",
	"Comment": "resolve any strings in the ranges so we have a number value for the fromand to of each range. the ranges are also sorted before being returned.",
	"Method": "Range[] processRanges(Function<Range, Range> rangeProcessor){\r\n    Range[] ranges = new Range[this.ranges.size()];\r\n    for (int i = 0; i < ranges.length; i++) {\r\n        ranges[i] = rangeProcessor.apply(this.ranges.get(i));\r\n    }\r\n    sortRanges(ranges);\r\n    return ranges;\r\n}"
}, {
	"Path": "org.elasticsearch.license.XPackLicenseState.update",
	"Comment": "updates the current state of the license, which will change what features are available.",
	"Method": "void update(OperationMode mode,boolean active,Version mostRecentTrialVersion){\r\n    synchronized (this) {\r\n        status = new Status(mode, active);\r\n        if (isSecurityEnabled == true && isSecurityExplicitlyEnabled == false && mode == OperationMode.TRIAL && isSecurityEnabledByTrialVersion == false) {\r\n            if (mostRecentTrialVersion == null || mostRecentTrialVersion.before(Version.V_6_3_0)) {\r\n                LogManager.getLogger(getClass()).info(\"Automatically enabling security for older trial license ({})\", mostRecentTrialVersion == null ? \"[pre 6.1.0]\" : mostRecentTrialVersion.toString());\r\n                isSecurityEnabledByTrialVersion = true;\r\n            }\r\n        }\r\n    }\r\n    listeners.forEach(Runnable::run);\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractQueryTestCase.copyQuery",
	"Comment": "we use the streaming infra to create a copy of the query provided as argument",
	"Method": "QB copyQuery(QB query){\r\n    Reader<QB> reader = (Reader<QB>) namedWriteableRegistry().getReader(QueryBuilder.class, query.getWriteableName());\r\n    return copyWriteable(query, namedWriteableRegistry(), reader);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoDistanceQueryBuilderTests.testToQuery",
	"Comment": "overridden here to ensure the test is only run if at least one type ispresent in the mappings. geo queries do not execute if the field is notexplicitly mapped",
	"Method": "void testToQuery(){\r\n    super.testToQuery();\r\n}"
}, {
	"Path": "org.elasticsearch.test.XContentTestUtils.differenceBetweenMapsIgnoringArrayOrder",
	"Comment": "compares two maps generated from xcontentobjects. the order of elements in arrays is ignored.",
	"Method": "String differenceBetweenMapsIgnoringArrayOrder(Map<String, Object> first,Map<String, Object> second,String differenceBetweenMapsIgnoringArrayOrder,String path,Map<String, Object> first,Map<String, Object> second){\r\n    if (first.size() != second.size()) {\r\n        return path + \": sizes of the maps don't match: \" + first.size() + \" != \" + second.size();\r\n    }\r\n    for (String key : first.keySet()) {\r\n        String reason = differenceBetweenObjectsIgnoringArrayOrder(path + \"/\" + key, first.get(key), second.get(key));\r\n        if (reason != null) {\r\n            return reason;\r\n        }\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.ExtendedStatsBucketPipelineAggregationBuilder.sigma",
	"Comment": "get the value of sigma to use when calculating the standard deviationbounds",
	"Method": "ExtendedStatsBucketPipelineAggregationBuilder sigma(double sigma,double sigma){\r\n    return sigma;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.TopHitsAggregationBuilder.scriptField",
	"Comment": "adds a script field under the given name with the provided script.",
	"Method": "TopHitsAggregationBuilder scriptField(String name,Script script,TopHitsAggregationBuilder scriptField,String name,Script script,boolean ignoreFailure){\r\n    if (name == null) {\r\n        throw new IllegalArgumentException(\"scriptField [name] must not be null: [\" + name + \"]\");\r\n    }\r\n    if (script == null) {\r\n        throw new IllegalArgumentException(\"scriptField [script] must not be null: [\" + name + \"]\");\r\n    }\r\n    if (scriptFields == null) {\r\n        scriptFields = new HashSet();\r\n    }\r\n    scriptFields.add(new ScriptField(name, script, ignoreFailure));\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.storedField",
	"Comment": "adds a stored field to load and return as part of thesearch request. if none are specified, the source of the document will bereturn.",
	"Method": "SearchSourceBuilder storedField(String name){\r\n    return storedFields(Collections.singletonList(name));\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TransportKeepAlive.receiveKeepAlive",
	"Comment": "called when a keep alive ping is received. if the channel that received the keep alive ping is aserver channel, a ping is sent back. if the channel that received the keep alive is a client channel,this method does nothing as the client initiated the ping in the first place.",
	"Method": "void receiveKeepAlive(TcpChannel channel){\r\n    if (channel.isServerChannel()) {\r\n        sendPing(channel);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.replication.IndexLevelReplicationTests.testLateDeliveryAfterGCTriggeredOnReplica",
	"Comment": "this test ensures the consistency between primary and replica with late and out of order delivery on the replica.an index operation on the primary is followed by a delete operation. the delete operation is delivered firstand processed on the replica but the index is delayed with an interval that is even longer the gc deletes cycle.this makes sure that that replica still remembers the delete operation and correctly ignores the stale index operation.",
	"Method": "void testLateDeliveryAfterGCTriggeredOnReplica(){\r\n    ThreadPool.terminate(this.threadPool, 10, TimeUnit.SECONDS);\r\n    this.threadPool = new TestThreadPool(getClass().getName(), Settings.builder().put(threadPoolSettings()).put(ThreadPool.ESTIMATED_TIME_INTERVAL_SETTING.getKey(), 0).build());\r\n    try (ReplicationGroup shards = createGroup(1)) {\r\n        shards.startAll();\r\n        final IndexShard primary = shards.getPrimary();\r\n        final IndexShard replica = shards.getReplicas().get(0);\r\n        final TimeValue gcInterval = TimeValue.timeValueMillis(between(1, 10));\r\n        updateGCDeleteCycle(replica, gcInterval);\r\n        final BulkShardRequest indexRequest = indexOnPrimary(new IndexRequest(index.getName(), \"type\", \"d1\").source(\"{}\", XContentType.JSON), primary);\r\n        final BulkShardRequest deleteRequest = deleteOnPrimary(new DeleteRequest(index.getName(), \"type\", \"d1\"), primary);\r\n        deleteOnReplica(deleteRequest, shards, replica);\r\n        final long deleteTimestamp = threadPool.relativeTimeInMillis();\r\n        replica.refresh(\"test\");\r\n        assertBusy(() -> assertThat(threadPool.relativeTimeInMillis() - deleteTimestamp, greaterThan(gcInterval.millis())));\r\n        getEngine(replica).maybePruneDeletes();\r\n        indexOnReplica(indexRequest, shards, replica);\r\n        shards.assertAllEqual(0);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.replication.ESIndexLevelReplicationTestCase.indexOnPrimary",
	"Comment": "indexes the given requests on the supplied primary, modifying it for replicas",
	"Method": "BulkShardRequest indexOnPrimary(IndexRequest request,IndexShard primary){\r\n    return executeReplicationRequestOnPrimary(primary, request);\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.CancellableTask.cancel",
	"Comment": "this method is called by the task manager when this task is cancelled.",
	"Method": "void cancel(String reason){\r\n    assert reason != null;\r\n    this.reason.compareAndSet(null, reason);\r\n    onCancelled();\r\n}"
}, {
	"Path": "org.elasticsearch.indices.cluster.AbstractIndicesClusterStateServiceTestCase.assertClusterStateMatchesNodeState",
	"Comment": "checks if cluster state matches internal state of indicesclusterstateservice instance",
	"Method": "void assertClusterStateMatchesNodeState(ClusterState state,IndicesClusterStateService indicesClusterStateService){\r\n    MockIndicesService indicesService = (MockIndicesService) indicesClusterStateService.indicesService;\r\n    ConcurrentMap<ShardId, ShardRouting> failedShardsCache = indicesClusterStateService.failedShardsCache;\r\n    RoutingNode localRoutingNode = state.getRoutingNodes().node(state.getNodes().getLocalNodeId());\r\n    if (localRoutingNode != null) {\r\n        if (enableRandomFailures == false) {\r\n            if (failedShardsCache.values().stream().anyMatch(ShardRouting::initializing)) {\r\n                fail(\"failed shard cache should not contain initializing shard routing: \" + failedShardsCache.values());\r\n            }\r\n        }\r\n        for (ShardRouting shardRouting : localRoutingNode) {\r\n            Index index = shardRouting.index();\r\n            IndexMetaData indexMetaData = state.metaData().getIndexSafe(index);\r\n            MockIndexShard shard = indicesService.getShardOrNull(shardRouting.shardId());\r\n            ShardRouting failedShard = failedShardsCache.get(shardRouting.shardId());\r\n            if (state.blocks().disableStatePersistence()) {\r\n                if (shard != null) {\r\n                    fail(\"Shard with id \" + shardRouting + \" should be removed from indicesService due to disabled state persistence\");\r\n                }\r\n            } else {\r\n                if (failedShard != null && failedShard.isSameAllocation(shardRouting) == false) {\r\n                    fail(\"Shard cache has not been properly cleaned for \" + failedShard);\r\n                }\r\n                if (shard == null && failedShard == null) {\r\n                    fail(\"Shard with id \" + shardRouting + \" expected but missing in indicesService and failedShardsCache\");\r\n                }\r\n                if (enableRandomFailures == false) {\r\n                    if (shard == null && shardRouting.initializing() && failedShard == shardRouting) {\r\n                        fail(\"Shard with id \" + shardRouting + \" expected but missing in indicesService \" + failedShard);\r\n                    }\r\n                }\r\n                if (shard != null) {\r\n                    AllocatedIndex<? extends Shard> indexService = indicesService.indexService(index);\r\n                    assertTrue(\"Index \" + index + \" expected but missing in indicesService\", indexService != null);\r\n                    assertThat(indexService.getIndexSettings().getIndexMetaData(), equalTo(indexMetaData));\r\n                    if (enableRandomFailures == false || failedShard == null) {\r\n                        assertTrue(\"Shard with id \" + shardRouting + \" expected but missing in indexService\", shard != null);\r\n                        assertThat(shard.routingEntry(), equalTo(shardRouting));\r\n                    }\r\n                    if (shard.routingEntry().primary() && shard.routingEntry().active()) {\r\n                        IndexShardRoutingTable shardRoutingTable = state.routingTable().shardRoutingTable(shard.shardId());\r\n                        Set<String> inSyncIds = state.metaData().index(shard.shardId().getIndex()).inSyncAllocationIds(shard.shardId().id());\r\n                        assertThat(shard.routingEntry() + \" isn't updated with in-sync aIDs\", shard.inSyncAllocationIds, equalTo(inSyncIds));\r\n                        assertThat(shard.routingEntry() + \" isn't updated with routing table\", shard.routingTable, equalTo(shardRoutingTable));\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    for (AllocatedIndex<? extends Shard> indexService : indicesService) {\r\n        if (state.blocks().disableStatePersistence()) {\r\n            fail(\"Index service \" + indexService.index() + \" should be removed from indicesService due to disabled state persistence\");\r\n        }\r\n        assertTrue(state.metaData().getIndexSafe(indexService.index()) != null);\r\n        boolean shardsFound = false;\r\n        for (Shard shard : indexService) {\r\n            shardsFound = true;\r\n            ShardRouting persistedShardRouting = shard.routingEntry();\r\n            ShardRouting shardRouting = localRoutingNode.getByShardId(persistedShardRouting.shardId());\r\n            if (shardRouting == null) {\r\n                fail(\"Shard with id \" + persistedShardRouting + \" locally exists but missing in routing table\");\r\n            }\r\n            if (shardRouting.equals(persistedShardRouting) == false) {\r\n                fail(\"Local shard \" + persistedShardRouting + \" has stale routing\" + shardRouting);\r\n            }\r\n        }\r\n        if (shardsFound == false) {\r\n            assertFalse(failedShardsCache.keySet().stream().noneMatch(shardId -> shardId.getIndex().equals(indexService.index())));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificantTextAggregationBuilder.includeExclude",
	"Comment": "get terms to include and exclude from the aggregation results",
	"Method": "SignificantTextAggregationBuilder includeExclude(IncludeExclude includeExclude,IncludeExclude includeExclude){\r\n    return includeExclude;\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.separator",
	"Comment": "get the separator that is used to separate terms in the bigram field.",
	"Method": "PhraseSuggestionBuilder separator(String separator,String separator){\r\n    return this.separator;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.monitoring.exporter.MonitoringTemplateUtils.oldTemplateName",
	"Comment": "get a template name for any template id for old templates in the previous version.",
	"Method": "String oldTemplateName(String id){\r\n    return \".monitoring-\" + id + \"-\" + OLD_TEMPLATE_VERSION;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.UnassignedInfoTests.testFailedShard",
	"Comment": "verifies that when a shard fails, reason is properly set and details are preserved.",
	"Method": "void testFailedShard(){\r\n    AllocationService allocation = createAllocationService();\r\n    MetaData metaData = MetaData.builder().put(IndexMetaData.builder(\"test\").settings(settings(Version.CURRENT)).numberOfShards(1).numberOfReplicas(1)).build();\r\n    ClusterState clusterState = ClusterState.builder(ClusterName.CLUSTER_NAME_SETTING.getDefault(Settings.EMPTY)).metaData(metaData).routingTable(RoutingTable.builder().addAsNew(metaData.index(\"test\")).build()).build();\r\n    clusterState = ClusterState.builder(clusterState).nodes(DiscoveryNodes.builder().add(newNode(\"node1\")).add(newNode(\"node2\"))).build();\r\n    clusterState = allocation.reroute(clusterState, \"reroute\");\r\n    clusterState = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));\r\n    clusterState = allocation.applyStartedShards(clusterState, clusterState.getRoutingNodes().shardsWithState(INITIALIZING));\r\n    assertThat(clusterState.getRoutingNodes().unassigned().size() > 0, equalTo(false));\r\n    ShardRouting shardToFail = clusterState.getRoutingNodes().shardsWithState(STARTED).get(0);\r\n    clusterState = allocation.applyFailedShards(clusterState, Collections.singletonList(new FailedShard(shardToFail, \"test fail\", null, randomBoolean())));\r\n    assertThat(clusterState.getRoutingNodes().unassigned().size() > 0, equalTo(true));\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).size(), equalTo(1));\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo(), notNullValue());\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getReason(), equalTo(UnassignedInfo.Reason.ALLOCATION_FAILED));\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getMessage(), equalTo(\"failed shard on node [\" + shardToFail.currentNodeId() + \"]: test fail\"));\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getDetails(), equalTo(\"failed shard on node [\" + shardToFail.currentNodeId() + \"]: test fail\"));\r\n    assertThat(clusterState.getRoutingNodes().shardsWithState(UNASSIGNED).get(0).unassignedInfo().getUnassignedTimeInMillis(), greaterThan(0L));\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.rewrite",
	"Comment": "rewrites this search source builder into its primitive form. e.g. byrewriting the querybuilder. if the builder did not change the identityreference must be returned otherwise the builder will be rewritteninfinitely.",
	"Method": "SearchSourceBuilder rewrite(QueryRewriteContext context){\r\n    assert (this.equals(shallowCopy(queryBuilder, postQueryBuilder, aggregations, sliceBuilder, sorts, rescoreBuilders, highlightBuilder)));\r\n    QueryBuilder queryBuilder = null;\r\n    if (this.queryBuilder != null) {\r\n        queryBuilder = this.queryBuilder.rewrite(context);\r\n    }\r\n    QueryBuilder postQueryBuilder = null;\r\n    if (this.postQueryBuilder != null) {\r\n        postQueryBuilder = this.postQueryBuilder.rewrite(context);\r\n    }\r\n    AggregatorFactories.Builder aggregations = null;\r\n    if (this.aggregations != null) {\r\n        aggregations = this.aggregations.rewrite(context);\r\n    }\r\n    List<SortBuilder<?>> sorts = Rewriteable.rewrite(this.sorts, context);\r\n    List<RescorerBuilder> rescoreBuilders = Rewriteable.rewrite(this.rescoreBuilders, context);\r\n    HighlightBuilder highlightBuilder = this.highlightBuilder;\r\n    if (highlightBuilder != null) {\r\n        highlightBuilder = this.highlightBuilder.rewrite(context);\r\n    }\r\n    boolean rewritten = queryBuilder != this.queryBuilder || postQueryBuilder != this.postQueryBuilder || aggregations != this.aggregations || rescoreBuilders != this.rescoreBuilders || sorts != this.sorts || this.highlightBuilder != highlightBuilder;\r\n    if (rewritten) {\r\n        return shallowCopy(queryBuilder, postQueryBuilder, aggregations, this.sliceBuilder, sorts, rescoreBuilders, highlightBuilder);\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.SettingTests.testValidator",
	"Comment": "the purpose of this test is merely to ensure that a validator is invoked with the appropriate values",
	"Method": "void testValidator(){\r\n    final Settings settings = Settings.builder().put(\"foo.bar\", \"foo.bar value\").put(\"baz.qux\", \"baz.qux value\").put(\"quux.quuz\", \"quux.quuz value\").build();\r\n    FOO_BAR_SETTING.get(settings);\r\n    assertTrue(FooBarValidator.invoked);\r\n}"
}, {
	"Path": "org.elasticsearch.common.unit.DistanceUnitTests.testDistanceUnitNames",
	"Comment": "this test ensures that we are aware of accidental reordering in the distance unit ordinals,since equality in e.g. circleshapebuilder, hashcode and serialization rely on them",
	"Method": "void testDistanceUnitNames(){\r\n    assertEquals(0, DistanceUnit.INCH.ordinal());\r\n    assertEquals(1, DistanceUnit.YARD.ordinal());\r\n    assertEquals(2, DistanceUnit.FEET.ordinal());\r\n    assertEquals(3, DistanceUnit.KILOMETERS.ordinal());\r\n    assertEquals(4, DistanceUnit.NAUTICALMILES.ordinal());\r\n    assertEquals(5, DistanceUnit.MILLIMETERS.ordinal());\r\n    assertEquals(6, DistanceUnit.CENTIMETERS.ordinal());\r\n    assertEquals(7, DistanceUnit.MILES.ordinal());\r\n    assertEquals(8, DistanceUnit.METERS.ordinal());\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.Task.taskInfo",
	"Comment": "build a version of the task status you can throw over the wire and backto the user.",
	"Method": "TaskInfo taskInfo(String localNodeId,boolean detailed,TaskInfo taskInfo,String localNodeId,String description,Status status){\r\n    return new TaskInfo(new TaskId(localNodeId, getId()), getType(), getAction(), description, status, startTime, System.nanoTime() - startTimeNanos, this instanceof CancellableTask, parentTask, headers);\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocatorTests.testRestoreForcesAllocateIfShardAvailable",
	"Comment": "tests that when restoring from a snapshot and we find a node with a shard copy but allocationdeciders say no, we still allocate to that node.",
	"Method": "void testRestoreForcesAllocateIfShardAvailable(){\r\n    RoutingAllocation allocation = getRestoreRoutingAllocation(noAllocationDeciders(), \"allocId\");\r\n    testAllocator.addData(node1, \"some allocId\", randomBoolean());\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodesChanged(), equalTo(true));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().isEmpty(), equalTo(true));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));\r\n    assertClusterHealthStatus(allocation, ClusterHealthStatus.YELLOW);\r\n}"
}, {
	"Path": "org.elasticsearch.test.junit.listeners.LoggingListener.getLoggersAndLevelsFromAnnotation",
	"Comment": "obtain the logging levels from the test logging annotation.",
	"Method": "Map<String, String> getLoggersAndLevelsFromAnnotation(TestLogging testLogging){\r\n    if (testLogging == null) {\r\n        return Collections.emptyMap();\r\n    }\r\n    final Map<String, String> map = new TreeMap();\r\n    final String[] loggersAndLevels = testLogging.value().split(\",\");\r\n    for (final String loggerAndLevel : loggersAndLevels) {\r\n        final String[] loggerAndLevelArray = loggerAndLevel.split(\":\");\r\n        if (loggerAndLevelArray.length == 2) {\r\n            map.put(loggerAndLevelArray[0], loggerAndLevelArray[1]);\r\n        } else {\r\n            throw new IllegalArgumentException(\"invalid test logging annotation [\" + loggerAndLevel + \"]\");\r\n        }\r\n    }\r\n    return map;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.DelayedAllocationIT.testDelayedAllocationChangeWithSettingTo0",
	"Comment": "verify that when explicitly changing the value of the index setting for the delayedallocation to 0, it kicks the allocation of the unassigned shardeven though the node it was hosted on will not come back.",
	"Method": "void testDelayedAllocationChangeWithSettingTo0(){\r\n    internalCluster().startNodes(3);\r\n    prepareCreate(\"test\").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1).put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), TimeValue.timeValueHours(1))).get();\r\n    ensureGreen(\"test\");\r\n    indexRandomData();\r\n    internalCluster().stopRandomNode(InternalTestCluster.nameFilter(findNodeWithShard()));\r\n    assertBusy(() -> assertThat(client().admin().cluster().prepareState().all().get().getState().getRoutingNodes().unassigned().size() > 0, equalTo(true)));\r\n    assertThat(client().admin().cluster().prepareHealth().get().getDelayedUnassignedShards(), equalTo(1));\r\n    assertAcked(client().admin().indices().prepareUpdateSettings(\"test\").setSettings(Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), TimeValue.timeValueMillis(0))).get());\r\n    ensureGreen(\"test\");\r\n    assertThat(client().admin().cluster().prepareHealth().get().getDelayedUnassignedShards(), equalTo(0));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificantTextAggregatorTests.testSignificance",
	"Comment": "uses the significant text aggregation to find the keywords in text fields",
	"Method": "void testSignificance(){\r\n    TextFieldType textFieldType = new TextFieldType();\r\n    textFieldType.setName(\"text\");\r\n    textFieldType.setIndexAnalyzer(new NamedAnalyzer(\"my_analyzer\", AnalyzerScope.GLOBAL, new StandardAnalyzer()));\r\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\r\n    indexWriterConfig.setMaxBufferedDocs(100);\r\n    indexWriterConfig.setRAMBufferSizeMB(100);\r\n    try (Directory dir = newDirectory();\r\n        IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {\r\n        indexDocuments(w, textFieldType);\r\n        SignificantTextAggregationBuilder sigAgg = new SignificantTextAggregationBuilder(\"sig_text\", \"text\").filterDuplicateText(true);\r\n        if (randomBoolean()) {\r\n            sigAgg.sourceFieldNames(Arrays.asList(new String[] { \"json_only_field\" }));\r\n        }\r\n        SamplerAggregationBuilder aggBuilder = new SamplerAggregationBuilder(\"sampler\").subAggregation(sigAgg);\r\n        try (IndexReader reader = DirectoryReader.open(w)) {\r\n            assertEquals(\"test expects a single segment\", 1, reader.leaves().size());\r\n            IndexSearcher searcher = new IndexSearcher(reader);\r\n            Sampler sampler = searchAndReduce(searcher, new TermQuery(new Term(\"text\", \"odd\")), aggBuilder, textFieldType);\r\n            SignificantTerms terms = sampler.getAggregations().get(\"sig_text\");\r\n            assertNull(terms.getBucketByKey(\"even\"));\r\n            assertNull(terms.getBucketByKey(\"duplicate\"));\r\n            assertNull(terms.getBucketByKey(\"common\"));\r\n            assertNotNull(terms.getBucketByKey(\"odd\"));\r\n            sampler = searchAndReduce(searcher, new TermQuery(new Term(\"text\", \"even\")), aggBuilder, textFieldType);\r\n            terms = sampler.getAggregations().get(\"sig_text\");\r\n            assertNull(terms.getBucketByKey(\"odd\"));\r\n            assertNull(terms.getBucketByKey(\"duplicate\"));\r\n            assertNull(terms.getBucketByKey(\"common\"));\r\n            assertNull(terms.getBucketByKey(\"separator2\"));\r\n            assertNull(terms.getBucketByKey(\"separator4\"));\r\n            assertNull(terms.getBucketByKey(\"separator6\"));\r\n            assertNotNull(terms.getBucketByKey(\"even\"));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.NetworkServiceTests.testPublishAnyLocalV4",
	"Comment": "ensure specifying wildcard ipv4 address selects reasonable publish address",
	"Method": "void testPublishAnyLocalV4(){\r\n    NetworkService service = new NetworkService(Collections.emptyList());\r\n    InetAddress address = service.resolvePublishHostAddresses(new String[] { \"0.0.0.0\" });\r\n    assertFalse(address.isAnyLocalAddress());\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.DoubleTermsIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=float\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1.5), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2.5));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(terms(\"terms\").field(\"d\").script(new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, \"_value + 1\", Collections.emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(terms(\"terms\").field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.license.CryptUtils.writeEncryptedPublicKey",
	"Comment": "returns encrypted public key file content with provided passphrase",
	"Method": "byte[] writeEncryptedPublicKey(PublicKey publicKey){\r\n    X509EncodedKeySpec encodedKeySpec = new X509EncodedKeySpec(publicKey.getEncoded());\r\n    return encrypt(encodedKeySpec.getEncoded(), DEFAULT_PASS_PHRASE);\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.TaskManager.getBanCount",
	"Comment": "returns the number of currently banned tasks.will be used in task manager stats and for debugging.",
	"Method": "int getBanCount(){\r\n    return banedParents.size();\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregationBuilder.minDocCount",
	"Comment": "set the minimum document count terms should have in order to appear inthe response.",
	"Method": "SignificantTermsAggregationBuilder minDocCount(long minDocCount){\r\n    if (minDocCount < 0) {\r\n        throw new IllegalArgumentException(\"[minDocCount] must be greater than or equal to 0. Found [\" + minDocCount + \"] in [\" + name + \"]\");\r\n    }\r\n    bucketCountThresholds.setMinDocCount(minDocCount);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.UnassignedInfoTests.testStateTransitionMetaHandling",
	"Comment": "the unassigned meta is kept when a shard goes to initializing, but cleared when it moves to started.",
	"Method": "void testStateTransitionMetaHandling(){\r\n    ShardRouting shard = TestShardRouting.newShardRouting(\"test\", 1, null, null, true, ShardRoutingState.UNASSIGNED, new UnassignedInfo(UnassignedInfo.Reason.INDEX_CREATED, null));\r\n    assertThat(shard.unassignedInfo(), notNullValue());\r\n    shard = shard.initialize(\"test_node\", null, -1);\r\n    assertThat(shard.state(), equalTo(ShardRoutingState.INITIALIZING));\r\n    assertThat(shard.unassignedInfo(), notNullValue());\r\n    shard = shard.moveToStarted();\r\n    assertThat(shard.state(), equalTo(ShardRoutingState.STARTED));\r\n    assertThat(shard.unassignedInfo(), nullValue());\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.NetworkUtilsTests.testAddressInterfaceLookup",
	"Comment": "test that selecting by name is possible and properly matches the addresses on all interfaces and virtualinterfaces.note that to avoid that this test fails when interfaces are down or they do not have addresses assigned to them,they are ignored.",
	"Method": "void testAddressInterfaceLookup(){\r\n    for (NetworkInterface netIf : NetworkUtils.getInterfaces()) {\r\n        if (!netIf.isUp() || Collections.list(netIf.getInetAddresses()).isEmpty()) {\r\n            continue;\r\n        }\r\n        String name = netIf.getName();\r\n        InetAddress[] expectedAddresses = Collections.list(netIf.getInetAddresses()).toArray(new InetAddress[0]);\r\n        InetAddress[] foundAddresses = NetworkUtils.getAddressesForInterface(name);\r\n        assertArrayEquals(expectedAddresses, foundAddresses);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.VersionUtils.allUnreleasedVersions",
	"Comment": "returns an immutable, sorted list containing all unreleased versions.",
	"Method": "List<Version> allUnreleasedVersions(){\r\n    return UNRELEASED_VERSIONS;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.InternalOrder.isKeyDesc",
	"Comment": "determine if the ordering strategy is sorting on bucket key descending.",
	"Method": "boolean isKeyDesc(BucketOrder order){\r\n    return isOrder(order, KEY_DESC);\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocatorTests.testShardLockObtainFailedException",
	"Comment": "tests that when the node returns a shardlockobtainfailedexception, it will be considered as a valid shard copy",
	"Method": "void testShardLockObtainFailedException(){\r\n    final RoutingAllocation allocation = routingAllocationWithOnePrimaryNoReplicas(yesAllocationDeciders(), CLUSTER_RECOVERED, \"allocId1\");\r\n    testAllocator.addData(node1, \"allocId1\", randomBoolean(), new ShardLockObtainFailedException(shardId, \"test\"));\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodesChanged(), equalTo(true));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().isEmpty(), equalTo(true));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo(node1.getId()));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).allocationId().getId(), equalTo(\"allocId1\"));\r\n    assertClusterHealthStatus(allocation, ClusterHealthStatus.YELLOW);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.CanMatchPreFilterSearchPhaseTests.testLotsOfShards",
	"Comment": "in cases that a query coordinating node held all the shards for a query, the can match phase would recurse and end in stack overflowwhen subjected to max concurrent search requests. this test is a test for that situation.",
	"Method": "void testLotsOfShards(){\r\n    final TransportSearchAction.SearchTimeProvider timeProvider = new TransportSearchAction.SearchTimeProvider(0, System.nanoTime(), System::nanoTime);\r\n    final Map<String, Transport.Connection> lookup = new ConcurrentHashMap();\r\n    final DiscoveryNode primaryNode = new DiscoveryNode(\"node_1\", buildNewFakeTransportAddress(), Version.CURRENT);\r\n    final DiscoveryNode replicaNode = new DiscoveryNode(\"node_2\", buildNewFakeTransportAddress(), Version.CURRENT);\r\n    lookup.put(\"node1\", new SearchAsyncActionTests.MockConnection(primaryNode));\r\n    lookup.put(\"node2\", new SearchAsyncActionTests.MockConnection(replicaNode));\r\n    final SearchTransportService searchTransportService = new SearchTransportService(null, null) {\r\n        @Override\r\n        public void sendCanMatch(Transport.Connection connection, ShardSearchTransportRequest request, SearchTask task, ActionListener<SearchService.CanMatchResponse> listener) {\r\n            listener.onResponse(new SearchService.CanMatchResponse(randomBoolean()));\r\n        }\r\n    };\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    final OriginalIndices originalIndices = new OriginalIndices(new String[] { \"idx\" }, SearchRequest.DEFAULT_INDICES_OPTIONS);\r\n    final GroupShardsIterator<SearchShardIterator> shardsIter = SearchAsyncActionTests.getShardsIter(\"idx\", originalIndices, 4096, randomBoolean(), primaryNode, replicaNode);\r\n    final ExecutorService executor = Executors.newFixedThreadPool(randomIntBetween(1, Runtime.getRuntime().availableProcessors()));\r\n    final SearchRequest searchRequest = new SearchRequest();\r\n    searchRequest.allowPartialSearchResults(true);\r\n    final CanMatchPreFilterSearchPhase canMatchPhase = new CanMatchPreFilterSearchPhase(logger, searchTransportService, (clusterAlias, node) -> lookup.get(node), Collections.singletonMap(\"_na_\", new AliasFilter(null, Strings.EMPTY_ARRAY)), Collections.emptyMap(), Collections.emptyMap(), EsExecutors.newDirectExecutorService(), searchRequest, null, shardsIter, timeProvider, 0, null, (iter) -> new InitialSearchPhase<SearchPhaseResult>(\"test\", searchRequest, iter, logger, randomIntBetween(1, 32), executor) {\r\n        @Override\r\n        void onPhaseDone() {\r\n            latch.countDown();\r\n        }\r\n        @Override\r\n        void onShardFailure(final int shardIndex, final SearchShardTarget shardTarget, final Exception ex) {\r\n        }\r\n        @Override\r\n        void onShardSuccess(final SearchPhaseResult result) {\r\n        }\r\n        @Override\r\n        protected void executePhaseOnShard(final SearchShardIterator shardIt, final ShardRouting shard, final SearchActionListener<SearchPhaseResult> listener) {\r\n            if (randomBoolean()) {\r\n                listener.onResponse(new SearchPhaseResult() {\r\n                });\r\n            } else {\r\n                listener.onFailure(new Exception(\"failure\"));\r\n            }\r\n        }\r\n    }, SearchResponse.Clusters.EMPTY);\r\n    canMatchPhase.start();\r\n    latch.await();\r\n    executor.shutdown();\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.CanMatchPreFilterSearchPhaseTests.testLotsOfShards",
	"Comment": "in cases that a query coordinating node held all the shards for a query, the can match phase would recurse and end in stack overflowwhen subjected to max concurrent search requests. this test is a test for that situation.",
	"Method": "void testLotsOfShards(){\r\n    listener.onResponse(new SearchService.CanMatchResponse(randomBoolean()));\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.CanMatchPreFilterSearchPhaseTests.testLotsOfShards",
	"Comment": "in cases that a query coordinating node held all the shards for a query, the can match phase would recurse and end in stack overflowwhen subjected to max concurrent search requests. this test is a test for that situation.",
	"Method": "void testLotsOfShards(){\r\n    latch.countDown();\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.CanMatchPreFilterSearchPhaseTests.testLotsOfShards",
	"Comment": "in cases that a query coordinating node held all the shards for a query, the can match phase would recurse and end in stack overflowwhen subjected to max concurrent search requests. this test is a test for that situation.",
	"Method": "void testLotsOfShards(){\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.CanMatchPreFilterSearchPhaseTests.testLotsOfShards",
	"Comment": "in cases that a query coordinating node held all the shards for a query, the can match phase would recurse and end in stack overflowwhen subjected to max concurrent search requests. this test is a test for that situation.",
	"Method": "void testLotsOfShards(){\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.CanMatchPreFilterSearchPhaseTests.testLotsOfShards",
	"Comment": "in cases that a query coordinating node held all the shards for a query, the can match phase would recurse and end in stack overflowwhen subjected to max concurrent search requests. this test is a test for that situation.",
	"Method": "void testLotsOfShards(){\r\n    if (randomBoolean()) {\r\n        listener.onResponse(new SearchPhaseResult() {\r\n        });\r\n    } else {\r\n        listener.onFailure(new Exception(\"failure\"));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShardTestCase.allowShardFailures",
	"Comment": "by default, tests will fail if any shard created by this class fails. tests that cause failures by designcan call this method to ignore those failures",
	"Method": "void allowShardFailures(){\r\n    failOnShardFailures.set(false);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.EngineTestCase.getTranslog",
	"Comment": "exposes a translog associated with the given engine for testing purpose.",
	"Method": "Translog getTranslog(Engine engine){\r\n    assert engine instanceof InternalEngine : \"only InternalEngines have translogs, got: \" + engine.getClass();\r\n    InternalEngine internalEngine = (InternalEngine) engine;\r\n    return internalEngine.getTranslog();\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.GeoDistanceSortBuilder.getNestedPath",
	"Comment": "returns the nested path if sorting occurs on a field that is inside a nested object. by default when sorting on afield inside a nested object, the nearest upper nested object is selected as nested path.",
	"Method": "String getNestedPath(){\r\n    return this.nestedPath;\r\n}"
}, {
	"Path": "org.elasticsearch.threadpool.Scheduler.preserveContext",
	"Comment": "does nothing by default but can be used by subclasses to save the current thread context and wraps the command in a runnablethat restores that context before running the command.",
	"Method": "Runnable preserveContext(Runnable command){\r\n    return command;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.terms.TermsAggregationBuilder.showTermDocCountError",
	"Comment": "set whether doc count error will be return for individual terms",
	"Method": "boolean showTermDocCountError(TermsAggregationBuilder showTermDocCountError,boolean showTermDocCountError){\r\n    this.showTermDocCountError = showTermDocCountError;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.rest.RestResponse.getHeaders",
	"Comment": "returns custom headers that have been added. this method should not be used to mutate headers.",
	"Method": "Map<String, List<String>> getHeaders(){\r\n    if (customHeaders == null) {\r\n        return Collections.emptyMap();\r\n    } else {\r\n        return customHeaders;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.license.XPackLicenseState.isBeatsAllowed",
	"Comment": "beats is allowed as long as there is an active license of type trial, standard, gold or platinum",
	"Method": "boolean isBeatsAllowed(){\r\n    Status localStatus = status;\r\n    return localStatus.active && (isBasic(localStatus.mode) == false);\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.phrase.LinearInterpolationModelTests.createMutation",
	"Comment": "mutate the given model so the returned smoothing model is different",
	"Method": "LinearInterpolation createMutation(SmoothingModel input){\r\n    LinearInterpolation original = (LinearInterpolation) input;\r\n    switch(randomIntBetween(0, 2)) {\r\n        case 0:\r\n            return new LinearInterpolation(original.getBigramLambda(), original.getTrigramLambda(), original.getUnigramLambda());\r\n        case 1:\r\n            return new LinearInterpolation(original.getTrigramLambda(), original.getUnigramLambda(), original.getBigramLambda());\r\n        case 2:\r\n        default:\r\n            return new LinearInterpolation(original.getUnigramLambda(), original.getBigramLambda(), original.getTrigramLambda());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.SumIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(sum(\"foo\").field(\"d\").script(new Script(ScriptType.INLINE, METRIC_SCRIPT_ENGINE, VALUE_SCRIPT, Collections.emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(sum(\"foo\").field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.assertMappingOnMaster",
	"Comment": "waits for the given mapping type to exists on the master node.",
	"Method": "void assertMappingOnMaster(String index,String type,String fieldNames){\r\n    GetMappingsResponse response = client().admin().indices().prepareGetMappings(index).setTypes(type).get();\r\n    ImmutableOpenMap<String, MappingMetaData> mappings = response.getMappings().get(index);\r\n    assertThat(mappings, notNullValue());\r\n    MappingMetaData mappingMetaData = mappings.get(type);\r\n    assertThat(mappingMetaData, notNullValue());\r\n    Map<String, Object> mappingSource = mappingMetaData.getSourceAsMap();\r\n    assertFalse(mappingSource.isEmpty());\r\n    assertTrue(mappingSource.containsKey(\"properties\"));\r\n    for (String fieldName : fieldNames) {\r\n        Map<String, Object> mappingProperties = (Map<String, Object>) mappingSource.get(\"properties\");\r\n        if (fieldName.indexOf('.') != -1) {\r\n            fieldName = fieldName.replace(\".\", \".properties.\");\r\n        }\r\n        assertThat(\"field \" + fieldName + \" doesn't exists in mapping \" + mappingMetaData.source().string(), XContentMapValues.extractValue(fieldName, mappingProperties), notNullValue());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.startNodes",
	"Comment": "starts multiple nodes with the given settings and returns their names",
	"Method": "List<String> startNodes(int numOfNodes,List<String> startNodes,int numOfNodes,Settings settings,List<String> startNodes,Settings settings){\r\n    final int defaultMinMasterNodes;\r\n    if (autoManageMinMasterNodes) {\r\n        int mastersDelta = (int) Stream.of(settings).filter(Node.NODE_MASTER_SETTING::get).count();\r\n        defaultMinMasterNodes = getMinMasterNodes(getMasterNodesCount() + mastersDelta);\r\n    } else {\r\n        defaultMinMasterNodes = -1;\r\n    }\r\n    final List<NodeAndClient> nodes = new ArrayList();\r\n    for (Settings nodeSettings : settings) {\r\n        nodes.add(buildNode(nodeSettings, defaultMinMasterNodes, () -> rebuildUnicastHostFiles(nodes)));\r\n    }\r\n    startAndPublishNodesAndClients(nodes);\r\n    if (autoManageMinMasterNodes) {\r\n        validateClusterFormed();\r\n    }\r\n    return nodes.stream().map(NodeAndClient::getName).collect(Collectors.toList());\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.rollingRestart",
	"Comment": "restarts all nodes in a rolling restart fashion ie. only restarts on node a time.",
	"Method": "void rollingRestart(RestartCallback callback){\r\n    int numNodesRestarted = 0;\r\n    for (NodeAndClient nodeAndClient : nodes.values()) {\r\n        callback.doAfterNodes(numNodesRestarted++, nodeAndClient.nodeClient());\r\n        restartNode(nodeAndClient, callback);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.script.FieldScript.getLeafLookup",
	"Comment": "the leaf lookup for the lucene segment this script was created for.",
	"Method": "LeafSearchLookup getLeafLookup(){\r\n    return leafLookup;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.filter.FiltersAggregationBuilder.otherBucketKey",
	"Comment": "get the key to use for the bucket for documents not matching anyfilter.",
	"Method": "FiltersAggregationBuilder otherBucketKey(String otherBucketKey,String otherBucketKey){\r\n    return otherBucketKey;\r\n}"
}, {
	"Path": "org.elasticsearch.watcher.FileWatcher.clearState",
	"Comment": "clears any state with the filewatcher, making all files show up as new",
	"Method": "void clearState(){\r\n    rootFileObserver = new FileObserver(file);\r\n    try {\r\n        rootFileObserver.init(false);\r\n    } catch (IOException e) {\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.rescores",
	"Comment": "gets the bytes representing the rescore builders for this request.",
	"Method": "List<RescorerBuilder> rescores(){\r\n    return rescoreBuilders;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ClientHelper.executeWithHeaders",
	"Comment": "execute a client operation and return the response, try to run an actionwith least privileges, when headers exist",
	"Method": "T executeWithHeaders(Map<String, String> headers,String origin,Client client,Supplier<T> supplier){\r\n    Map<String, String> filteredHeaders = headers.entrySet().stream().filter(e -> SECURITY_HEADER_FILTERS.contains(e.getKey())).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\r\n    if (filteredHeaders.isEmpty()) {\r\n        try (ThreadContext.StoredContext ignore = stashWithOrigin(client.threadPool().getThreadContext(), origin)) {\r\n            return supplier.get();\r\n        }\r\n    } else {\r\n        try (ThreadContext.StoredContext ignore = client.threadPool().getThreadContext().stashContext()) {\r\n            client.threadPool().getThreadContext().copyHeaders(filteredHeaders.entrySet());\r\n            return supplier.get();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.support.ValuesSourceConfig.toValuesSource",
	"Comment": "get a value source given its configuration. a return value of null indicates that no value source could be built.",
	"Method": "VS toValuesSource(QueryShardContext context){\r\n    if (!valid()) {\r\n        throw new IllegalStateException(\"value source config is invalid; must have either a field context or a script or marked as unwrapped\");\r\n    }\r\n    final VS vs;\r\n    if (unmapped()) {\r\n        if (missing() == null) {\r\n            vs = null;\r\n        } else if (valueSourceType() == ValuesSourceType.NUMERIC) {\r\n            vs = (VS) ValuesSource.Numeric.EMPTY;\r\n        } else if (valueSourceType() == ValuesSourceType.GEOPOINT) {\r\n            vs = (VS) ValuesSource.GeoPoint.EMPTY;\r\n        } else if (valueSourceType() == ValuesSourceType.ANY || valueSourceType() == ValuesSourceType.BYTES) {\r\n            vs = (VS) ValuesSource.Bytes.WithOrdinals.EMPTY;\r\n        } else {\r\n            throw new IllegalArgumentException(\"Can't deal with unmapped ValuesSource type \" + valueSourceType());\r\n        }\r\n    } else {\r\n        vs = originalValuesSource();\r\n    }\r\n    if (missing() == null) {\r\n        return vs;\r\n    }\r\n    if (vs instanceof ValuesSource.Bytes) {\r\n        final BytesRef missing = format.parseBytesRef(missing().toString());\r\n        if (vs instanceof ValuesSource.Bytes.WithOrdinals) {\r\n            return (VS) MissingValues.replaceMissing((ValuesSource.Bytes.WithOrdinals) vs, missing);\r\n        } else {\r\n            return (VS) MissingValues.replaceMissing((ValuesSource.Bytes) vs, missing);\r\n        }\r\n    } else if (vs instanceof ValuesSource.Numeric) {\r\n        Number missing = format.parseDouble(missing().toString(), false, context::nowInMillis);\r\n        return (VS) MissingValues.replaceMissing((ValuesSource.Numeric) vs, missing);\r\n    } else if (vs instanceof ValuesSource.GeoPoint) {\r\n        final GeoPoint missing = new GeoPoint(missing().toString());\r\n        return (VS) MissingValues.replaceMissing((ValuesSource.GeoPoint) vs, missing);\r\n    } else {\r\n        throw new IllegalArgumentException(\"Can't apply missing values on a \" + vs.getClass());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.ReplicaShardAllocatorTests.testNoMatchingFilesForReplicaOnAnyNode",
	"Comment": "verifies that when there is primary data, but no matching data at all on other nodes, the shard keepsunassigned to be allocated later on.",
	"Method": "void testNoMatchingFilesForReplicaOnAnyNode(){\r\n    RoutingAllocation allocation = onePrimaryOnNode1And1Replica(yesAllocationDeciders());\r\n    testAllocator.addData(node1, \"MATCH\", new StoreFileMetaData(\"file1\", 10, \"MATCH_CHECKSUM\", MIN_SUPPORTED_LUCENE_VERSION)).addData(node2, \"NO_MATCH\", new StoreFileMetaData(\"file1\", 10, \"NO_MATCH_CHECKSUM\", MIN_SUPPORTED_LUCENE_VERSION));\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.UNASSIGNED).get(0).shardId(), equalTo(shardId));\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.waitForDocs",
	"Comment": "waits until at least a give number of document is visible for searchers",
	"Method": "long waitForDocs(long numDocs,long waitForDocs,long numDocs,BackgroundIndexer indexer,long waitForDocs,long numDocs,int maxWaitTime,TimeUnit maxWaitTimeUnit,BackgroundIndexer indexer){\r\n    final AtomicLong lastKnownCount = new AtomicLong(-1);\r\n    long lastStartCount = -1;\r\n    BooleanSupplier testDocs = () -> {\r\n        if (indexer != null) {\r\n            lastKnownCount.set(indexer.totalIndexedDocs());\r\n        }\r\n        if (lastKnownCount.get() >= numDocs) {\r\n            try {\r\n                long count = client().prepareSearch().setSize(0).setQuery(matchAllQuery()).get().getHits().getTotalHits();\r\n                if (count == lastKnownCount.get()) {\r\n                    client().admin().indices().prepareRefresh().get();\r\n                }\r\n                lastKnownCount.set(count);\r\n            } catch (Exception e) {\r\n                logger.debug(\"failed to executed count\", e);\r\n                return false;\r\n            }\r\n            logger.debug(\"[{}] docs visible for search. waiting for [{}]\", lastKnownCount.get(), numDocs);\r\n        } else {\r\n            logger.debug(\"[{}] docs indexed. waiting for [{}]\", lastKnownCount.get(), numDocs);\r\n        }\r\n        return lastKnownCount.get() >= numDocs;\r\n    };\r\n    while (!awaitBusy(testDocs, maxWaitTime, maxWaitTimeUnit)) {\r\n        if (lastStartCount == lastKnownCount.get()) {\r\n            fail(\"failed to reach \" + numDocs + \"docs\");\r\n        }\r\n        lastStartCount = lastKnownCount.get();\r\n    }\r\n    return lastKnownCount.get();\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.ESRestTestCase.restAdminSettings",
	"Comment": "returns the rest client settings used for admin actions like cleaning up after the test has completed.",
	"Method": "Settings restAdminSettings(){\r\n    return restClientSettings();\r\n}"
}, {
	"Path": "org.elasticsearch.search.query.QuerySearchResult.hasAggs",
	"Comment": "returns true if this query result has unconsumed aggregations",
	"Method": "boolean hasAggs(){\r\n    return hasAggs;\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.ESRestTestCase.adminClient",
	"Comment": "get the client used for test administrative actions. do not use this while writing a test. only use it for cleaning up after tests.",
	"Method": "RestClient adminClient(){\r\n    return adminClient;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.TasksIT.findEvents",
	"Comment": "returns all events that satisfy the criteria across all nodes",
	"Method": "List<TaskInfo> findEvents(String actionMasks,Function<Tuple<Boolean, TaskInfo>, Boolean> criteria){\r\n    List<TaskInfo> events = new ArrayList();\r\n    for (Map.Entry<Tuple<String, String>, RecordingTaskManagerListener> entry : listeners.entrySet()) {\r\n        if (actionMasks == null || entry.getKey().v2().equals(actionMasks)) {\r\n            for (Tuple<Boolean, TaskInfo> taskEvent : entry.getValue().getEvents()) {\r\n                if (criteria.apply(taskEvent)) {\r\n                    events.add(taskEvent.v2());\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return events;\r\n}"
}, {
	"Path": "org.elasticsearch.search.fetch.subphase.highlight.AbstractHighlighterBuilder.requireFieldMatch",
	"Comment": "set to true to cause a field to be highlighted only if a query matches that field.default is false meaning that terms are highlighted on all requested fields regardlessif the query matches specifically on them.",
	"Method": "HB requireFieldMatch(Boolean requireFieldMatch,Boolean requireFieldMatch){\r\n    return this.requireFieldMatch;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.TestTaskPlugin.getTransportInterceptors",
	"Comment": "intercept transport requests to verify that all of the ones that shouldhave the origin set do have the origin set and the onesthat should not have the origin set do not have it set.",
	"Method": "List<TransportInterceptor> getTransportInterceptors(NamedWriteableRegistry namedWriteableRegistry,ThreadContext threadContext){\r\n    return Collections.singletonList(new OriginAssertingInterceptor(threadContext));\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.ObjectPath.evaluate",
	"Comment": "returns the object corresponding to the provided path if present, null otherwise",
	"Method": "T evaluate(Object object,String path,T evaluate,String path,T evaluate,String path,Stash stash,Object evaluate,String key,Object object,Stash stash){\r\n    if (stash.containsStashedValue(key)) {\r\n        key = stash.getValue(key).toString();\r\n    }\r\n    if (object instanceof Map) {\r\n        return ((Map<String, Object>) object).get(key);\r\n    }\r\n    if (object instanceof List) {\r\n        List<Object> list = (List<Object>) object;\r\n        try {\r\n            return list.get(Integer.valueOf(key));\r\n        } catch (NumberFormatException e) {\r\n            throw new IllegalArgumentException(\"element was a list, but [\" + key + \"] was not numeric\", e);\r\n        } catch (IndexOutOfBoundsException e) {\r\n            throw new IllegalArgumentException(\"element was a list with \" + list.size() + \" elements, but [\" + key + \"] was out of bounds\", e);\r\n        }\r\n    }\r\n    throw new IllegalArgumentException(\"no object found for [\" + key + \"] within object of class [\" + object.getClass() + \"]\");\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.NetworkServiceTests.testPublishAnyLocalV6",
	"Comment": "ensure specifying wildcard ipv6 address selects reasonable publish address",
	"Method": "void testPublishAnyLocalV6(){\r\n    NetworkService service = new NetworkService(Collections.emptyList());\r\n    InetAddress address = service.resolvePublishHostAddresses(new String[] { \"::\" });\r\n    assertFalse(address.isAnyLocalAddress());\r\n}"
}, {
	"Path": "org.elasticsearch.license.XPackLicenseState.isRollupAllowed",
	"Comment": "rollup is always available as long as there is a valid license",
	"Method": "boolean isRollupAllowed(){\r\n    return status.active;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TcpTransport.sendErrorResponse",
	"Comment": "sends back an error response to the caller via the given channel",
	"Method": "void sendErrorResponse(Version nodeVersion,Set<String> features,TcpChannel channel,Exception error,long requestId,String action){\r\n    try (BytesStreamOutput stream = new BytesStreamOutput()) {\r\n        stream.setVersion(nodeVersion);\r\n        stream.setFeatures(features);\r\n        RemoteTransportException tx = new RemoteTransportException(nodeName, new TransportAddress(channel.getLocalAddress()), action, error);\r\n        threadPool.getThreadContext().writeTo(stream);\r\n        stream.writeException(tx);\r\n        byte status = 0;\r\n        status = TransportStatus.setResponse(status);\r\n        status = TransportStatus.setError(status);\r\n        final BytesReference bytes = stream.bytes();\r\n        final BytesReference header = buildHeader(requestId, status, nodeVersion, bytes.length());\r\n        CompositeBytesReference message = new CompositeBytesReference(header, bytes);\r\n        ReleaseListener releaseListener = new ReleaseListener(null, () -> messageListener.onResponseSent(requestId, action, error));\r\n        internalSendMessage(channel, message, releaseListener);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.transport.RemoteClusterAware.listenForUpdates",
	"Comment": "registers this instance to listen to updates on the cluster settings.",
	"Method": "void listenForUpdates(ClusterSettings clusterSettings){\r\n    clusterSettings.addAffixUpdateConsumer(RemoteClusterAware.REMOTE_CLUSTERS_PROXY, RemoteClusterAware.REMOTE_CLUSTERS_SEEDS, (key, value) -> updateRemoteCluster(key, value.v2(), value.v1()), (namespace, value) -> {\r\n    });\r\n    clusterSettings.addAffixUpdateConsumer(RemoteClusterAware.SEARCH_REMOTE_CLUSTERS_PROXY, RemoteClusterAware.SEARCH_REMOTE_CLUSTERS_SEEDS, (key, value) -> updateRemoteCluster(key, value.v2(), value.v1()), (namespace, value) -> {\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.newTestIndicesModule",
	"Comment": "creates an indicesmodule for testing with the given mappers and metadata mappers.",
	"Method": "IndicesModule newTestIndicesModule(Map<String, Mapper.TypeParser> extraMappers,Map<String, MetadataFieldMapper.TypeParser> extraMetadataMappers){\r\n    return new IndicesModule(Collections.singletonList(new MapperPlugin() {\r\n        @Override\r\n        public Map<String, Mapper.TypeParser> getMappers() {\r\n            return extraMappers;\r\n        }\r\n        @Override\r\n        public Map<String, MetadataFieldMapper.TypeParser> getMetadataMappers() {\r\n            return extraMetadataMappers;\r\n        }\r\n    }));\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.newTestIndicesModule",
	"Comment": "creates an indicesmodule for testing with the given mappers and metadata mappers.",
	"Method": "IndicesModule newTestIndicesModule(Map<String, Mapper.TypeParser> extraMappers,Map<String, MetadataFieldMapper.TypeParser> extraMetadataMappers){\r\n    return extraMappers;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.newTestIndicesModule",
	"Comment": "creates an indicesmodule for testing with the given mappers and metadata mappers.",
	"Method": "IndicesModule newTestIndicesModule(Map<String, Mapper.TypeParser> extraMappers,Map<String, MetadataFieldMapper.TypeParser> extraMetadataMappers){\r\n    return extraMetadataMappers;\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.slice",
	"Comment": "gets the slice used to filter the search hits, the top hits and the aggregations.",
	"Method": "SearchSourceBuilder slice(SliceBuilder builder,SliceBuilder slice){\r\n    return sliceBuilder;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.ConnectionProfile.getHandshakeTimeout",
	"Comment": "returns the handshake timeout or null if no explicit timeout is set on this profile.",
	"Method": "TimeValue getHandshakeTimeout(){\r\n    return handshakeTimeout;\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.IngestDocumentMatcher.assertIngestDocument",
	"Comment": "helper method to assert the equivalence between two ingestdocuments.",
	"Method": "void assertIngestDocument(IngestDocument docA,IngestDocument docB){\r\n    if ((deepEquals(docA.getIngestMetadata(), docB.getIngestMetadata(), true) && deepEquals(docA.getSourceAndMetadata(), docB.getSourceAndMetadata(), false)) == false) {\r\n        throw new AssertionError(\"Expected [\" + docA + \"] but received [\" + docB + \"].\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESSingleNodeTestCase.addMockHttpTransport",
	"Comment": "true if a dummy http transport should be used, or false if the real http transport should be used.",
	"Method": "boolean addMockHttpTransport(){\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.TaskManager.getCancellableTask",
	"Comment": "returns a cancellable task with given id, or null if the task is not found.",
	"Method": "CancellableTask getCancellableTask(long id){\r\n    CancellableTaskHolder holder = cancellableTasks.get(id);\r\n    if (holder != null) {\r\n        return holder.getTask();\r\n    } else {\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.ElasticsearchExceptionTests.testFromXContentWithIgnoredMetadataAndHeaders",
	"Comment": "test that some values like arrays of numbers are ignored when parsing backan exception.",
	"Method": "void testFromXContentWithIgnoredMetadataAndHeaders(){\r\n    final XContent xContent = randomFrom(XContentType.values()).xContent();\r\n    BytesReference originalBytes;\r\n    try (XContentBuilder builder = XContentBuilder.builder(xContent)) {\r\n        builder.startObject().field(\"metadata_int\", 1).array(\"metadata_array_of_ints\", new int[] { 8, 13, 21 }).field(\"reason\", \"Custom reason\").array(\"metadata_array_of_boolean\", new boolean[] { false, false }).startArray(\"metadata_array_of_objects\").startObject().field(\"object_array_one\", \"value_one\").endObject().startObject().field(\"object_array_two\", \"value_two\").endObject().endArray().field(\"type\", \"custom_exception\").field(\"metadata_long\", 1L).array(\"metadata_array_of_longs\", new long[] { 2L, 3L, 5L }).field(\"metadata_other\", \"some metadata\").startObject(\"header\").field(\"header_string\", \"some header\").array(\"header_array_of_strings\", new String[] { \"foo\", \"bar\", \"baz\" }).endObject().startObject(\"metadata_object\").field(\"object_field\", \"value\").endObject().endObject();\r\n        try (XContentBuilder shuffledBuilder = shuffleXContent(builder)) {\r\n            originalBytes = BytesReference.bytes(shuffledBuilder);\r\n        }\r\n    }\r\n    ElasticsearchException parsedException;\r\n    try (XContentParser parser = createParser(xContent, originalBytes)) {\r\n        assertEquals(XContentParser.Token.START_OBJECT, parser.nextToken());\r\n        parsedException = ElasticsearchException.fromXContent(parser);\r\n        assertEquals(XContentParser.Token.END_OBJECT, parser.currentToken());\r\n        assertNull(parser.nextToken());\r\n    }\r\n    assertNotNull(parsedException);\r\n    assertEquals(\"Elasticsearch exception [type=custom_exception, reason=Custom reason]\", parsedException.getMessage());\r\n    assertEquals(2, parsedException.getHeaderKeys().size());\r\n    assertThat(parsedException.getHeader(\"header_string\"), hasItem(\"some header\"));\r\n    assertThat(parsedException.getHeader(\"header_array_of_strings\"), hasItems(\"foo\", \"bar\", \"baz\"));\r\n    assertEquals(1, parsedException.getMetadataKeys().size());\r\n    assertThat(parsedException.getMetadata(\"es.metadata_other\"), hasItem(\"some metadata\"));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.LongTermsIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(terms(\"terms\").field(\"d\").script(new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, \"_value + 1\", Collections.emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(terms(\"terms\").field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.results.ReservedFieldNames.isValidFieldName",
	"Comment": "test if fieldname is one of the reserved names or if it contains dots thenthat the segment before the first dot is not a reserved name. a fieldnamecontaining dots represents nested fields in which case we only care aboutthe top level.",
	"Method": "boolean isValidFieldName(String fieldName){\r\n    String[] segments = DOT_PATTERN.split(fieldName);\r\n    return !RESERVED_FIELD_NAMES.contains(segments[0]);\r\n}"
}, {
	"Path": "org.elasticsearch.rest.action.admin.indices.RestGetFieldMappingAction.isFieldMappingMissingField",
	"Comment": "helper method to find out if the only included fieldmapping metadata is typed null, which meansthat type and index exist, but the field did not",
	"Method": "boolean isFieldMappingMissingField(Map<String, Map<String, Map<String, FieldMappingMetaData>>> mappingsByIndex){\r\n    if (mappingsByIndex.size() != 1) {\r\n        return false;\r\n    }\r\n    for (Map<String, Map<String, FieldMappingMetaData>> value : mappingsByIndex.values()) {\r\n        for (Map<String, FieldMappingMetaData> fieldValue : value.values()) {\r\n            for (Map.Entry<String, FieldMappingMetaData> fieldMappingMetaDataEntry : fieldValue.entrySet()) {\r\n                if (fieldMappingMetaDataEntry.getValue().isNull()) {\r\n                    return true;\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.joda.time.format.StrictISODateTimeFormat.checkNotStrictISO",
	"Comment": "checks that the iso only flag is not set, throwing an exception if it is.",
	"Method": "void checkNotStrictISO(Collection<DateTimeFieldType> fields,boolean strictISO){\r\n    if (strictISO) {\r\n        throw new IllegalArgumentException(\"No valid ISO8601 format for fields: \" + fields);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.randomFrom",
	"Comment": "pick a random object from the given array. the array must not be empty.",
	"Method": "T randomFrom(T array,T randomFrom,Random random,T array,T randomFrom,List<T> list,T randomFrom,Collection<T> collection,T randomFrom,Random random,Collection<T> collection){\r\n    return RandomPicks.randomFrom(random, collection);\r\n}"
}, {
	"Path": "org.elasticsearch.license.XPackLicenseState.isMonitoringClusterAlertsAllowed",
	"Comment": "monitoring cluster alerts requires the equivalent license to use watcher.",
	"Method": "boolean isMonitoringClusterAlertsAllowed(){\r\n    return isWatcherAllowed();\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocatorTests.testPreferAllocatingPreviousPrimary",
	"Comment": "tests that when there was a node that previously had the primary, it will be allocated to that same node again.",
	"Method": "void testPreferAllocatingPreviousPrimary(){\r\n    String primaryAllocId = UUIDs.randomBase64UUID();\r\n    String replicaAllocId = UUIDs.randomBase64UUID();\r\n    RoutingAllocation allocation = routingAllocationWithOnePrimaryNoReplicas(yesAllocationDeciders(), randomFrom(CLUSTER_RECOVERED, INDEX_REOPENED), primaryAllocId, replicaAllocId);\r\n    boolean node1HasPrimaryShard = randomBoolean();\r\n    testAllocator.addData(node1, node1HasPrimaryShard ? primaryAllocId : replicaAllocId, node1HasPrimaryShard);\r\n    testAllocator.addData(node2, node1HasPrimaryShard ? replicaAllocId : primaryAllocId, !node1HasPrimaryShard);\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodesChanged(), equalTo(true));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().isEmpty(), equalTo(true));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));\r\n    DiscoveryNode allocatedNode = node1HasPrimaryShard ? node1 : node2;\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo(allocatedNode.getId()));\r\n    assertClusterHealthStatus(allocation, ClusterHealthStatus.YELLOW);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEventTests.nextState",
	"Comment": "create a modified cluster state from another one, but with some number of indices added and deleted.",
	"Method": "ClusterState nextState(ClusterState previousState,List<TestCustomMetaData> customMetaDataList,ClusterState nextState,ClusterState previousState,boolean changeClusterUUID,List<Index> addedIndices,List<Index> deletedIndices,int numNodesToRemove){\r\n    final ClusterState.Builder builder = ClusterState.builder(previousState);\r\n    builder.stateUUID(UUIDs.randomBase64UUID());\r\n    final MetaData.Builder metaBuilder = MetaData.builder(previousState.metaData());\r\n    if (changeClusterUUID || addedIndices.size() > 0 || deletedIndices.size() > 0) {\r\n        if (changeClusterUUID) {\r\n            metaBuilder.clusterUUID(UUIDs.randomBase64UUID());\r\n        }\r\n        for (Index index : addedIndices) {\r\n            metaBuilder.put(createIndexMetadata(index), true);\r\n        }\r\n        for (Index index : deletedIndices) {\r\n            metaBuilder.remove(index.getName());\r\n            IndexGraveyard.Builder graveyardBuilder = IndexGraveyard.builder(metaBuilder.indexGraveyard());\r\n            graveyardBuilder.addTombstone(index);\r\n            metaBuilder.indexGraveyard(graveyardBuilder.build());\r\n        }\r\n        builder.metaData(metaBuilder);\r\n    }\r\n    if (numNodesToRemove > 0) {\r\n        final int discoveryNodesSize = previousState.getNodes().getSize();\r\n        final DiscoveryNodes.Builder nodesBuilder = DiscoveryNodes.builder(previousState.getNodes());\r\n        for (int i = 0; i < numNodesToRemove && i < discoveryNodesSize; i++) {\r\n            nodesBuilder.remove(NODE_ID_PREFIX + i);\r\n        }\r\n        builder.nodes(nodesBuilder);\r\n    }\r\n    builder.blocks(ClusterBlocks.builder().build());\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.common.RoundingTests.testDST_America_St_Johns",
	"Comment": "test for a time zone whose days overlap because the clocks are set back across midnight at the end of dst.",
	"Method": "void testDST_America_St_Johns(){\r\n    ZoneId tz = ZoneId.of(\"America/St_Johns\");\r\n    Rounding rounding = new Rounding.TimeUnitRounding(Rounding.DateTimeUnit.DAY_OF_MONTH, tz);\r\n    {\r\n        long timeBeforeFirstMidnight = time(\"2006-10-28T23:30:00.000-02:30\");\r\n        long floor = rounding.round(timeBeforeFirstMidnight);\r\n        assertThat(floor, isDate(time(\"2006-10-28T00:00:00.000-02:30\"), tz));\r\n        long ceiling = rounding.nextRoundingValue(timeBeforeFirstMidnight);\r\n        assertThat(ceiling, isDate(time(\"2006-10-29T00:00:00.000-02:30\"), tz));\r\n        assertInterval(floor, timeBeforeFirstMidnight, ceiling, rounding, tz);\r\n    }\r\n    {\r\n        long timeBetweenMidnights = time(\"2006-10-29T00:00:30.000-02:30\");\r\n        long floor = rounding.round(timeBetweenMidnights);\r\n        assertThat(floor, isDate(time(\"2006-10-29T00:00:00.000-02:30\"), tz));\r\n        long ceiling = rounding.nextRoundingValue(timeBetweenMidnights);\r\n        assertThat(ceiling, isDate(time(\"2006-10-30T00:00:00.000-03:30\"), tz));\r\n        assertInterval(floor, timeBetweenMidnights, ceiling, rounding, tz);\r\n    }\r\n    {\r\n        long timeBetweenMidnights = time(\"2006-10-28T23:30:00.000-03:30\");\r\n        long floor = rounding.round(timeBetweenMidnights);\r\n        assertThat(floor, isDate(time(\"2006-10-28T00:00:00.000-02:30\"), tz));\r\n        long ceiling = rounding.nextRoundingValue(timeBetweenMidnights);\r\n        assertThat(ceiling, isDate(time(\"2006-10-29T00:00:00.000-02:30\"), tz));\r\n        assertInterval(floor, timeBetweenMidnights, ceiling, rounding, tz);\r\n    }\r\n    {\r\n        long timeAfterSecondMidnight = time(\"2006-10-29T06:00:00.000-03:30\");\r\n        long floor = rounding.round(timeAfterSecondMidnight);\r\n        assertThat(floor, isDate(time(\"2006-10-29T00:00:00.000-02:30\"), tz));\r\n        long ceiling = rounding.nextRoundingValue(timeAfterSecondMidnight);\r\n        assertThat(ceiling, isDate(time(\"2006-10-30T00:00:00.000-03:30\"), tz));\r\n        assertInterval(floor, timeAfterSecondMidnight, ceiling, rounding, tz);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.rest.RestController.dispatchRequest",
	"Comment": "dispatch the request, if possible, returning true if a response was sent or false otherwise.",
	"Method": "void dispatchRequest(RestRequest request,RestChannel channel,ThreadContext threadContext,boolean dispatchRequest,RestRequest request,RestChannel channel,NodeClient client,Optional<RestHandler> mHandler){\r\n    final int contentLength = request.hasContent() ? request.content().length() : 0;\r\n    RestChannel responseChannel = channel;\r\n    boolean requestHandled;\r\n    if (contentLength > 0 && mHandler.map(h -> hasContentType(request, h) == false).orElse(false)) {\r\n        sendContentTypeErrorMessage(request, channel);\r\n        requestHandled = true;\r\n    } else if (contentLength > 0 && mHandler.map(h -> h.supportsContentStream()).orElse(false) && request.getXContentType() != XContentType.JSON && request.getXContentType() != XContentType.SMILE) {\r\n        channel.sendResponse(BytesRestResponse.createSimpleErrorResponse(channel, RestStatus.NOT_ACCEPTABLE, \"Content-Type [\" + request.getXContentType() + \"] does not support stream parsing. Use JSON or SMILE instead\"));\r\n        requestHandled = true;\r\n    } else if (mHandler.isPresent()) {\r\n        try {\r\n            if (canTripCircuitBreaker(mHandler)) {\r\n                inFlightRequestsBreaker(circuitBreakerService).addEstimateBytesAndMaybeBreak(contentLength, \"<http_request>\");\r\n            } else {\r\n                inFlightRequestsBreaker(circuitBreakerService).addWithoutBreaking(contentLength);\r\n            }\r\n            responseChannel = new ResourceHandlingHttpChannel(channel, circuitBreakerService, contentLength);\r\n            final RestHandler wrappedHandler = mHandler.map(h -> handlerWrapper.apply(h)).get();\r\n            wrappedHandler.handleRequest(request, responseChannel, client);\r\n            requestHandled = true;\r\n        } catch (Exception e) {\r\n            responseChannel.sendResponse(new BytesRestResponse(responseChannel, e));\r\n            requestHandled = true;\r\n        }\r\n    } else {\r\n        final Set<RestRequest.Method> validMethodSet = getValidHandlerMethodSet(request);\r\n        if (validMethodSet.size() > 0 && validMethodSet.contains(request.method()) == false && request.method() != RestRequest.Method.OPTIONS) {\r\n            handleUnsupportedHttpMethod(request, channel, validMethodSet);\r\n            requestHandled = true;\r\n        } else if (validMethodSet.contains(request.method()) == false && (request.method() == RestRequest.Method.OPTIONS)) {\r\n            handleOptionsRequest(request, channel, validMethodSet);\r\n            requestHandled = true;\r\n        } else {\r\n            requestHandled = false;\r\n        }\r\n    }\r\n    return requestHandled;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.RemoteClusterService.isRemoteClusterRegistered",
	"Comment": "returns true iff the given cluster is configured as a remote cluster. otherwise false",
	"Method": "boolean isRemoteClusterRegistered(String clusterName){\r\n    return remoteClusters.containsKey(clusterName);\r\n}"
}, {
	"Path": "org.elasticsearch.test.VersionUtils.allVersions",
	"Comment": "returns an immutable, sorted list containing all versions, both released and unreleased.",
	"Method": "List<Version> allVersions(){\r\n    return ALL_VERSIONS;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.ccr.action.ShardChangesAction.getOperations",
	"Comment": "returns at most the specified maximum number of operations from the specified from sequence number. this method will never returnoperations above the specified global checkpoint.also if the sum of collected operations size is above the specified maximum batch size then this method stops collecting moreoperations and returns what has been collected so far.",
	"Method": "Translog.Operation[] getOperations(Translog.Operation[] getOperations,IndexShard indexShard,long globalCheckpoint,long fromSeqNo,int maxOperationCount,String expectedHistoryUUID,ByteSizeValue maxBatchSize){\r\n    if (indexShard.state() != IndexShardState.STARTED) {\r\n        throw new IndexShardNotStartedException(indexShard.shardId(), indexShard.state());\r\n    }\r\n    final String historyUUID = indexShard.getHistoryUUID();\r\n    if (historyUUID.equals(expectedHistoryUUID) == false) {\r\n        throw new IllegalStateException(\"unexpected history uuid, expected [\" + expectedHistoryUUID + \"], actual [\" + historyUUID + \"]\");\r\n    }\r\n    if (fromSeqNo > globalCheckpoint) {\r\n        throw new IllegalStateException(\"not exposing operations from [\" + fromSeqNo + \"] greater than the global checkpoint [\" + globalCheckpoint + \"]\");\r\n    }\r\n    int seenBytes = 0;\r\n    long toSeqNo = Math.min(globalCheckpoint, (fromSeqNo + maxOperationCount) - 1);\r\n    assert fromSeqNo <= toSeqNo : \"invalid range from_seqno[\" + fromSeqNo + \"] > to_seqno[\" + toSeqNo + \"]\";\r\n    final List<Translog.Operation> operations = new ArrayList();\r\n    try (Translog.Snapshot snapshot = indexShard.newChangesSnapshot(\"ccr\", fromSeqNo, toSeqNo, true)) {\r\n        Translog.Operation op;\r\n        while ((op = snapshot.next()) != null) {\r\n            operations.add(op);\r\n            seenBytes += op.estimateSize();\r\n            if (seenBytes > maxBatchSize.getBytes()) {\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    return operations.toArray(EMPTY_OPERATIONS_ARRAY);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.CardinalityAggregationBuilder.precisionThreshold",
	"Comment": "get the precision threshold. higher values improve accuracy but alsoincrease memory usage. will return null if theprecisionthreshold has not been set yet.",
	"Method": "CardinalityAggregationBuilder precisionThreshold(long precisionThreshold,Long precisionThreshold){\r\n    return precisionThreshold;\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationActionTests.testPrimaryActionRejectsWrongAidOrWrongTerm",
	"Comment": "test that a primary request is rejected if it arrives at a shard with a wrong allocation id or term",
	"Method": "void testPrimaryActionRejectsWrongAidOrWrongTerm(){\r\n    final String index = \"test\";\r\n    final ShardId shardId = new ShardId(index, \"_na_\", 0);\r\n    setState(clusterService, state(index, true, ShardRoutingState.STARTED));\r\n    final ShardRouting primary = clusterService.state().routingTable().shardRoutingTable(shardId).primaryShard();\r\n    final long primaryTerm = clusterService.state().metaData().index(shardId.getIndexName()).primaryTerm(shardId.id());\r\n    PlainActionFuture<TestResponse> listener = new PlainActionFuture();\r\n    final boolean wrongAllocationId = randomBoolean();\r\n    final long requestTerm = wrongAllocationId && randomBoolean() ? primaryTerm : primaryTerm + randomIntBetween(1, 10);\r\n    Request request = new Request(shardId).timeout(\"1ms\");\r\n    action.new PrimaryOperationTransportHandler().messageReceived(new TransportReplicationAction.ConcreteShardRequest(request, wrongAllocationId ? \"_not_a_valid_aid_\" : primary.allocationId().getId(), requestTerm), createTransportChannel(listener), maybeTask());\r\n    try {\r\n        listener.get();\r\n        fail(\"using a wrong aid didn't fail the operation\");\r\n    } catch (ExecutionException execException) {\r\n        Throwable throwable = execException.getCause();\r\n        logger.debug(\"got exception:\", throwable);\r\n        assertTrue(throwable.getClass() + \" is not a retry exception\", action.retryPrimaryException(throwable));\r\n        if (wrongAllocationId) {\r\n            assertThat(throwable.getMessage(), containsString(\"expected allocation id [_not_a_valid_aid_] but found [\" + primary.allocationId().getId() + \"]\"));\r\n        } else {\r\n            assertThat(throwable.getMessage(), containsString(\"expected allocation id [\" + primary.allocationId().getId() + \"] with term [\" + requestTerm + \"] but found [\" + primaryTerm + \"]\"));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.checkStaticState",
	"Comment": "separate method so that this can be checked again after suite scoped cluster is shut down",
	"Method": "void checkStaticState(boolean afterClass){\r\n    if (afterClass) {\r\n        MockPageCacheRecycler.ensureAllPagesAreReleased();\r\n    }\r\n    MockBigArrays.ensureAllArraysAreReleased();\r\n    assertThat(StatusLogger.getLogger().getLevel(), equalTo(Level.WARN));\r\n    synchronized (statusData) {\r\n        try {\r\n            assertThat(statusData.stream().map(status -> status.getMessage().getFormattedMessage()).collect(Collectors.toList()), empty());\r\n        } finally {\r\n            statusData.clear();\r\n        }\r\n    }\r\n    synchronized (nettyLoggedLeaks) {\r\n        try {\r\n            assertThat(nettyLoggedLeaks, empty());\r\n        } finally {\r\n            nettyLoggedLeaks.clear();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.query.QueryCollectorContext.createEarlyTerminationCollectorContext",
	"Comment": "creates collector limiting the collection to the first numhits documents",
	"Method": "QueryCollectorContext createEarlyTerminationCollectorContext(int numHits){\r\n    return new QueryCollectorContext(REASON_SEARCH_TERMINATE_AFTER_COUNT) {\r\n        private EarlyTerminatingCollector collector;\r\n        @Override\r\n        Collector create(Collector in) throws IOException {\r\n            assert collector == null;\r\n            this.collector = new EarlyTerminatingCollector(in, numHits, true);\r\n            return collector;\r\n        }\r\n    };\r\n}"
}, {
	"Path": "org.elasticsearch.search.query.QueryCollectorContext.createEarlyTerminationCollectorContext",
	"Comment": "creates collector limiting the collection to the first numhits documents",
	"Method": "QueryCollectorContext createEarlyTerminationCollectorContext(int numHits){\r\n    assert collector == null;\r\n    this.collector = new EarlyTerminatingCollector(in, numHits, true);\r\n    return collector;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TcpTransport.validateRequest",
	"Comment": "this template method is needed to inject custom error checking logic in tests.",
	"Method": "void validateRequest(StreamInput stream,long requestId,String action){\r\n    final int nextByte = stream.read();\r\n    if (nextByte != -1) {\r\n        throw new IllegalStateException(\"Message not fully read (request) for requestId [\" + requestId + \"], action [\" + action + \"], available [\" + stream.available() + \"]; resetting\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.randomByteArrayOfLength",
	"Comment": "helper method to create a byte array of a given length populated with random byte values",
	"Method": "byte[] randomByteArrayOfLength(int size){\r\n    byte[] bytes = new byte[size];\r\n    for (int i = 0; i < size; i++) {\r\n        bytes[i] = randomByte();\r\n    }\r\n    return bytes;\r\n}"
}, {
	"Path": "org.elasticsearch.search.geo.GeoShapeIntegrationIT.testIndexShapeRouting",
	"Comment": "test that the indexed shape routing can be provided if it is required",
	"Method": "void testIndexShapeRouting(){\r\n    String mapping = \"{\\n\" + \"    \\\"_routing\\\": {\\n\" + \"      \\\"required\\\": true\\n\" + \"    },\\n\" + \"    \\\"properties\\\": {\\n\" + \"      \\\"shape\\\": {\\n\" + \"        \\\"type\\\": \\\"geo_shape\\\"\\n\" + \"      }\\n\" + \"    }\\n\" + \"  }\";\r\n    assertAcked(client().admin().indices().prepareCreate(\"test\").addMapping(\"doc\", mapping, XContentType.JSON).get());\r\n    ensureGreen();\r\n    String source = \"{\\n\" + \"    \\\"shape\\\" : {\\n\" + \"        \\\"type\\\" : \\\"circle\\\",\\n\" + \"        \\\"coordinates\\\" : [-45.0, 45.0],\\n\" + \"        \\\"radius\\\" : \\\"100m\\\"\\n\" + \"    }\\n\" + \"}\";\r\n    indexRandom(true, client().prepareIndex(\"test\", \"doc\", \"0\").setSource(source, XContentType.JSON).setRouting(\"ABC\"));\r\n    SearchResponse searchResponse = client().prepareSearch(\"test\").setQuery(geoShapeQuery(\"shape\", \"0\", \"doc\").indexedShapeIndex(\"test\").indexedShapeRouting(\"ABC\")).get();\r\n    assertThat(searchResponse.getHits().getTotalHits(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.test.NodeConfigurationSource.transportClientPlugins",
	"Comment": "returns plugins that should be loaded in the transport client",
	"Method": "Collection<Class<? extends Plugin>> transportClientPlugins(){\r\n    return Collections.emptyList();\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.ESPolicyTests.testRestrictPrivileges",
	"Comment": "test restricting privileges to no permissions actually works",
	"Method": "void testRestrictPrivileges(){\r\n    assumeTrue(\"test requires security manager\", System.getSecurityManager() != null);\r\n    try {\r\n        System.getProperty(\"user.home\");\r\n    } catch (SecurityException e) {\r\n        fail(\"this test needs to be fixed: user.home not available by policy\");\r\n    }\r\n    PermissionCollection noPermissions = new Permissions();\r\n    AccessControlContext noPermissionsAcc = new AccessControlContext(new ProtectionDomain[] { new ProtectionDomain(null, noPermissions) });\r\n    try {\r\n        AccessController.doPrivileged(new PrivilegedAction<Void>() {\r\n            public Void run() {\r\n                System.getProperty(\"user.home\");\r\n                fail(\"access should have been denied\");\r\n                return null;\r\n            }\r\n        }, noPermissionsAcc);\r\n    } catch (SecurityException expected) {\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.ESPolicyTests.testRestrictPrivileges",
	"Comment": "test restricting privileges to no permissions actually works",
	"Method": "void testRestrictPrivileges(){\r\n    System.getProperty(\"user.home\");\r\n    fail(\"access should have been denied\");\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShardIT.checkAccountingBreaker",
	"Comment": "check that the accounting breaker correctly matches the segments api for memory usage",
	"Method": "void checkAccountingBreaker(){\r\n    CircuitBreakerService breakerService = getInstanceFromNode(CircuitBreakerService.class);\r\n    CircuitBreaker acctBreaker = breakerService.getBreaker(CircuitBreaker.ACCOUNTING);\r\n    long usedMem = acctBreaker.getUsed();\r\n    assertThat(usedMem, greaterThan(0L));\r\n    NodesStatsResponse response = client().admin().cluster().prepareNodesStats().setIndices(true).setBreaker(true).get();\r\n    NodeStats stats = response.getNodes().get(0);\r\n    assertNotNull(stats);\r\n    SegmentsStats segmentsStats = stats.getIndices().getSegments();\r\n    CircuitBreakerStats breakerStats = stats.getBreaker().getStats(CircuitBreaker.ACCOUNTING);\r\n    assertEquals(usedMem, segmentsStats.getMemoryInBytes());\r\n    assertEquals(usedMem, breakerStats.getEstimated());\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.ensureClusterStateConsistency",
	"Comment": "verifies that all nodes that have the same version of the cluster state as master have same cluster state",
	"Method": "void ensureClusterStateConsistency(){\r\n    if (cluster() != null && cluster().size() > 0) {\r\n        final NamedWriteableRegistry namedWriteableRegistry = cluster().getNamedWriteableRegistry();\r\n        final Client masterClient = client();\r\n        ClusterState masterClusterState = masterClient.admin().cluster().prepareState().all().get().getState();\r\n        byte[] masterClusterStateBytes = ClusterState.Builder.toBytes(masterClusterState);\r\n        masterClusterState = ClusterState.Builder.fromBytes(masterClusterStateBytes, null, namedWriteableRegistry);\r\n        Map<String, Object> masterStateMap = convertToMap(masterClusterState);\r\n        int masterClusterStateSize = ClusterState.Builder.toBytes(masterClusterState).length;\r\n        String masterId = masterClusterState.nodes().getMasterNodeId();\r\n        for (Client client : cluster().getClients()) {\r\n            ClusterState localClusterState = client.admin().cluster().prepareState().all().setLocal(true).get().getState();\r\n            byte[] localClusterStateBytes = ClusterState.Builder.toBytes(localClusterState);\r\n            localClusterState = ClusterState.Builder.fromBytes(localClusterStateBytes, null, namedWriteableRegistry);\r\n            final Map<String, Object> localStateMap = convertToMap(localClusterState);\r\n            final int localClusterStateSize = ClusterState.Builder.toBytes(localClusterState).length;\r\n            if (masterClusterState.version() == localClusterState.version() && masterId.equals(localClusterState.nodes().getMasterNodeId())) {\r\n                try {\r\n                    assertEquals(\"cluster state UUID does not match\", masterClusterState.stateUUID(), localClusterState.stateUUID());\r\n                    if (isTransportClient(masterClient) == isTransportClient(client)) {\r\n                        assertEquals(\"cluster state size does not match\", masterClusterStateSize, localClusterStateSize);\r\n                        assertNull(\"cluster state JSON serialization does not match\", differenceBetweenMapsIgnoringArrayOrder(masterStateMap, localStateMap));\r\n                    } else {\r\n                        assertNull(\"cluster state JSON serialization does not match (after removing some customs)\", differenceBetweenMapsIgnoringArrayOrder(convertToMap(removePluginCustoms(masterClusterState)), convertToMap(removePluginCustoms(localClusterState))));\r\n                    }\r\n                } catch (final AssertionError error) {\r\n                    logger.error(\"Cluster state from master:\\n{}\\nLocal cluster state:\\n{}\", masterClusterState.toString(), localClusterState.toString());\r\n                    throw error;\r\n                }\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.rescore.QueryRescorerBuilderTests.testFromXContent",
	"Comment": "creates random rescorer, renders it to xcontent and back to new instance that should be equal to original",
	"Method": "void testFromXContent(){\r\n    for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {\r\n        RescorerBuilder<?> rescoreBuilder = randomRescoreBuilder();\r\n        XContentBuilder builder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));\r\n        if (randomBoolean()) {\r\n            builder.prettyPrint();\r\n        }\r\n        rescoreBuilder.toXContent(builder, ToXContent.EMPTY_PARAMS);\r\n        XContentBuilder shuffled = shuffleXContent(builder);\r\n        try (XContentParser parser = createParser(shuffled)) {\r\n            parser.nextToken();\r\n            RescorerBuilder<?> secondRescoreBuilder = RescorerBuilder.parseFromXContent(parser);\r\n            assertNotSame(rescoreBuilder, secondRescoreBuilder);\r\n            assertEquals(rescoreBuilder, secondRescoreBuilder);\r\n            assertEquals(rescoreBuilder.hashCode(), secondRescoreBuilder.hashCode());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.transport.RemoteClusterService.ensureConnected",
	"Comment": "ensures that the given cluster alias is connected. if the cluster is connected this operationwill invoke the listener immediately.",
	"Method": "void ensureConnected(String clusterAlias,ActionListener<Void> listener){\r\n    getRemoteClusterConnection(clusterAlias).ensureConnected(listener);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.HyperLogLogPlusPlus.encodeHash",
	"Comment": "encode the hash on 32 bits. the encoded hash cannot be equal to 0.",
	"Method": "int encodeHash(long hash,int p){\r\n    final long e = hash >>> (64 - P2);\r\n    final long encoded;\r\n    if ((e & mask(P2 - p)) == 0) {\r\n        final int runLen = 1 + Math.min(Long.numberOfLeadingZeros(hash << P2), 64 - P2);\r\n        encoded = (e << 7) | (runLen << 1) | 1;\r\n    } else {\r\n        encoded = e << 1;\r\n    }\r\n    assert PackedInts.bitsRequired(encoded) <= 32;\r\n    assert encoded != 0;\r\n    return (int) encoded;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShardTestCase.newShard",
	"Comment": "creates a new initializing shard. the shard will will be put in its proper path under thecurrent node id the shard is assigned to.",
	"Method": "IndexShard newShard(boolean primary,IndexShard newShard,boolean primary,Settings settings,IndexShard newShard,boolean primary,Settings settings,EngineFactory engineFactory,IndexShard newShard,ShardRouting shardRouting,IndexingOperationListener listeners,IndexShard newShard,ShardRouting shardRouting,Settings settings,EngineFactory engineFactory,IndexingOperationListener listeners,IndexShard newShard,ShardId shardId,boolean primary,IndexingOperationListener listeners,IndexShard newShard,ShardId shardId,boolean primary,String nodeId,IndexMetaData indexMetaData,IndexSearcherWrapper searcherWrapper,IndexShard newShard,ShardId shardId,boolean primary,String nodeId,IndexMetaData indexMetaData,IndexSearcherWrapper searcherWrapper,Runnable globalCheckpointSyncer,IndexShard newShard,ShardRouting routing,IndexMetaData indexMetaData,EngineFactory engineFactory,IndexingOperationListener listeners,IndexShard newShard,ShardRouting routing,IndexMetaData indexMetaData,IndexSearcherWrapper indexSearcherWrapper,EngineFactory engineFactory,Runnable globalCheckpointSyncer,IndexingOperationListener listeners,IndexShard newShard,ShardRouting routing,ShardPath shardPath,IndexMetaData indexMetaData,CheckedFunction<IndexSettings, Store, IOException> storeProvider,IndexSearcherWrapper indexSearcherWrapper,EngineFactory engineFactory,Runnable globalCheckpointSyncer,IndexEventListener indexEventListener,IndexingOperationListener listeners){\r\n    final Settings nodeSettings = Settings.builder().put(\"node.name\", routing.currentNodeId()).build();\r\n    final IndexSettings indexSettings = new IndexSettings(indexMetaData, nodeSettings);\r\n    final IndexShard indexShard;\r\n    if (storeProvider == null) {\r\n        storeProvider = is -> createStore(is, shardPath);\r\n    }\r\n    final Store store = storeProvider.apply(indexSettings);\r\n    boolean success = false;\r\n    try {\r\n        IndexCache indexCache = new IndexCache(indexSettings, new DisabledQueryCache(indexSettings), null);\r\n        MapperService mapperService = MapperTestUtils.newMapperService(xContentRegistry(), createTempDir(), indexSettings.getSettings(), \"index\");\r\n        mapperService.merge(indexMetaData, MapperService.MergeReason.MAPPING_RECOVERY);\r\n        SimilarityService similarityService = new SimilarityService(indexSettings, null, Collections.emptyMap());\r\n        final Engine.Warmer warmer = searcher -> {\r\n        };\r\n        ClusterSettings clusterSettings = new ClusterSettings(nodeSettings, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS);\r\n        CircuitBreakerService breakerService = new HierarchyCircuitBreakerService(nodeSettings, clusterSettings);\r\n        indexShard = new IndexShard(routing, indexSettings, shardPath, store, () -> null, indexCache, mapperService, similarityService, engineFactory, indexEventListener, indexSearcherWrapper, threadPool, BigArrays.NON_RECYCLING_INSTANCE, warmer, Collections.emptyList(), Arrays.asList(listeners), globalCheckpointSyncer, breakerService);\r\n        indexShard.addShardFailureCallback(DEFAULT_SHARD_FAILURE_HANDLER);\r\n        success = true;\r\n    } finally {\r\n        if (success == false) {\r\n            IOUtils.close(store);\r\n        }\r\n    }\r\n    return indexShard;\r\n}"
}, {
	"Path": "org.elasticsearch.script.AggregationScript.getDoc",
	"Comment": "the doc lookup for the lucene segment this script was created for.",
	"Method": "Map<String, ScriptDocValues<?>> getDoc(){\r\n    return leafLookup.doc();\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.ESRestTestCase.ensureGreen",
	"Comment": "checks that the specific index is green. we force a selection of an index as the tests share a cluster and often leave indicesin an non green state",
	"Method": "void ensureGreen(String index){\r\n    Request request = new Request(\"GET\", \"/_cluster/health/\" + index);\r\n    request.addParameter(\"wait_for_status\", \"green\");\r\n    request.addParameter(\"wait_for_no_relocating_shards\", \"true\");\r\n    request.addParameter(\"timeout\", \"70s\");\r\n    request.addParameter(\"level\", \"shards\");\r\n    client().performRequest(request);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.TDigestPercentileRanksIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(percentileRanks(\"foo\", new double[] { 50.0 }).field(\"d\").script(new Script(ScriptType.INLINE, AggregationTestScriptsPlugin.NAME, \"_value - 1\", emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(percentileRanks(\"foo\", new double[] { 50.0 }).field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.MedianAbsoluteDeviationIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(randomBuilder().field(\"d\").script(new Script(ScriptType.INLINE, AggregationTestScriptsPlugin.NAME, \"_value - 1\", emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(randomBuilder().field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.admin",
	"Comment": "returns a random admin client. this client can either be a node or a transport client pointing to any ofthe nodes in the cluster.",
	"Method": "AdminClient admin(){\r\n    return client().admin();\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogTests.testPendingDelete",
	"Comment": "tests that closing views after the translog is fine and we can reopen the translog",
	"Method": "void testPendingDelete(){\r\n    translog.add(new Translog.Index(\"test\", \"1\", 0, primaryTerm.get(), new byte[] { 1 }));\r\n    translog.rollGeneration();\r\n    TranslogConfig config = translog.getConfig();\r\n    final String translogUUID = translog.getTranslogUUID();\r\n    final TranslogDeletionPolicy deletionPolicy = createTranslogDeletionPolicy(config.getIndexSettings());\r\n    translog.close();\r\n    translog = new Translog(config, translogUUID, deletionPolicy, () -> SequenceNumbers.NO_OPS_PERFORMED, primaryTerm::get);\r\n    translog.add(new Translog.Index(\"test\", \"2\", 1, primaryTerm.get(), new byte[] { 2 }));\r\n    translog.rollGeneration();\r\n    Closeable lock = translog.acquireRetentionLock();\r\n    translog.add(new Translog.Index(\"test\", \"3\", 2, primaryTerm.get(), new byte[] { 3 }));\r\n    translog.close();\r\n    IOUtils.close(lock);\r\n    translog = new Translog(config, translogUUID, deletionPolicy, () -> SequenceNumbers.NO_OPS_PERFORMED, primaryTerm::get);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngineTests.testConcurrentEngineClosed",
	"Comment": "tests that when the close method returns the engine is actually guaranteed to have cleaned up and that resources are closed",
	"Method": "void testConcurrentEngineClosed(){\r\n    Thread[] closingThreads = new Thread[3];\r\n    CyclicBarrier barrier = new CyclicBarrier(1 + closingThreads.length + 1);\r\n    Thread failEngine = new Thread(new AbstractRunnable() {\r\n        @Override\r\n        public void onFailure(Exception e) {\r\n            throw new AssertionError(e);\r\n        }\r\n        @Override\r\n        protected void doRun() throws Exception {\r\n            barrier.await();\r\n            engine.failEngine(\"test\", new RuntimeException(\"test\"));\r\n        }\r\n    });\r\n    failEngine.start();\r\n    for (int i = 0; i < closingThreads.length; i++) {\r\n        boolean flushAndClose = randomBoolean();\r\n        closingThreads[i] = new Thread(new AbstractRunnable() {\r\n            @Override\r\n            public void onFailure(Exception e) {\r\n                throw new AssertionError(e);\r\n            }\r\n            @Override\r\n            protected void doRun() throws Exception {\r\n                barrier.await();\r\n                if (flushAndClose) {\r\n                    engine.flushAndClose();\r\n                } else {\r\n                    engine.close();\r\n                }\r\n                synchronized (closingThreads) {\r\n                    try (Lock ignored = store.directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {\r\n                    }\r\n                }\r\n            }\r\n        });\r\n        closingThreads[i].setName(\"closingThread_\" + i);\r\n        closingThreads[i].start();\r\n    }\r\n    barrier.await();\r\n    failEngine.join();\r\n    for (Thread t : closingThreads) {\r\n        t.join();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngineTests.testConcurrentEngineClosed",
	"Comment": "tests that when the close method returns the engine is actually guaranteed to have cleaned up and that resources are closed",
	"Method": "void testConcurrentEngineClosed(){\r\n    throw new AssertionError(e);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngineTests.testConcurrentEngineClosed",
	"Comment": "tests that when the close method returns the engine is actually guaranteed to have cleaned up and that resources are closed",
	"Method": "void testConcurrentEngineClosed(){\r\n    barrier.await();\r\n    engine.failEngine(\"test\", new RuntimeException(\"test\"));\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngineTests.testConcurrentEngineClosed",
	"Comment": "tests that when the close method returns the engine is actually guaranteed to have cleaned up and that resources are closed",
	"Method": "void testConcurrentEngineClosed(){\r\n    throw new AssertionError(e);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngineTests.testConcurrentEngineClosed",
	"Comment": "tests that when the close method returns the engine is actually guaranteed to have cleaned up and that resources are closed",
	"Method": "void testConcurrentEngineClosed(){\r\n    barrier.await();\r\n    if (flushAndClose) {\r\n        engine.flushAndClose();\r\n    } else {\r\n        engine.close();\r\n    }\r\n    synchronized (closingThreads) {\r\n        try (Lock ignored = store.directory().obtainLock(IndexWriter.WRITE_LOCK_NAME)) {\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.ESRestTestCase.preserveClusterSettings",
	"Comment": "controls whether or not to preserve cluster settings upon completion of the test. the default implementation is to remove all clustersettings.",
	"Method": "boolean preserveClusterSettings(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogTests.testWithRandomException",
	"Comment": "this test adds operations to the translog which might randomly throw an ioexception. the only thing this test verifies isthat we can, after we hit an exception, open and recover the translog successfully and retrieve all successfully synced operationsfrom the transaction log.",
	"Method": "void testWithRandomException(){\r\n    final int runs = randomIntBetween(5, 10);\r\n    for (int run = 0; run < runs; run++) {\r\n        Path tempDir = createTempDir();\r\n        final FailSwitch fail = new FailSwitch();\r\n        fail.failRandomly();\r\n        TranslogConfig config = getTranslogConfig(tempDir);\r\n        final int numOps = randomIntBetween(100, 200);\r\n        long minGenForRecovery = 1;\r\n        List<String> syncedDocs = new ArrayList();\r\n        List<String> unsynced = new ArrayList();\r\n        if (randomBoolean()) {\r\n            fail.onceFailedFailAlways();\r\n        }\r\n        String generationUUID = null;\r\n        try {\r\n            boolean committing = false;\r\n            final Translog failableTLog = getFailableTranslog(fail, config, randomBoolean(), false, generationUUID, createTranslogDeletionPolicy());\r\n            try {\r\n                LineFileDocs lineFileDocs = new LineFileDocs(random());\r\n                for (int opsAdded = 0; opsAdded < numOps; opsAdded++) {\r\n                    String doc = lineFileDocs.nextDoc().toString();\r\n                    failableTLog.add(new Translog.Index(\"test\", \"\" + opsAdded, opsAdded, primaryTerm.get(), doc.getBytes(Charset.forName(\"UTF-8\"))));\r\n                    unsynced.add(doc);\r\n                    if (randomBoolean()) {\r\n                        failableTLog.sync();\r\n                        syncedDocs.addAll(unsynced);\r\n                        unsynced.clear();\r\n                    }\r\n                    if (randomFloat() < 0.1) {\r\n                        failableTLog.sync();\r\n                        syncedDocs.addAll(unsynced);\r\n                        unsynced.clear();\r\n                        failableTLog.rollGeneration();\r\n                        committing = true;\r\n                        failableTLog.getDeletionPolicy().setTranslogGenerationOfLastCommit(failableTLog.currentFileGeneration());\r\n                        failableTLog.getDeletionPolicy().setMinTranslogGenerationForRecovery(failableTLog.currentFileGeneration());\r\n                        failableTLog.trimUnreferencedReaders();\r\n                        committing = false;\r\n                        syncedDocs.clear();\r\n                    }\r\n                }\r\n                failableTLog.close();\r\n                syncedDocs.addAll(unsynced);\r\n                unsynced.clear();\r\n            } catch (TranslogException | MockDirectoryWrapper.FakeIOException ex) {\r\n                assertEquals(failableTLog.getTragicException(), ex);\r\n            } catch (IOException ex) {\r\n                assertEquals(ex.getMessage(), \"__FAKE__ no space left on device\");\r\n                assertEquals(failableTLog.getTragicException(), ex);\r\n            } catch (RuntimeException ex) {\r\n                assertEquals(ex.getMessage(), \"simulated\");\r\n                assertEquals(failableTLog.getTragicException(), ex);\r\n            } finally {\r\n                Checkpoint checkpoint = Translog.readCheckpoint(config.getTranslogPath());\r\n                if (checkpoint.numOps == unsynced.size() + syncedDocs.size()) {\r\n                    syncedDocs.addAll(unsynced);\r\n                    unsynced.clear();\r\n                }\r\n                if (committing && checkpoint.minTranslogGeneration == checkpoint.generation) {\r\n                    syncedDocs.clear();\r\n                    assertThat(unsynced, empty());\r\n                }\r\n                generationUUID = failableTLog.getTranslogUUID();\r\n                minGenForRecovery = failableTLog.getDeletionPolicy().getMinTranslogGenerationForRecovery();\r\n                IOUtils.closeWhileHandlingException(failableTLog);\r\n            }\r\n        } catch (TranslogException | MockDirectoryWrapper.FakeIOException ex) {\r\n        } catch (IOException ex) {\r\n            assertEquals(ex.getMessage(), \"__FAKE__ no space left on device\");\r\n        }\r\n        if (randomBoolean()) {\r\n            try {\r\n                TranslogDeletionPolicy deletionPolicy = createTranslogDeletionPolicy();\r\n                deletionPolicy.setTranslogGenerationOfLastCommit(minGenForRecovery);\r\n                deletionPolicy.setMinTranslogGenerationForRecovery(minGenForRecovery);\r\n                IOUtils.close(getFailableTranslog(fail, config, randomBoolean(), false, generationUUID, deletionPolicy));\r\n            } catch (TranslogException | MockDirectoryWrapper.FakeIOException ex) {\r\n            } catch (IOException ex) {\r\n                assertEquals(ex.getMessage(), \"__FAKE__ no space left on device\");\r\n            }\r\n        }\r\n        fail.failNever();\r\n        TranslogDeletionPolicy deletionPolicy = createTranslogDeletionPolicy();\r\n        deletionPolicy.setTranslogGenerationOfLastCommit(minGenForRecovery);\r\n        deletionPolicy.setMinTranslogGenerationForRecovery(minGenForRecovery);\r\n        if (generationUUID == null) {\r\n            generationUUID = Translog.createEmptyTranslog(config.getTranslogPath(), SequenceNumbers.NO_OPS_PERFORMED, shardId, primaryTerm.get());\r\n        }\r\n        try (Translog translog = new Translog(config, generationUUID, deletionPolicy, () -> SequenceNumbers.NO_OPS_PERFORMED, primaryTerm::get);\r\n            Translog.Snapshot snapshot = translog.newSnapshotFromGen(new Translog.TranslogGeneration(generationUUID, minGenForRecovery), Long.MAX_VALUE)) {\r\n            assertEquals(syncedDocs.size(), snapshot.totalOperations());\r\n            for (int i = 0; i < syncedDocs.size(); i++) {\r\n                Translog.Operation next = snapshot.next();\r\n                assertEquals(syncedDocs.get(i), next.getSource().source.utf8ToString());\r\n                assertNotNull(\"operation \" + i + \" must be non-null\", next);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.BestBucketsDeferringCollector.prepareSelectedBuckets",
	"Comment": "replay the wrapped collector, but only on a selection of buckets.",
	"Method": "void prepareSelectedBuckets(long selectedBuckets){\r\n    if (!finished) {\r\n        throw new IllegalStateException(\"Cannot replay yet, collection is not finished: postCollect() has not been called\");\r\n    }\r\n    if (this.selectedBuckets != null) {\r\n        throw new IllegalStateException(\"Already been replayed\");\r\n    }\r\n    final LongHash hash = new LongHash(selectedBuckets.length, BigArrays.NON_RECYCLING_INSTANCE);\r\n    for (long bucket : selectedBuckets) {\r\n        hash.add(bucket);\r\n    }\r\n    this.selectedBuckets = hash;\r\n    boolean needsScores = scoreMode().needsScores();\r\n    Weight weight = null;\r\n    if (needsScores) {\r\n        Query query = isGlobal ? new MatchAllDocsQuery() : searchContext.query();\r\n        weight = searchContext.searcher().createWeight(searchContext.searcher().rewrite(query), ScoreMode.COMPLETE, 1f);\r\n    }\r\n    for (Entry entry : entries) {\r\n        final LeafBucketCollector leafCollector = collector.getLeafCollector(entry.context);\r\n        DocIdSetIterator docIt = null;\r\n        if (needsScores && entry.docDeltas.size() > 0) {\r\n            Scorer scorer = weight.scorer(entry.context);\r\n            docIt = scorer.iterator();\r\n            leafCollector.setScorer(scorer);\r\n        }\r\n        final PackedLongValues.Iterator docDeltaIterator = entry.docDeltas.iterator();\r\n        final PackedLongValues.Iterator buckets = entry.buckets.iterator();\r\n        int doc = 0;\r\n        for (long i = 0, end = entry.docDeltas.size(); i < end; ++i) {\r\n            doc += docDeltaIterator.next();\r\n            final long bucket = buckets.next();\r\n            final long rebasedBucket = hash.find(bucket);\r\n            if (rebasedBucket != -1) {\r\n                if (needsScores) {\r\n                    if (docIt.docID() < doc) {\r\n                        docIt.advance(doc);\r\n                    }\r\n                    assert docIt.docID() == doc;\r\n                }\r\n                leafCollector.collect(doc, rebasedBucket);\r\n            }\r\n        }\r\n    }\r\n    collector.postCollection();\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.AggregatorFactories.parseAggregators",
	"Comment": "parses the aggregation request recursively generating aggregatorfactories in turn.",
	"Method": "AggregatorFactories.Builder parseAggregators(XContentParser parser,AggregatorFactories.Builder parseAggregators,XContentParser parser,int level){\r\n    Matcher validAggMatcher = VALID_AGG_NAME.matcher(\"\");\r\n    AggregatorFactories.Builder factories = new AggregatorFactories.Builder();\r\n    XContentParser.Token token = null;\r\n    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\r\n        if (token != XContentParser.Token.FIELD_NAME) {\r\n            throw new ParsingException(parser.getTokenLocation(), \"Unexpected token \" + token + \" in [aggs]: aggregations definitions must start with the name of the aggregation.\");\r\n        }\r\n        final String aggregationName = parser.currentName();\r\n        if (!validAggMatcher.reset(aggregationName).matches()) {\r\n            throw new ParsingException(parser.getTokenLocation(), \"Invalid aggregation name [\" + aggregationName + \"]. Aggregation names must be alpha-numeric and can only contain '_' and '-'\");\r\n        }\r\n        token = parser.nextToken();\r\n        if (token != XContentParser.Token.START_OBJECT) {\r\n            throw new ParsingException(parser.getTokenLocation(), \"Aggregation definition for [\" + aggregationName + \" starts with a [\" + token + \"], expected a [\" + XContentParser.Token.START_OBJECT + \"].\");\r\n        }\r\n        BaseAggregationBuilder aggBuilder = null;\r\n        AggregatorFactories.Builder subFactories = null;\r\n        Map<String, Object> metaData = null;\r\n        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\r\n            if (token != XContentParser.Token.FIELD_NAME) {\r\n                throw new ParsingException(parser.getTokenLocation(), \"Expected [\" + XContentParser.Token.FIELD_NAME + \"] under a [\" + XContentParser.Token.START_OBJECT + \"], but got a [\" + token + \"] in [\" + aggregationName + \"]\", parser.getTokenLocation());\r\n            }\r\n            final String fieldName = parser.currentName();\r\n            token = parser.nextToken();\r\n            if (token == XContentParser.Token.START_OBJECT) {\r\n                switch(fieldName) {\r\n                    case \"meta\":\r\n                        metaData = parser.map();\r\n                        break;\r\n                    case \"aggregations\":\r\n                    case \"aggs\":\r\n                        if (subFactories != null) {\r\n                            throw new ParsingException(parser.getTokenLocation(), \"Found two sub aggregation definitions under [\" + aggregationName + \"]\");\r\n                        }\r\n                        subFactories = parseAggregators(parser, level + 1);\r\n                        break;\r\n                    default:\r\n                        if (aggBuilder != null) {\r\n                            throw new ParsingException(parser.getTokenLocation(), \"Found two aggregation type definitions in [\" + aggregationName + \"]: [\" + aggBuilder.getType() + \"] and [\" + fieldName + \"]\");\r\n                        }\r\n                        aggBuilder = parser.namedObject(BaseAggregationBuilder.class, fieldName, new AggParseContext(aggregationName));\r\n                }\r\n            } else {\r\n                throw new ParsingException(parser.getTokenLocation(), \"Expected [\" + XContentParser.Token.START_OBJECT + \"] under [\" + fieldName + \"], but got a [\" + token + \"] in [\" + aggregationName + \"]\");\r\n            }\r\n        }\r\n        if (aggBuilder == null) {\r\n            throw new ParsingException(parser.getTokenLocation(), \"Missing definition for aggregation [\" + aggregationName + \"]\", parser.getTokenLocation());\r\n        } else {\r\n            if (metaData != null) {\r\n                aggBuilder.setMetaData(metaData);\r\n            }\r\n            if (subFactories != null) {\r\n                aggBuilder.subAggregations(subFactories);\r\n            }\r\n            if (aggBuilder instanceof AggregationBuilder) {\r\n                factories.addAggregator((AggregationBuilder) aggBuilder);\r\n            } else {\r\n                factories.addPipelineAggregator((PipelineAggregationBuilder) aggBuilder);\r\n            }\r\n        }\r\n    }\r\n    return factories;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.ScriptSortBuilderTests.testMultiValueMode",
	"Comment": "test that the sort builder mode gets transferred correctly to the sortfield",
	"Method": "void testMultiValueMode(){\r\n    QueryShardContext shardContextMock = createMockShardContext();\r\n    for (SortMode mode : SortMode.values()) {\r\n        ScriptSortBuilder sortBuilder = new ScriptSortBuilder(mockScript(MOCK_SCRIPT_NAME), ScriptSortType.NUMBER);\r\n        sortBuilder.sortMode(mode);\r\n        SortField sortField = sortBuilder.build(shardContextMock).field;\r\n        assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n        XFieldComparatorSource comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n        assertEquals(MultiValueMode.fromString(mode.toString()), comparatorSource.sortMode());\r\n    }\r\n    ScriptSortBuilder sortBuilder = new ScriptSortBuilder(mockScript(MOCK_SCRIPT_NAME), ScriptSortType.NUMBER);\r\n    sortBuilder.order(SortOrder.ASC);\r\n    SortField sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    XFieldComparatorSource comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertEquals(MultiValueMode.MIN, comparatorSource.sortMode());\r\n    sortBuilder = new ScriptSortBuilder(mockScript(MOCK_SCRIPT_NAME), ScriptSortType.NUMBER);\r\n    sortBuilder.order(SortOrder.DESC);\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertEquals(MultiValueMode.MAX, comparatorSource.sortMode());\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.TranslogHandler.getRecoveredTypes",
	"Comment": "returns the recovered types modifying the mapping during the recovery",
	"Method": "Map<String, Mapping> getRecoveredTypes(){\r\n    return recoveredTypes;\r\n}"
}, {
	"Path": "org.elasticsearch.test.VersionUtils.allReleasedVersions",
	"Comment": "returns an immutable, sorted list containing all released versions.",
	"Method": "List<Version> allReleasedVersions(){\r\n    return RELEASED_VERSIONS;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.recovery.IndexRecoveryIT.testDisconnectsDuringRecovery",
	"Comment": "tests scenario where recovery target successfully sends recovery request to source but then the channel gets closed whilethe source is working on the recovery process.",
	"Method": "void testDisconnectsDuringRecovery(){\r\n    boolean primaryRelocation = randomBoolean();\r\n    final String indexName = \"test\";\r\n    final Settings nodeSettings = Settings.builder().put(RecoverySettings.INDICES_RECOVERY_RETRY_DELAY_NETWORK_SETTING.getKey(), TimeValue.timeValueMillis(randomIntBetween(0, 100))).build();\r\n    TimeValue disconnectAfterDelay = TimeValue.timeValueMillis(randomIntBetween(0, 100));\r\n    String masterNodeName = internalCluster().startMasterOnlyNode(nodeSettings);\r\n    final String blueNodeName = internalCluster().startNode(Settings.builder().put(\"node.attr.color\", \"blue\").put(nodeSettings).build());\r\n    final String redNodeName = internalCluster().startNode(Settings.builder().put(\"node.attr.color\", \"red\").put(nodeSettings).build());\r\n    client().admin().indices().prepareCreate(indexName).setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_INCLUDE_GROUP_SETTING.getKey() + \"color\", \"blue\").put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 0)).get();\r\n    List<IndexRequestBuilder> requests = new ArrayList();\r\n    int numDocs = scaledRandomIntBetween(25, 250);\r\n    for (int i = 0; i < numDocs; i++) {\r\n        requests.add(client().prepareIndex(indexName, \"type\").setSource(\"{}\", XContentType.JSON));\r\n    }\r\n    indexRandom(true, requests);\r\n    ensureSearchable(indexName);\r\n    assertHitCount(client().prepareSearch(indexName).get(), numDocs);\r\n    MockTransportService masterTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, masterNodeName);\r\n    MockTransportService blueMockTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, blueNodeName);\r\n    MockTransportService redMockTransportService = (MockTransportService) internalCluster().getInstance(TransportService.class, redNodeName);\r\n    redMockTransportService.addSendBehavior(blueMockTransportService, new StubbableTransport.SendRequestBehavior() {\r\n        private final AtomicInteger count = new AtomicInteger();\r\n        @Override\r\n        public void sendRequest(Transport.Connection connection, long requestId, String action, TransportRequest request, TransportRequestOptions options) throws IOException {\r\n            logger.info(\"--> sending request {} on {}\", action, connection.getNode());\r\n            if (PeerRecoverySourceService.Actions.START_RECOVERY.equals(action) && count.incrementAndGet() == 1) {\r\n                try {\r\n                    awaitBusy(() -> client(blueNodeName).admin().cluster().prepareState().setLocal(true).get().getState().getRoutingTable().index(\"test\").shard(0).getAllInitializingShards().isEmpty() == false);\r\n                } catch (InterruptedException e) {\r\n                    throw new RuntimeException(e);\r\n                }\r\n                connection.sendRequest(requestId, action, request, options);\r\n                try {\r\n                    Thread.sleep(disconnectAfterDelay.millis());\r\n                } catch (InterruptedException e) {\r\n                    throw new RuntimeException(e);\r\n                }\r\n                throw new ConnectTransportException(connection.getNode(), \"DISCONNECT: simulation disconnect after successfully sending \" + action + \" request\");\r\n            } else {\r\n                connection.sendRequest(requestId, action, request, options);\r\n            }\r\n        }\r\n    });\r\n    final AtomicBoolean finalized = new AtomicBoolean();\r\n    blueMockTransportService.addSendBehavior(redMockTransportService, (connection, requestId, action, request, options) -> {\r\n        logger.info(\"--> sending request {} on {}\", action, connection.getNode());\r\n        if (action.equals(PeerRecoveryTargetService.Actions.FINALIZE)) {\r\n            finalized.set(true);\r\n        }\r\n        connection.sendRequest(requestId, action, request, options);\r\n    });\r\n    for (MockTransportService mockTransportService : Arrays.asList(redMockTransportService, blueMockTransportService)) {\r\n        mockTransportService.addSendBehavior(masterTransportService, (connection, requestId, action, request, options) -> {\r\n            logger.info(\"--> sending request {} on {}\", action, connection.getNode());\r\n            if ((primaryRelocation && finalized.get()) == false) {\r\n                assertNotEquals(action, ShardStateAction.SHARD_FAILED_ACTION_NAME);\r\n            }\r\n            connection.sendRequest(requestId, action, request, options);\r\n        });\r\n    }\r\n    if (primaryRelocation) {\r\n        logger.info(\"--> starting primary relocation recovery from blue to red\");\r\n        client().admin().indices().prepareUpdateSettings(indexName).setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_INCLUDE_GROUP_SETTING.getKey() + \"color\", \"red\")).get();\r\n        ensureGreen();\r\n        client().admin().indices().prepareRefresh(indexName).get();\r\n    } else {\r\n        logger.info(\"--> starting replica recovery from blue to red\");\r\n        client().admin().indices().prepareUpdateSettings(indexName).setSettings(Settings.builder().put(IndexMetaData.INDEX_ROUTING_INCLUDE_GROUP_SETTING.getKey() + \"color\", \"red,blue\").put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1)).get();\r\n        ensureGreen();\r\n    }\r\n    for (int i = 0; i < 10; i++) {\r\n        assertHitCount(client().prepareSearch(indexName).get(), numDocs);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.indices.recovery.IndexRecoveryIT.testDisconnectsDuringRecovery",
	"Comment": "tests scenario where recovery target successfully sends recovery request to source but then the channel gets closed whilethe source is working on the recovery process.",
	"Method": "void testDisconnectsDuringRecovery(){\r\n    logger.info(\"--> sending request {} on {}\", action, connection.getNode());\r\n    if (PeerRecoverySourceService.Actions.START_RECOVERY.equals(action) && count.incrementAndGet() == 1) {\r\n        try {\r\n            awaitBusy(() -> client(blueNodeName).admin().cluster().prepareState().setLocal(true).get().getState().getRoutingTable().index(\"test\").shard(0).getAllInitializingShards().isEmpty() == false);\r\n        } catch (InterruptedException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n        connection.sendRequest(requestId, action, request, options);\r\n        try {\r\n            Thread.sleep(disconnectAfterDelay.millis());\r\n        } catch (InterruptedException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n        throw new ConnectTransportException(connection.getNode(), \"DISCONNECT: simulation disconnect after successfully sending \" + action + \" request\");\r\n    } else {\r\n        connection.sendRequest(requestId, action, request, options);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.restspec.ClientYamlSuiteRestSpec.load",
	"Comment": "parses the complete set of rest spec available under the provided directories",
	"Method": "ClientYamlSuiteRestSpec load(String classpathPrefix){\r\n    Path dir = PathUtils.get(ClientYamlSuiteRestSpec.class.getResource(classpathPrefix).toURI());\r\n    ClientYamlSuiteRestSpec restSpec = new ClientYamlSuiteRestSpec();\r\n    ClientYamlSuiteRestApiParser restApiParser = new ClientYamlSuiteRestApiParser();\r\n    try (Stream<Path> stream = Files.walk(dir)) {\r\n        stream.forEach(item -> {\r\n            if (item.toString().endsWith(\".json\")) {\r\n                parseSpecFile(restApiParser, item, restSpec);\r\n            }\r\n        });\r\n    }\r\n    return restSpec;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.FieldSortBuilder.setNestedPath",
	"Comment": "sets the nested path if sorting occurs on a field that is inside a nestedobject. by default when sorting on a field inside a nested object, thenearest upper nested object is selected as nested path.",
	"Method": "FieldSortBuilder setNestedPath(String nestedPath){\r\n    if (this.nestedSort != null) {\r\n        throw new IllegalArgumentException(\"Setting both nested_path/nested_filter and nested not allowed\");\r\n    }\r\n    this.nestedPath = nestedPath;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.indexing.IterationResult.isDone",
	"Comment": "returns true if this indexing iteration is done and job should go into sleep mode.",
	"Method": "boolean isDone(){\r\n    return isDone;\r\n}"
}, {
	"Path": "org.elasticsearch.test.TestCluster.beforeTest",
	"Comment": "this method should be executed before each test to reset the cluster to its initial state.",
	"Method": "void beforeTest(Random random,double transportClientRatio){\r\n    assert transportClientRatio >= 0.0 && transportClientRatio <= 1.0;\r\n    logger.debug(\"Reset test cluster with transport client ratio: [{}]\", transportClientRatio);\r\n    this.transportClientRatio = transportClientRatio;\r\n    this.random = new Random(random.nextLong());\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractStreamableXContentTestCase.getRandomFieldsExcludeFilter",
	"Comment": "returns a predicate that given the field name indicates whether the field has to be excluded from random fields insertion or not",
	"Method": "Predicate<String> getRandomFieldsExcludeFilter(){\r\n    return field -> false;\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.ReplicaShardAllocatorTests.testNoOrThrottleDecidersRemainsInUnassigned",
	"Comment": "when there is no decision or throttle decision across all nodes for the shard, make sure the shardmoves to the ignore unassigned list.",
	"Method": "void testNoOrThrottleDecidersRemainsInUnassigned(){\r\n    RoutingAllocation allocation = onePrimaryOnNode1And1Replica(randomBoolean() ? noAllocationDeciders() : throttleAllocationDeciders());\r\n    testAllocator.addData(node1, \"MATCH\", new StoreFileMetaData(\"file1\", 10, \"MATCH_CHECKSUM\", MIN_SUPPORTED_LUCENE_VERSION)).addData(node2, \"MATCH\", new StoreFileMetaData(\"file1\", 10, \"MATCH_CHECKSUM\", MIN_SUPPORTED_LUCENE_VERSION));\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().get(0).shardId(), equalTo(shardId));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.CumulativeSumAggregatorTests.testDerivative",
	"Comment": "first value from a derivative is null, so this makes sure the cusum can handle that",
	"Method": "void testDerivative(){\r\n    Query query = new MatchAllDocsQuery();\r\n    DateHistogramAggregationBuilder aggBuilder = new DateHistogramAggregationBuilder(\"histo\");\r\n    aggBuilder.dateHistogramInterval(DateHistogramInterval.DAY).field(HISTO_FIELD);\r\n    aggBuilder.subAggregation(new AvgAggregationBuilder(\"the_avg\").field(VALUE_FIELD));\r\n    aggBuilder.subAggregation(new DerivativePipelineAggregationBuilder(\"the_deriv\", \"the_avg\"));\r\n    aggBuilder.subAggregation(new CumulativeSumPipelineAggregationBuilder(\"cusum\", \"the_deriv\"));\r\n    executeTestCase(query, aggBuilder, histogram -> {\r\n        assertEquals(10, ((Histogram) histogram).getBuckets().size());\r\n        List<? extends Histogram.Bucket> buckets = ((Histogram) histogram).getBuckets();\r\n        double sum = 0.0;\r\n        for (int i = 0; i < buckets.size(); i++) {\r\n            if (i == 0) {\r\n                assertThat(((InternalSimpleValue) (buckets.get(i).getAggregations().get(\"cusum\"))).value(), equalTo(0.0));\r\n            } else {\r\n                sum += 1.0;\r\n                assertThat(((InternalSimpleValue) (buckets.get(i).getAggregations().get(\"cusum\"))).value(), equalTo(sum));\r\n            }\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.indexing.IterationResult.getPosition",
	"Comment": "return the position of the job, a generic to be passed to the next query construction.",
	"Method": "JobPosition getPosition(){\r\n    return position;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.HistogramIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=float\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"d\", -0.6), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"d\", 0.1));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(histogram(\"histo\").field(\"d\").script(new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, \"_value + 1\", emptyMap())).interval(0.7).offset(0.05)).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(histogram(\"histo\").field(\"d\").interval(0.7).offset(0.05)).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.script.ScoreScript.getDoc",
	"Comment": "the doc lookup for the lucene segment this script was created for.",
	"Method": "Map<String, ScriptDocValues<?>> getDoc(){\r\n    return leafLookup.doc();\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.RandomDocumentPicks.randomFieldName",
	"Comment": "returns a random field name. can be a leaf field name or thepath to refer to a field name using the dot notation.",
	"Method": "String randomFieldName(Random random){\r\n    int numLevels = RandomNumbers.randomIntBetween(random, 1, 5);\r\n    StringBuilder fieldName = new StringBuilder();\r\n    for (int i = 0; i < numLevels - 1; i++) {\r\n        if (i > 0) {\r\n            fieldName.append('.');\r\n        }\r\n        fieldName.append(randomString(random));\r\n    }\r\n    if (numLevels > 1) {\r\n        fieldName.append('.');\r\n    }\r\n    fieldName.append(randomLeafFieldName(random));\r\n    return fieldName.toString();\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.SortOrderTests.testDistanceUnitNames",
	"Comment": "check that ordinals remain stable as we rely on them for serialisation.",
	"Method": "void testDistanceUnitNames(){\r\n    assertEquals(0, SortOrder.ASC.ordinal());\r\n    assertEquals(1, SortOrder.DESC.ordinal());\r\n}"
}, {
	"Path": "org.elasticsearch.search.SearchShardTarget.getFullyQualifiedIndexName",
	"Comment": "returns the fully qualified index name, including the cluster alias.",
	"Method": "String getFullyQualifiedIndexName(){\r\n    return RemoteClusterAware.buildRemoteIndexName(getClusterAlias(), getIndex());\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocatorTests.testStoreException",
	"Comment": "tests when the node returns that no data was found for it, it will be moved to ignore unassigned.",
	"Method": "void testStoreException(){\r\n    final RoutingAllocation allocation = routingAllocationWithOnePrimaryNoReplicas(yesAllocationDeciders(), CLUSTER_RECOVERED, \"allocId1\");\r\n    testAllocator.addData(node1, \"allocId1\", randomBoolean(), new CorruptIndexException(\"test\", \"test\"));\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodesChanged(), equalTo(true));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().get(0).shardId(), equalTo(shardId));\r\n    assertClusterHealthStatus(allocation, ClusterHealthStatus.YELLOW);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomMetaDataCustoms",
	"Comment": "randomly adds, deletes or updates repositories in the metadata",
	"Method": "MetaData randomMetaDataCustoms(MetaData metaData){\r\n    return randomParts(metaData, \"custom\", new RandomPart<MetaData.Custom>() {\r\n        @Override\r\n        public ImmutableOpenMap<String, MetaData.Custom> parts(MetaData metaData) {\r\n            return metaData.customs();\r\n        }\r\n        @Override\r\n        public MetaData.Builder put(MetaData.Builder builder, MetaData.Custom part) {\r\n            return builder.putCustom(part.getWriteableName(), part);\r\n        }\r\n        @Override\r\n        public MetaData.Builder remove(MetaData.Builder builder, String name) {\r\n            if (IndexGraveyard.TYPE.equals(name)) {\r\n                return builder.indexGraveyard(IndexGraveyard.builder().build());\r\n            } else {\r\n                return builder.removeCustom(name);\r\n            }\r\n        }\r\n        @Override\r\n        public MetaData.Custom randomCreate(String name) {\r\n            if (randomBoolean()) {\r\n                return new RepositoriesMetaData(Collections.emptyList());\r\n            } else {\r\n                return IndexGraveyardTests.createRandom();\r\n            }\r\n        }\r\n        @Override\r\n        public MetaData.Custom randomChange(MetaData.Custom part) {\r\n            return part;\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomMetaDataCustoms",
	"Comment": "randomly adds, deletes or updates repositories in the metadata",
	"Method": "MetaData randomMetaDataCustoms(MetaData metaData){\r\n    return metaData.customs();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomMetaDataCustoms",
	"Comment": "randomly adds, deletes or updates repositories in the metadata",
	"Method": "MetaData randomMetaDataCustoms(MetaData metaData){\r\n    return builder.putCustom(part.getWriteableName(), part);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomMetaDataCustoms",
	"Comment": "randomly adds, deletes or updates repositories in the metadata",
	"Method": "MetaData randomMetaDataCustoms(MetaData metaData){\r\n    if (IndexGraveyard.TYPE.equals(name)) {\r\n        return builder.indexGraveyard(IndexGraveyard.builder().build());\r\n    } else {\r\n        return builder.removeCustom(name);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomMetaDataCustoms",
	"Comment": "randomly adds, deletes or updates repositories in the metadata",
	"Method": "MetaData randomMetaDataCustoms(MetaData metaData){\r\n    if (randomBoolean()) {\r\n        return new RepositoriesMetaData(Collections.emptyList());\r\n    } else {\r\n        return IndexGraveyardTests.createRandom();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomMetaDataCustoms",
	"Comment": "randomly adds, deletes or updates repositories in the metadata",
	"Method": "MetaData randomMetaDataCustoms(MetaData metaData){\r\n    return part;\r\n}"
}, {
	"Path": "org.elasticsearch.search.nested.SimpleNestedIT.testDeleteNestedDocsWithAlias",
	"Comment": "this includenesteddocsquery also needs to be aware of the filter from alias",
	"Method": "void testDeleteNestedDocsWithAlias(){\r\n    assertAcked(prepareCreate(\"test\").setSettings(Settings.builder().put(indexSettings()).put(\"index.refresh_interval\", -1).build()).addMapping(\"type1\", jsonBuilder().startObject().startObject(\"type1\").startObject(\"properties\").startObject(\"field1\").field(\"type\", \"text\").endObject().startObject(\"nested1\").field(\"type\", \"nested\").endObject().endObject().endObject().endObject()));\r\n    client().admin().indices().prepareAliases().addAlias(\"test\", \"alias1\", QueryBuilders.termQuery(\"field1\", \"value1\")).execute().actionGet();\r\n    ensureGreen();\r\n    client().prepareIndex(\"test\", \"type1\", \"1\").setSource(jsonBuilder().startObject().field(\"field1\", \"value1\").startArray(\"nested1\").startObject().field(\"n_field1\", \"n_value1_1\").field(\"n_field2\", \"n_value2_1\").endObject().startObject().field(\"n_field1\", \"n_value1_2\").field(\"n_field2\", \"n_value2_2\").endObject().endArray().endObject()).execute().actionGet();\r\n    client().prepareIndex(\"test\", \"type1\", \"2\").setSource(jsonBuilder().startObject().field(\"field1\", \"value2\").startArray(\"nested1\").startObject().field(\"n_field1\", \"n_value1_1\").field(\"n_field2\", \"n_value2_1\").endObject().startObject().field(\"n_field1\", \"n_value1_2\").field(\"n_field2\", \"n_value2_2\").endObject().endArray().endObject()).execute().actionGet();\r\n    flush();\r\n    refresh();\r\n    assertDocumentCount(\"test\", 6);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.ClusterStateCreationUtils.state",
	"Comment": "creates a cluster state where local node and master node can be specified",
	"Method": "ClusterState state(String index,boolean activePrimaryLocal,ShardRoutingState primaryState,ShardRoutingState replicaStates,ClusterState state,String index,int numberOfNodes,int numberOfPrimaries,ClusterState state,int numberOfNodes,String[] indices,int numberOfPrimaries,ClusterState state,DiscoveryNode localNode,DiscoveryNode masterNode,DiscoveryNode allNodes){\r\n    DiscoveryNodes.Builder discoBuilder = DiscoveryNodes.builder();\r\n    for (DiscoveryNode node : allNodes) {\r\n        discoBuilder.add(node);\r\n    }\r\n    if (masterNode != null) {\r\n        discoBuilder.masterNodeId(masterNode.getId());\r\n    }\r\n    discoBuilder.localNodeId(localNode.getId());\r\n    ClusterState.Builder state = ClusterState.builder(new ClusterName(\"test\"));\r\n    state.nodes(discoBuilder);\r\n    state.metaData(MetaData.builder().generateClusterUuidIfNeeded());\r\n    return state.build();\r\n}"
}, {
	"Path": "org.elasticsearch.search.SearchService.getActiveContexts",
	"Comment": "returns the number of active contexts in thissearchservice",
	"Method": "int getActiveContexts(){\r\n    return this.activeContexts.size();\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.GatewayIndexStateIT.testRecoverBrokenIndexMetadata",
	"Comment": "this test really tests worst case scenario where we have a broken setting or any setting that prevents an index from beingallocated in our metadata that we recover. in that case we now have the ability to check the index on local recovery from diskif it is sane and if we can successfully create an indexservice. this also includes plugins etc.",
	"Method": "void testRecoverBrokenIndexMetadata(){\r\n    logger.info(\"--> starting one node\");\r\n    internalCluster().startNode();\r\n    logger.info(\"--> indexing a simple document\");\r\n    client().prepareIndex(\"test\", \"type1\", \"1\").setSource(\"field1\", \"value1\").setRefreshPolicy(IMMEDIATE).get();\r\n    logger.info(\"--> waiting for green status\");\r\n    if (usually()) {\r\n        ensureYellow();\r\n    } else {\r\n        internalCluster().startNode();\r\n        client().admin().cluster().health(Requests.clusterHealthRequest().waitForGreenStatus().waitForEvents(Priority.LANGUID).waitForNoRelocatingShards(true).waitForNodes(\"2\")).actionGet();\r\n    }\r\n    ClusterState state = client().admin().cluster().prepareState().get().getState();\r\n    IndexMetaData metaData = state.getMetaData().index(\"test\");\r\n    for (NodeEnvironment services : internalCluster().getInstances(NodeEnvironment.class)) {\r\n        IndexMetaData brokenMeta = IndexMetaData.builder(metaData).settings(// this is invalid but should be archived\r\n        Settings.builder().put(metaData.getSettings()).put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT.minimumIndexCompatibilityVersion().id).put(\"index.similarity.BM25.type\", // this one is not validated ahead of time and breaks allocation\r\n        \"classic\").put(\"index.analysis.filter.myCollator.type\", \"icu_collation\")).build();\r\n        IndexMetaData.FORMAT.write(brokenMeta, services.indexPaths(brokenMeta.getIndex()));\r\n    }\r\n    internalCluster().fullRestart();\r\n    ensureGreen(metaData.getIndex().getName());\r\n    state = client().admin().cluster().prepareState().get().getState();\r\n    assertEquals(IndexMetaData.State.CLOSE, state.getMetaData().index(metaData.getIndex()).getState());\r\n    assertEquals(\"classic\", state.getMetaData().index(metaData.getIndex()).getSettings().get(\"archived.index.similarity.BM25.type\"));\r\n    ElasticsearchException ex = expectThrows(ElasticsearchException.class, () -> client().admin().indices().prepareOpen(\"test\").get());\r\n    assertEquals(ex.getMessage(), \"Failed to verify index \" + metaData.getIndex());\r\n    assertNotNull(ex.getCause());\r\n    assertEquals(IllegalArgumentException.class, ex.getCause().getClass());\r\n    assertEquals(ex.getCause().getMessage(), \"Unknown filter type [icu_collation] for [myCollator]\");\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TcpTransportTests.testParseV6UnBracketed",
	"Comment": "test unbracketed ipv6 hosts in configuration fail. leave no ambiguity",
	"Method": "void testParseV6UnBracketed(){\r\n    try {\r\n        TcpTransport.parse(\"::1\", \"1234\", Integer.MAX_VALUE);\r\n        fail(\"should have gotten exception\");\r\n    } catch (IllegalArgumentException expected) {\r\n        assertTrue(expected.getMessage().contains(\"must be bracketed\"));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.FieldSortBuilder.getNestedFilter",
	"Comment": "returns the nested filter that the nested objects should match with inorder to be taken into account for sorting.",
	"Method": "QueryBuilder getNestedFilter(){\r\n    return this.nestedFilter;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.SnapshotsInProgressTests.testWaitingIndices",
	"Comment": "makes sure that the indices being waited on before snapshotting commencesare populated with all shards in the relocating or initializing state.",
	"Method": "void testWaitingIndices(){\r\n    final Snapshot snapshot = new Snapshot(\"repo\", new SnapshotId(\"snap\", randomAlphaOfLength(5)));\r\n    final String idx1Name = \"idx1\";\r\n    final String idx2Name = \"idx2\";\r\n    final String idx3Name = \"idx3\";\r\n    final String idx1UUID = randomAlphaOfLength(5);\r\n    final String idx2UUID = randomAlphaOfLength(5);\r\n    final String idx3UUID = randomAlphaOfLength(5);\r\n    final List<IndexId> indices = Arrays.asList(new IndexId(idx1Name, randomAlphaOfLength(5)), new IndexId(idx2Name, randomAlphaOfLength(5)), new IndexId(idx3Name, randomAlphaOfLength(5)));\r\n    ImmutableOpenMap.Builder<ShardId, ShardSnapshotStatus> shards = ImmutableOpenMap.builder();\r\n    shards.put(new ShardId(idx1Name, idx1UUID, 0), new ShardSnapshotStatus(randomAlphaOfLength(2), State.WAITING));\r\n    shards.put(new ShardId(idx1Name, idx1UUID, 1), new ShardSnapshotStatus(randomAlphaOfLength(2), State.WAITING));\r\n    shards.put(new ShardId(idx1Name, idx1UUID, 2), new ShardSnapshotStatus(randomAlphaOfLength(2), randomNonWaitingState(), \"\"));\r\n    shards.put(new ShardId(idx2Name, idx2UUID, 0), new ShardSnapshotStatus(randomAlphaOfLength(2), State.WAITING));\r\n    shards.put(new ShardId(idx2Name, idx2UUID, 1), new ShardSnapshotStatus(randomAlphaOfLength(2), randomNonWaitingState(), \"\"));\r\n    shards.put(new ShardId(idx3Name, idx3UUID, 0), new ShardSnapshotStatus(randomAlphaOfLength(2), randomNonWaitingState(), \"\"));\r\n    Entry entry = new Entry(snapshot, randomBoolean(), randomBoolean(), State.INIT, indices, System.currentTimeMillis(), randomLong(), shards.build());\r\n    ImmutableOpenMap<String, List<ShardId>> waitingIndices = entry.waitingIndices();\r\n    assertEquals(2, waitingIndices.get(idx1Name).size());\r\n    assertEquals(1, waitingIndices.get(idx2Name).size());\r\n    assertFalse(waitingIndices.containsKey(idx3Name));\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.persistence.AnomalyDetectorsIndex.getPhysicalIndexFromState",
	"Comment": "retrieves the currently defined physical index from the job state",
	"Method": "String getPhysicalIndexFromState(ClusterState state,String jobId){\r\n    return MlMetadata.getMlMetadata(state).getJobs().get(jobId).getResultsIndexName();\r\n}"
}, {
	"Path": "org.elasticsearch.common.rounding.TimeZoneRoundingTests.nastyDate",
	"Comment": "to be even more nasty, go to a transition in the selected time zone.in one third of the cases stay there, otherwise go half a unit back or forth",
	"Method": "long nastyDate(long initialDate,DateTimeZone timezone,long unitMillis){\r\n    long date = timezone.nextTransition(initialDate);\r\n    if (randomBoolean()) {\r\n        return date + (randomLong() % unitMillis);\r\n    } else {\r\n        return date;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.AggregatorTestCase.getFieldAliases",
	"Comment": "allows subclasses to provide alternate names for the provided field type, whichcan be useful when testing aggregations on field aliases.",
	"Method": "Map<String, MappedFieldType> getFieldAliases(MappedFieldType fieldTypes){\r\n    return Collections.emptyMap();\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.shuffleList",
	"Comment": "shuffle fields of objects in the list, but not the list itself",
	"Method": "List<Object> shuffleList(List<Object> list,Set<String> exceptFields){\r\n    List<Object> targetList = new ArrayList();\r\n    for (Object value : list) {\r\n        if (value instanceof Map) {\r\n            LinkedHashMap<String, Object> valueMap = (LinkedHashMap<String, Object>) value;\r\n            targetList.add(shuffleMap(valueMap, exceptFields));\r\n        } else if (value instanceof List) {\r\n            targetList.add(shuffleList((List) value, exceptFields));\r\n        } else {\r\n            targetList.add(value);\r\n        }\r\n    }\r\n    return targetList;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogTests.testRecoveryFromFailureOnTrimming",
	"Comment": "tests the situation where the node crashes after a translog gen was committed to lucene, but before the translog had the chanceto clean up its files.",
	"Method": "void testRecoveryFromFailureOnTrimming(){\r\n    Path tempDir = createTempDir();\r\n    final FailSwitch fail = new FailSwitch();\r\n    fail.failNever();\r\n    final TranslogConfig config = getTranslogConfig(tempDir);\r\n    final long comittedGeneration;\r\n    final String translogUUID;\r\n    try (Translog translog = getFailableTranslog(fail, config)) {\r\n        final TranslogDeletionPolicy deletionPolicy = translog.getDeletionPolicy();\r\n        deletionPolicy.setRetentionSizeInBytes(-1);\r\n        deletionPolicy.setRetentionAgeInMillis(-1);\r\n        translogUUID = translog.getTranslogUUID();\r\n        int translogOperations = randomIntBetween(10, 100);\r\n        for (int op = 0; op < translogOperations / 2; op++) {\r\n            translog.add(new Translog.Index(\"test\", \"\" + op, op, primaryTerm.get(), Integer.toString(op).getBytes(Charset.forName(\"UTF-8\"))));\r\n            if (rarely()) {\r\n                translog.rollGeneration();\r\n            }\r\n        }\r\n        translog.rollGeneration();\r\n        comittedGeneration = randomLongBetween(2, translog.currentFileGeneration());\r\n        for (int op = translogOperations / 2; op < translogOperations; op++) {\r\n            translog.add(new Translog.Index(\"test\", \"\" + op, op, primaryTerm.get(), Integer.toString(op).getBytes(Charset.forName(\"UTF-8\"))));\r\n            if (rarely()) {\r\n                translog.rollGeneration();\r\n            }\r\n        }\r\n        deletionPolicy.setTranslogGenerationOfLastCommit(randomLongBetween(comittedGeneration, translog.currentFileGeneration()));\r\n        deletionPolicy.setMinTranslogGenerationForRecovery(comittedGeneration);\r\n        fail.failRandomly();\r\n        try {\r\n            translog.trimUnreferencedReaders();\r\n        } catch (Exception e) {\r\n        }\r\n    }\r\n    final TranslogDeletionPolicy deletionPolicy = new TranslogDeletionPolicy(-1, -1);\r\n    deletionPolicy.setTranslogGenerationOfLastCommit(randomLongBetween(comittedGeneration, Long.MAX_VALUE));\r\n    deletionPolicy.setMinTranslogGenerationForRecovery(comittedGeneration);\r\n    try (Translog translog = new Translog(config, translogUUID, deletionPolicy, () -> SequenceNumbers.NO_OPS_PERFORMED, primaryTerm::get)) {\r\n        assertThat(translog.getMinFileGeneration(), greaterThanOrEqualTo(1L));\r\n        assertThat(translog.getMinFileGeneration(), lessThanOrEqualTo(comittedGeneration));\r\n        assertFilePresences(translog);\r\n        translog.trimUnreferencedReaders();\r\n        assertThat(translog.getMinFileGeneration(), equalTo(comittedGeneration));\r\n        assertFilePresences(translog);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.rest.RestController.registerHandler",
	"Comment": "registers a rest handler to be executed when one of the provided methods and path match the request.",
	"Method": "void registerHandler(RestRequest.Method method,String path,RestHandler handler){\r\n    if (handler instanceof BaseRestHandler) {\r\n        usageService.addRestHandler((BaseRestHandler) handler);\r\n    }\r\n    handlers.insertOrUpdate(path, new MethodHandlers(path, handler, method), (mHandlers, newMHandler) -> {\r\n        return mHandlers.addMethods(handler, method);\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.ClusterStateCreationUtils.stateWithAssignedPrimariesAndReplicas",
	"Comment": "creates cluster state with several indexes, shards and replicas and all shards started.",
	"Method": "ClusterState stateWithAssignedPrimariesAndReplicas(String[] indices,int numberOfShards,int numberOfReplicas){\r\n    int numberOfDataNodes = numberOfReplicas + 1;\r\n    DiscoveryNodes.Builder discoBuilder = DiscoveryNodes.builder();\r\n    for (int i = 0; i < numberOfDataNodes + 1; i++) {\r\n        final DiscoveryNode node = newNode(i);\r\n        discoBuilder = discoBuilder.add(node);\r\n    }\r\n    discoBuilder.localNodeId(newNode(0).getId());\r\n    discoBuilder.masterNodeId(newNode(numberOfDataNodes + 1).getId());\r\n    ClusterState.Builder state = ClusterState.builder(new ClusterName(\"test\"));\r\n    state.nodes(discoBuilder);\r\n    Builder routingTableBuilder = RoutingTable.builder();\r\n    org.elasticsearch.cluster.metadata.MetaData.Builder metadataBuilder = MetaData.builder();\r\n    for (String index : indices) {\r\n        IndexMetaData indexMetaData = IndexMetaData.builder(index).settings(Settings.builder().put(SETTING_VERSION_CREATED, Version.CURRENT).put(SETTING_NUMBER_OF_SHARDS, numberOfShards).put(SETTING_NUMBER_OF_REPLICAS, numberOfReplicas).put(SETTING_CREATION_DATE, System.currentTimeMillis())).build();\r\n        metadataBuilder.put(indexMetaData, false).generateClusterUuidIfNeeded();\r\n        IndexRoutingTable.Builder indexRoutingTableBuilder = IndexRoutingTable.builder(indexMetaData.getIndex());\r\n        for (int i = 0; i < numberOfShards; i++) {\r\n            final ShardId shardId = new ShardId(index, \"_na_\", i);\r\n            IndexShardRoutingTable.Builder indexShardRoutingBuilder = new IndexShardRoutingTable.Builder(shardId);\r\n            indexShardRoutingBuilder.addShard(TestShardRouting.newShardRouting(index, i, newNode(0).getId(), null, true, ShardRoutingState.STARTED));\r\n            for (int replica = 0; replica < numberOfReplicas; replica++) {\r\n                indexShardRoutingBuilder.addShard(TestShardRouting.newShardRouting(index, i, newNode(replica + 1).getId(), null, false, ShardRoutingState.STARTED));\r\n            }\r\n            indexRoutingTableBuilder.addIndexShard(indexShardRoutingBuilder.build());\r\n        }\r\n        routingTableBuilder.add(indexRoutingTableBuilder.build());\r\n    }\r\n    state.metaData(metadataBuilder);\r\n    state.routingTable(routingTableBuilder.build());\r\n    return state.build();\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.CorruptedFileIT.testCorruptPrimaryNoReplica",
	"Comment": "tests corruption that happens on a single shard when no replicas are present. we make sure that the primary stays unassignedand all other replicas for the healthy shards happens",
	"Method": "void testCorruptPrimaryNoReplica(){\r\n    int numDocs = scaledRandomIntBetween(100, 1000);\r\n    internalCluster().ensureAtLeastNumDataNodes(2);\r\n    assertAcked(prepareCreate(\"test\").setSettings(// no checkindex - we corrupt shards on purpose\r\n    Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, \"0\").put(MergePolicyConfig.INDEX_MERGE_ENABLED, false).put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), // no translog based flush - it might change the .liv / segments.N files\r\n    false).put(IndexSettings.INDEX_TRANSLOG_FLUSH_THRESHOLD_SIZE_SETTING.getKey(), new ByteSizeValue(1, ByteSizeUnit.PB))));\r\n    ensureGreen();\r\n    IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs];\r\n    for (int i = 0; i < builders.length; i++) {\r\n        builders[i] = client().prepareIndex(\"test\", \"type\").setSource(\"field\", \"value\");\r\n    }\r\n    indexRandom(true, builders);\r\n    ensureGreen();\r\n    assertAllSuccessful(client().admin().indices().prepareFlush().setForce(true).execute().actionGet());\r\n    SearchResponse countResponse = client().prepareSearch().setSize(0).get();\r\n    assertHitCount(countResponse, numDocs);\r\n    ShardRouting shardRouting = corruptRandomPrimaryFile();\r\n    Settings build = Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, \"1\").build();\r\n    client().admin().indices().prepareUpdateSettings(\"test\").setSettings(build).get();\r\n    client().admin().cluster().prepareReroute().get();\r\n    boolean didClusterTurnRed = awaitBusy(() -> {\r\n        ClusterHealthStatus test = client().admin().cluster().health(Requests.clusterHealthRequest(\"test\")).actionGet().getStatus();\r\n        return test == ClusterHealthStatus.RED;\r\n    }, 5, TimeUnit.MINUTES);\r\n    final ClusterHealthResponse response = client().admin().cluster().health(Requests.clusterHealthRequest(\"test\")).get();\r\n    if (response.getStatus() != ClusterHealthStatus.RED) {\r\n        logger.info(\"Cluster turned red in busy loop: {}\", didClusterTurnRed);\r\n        logger.info(\"cluster state:\\n{}\\n{}\", client().admin().cluster().prepareState().get().getState(), client().admin().cluster().preparePendingClusterTasks().get());\r\n    }\r\n    assertThat(response.getStatus(), is(ClusterHealthStatus.RED));\r\n    ClusterState state = client().admin().cluster().prepareState().get().getState();\r\n    GroupShardsIterator<ShardIterator> shardIterators = state.getRoutingTable().activePrimaryShardsGrouped(new String[] { \"test\" }, false);\r\n    for (ShardIterator iterator : shardIterators) {\r\n        ShardRouting routing;\r\n        while ((routing = iterator.nextOrNull()) != null) {\r\n            if (routing.getId() == shardRouting.getId()) {\r\n                assertThat(routing.state(), equalTo(ShardRoutingState.UNASSIGNED));\r\n            } else {\r\n                assertThat(routing.state(), anyOf(equalTo(ShardRoutingState.RELOCATING), equalTo(ShardRoutingState.STARTED)));\r\n            }\r\n        }\r\n    }\r\n    final List<Path> files = listShardFiles(shardRouting);\r\n    Path corruptedFile = null;\r\n    for (Path file : files) {\r\n        if (file.getFileName().toString().startsWith(\"corrupted_\")) {\r\n            corruptedFile = file;\r\n            break;\r\n        }\r\n    }\r\n    assertThat(corruptedFile, notNullValue());\r\n}"
}, {
	"Path": "org.elasticsearch.common.cache.CacheTests.testNotificationOnInvalidateAll",
	"Comment": "invalidate all cached entries, then check that we receive invalidate notifications for all entries",
	"Method": "void testNotificationOnInvalidateAll(){\r\n    Set<Integer> notifications = new HashSet();\r\n    Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().removalListener(notification -> {\r\n        assertEquals(RemovalNotification.RemovalReason.INVALIDATED, notification.getRemovalReason());\r\n        notifications.add(notification.getKey());\r\n    }).build();\r\n    Set<Integer> invalidated = new HashSet();\r\n    for (int i = 0; i < numberOfEntries; i++) {\r\n        cache.put(i, Integer.toString(i));\r\n        invalidated.add(i);\r\n    }\r\n    cache.invalidateAll();\r\n    assertEquals(invalidated, notifications);\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.randomDateTimeZone",
	"Comment": "generate a random datetimezone from the ones available in joda library",
	"Method": "DateTimeZone randomDateTimeZone(){\r\n    return DateTimeZone.forID(randomFrom(JODA_TIMEZONE_IDS));\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.term.TermSuggestionBuilder.minDocFreq",
	"Comment": "get the minimal threshold for the frequency of a term appearing in thedocument set setting.",
	"Method": "TermSuggestionBuilder minDocFreq(float minDocFreq,float minDocFreq){\r\n    return minDocFreq;\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.restspec.ClientYamlSuiteRestApi.getPathParts",
	"Comment": "gets all path parts supported by the api. for every path part defines if itis required or optional.",
	"Method": "Map<String, Boolean> getPathParts(){\r\n    return pathParts;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.BaseAggregationTestCase.testToString",
	"Comment": "generic test that checks that the tostring method renders the xcontentcorrectly.",
	"Method": "void testToString(){\r\n    AB testAgg = createTestAggregatorBuilder();\r\n    String toString = randomBoolean() ? Strings.toString(testAgg) : testAgg.toString();\r\n    XContentParser parser = createParser(XContentType.JSON.xContent(), toString);\r\n    AggregationBuilder newAgg = parse(parser);\r\n    assertNotSame(newAgg, testAgg);\r\n    assertEquals(testAgg, newAgg);\r\n    assertEquals(testAgg.hashCode(), newAgg.hashCode());\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.assertAllShardsOnNodes",
	"Comment": "asserts that all shards are allocated on nodes matching the given node pattern.",
	"Method": "Set<String> assertAllShardsOnNodes(String index,String pattern){\r\n    Set<String> nodes = new HashSet();\r\n    ClusterState clusterState = client().admin().cluster().prepareState().execute().actionGet().getState();\r\n    for (IndexRoutingTable indexRoutingTable : clusterState.routingTable()) {\r\n        for (IndexShardRoutingTable indexShardRoutingTable : indexRoutingTable) {\r\n            for (ShardRouting shardRouting : indexShardRoutingTable) {\r\n                if (shardRouting.currentNodeId() != null && index.equals(shardRouting.getIndexName())) {\r\n                    String name = clusterState.nodes().get(shardRouting.currentNodeId()).getName();\r\n                    nodes.add(name);\r\n                    assertThat(\"Allocated on new node: \" + name, Regex.simpleMatch(pattern, name), is(true));\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return nodes;\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.highlight",
	"Comment": "a static factory method to construct new search highlights.",
	"Method": "HighlightBuilder highlight(){\r\n    return new HighlightBuilder();\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.EngineTestCase.assertConsistentHistoryBetweenTranslogAndLuceneIndex",
	"Comment": "asserts the provided engine has a consistent document history between translog and lucene index.",
	"Method": "void assertConsistentHistoryBetweenTranslogAndLuceneIndex(Engine engine,MapperService mapper){\r\n    if (mapper.documentMapper() == null || engine.config().getIndexSettings().isSoftDeleteEnabled() == false || (engine instanceof InternalEngine) == false) {\r\n        return;\r\n    }\r\n    final long maxSeqNo = ((InternalEngine) engine).getLocalCheckpointTracker().getMaxSeqNo();\r\n    if (maxSeqNo < 0) {\r\n        return;\r\n    }\r\n    final Map<Long, Translog.Operation> translogOps = new HashMap();\r\n    try (Translog.Snapshot snapshot = EngineTestCase.getTranslog(engine).newSnapshot()) {\r\n        Translog.Operation op;\r\n        while ((op = snapshot.next()) != null) {\r\n            translogOps.put(op.seqNo(), op);\r\n        }\r\n    }\r\n    final Map<Long, Translog.Operation> luceneOps = readAllOperationsInLucene(engine, mapper).stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\r\n    final long globalCheckpoint = EngineTestCase.getTranslog(engine).getLastSyncedGlobalCheckpoint();\r\n    final long retainedOps = engine.config().getIndexSettings().getSoftDeleteRetentionOperations();\r\n    final long seqNoForRecovery;\r\n    try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\r\n        seqNoForRecovery = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY)) + 1;\r\n    }\r\n    final long minSeqNoToRetain = Math.min(seqNoForRecovery, globalCheckpoint + 1 - retainedOps);\r\n    for (Translog.Operation translogOp : translogOps.values()) {\r\n        final Translog.Operation luceneOp = luceneOps.get(translogOp.seqNo());\r\n        if (luceneOp == null) {\r\n            if (minSeqNoToRetain <= translogOp.seqNo() && translogOp.seqNo() <= maxSeqNo) {\r\n                fail(\"Operation not found seq# [\" + translogOp.seqNo() + \"], global checkpoint [\" + globalCheckpoint + \"], \" + \"retention policy [\" + retainedOps + \"], maxSeqNo [\" + maxSeqNo + \"], translog op [\" + translogOp + \"]\");\r\n            } else {\r\n                continue;\r\n            }\r\n        }\r\n        assertThat(luceneOp, notNullValue());\r\n        assertThat(luceneOp.toString(), luceneOp.primaryTerm(), equalTo(translogOp.primaryTerm()));\r\n        assertThat(luceneOp.opType(), equalTo(translogOp.opType()));\r\n        if (luceneOp.opType() == Translog.Operation.Type.INDEX) {\r\n            assertThat(luceneOp.getSource().source, equalTo(translogOp.getSource().source));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.GeoDistanceSortBuilderTests.testBuildInvalidPoints",
	"Comment": "test that if validation is strict, invalid points throw an error",
	"Method": "void testBuildInvalidPoints(){\r\n    QueryShardContext shardContextMock = createMockShardContext();\r\n    {\r\n        GeoDistanceSortBuilder sortBuilder = new GeoDistanceSortBuilder(\"fieldName\", -180.0, 0.0);\r\n        sortBuilder.validation(GeoValidationMethod.STRICT);\r\n        ElasticsearchParseException ex = expectThrows(ElasticsearchParseException.class, () -> sortBuilder.build(shardContextMock));\r\n        assertEquals(\"illegal latitude value [-180.0] for [GeoDistanceSort] for field [fieldName].\", ex.getMessage());\r\n    }\r\n    {\r\n        GeoDistanceSortBuilder sortBuilder = new GeoDistanceSortBuilder(\"fieldName\", 0.0, -360.0);\r\n        sortBuilder.validation(GeoValidationMethod.STRICT);\r\n        ElasticsearchParseException ex = expectThrows(ElasticsearchParseException.class, () -> sortBuilder.build(shardContextMock));\r\n        assertEquals(\"illegal longitude value [-360.0] for [GeoDistanceSort] for field [fieldName].\", ex.getMessage());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalAggregationTestCase.excludePathsFromXContentInsertion",
	"Comment": "overwrite this in your test if other than the basic xcontent paths should be excluded during insertion of random fields",
	"Method": "Predicate<String> excludePathsFromXContentInsertion(){\r\n    return path -> false;\r\n}"
}, {
	"Path": "org.elasticsearch.common.io.stream.AbstractWriteableEnumTestCase.assertWriteToStream",
	"Comment": "a convenience method for testing the write of a writeable enum",
	"Method": "void assertWriteToStream(Writeable writeableEnum,int ordinal){\r\n    try (BytesStreamOutput out = new BytesStreamOutput()) {\r\n        writeableEnum.writeTo(out);\r\n        try (StreamInput in = out.bytes().streamInput()) {\r\n            assertThat(in.readVInt(), equalTo(ordinal));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregationBuilder.offset",
	"Comment": "set the offset on this builder, and return the builder so that calls can be chained.",
	"Method": "double offset(HistogramAggregationBuilder offset,double offset){\r\n    this.offset = offset;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.node.tasks.TasksIT.registerTaskManageListeners",
	"Comment": "registers recording task event listeners with the given action mask on all nodes",
	"Method": "void registerTaskManageListeners(String actionMasks){\r\n    for (String nodeName : internalCluster().getNodeNames()) {\r\n        DiscoveryNode node = internalCluster().getInstance(ClusterService.class, nodeName).localNode();\r\n        RecordingTaskManagerListener listener = new RecordingTaskManagerListener(node.getId(), actionMasks.split(\",\"));\r\n        ((MockTaskManager) internalCluster().getInstance(TransportService.class, nodeName).getTaskManager()).addListener(listener);\r\n        RecordingTaskManagerListener oldListener = listeners.put(new Tuple(node.getName(), actionMasks), listener);\r\n        assertNull(oldListener);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TestTranslog.minTranslogGenUsedInRecovery",
	"Comment": "lists all existing commits in a given index path, then read the minimum translog generation that will be used in recoverfromtranslog.",
	"Method": "long minTranslogGenUsedInRecovery(Path translogPath){\r\n    try (NIOFSDirectory directory = new NIOFSDirectory(translogPath.getParent().resolve(\"index\"))) {\r\n        List<IndexCommit> commits = DirectoryReader.listCommits(directory);\r\n        final String translogUUID = commits.get(commits.size() - 1).getUserData().get(Translog.TRANSLOG_UUID_KEY);\r\n        long globalCheckpoint = Translog.readGlobalCheckpoint(translogPath, translogUUID);\r\n        IndexCommit recoveringCommit = CombinedDeletionPolicy.findSafeCommitPoint(commits, globalCheckpoint);\r\n        return Long.parseLong(recoveringCommit.getUserData().get(Translog.TRANSLOG_GENERATION_KEY));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.transport.RemoteClusterConnection.fetchSearchShards",
	"Comment": "fetches all shards for the search request from this remote connection. this is used to later run the search on the remote end.",
	"Method": "void fetchSearchShards(ClusterSearchShardsRequest searchRequest,ActionListener<ClusterSearchShardsResponse> listener){\r\n    final ActionListener<ClusterSearchShardsResponse> searchShardsListener;\r\n    final Consumer<Exception> onConnectFailure;\r\n    if (skipUnavailable) {\r\n        onConnectFailure = (exception) -> listener.onResponse(ClusterSearchShardsResponse.EMPTY);\r\n        searchShardsListener = ActionListener.wrap(listener::onResponse, (e) -> listener.onResponse(ClusterSearchShardsResponse.EMPTY));\r\n    } else {\r\n        onConnectFailure = listener::onFailure;\r\n        searchShardsListener = listener;\r\n    }\r\n    ensureConnected(ActionListener.wrap((x) -> fetchShardsInternal(searchRequest, searchShardsListener), onConnectFailure));\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ClientHelper.executeWithHeadersAsync",
	"Comment": "execute a client operation asynchronously, try to run an action withleast privileges, when headers exist",
	"Method": "void executeWithHeadersAsync(Map<String, String> headers,String origin,Client client,Action<Response> action,Request request,ActionListener<Response> listener){\r\n    Map<String, String> filteredHeaders = headers.entrySet().stream().filter(e -> SECURITY_HEADER_FILTERS.contains(e.getKey())).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\r\n    final ThreadContext threadContext = client.threadPool().getThreadContext();\r\n    if (filteredHeaders.isEmpty()) {\r\n        ClientHelper.executeAsyncWithOrigin(client, origin, action, request, listener);\r\n    } else {\r\n        final Supplier<ThreadContext.StoredContext> supplier = threadContext.newRestorableContext(false);\r\n        try (ThreadContext.StoredContext ignore = stashWithHeaders(threadContext, filteredHeaders)) {\r\n            client.execute(action, request, new ContextPreservingActionListener(supplier, listener));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationActionTests.testReplicaActionRejectsWrongAid",
	"Comment": "test that a replica request is rejected if it arrives at a shard with a wrong allocation id",
	"Method": "void testReplicaActionRejectsWrongAid(){\r\n    final String index = \"test\";\r\n    final ShardId shardId = new ShardId(index, \"_na_\", 0);\r\n    ClusterState state = state(index, false, ShardRoutingState.STARTED, ShardRoutingState.STARTED);\r\n    final ShardRouting replica = state.routingTable().shardRoutingTable(shardId).replicaShards().get(0);\r\n    state = ClusterState.builder(state).nodes(DiscoveryNodes.builder(state.nodes()).localNodeId(replica.currentNodeId())).build();\r\n    setState(clusterService, state);\r\n    PlainActionFuture<TestResponse> listener = new PlainActionFuture();\r\n    Request request = new Request(shardId).timeout(\"1ms\");\r\n    action.new ReplicaOperationTransportHandler().messageReceived(new TransportReplicationAction.ConcreteReplicaRequest(request, \"_not_a_valid_aid_\", randomNonNegativeLong(), randomNonNegativeLong(), randomNonNegativeLong()), createTransportChannel(listener), maybeTask());\r\n    try {\r\n        listener.get();\r\n        fail(\"using a wrong aid didn't fail the operation\");\r\n    } catch (ExecutionException execException) {\r\n        Throwable throwable = execException.getCause();\r\n        if (action.retryPrimaryException(throwable) == false) {\r\n            throw new AssertionError(\"thrown exception is not retriable\", throwable);\r\n        }\r\n        assertThat(throwable.getMessage(), containsString(\"_not_a_valid_aid_\"));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.stopRandomDataNode",
	"Comment": "stops a random data node in the cluster. returns true if a node was found to stop, false otherwise.",
	"Method": "boolean stopRandomDataNode(){\r\n    ensureOpen();\r\n    NodeAndClient nodeAndClient = getRandomNodeAndClient(new DataNodePredicate());\r\n    if (nodeAndClient != null) {\r\n        logger.info(\"Closing random node [{}] \", nodeAndClient.name);\r\n        stopNodesAndClient(nodeAndClient);\r\n        return true;\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilder.useExplicitFieldOrder",
	"Comment": "send the fields to be highlighted using a syntax that is specific about the order in which they should be highlighted.",
	"Method": "HighlightBuilder useExplicitFieldOrder(boolean useExplicitFieldOrder,Boolean useExplicitFieldOrder){\r\n    return this.useExplicitFieldOrder;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.StatsIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(stats(\"foo\").field(\"d\").script(new Script(ScriptType.INLINE, AggregationTestScriptsPlugin.NAME, \"_value + 1\", Collections.emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(stats(\"foo\").field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEventTests.testLocalNodeIsMaster",
	"Comment": "test whether the clusterchangedevent returns the correct value for whether the local node is master,based on what was set on the cluster state.",
	"Method": "void testLocalNodeIsMaster(){\r\n    final int numNodesInCluster = 3;\r\n    ClusterState previousState = createSimpleClusterState();\r\n    ClusterState newState = createState(numNodesInCluster, true, initialIndices);\r\n    ClusterChangedEvent event = new ClusterChangedEvent(\"_na_\", newState, previousState);\r\n    assertTrue(\"local node should be master\", event.localNodeMaster());\r\n    newState = createState(numNodesInCluster, false, initialIndices);\r\n    event = new ClusterChangedEvent(\"_na_\", newState, previousState);\r\n    assertFalse(\"local node should not be master\", event.localNodeMaster());\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.monitoring.action.MonitoringBulkRequest.add",
	"Comment": "parses a monitoring bulk request and builds the list of documents to be indexed.",
	"Method": "MonitoringBulkRequest add(MonitoringBulkDoc doc,MonitoringBulkRequest add,MonitoredSystem system,String defaultType,BytesReference content,XContentType xContentType,long timestamp,long intervalMillis){\r\n    final BulkRequest bulkRequest = Requests.bulkRequest().add(content, null, defaultType, xContentType);\r\n    for (DocWriteRequest request : bulkRequest.requests()) {\r\n        if (request instanceof IndexRequest) {\r\n            final IndexRequest indexRequest = (IndexRequest) request;\r\n            if (MonitoringIndex.from(indexRequest.index()) != MonitoringIndex.TIMESTAMPED) {\r\n                continue;\r\n            }\r\n            final BytesReference source = indexRequest.source();\r\n            if (source.length() == 0) {\r\n                throw new IllegalArgumentException(\"source is missing for monitoring document [\" + indexRequest.index() + \"][\" + indexRequest.type() + \"][\" + indexRequest.id() + \"]\");\r\n            }\r\n            add(new MonitoringBulkDoc(system, indexRequest.type(), indexRequest.id(), timestamp, intervalMillis, source, xContentType));\r\n        } else {\r\n            throw new IllegalArgumentException(\"monitoring bulk requests should only contain index requests\");\r\n        }\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.ESRestTestCase.preserveRollupJobsUponCompletion",
	"Comment": "returns whether to preserve the rollup jobs of this test. defaults tonot preserving them. only runs at all if xpack is installed on thecluster being tested.",
	"Method": "boolean preserveRollupJobsUponCompletion(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.search.fetch.subphase.FetchSourceContext.getFilter",
	"Comment": "returns a filter function that expects the source map as an input and returnsthe filtered map.",
	"Method": "Function<Map<String, ?>, Map<String, Object>> getFilter(){\r\n    if (filter == null) {\r\n        filter = XContentMapValues.filter(includes, excludes);\r\n    }\r\n    return filter;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TcpTransport.onNonChannelException",
	"Comment": "exception handler for exceptions that are not associated with a specific channel.",
	"Method": "void onNonChannelException(Exception exception){\r\n    logger.warn(new ParameterizedMessage(\"exception caught on transport layer [thread={}]\", Thread.currentThread().getName()), exception);\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotShardsService.notifySuccessfulSnapshotShard",
	"Comment": "notify the master node that the given shard has been successfully snapshotted",
	"Method": "void notifySuccessfulSnapshotShard(Snapshot snapshot,ShardId shardId,String localNodeId){\r\n    sendSnapshotShardUpdate(snapshot, shardId, new ShardSnapshotStatus(localNodeId, State.SUCCESS));\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.storedFields",
	"Comment": "sets the stored fields to load and return as part of the search request. if noneare specified, the source of the document will be returned.",
	"Method": "SearchSourceBuilder storedFields(List<String> fields,SearchSourceBuilder storedFields,StoredFieldsContext context,StoredFieldsContext storedFields){\r\n    return storedFieldsContext;\r\n}"
}, {
	"Path": "org.elasticsearch.search.SearchService.canRewriteToMatchNone",
	"Comment": "returns true iff the given search source builder can be early terminated by rewriting to a match none query. or in other wordsif the execution of the search request can be early terminated without executing it. this is for instance not possible ifa global aggregation is part of this request or if there is a suggest builder present.",
	"Method": "boolean canRewriteToMatchNone(SearchSourceBuilder source){\r\n    if (source == null || source.query() == null || source.query() instanceof MatchAllQueryBuilder || source.suggest() != null) {\r\n        return false;\r\n    }\r\n    AggregatorFactories.Builder aggregations = source.aggregations();\r\n    return aggregations == null || aggregations.mustVisitAllDocs() == false;\r\n}"
}, {
	"Path": "org.elasticsearch.license.RemoteClusterLicenseChecker.isRemoteIndex",
	"Comment": "predicate to test if the index name represents the name of a remote index.",
	"Method": "boolean isRemoteIndex(String index){\r\n    return index.indexOf(RemoteClusterAware.REMOTE_CLUSTER_INDEX_SEPARATOR) != -1;\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksClusterServiceTests.createService",
	"Comment": "creates a persistenttasksclusterservice with a single persistenttasksexecutor implemented by a bifunction",
	"Method": "PersistentTasksClusterService createService(BiFunction<P, ClusterState, Assignment> fn){\r\n    PersistentTasksExecutorRegistry registry = new PersistentTasksExecutorRegistry(singleton(new PersistentTasksExecutor<P>(TestPersistentTasksExecutor.NAME, null) {\r\n        @Override\r\n        public Assignment getAssignment(P params, ClusterState clusterState) {\r\n            return fn.apply(params, clusterState);\r\n        }\r\n        @Override\r\n        protected void nodeOperation(AllocatedPersistentTask task, P params, PersistentTaskState state) {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n    }));\r\n    return new PersistentTasksClusterService(Settings.EMPTY, registry, clusterService);\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksClusterServiceTests.createService",
	"Comment": "creates a persistenttasksclusterservice with a single persistenttasksexecutor implemented by a bifunction",
	"Method": "PersistentTasksClusterService createService(BiFunction<P, ClusterState, Assignment> fn){\r\n    return fn.apply(params, clusterState);\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksClusterServiceTests.createService",
	"Comment": "creates a persistenttasksclusterservice with a single persistenttasksexecutor implemented by a bifunction",
	"Method": "PersistentTasksClusterService createService(BiFunction<P, ClusterState, Assignment> fn){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.indicesWithMissingShards",
	"Comment": "returns list of indices with missing shards, and list of indices that are closed",
	"Method": "Tuple<Set<String>, Set<String>> indicesWithMissingShards(ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards,MetaData metaData){\r\n    Set<String> missing = new HashSet();\r\n    Set<String> closed = new HashSet();\r\n    for (ObjectObjectCursor<ShardId, SnapshotsInProgress.ShardSnapshotStatus> entry : shards) {\r\n        if (entry.value.state() == State.MISSING) {\r\n            if (metaData.hasIndex(entry.key.getIndex().getName()) && metaData.getIndexSafe(entry.key.getIndex()).getState() == IndexMetaData.State.CLOSE) {\r\n                closed.add(entry.key.getIndex().getName());\r\n            } else {\r\n                missing.add(entry.key.getIndex().getName());\r\n            }\r\n        }\r\n    }\r\n    return new Tuple(missing, closed);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationActionTests.maybeTask",
	"Comment": "sometimes build a replicationtask for tracking the phase of thetransportreplicationaction. since transportreplicationaction has to workif the task as null just as well as if it is supplied this returns nullhalf the time.",
	"Method": "ReplicationTask maybeTask(){\r\n    return random().nextBoolean() ? new ReplicationTask(0, null, null, null, null, null) : null;\r\n}"
}, {
	"Path": "org.elasticsearch.search.fetch.subphase.highlight.HighlighterSearchIT.testFVHManyMatches",
	"Comment": "the fhv can spend a long time highlighting degenerate documents ifphraselimit is not set. its default is now reasonably low.",
	"Method": "void testFVHManyMatches(){\r\n    assertAcked(prepareCreate(\"test\").addMapping(\"type1\", type1TermVectorMapping()));\r\n    ensureGreen();\r\n    String pattern = \"t   \";\r\n    String value = new String(new char[1024 * 256 / pattern.length()]).replace(\"\\0\", pattern);\r\n    client().prepareIndex(\"test\", \"type1\").setSource(\"field1\", value).get();\r\n    refresh();\r\n    logger.info(\"--> highlighting and searching on field1 with default phrase limit\");\r\n    SearchSourceBuilder source = searchSource().query(termQuery(\"field1\", \"t\")).highlighter(highlight().highlighterType(\"fvh\").field(\"field1\", 20, 1).order(\"score\").preTags(\"<xxx>\").postTags(\"<\/xxx>\"));\r\n    SearchResponse defaultPhraseLimit = client().search(searchRequest(\"test\").source(source)).actionGet();\r\n    assertHighlight(defaultPhraseLimit, 0, \"field1\", 0, 1, containsString(\"<xxx>t<\/xxx>\"));\r\n    logger.info(\"--> highlighting and searching on field1 with large phrase limit\");\r\n    source = searchSource().query(termQuery(\"field1\", \"t\")).highlighter(highlight().highlighterType(\"fvh\").field(\"field1\", 20, 1).order(\"score\").preTags(\"<xxx>\").postTags(\"<\/xxx>\").phraseLimit(30000));\r\n    SearchResponse largePhraseLimit = client().search(searchRequest(\"test\").source(source)).actionGet();\r\n    assertHighlight(largePhraseLimit, 0, \"field1\", 0, 1, containsString(\"<xxx>t<\/xxx>\"));\r\n    assertThat(defaultPhraseLimit.getTook().getMillis(), lessThan(largePhraseLimit.getTook().getMillis()));\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.MasterDisruptionIT.testVerifyApiBlocksDuringPartition",
	"Comment": "verify that the proper block is applied when nodes loose their master",
	"Method": "void testVerifyApiBlocksDuringPartition(){\r\n    startCluster(3);\r\n    assertAcked(prepareCreate(\"test\").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 2)));\r\n    ensureGreen(\"test\");\r\n    TwoPartitions partitions = TwoPartitions.random(random(), internalCluster().getNodeNames());\r\n    NetworkDisruption networkDisruption = addRandomDisruptionType(partitions);\r\n    assertEquals(1, partitions.getMinoritySide().size());\r\n    final String isolatedNode = partitions.getMinoritySide().iterator().next();\r\n    assertEquals(2, partitions.getMajoritySide().size());\r\n    final String nonIsolatedNode = partitions.getMajoritySide().iterator().next();\r\n    networkDisruption.startDisrupting();\r\n    logger.info(\"waiting for isolated node [{}] to have no master\", isolatedNode);\r\n    assertNoMaster(isolatedNode, DiscoverySettings.NO_MASTER_BLOCK_WRITES, TimeValue.timeValueSeconds(10));\r\n    logger.info(\"wait until elected master has been removed and a new 2 node cluster was from (via [{}])\", isolatedNode);\r\n    ensureStableCluster(2, nonIsolatedNode);\r\n    for (String node : partitions.getMajoritySide()) {\r\n        ClusterState nodeState = getNodeClusterState(node);\r\n        boolean success = true;\r\n        if (nodeState.nodes().getMasterNode() == null) {\r\n            success = false;\r\n        }\r\n        if (!nodeState.blocks().global().isEmpty()) {\r\n            success = false;\r\n        }\r\n        if (!success) {\r\n            fail(\"node [\" + node + \"] has no master or has blocks, despite of being on the right side of the partition. State dump:\\n\" + nodeState);\r\n        }\r\n    }\r\n    networkDisruption.stopDisrupting();\r\n    ensureStableCluster(3, new TimeValue(DISRUPTION_HEALING_OVERHEAD.millis() + networkDisruption.expectedTimeToHeal().millis()));\r\n    logger.info(\"Verify no master block with {} set to {}\", DiscoverySettings.NO_MASTER_BLOCK_SETTING.getKey(), \"all\");\r\n    client().admin().cluster().prepareUpdateSettings().setTransientSettings(Settings.builder().put(DiscoverySettings.NO_MASTER_BLOCK_SETTING.getKey(), \"all\")).get();\r\n    networkDisruption.startDisrupting();\r\n    logger.info(\"waiting for isolated node [{}] to have no master\", isolatedNode);\r\n    assertNoMaster(isolatedNode, DiscoverySettings.NO_MASTER_BLOCK_ALL, TimeValue.timeValueSeconds(10));\r\n    ensureStableCluster(2, nonIsolatedNode);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorTests.testSignificance",
	"Comment": "uses the significant terms aggregation to find the keywords in text fields",
	"Method": "void testSignificance(){\r\n    TextFieldType textFieldType = new TextFieldType();\r\n    textFieldType.setName(\"text\");\r\n    textFieldType.setFielddata(true);\r\n    textFieldType.setIndexAnalyzer(new NamedAnalyzer(\"my_analyzer\", AnalyzerScope.GLOBAL, new StandardAnalyzer()));\r\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\r\n    indexWriterConfig.setMaxBufferedDocs(100);\r\n    indexWriterConfig.setRAMBufferSizeMB(100);\r\n    try (Directory dir = newDirectory();\r\n        IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {\r\n        addMixedTextDocs(textFieldType, w);\r\n        SignificantTermsAggregationBuilder sigAgg = new SignificantTermsAggregationBuilder(\"sig_text\", null).field(\"text\");\r\n        sigAgg.executionHint(randomExecutionHint());\r\n        if (randomBoolean()) {\r\n            sigAgg.backgroundFilter(QueryBuilders.termsQuery(\"text\", \"common\"));\r\n        }\r\n        SignificantTermsAggregationBuilder sigNumAgg = new SignificantTermsAggregationBuilder(\"sig_number\", null).field(\"long_field\");\r\n        sigNumAgg.executionHint(randomExecutionHint());\r\n        try (IndexReader reader = DirectoryReader.open(w)) {\r\n            assertEquals(\"test expects a single segment\", 1, reader.leaves().size());\r\n            IndexSearcher searcher = new IndexSearcher(reader);\r\n            SignificantTerms terms = searchAndReduce(searcher, new TermQuery(new Term(\"text\", \"odd\")), sigAgg, textFieldType);\r\n            assertEquals(1, terms.getBuckets().size());\r\n            assertNull(terms.getBucketByKey(\"even\"));\r\n            assertNull(terms.getBucketByKey(\"common\"));\r\n            assertNotNull(terms.getBucketByKey(\"odd\"));\r\n            terms = searchAndReduce(searcher, new TermQuery(new Term(\"text\", \"even\")), sigAgg, textFieldType);\r\n            assertEquals(1, terms.getBuckets().size());\r\n            assertNull(terms.getBucketByKey(\"odd\"));\r\n            assertNull(terms.getBucketByKey(\"common\"));\r\n            assertNotNull(terms.getBucketByKey(\"even\"));\r\n            sigAgg.includeExclude(new IncludeExclude(\"o.d\", null));\r\n            terms = searchAndReduce(searcher, new TermQuery(new Term(\"text\", \"odd\")), sigAgg, textFieldType);\r\n            assertEquals(1, terms.getBuckets().size());\r\n            assertNotNull(terms.getBucketByKey(\"odd\"));\r\n            assertNull(terms.getBucketByKey(\"common\"));\r\n            assertNull(terms.getBucketByKey(\"even\"));\r\n            String[] oddStrings = new String[] { \"odd\", \"weird\" };\r\n            String[] evenStrings = new String[] { \"even\", \"regular\" };\r\n            sigAgg.includeExclude(new IncludeExclude(oddStrings, evenStrings));\r\n            sigAgg.significanceHeuristic(SignificanceHeuristicTests.getRandomSignificanceheuristic());\r\n            terms = searchAndReduce(searcher, new TermQuery(new Term(\"text\", \"odd\")), sigAgg, textFieldType);\r\n            assertEquals(1, terms.getBuckets().size());\r\n            assertNotNull(terms.getBucketByKey(\"odd\"));\r\n            assertNull(terms.getBucketByKey(\"weird\"));\r\n            assertNull(terms.getBucketByKey(\"common\"));\r\n            assertNull(terms.getBucketByKey(\"even\"));\r\n            assertNull(terms.getBucketByKey(\"regular\"));\r\n            sigAgg.includeExclude(new IncludeExclude(evenStrings, oddStrings));\r\n            terms = searchAndReduce(searcher, new TermQuery(new Term(\"text\", \"odd\")), sigAgg, textFieldType);\r\n            assertEquals(0, terms.getBuckets().size());\r\n            assertNull(terms.getBucketByKey(\"odd\"));\r\n            assertNull(terms.getBucketByKey(\"weird\"));\r\n            assertNull(terms.getBucketByKey(\"common\"));\r\n            assertNull(terms.getBucketByKey(\"even\"));\r\n            assertNull(terms.getBucketByKey(\"regular\"));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.MovAvgModel.minimizeByDefault",
	"Comment": "should this model be fit to the data via a cost minimizing algorithm by default?",
	"Method": "boolean minimizeByDefault(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.logSegmentsState",
	"Comment": "prints the segments info for the given indices as debug logging.",
	"Method": "void logSegmentsState(String indices){\r\n    IndicesSegmentResponse segsRsp = client().admin().indices().prepareSegments(indices).get();\r\n    logger.debug(\"segments {} state: \\n{}\", indices.length == 0 ? \"[_all]\" : indices, Strings.toString(segsRsp.toXContent(JsonXContent.contentBuilder().prettyPrint(), ToXContent.EMPTY_PARAMS)));\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksDecidersTestCase.assertNbAssignedTasks",
	"Comment": "asserts that the given cluster state contains nbtasks tasks that are assigned",
	"Method": "void assertNbAssignedTasks(long nbTasks,ClusterState clusterState){\r\n    assertPersistentTasks(nbTasks, clusterState, PersistentTasksCustomMetaData.PersistentTask::isAssigned);\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.BulkByScrollTaskTests.checkStatusNegatives",
	"Comment": "build a task status with only some values. used for testing negative values.",
	"Method": "void checkStatusNegatives(Integer sliceId,long total,long updated,long created,long deleted,int batches,long versionConflicts,long noops,long bulkRetries,long searchRetries,String fieldName){\r\n    TimeValue throttle = parseTimeValue(randomPositiveTimeValue(), \"test\");\r\n    TimeValue throttledUntil = parseTimeValue(randomPositiveTimeValue(), \"test\");\r\n    IllegalArgumentException e = expectThrows(IllegalArgumentException.class, () -> new BulkByScrollTask.Status(sliceId, total, updated, created, deleted, batches, versionConflicts, noops, bulkRetries, searchRetries, throttle, 0f, null, throttledUntil));\r\n    assertEquals(e.getMessage(), fieldName + \" must be greater than 0 but was [-1]\");\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramAggregationBuilder.keyed",
	"Comment": "set whether to return buckets as a hash or as an array, and return the builder so that calls can be chained.",
	"Method": "boolean keyed(DateHistogramAggregationBuilder keyed,boolean keyed){\r\n    this.keyed = keyed;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.AdjacencyMatrixTests.testFiltersSameMap",
	"Comment": "test that when passing in keyed filters as a map they are equivalent",
	"Method": "void testFiltersSameMap(){\r\n    Map<String, QueryBuilder> original = new HashMap();\r\n    original.put(\"bbb\", new MatchNoneQueryBuilder());\r\n    original.put(\"aaa\", new MatchNoneQueryBuilder());\r\n    AdjacencyMatrixAggregationBuilder builder;\r\n    builder = new AdjacencyMatrixAggregationBuilder(\"my-agg\", original);\r\n    assertEquals(original, builder.filters());\r\n    assert original != builder.filters();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomMetaDataSettings",
	"Comment": "randomly updates persistent or transient settings of the given metadata",
	"Method": "MetaData randomMetaDataSettings(MetaData metaData){\r\n    if (randomBoolean()) {\r\n        return MetaData.builder(metaData).persistentSettings(randomSettings(metaData.persistentSettings())).build();\r\n    } else {\r\n        return MetaData.builder(metaData).transientSettings(randomSettings(metaData.transientSettings())).build();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilderTests.mutate",
	"Comment": "mutate the given highlighter builder so the returned one is different in one aspect",
	"Method": "HighlightBuilder mutate(HighlightBuilder original){\r\n    HighlightBuilder mutation = serializedCopy(original);\r\n    if (randomBoolean()) {\r\n        mutateCommonOptions(mutation);\r\n    } else {\r\n        switch(randomIntBetween(0, 2)) {\r\n            case 0:\r\n                mutation.useExplicitFieldOrder(!original.useExplicitFieldOrder());\r\n                break;\r\n            case 1:\r\n                mutation.encoder(original.encoder() + randomAlphaOfLength(2));\r\n                break;\r\n            case 2:\r\n                if (randomBoolean()) {\r\n                    mutation.field(new Field(randomAlphaOfLength(10)));\r\n                } else {\r\n                    List<Field> originalFields = original.fields();\r\n                    Field fieldToChange = originalFields.get(randomInt(originalFields.size() - 1));\r\n                    if (randomBoolean()) {\r\n                        fieldToChange.fragmentOffset(randomIntBetween(101, 200));\r\n                    } else {\r\n                        fieldToChange.matchedFields(randomStringArray(5, 10));\r\n                    }\r\n                }\r\n                break;\r\n        }\r\n    }\r\n    return mutation;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEventTests.testIndicesMetaDataChanges",
	"Comment": "test that the indices created and indices deleted lists between two cluster statesare correct when there is a change in indices added and deleted.also tests metadataequality between cluster states.",
	"Method": "void testIndicesMetaDataChanges(){\r\n    final int numNodesInCluster = 3;\r\n    ClusterState previousState = createState(numNodesInCluster, randomBoolean(), initialIndices);\r\n    for (TombstoneDeletionQuantity quantity : TombstoneDeletionQuantity.valuesInRandomizedOrder()) {\r\n        final ClusterState newState = executeIndicesChangesTest(previousState, quantity);\r\n        previousState = newState;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.PipelineAggregationHelperTests.generateHistogram",
	"Comment": "generates a mock histogram to use for testing.each mockbucket holds a doc count, key and document valueswhich can later be used to compute metrics and compare against the real aggregation results.gappiness can becontrolled via parameters",
	"Method": "ArrayList<MockBucket> generateHistogram(int interval,int size,double gapProbability,double runProbability){\r\n    ArrayList<MockBucket> values = new ArrayList(size);\r\n    boolean lastWasGap = false;\r\n    boolean emptyHisto = true;\r\n    for (int i = 0; i < size; i++) {\r\n        MockBucket bucket = new MockBucket();\r\n        if (randomDouble() < gapProbability) {\r\n            bucket.count = 0;\r\n            bucket.docValues = new double[0];\r\n            lastWasGap = true;\r\n        } else if (lastWasGap && randomDouble() < runProbability) {\r\n            bucket.count = 0;\r\n            bucket.docValues = new double[0];\r\n            lastWasGap = true;\r\n        } else {\r\n            bucket.count = randomIntBetween(1, 50);\r\n            bucket.docValues = new double[bucket.count];\r\n            for (int j = 0; j < bucket.count; j++) {\r\n                bucket.docValues[j] = randomDouble() * randomIntBetween(-20, 20);\r\n            }\r\n            lastWasGap = false;\r\n            emptyHisto = false;\r\n        }\r\n        bucket.key = i * interval;\r\n        values.add(bucket);\r\n    }\r\n    if (emptyHisto) {\r\n        int idx = randomIntBetween(0, values.size() - 1);\r\n        MockBucket bucket = values.get(idx);\r\n        bucket.count = randomIntBetween(1, 50);\r\n        bucket.docValues = new double[bucket.count];\r\n        for (int j = 0; j < bucket.count; j++) {\r\n            bucket.docValues[j] = randomDouble() * randomIntBetween(-20, 20);\r\n        }\r\n        values.set(idx, bucket);\r\n    }\r\n    return values;\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractQueryTestCase.testFromXContent",
	"Comment": "generic test that creates new query from the test query and checks both for equalityand asserts equality on the two queries.",
	"Method": "void testFromXContent(){\r\n    for (int runs = 0; runs < NUMBER_OF_TESTQUERIES; runs++) {\r\n        QB testQuery = createTestQueryBuilder();\r\n        XContentType xContentType = randomFrom(XContentType.values());\r\n        BytesReference shuffledXContent = toShuffledXContent(testQuery, xContentType, ToXContent.EMPTY_PARAMS, randomBoolean(), shuffleProtectedFields());\r\n        assertParsedQuery(createParser(xContentType.xContent(), shuffledXContent), testQuery);\r\n        for (Map.Entry<String, QB> alternateVersion : getAlternateVersions().entrySet()) {\r\n            String queryAsString = alternateVersion.getKey();\r\n            assertParsedQuery(createParser(JsonXContent.jsonXContent, queryAsString), alternateVersion.getValue());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.MergingBucketsDeferringCollector.prepareSelectedBuckets",
	"Comment": "replay the wrapped collector, but only on a selection of buckets.",
	"Method": "void prepareSelectedBuckets(long selectedBuckets){\r\n    if (finished == false) {\r\n        throw new IllegalStateException(\"Cannot replay yet, collection is not finished: postCollect() has not been called\");\r\n    }\r\n    if (this.selectedBuckets != null) {\r\n        throw new IllegalStateException(\"Already been replayed\");\r\n    }\r\n    final LongHash hash = new LongHash(selectedBuckets.length, BigArrays.NON_RECYCLING_INSTANCE);\r\n    for (long bucket : selectedBuckets) {\r\n        hash.add(bucket);\r\n    }\r\n    this.selectedBuckets = hash;\r\n    boolean needsScores = collector.scoreMode().needsScores();\r\n    Weight weight = null;\r\n    if (needsScores) {\r\n        weight = searchContext.searcher().createWeight(searchContext.searcher().rewrite(searchContext.query()), ScoreMode.COMPLETE, 1f);\r\n    }\r\n    for (Entry entry : entries) {\r\n        final LeafBucketCollector leafCollector = collector.getLeafCollector(entry.context);\r\n        DocIdSetIterator docIt = null;\r\n        if (needsScores && entry.docDeltas.size() > 0) {\r\n            Scorer scorer = weight.scorer(entry.context);\r\n            docIt = scorer.iterator();\r\n            leafCollector.setScorer(scorer);\r\n        }\r\n        final PackedLongValues.Iterator docDeltaIterator = entry.docDeltas.iterator();\r\n        final PackedLongValues.Iterator buckets = entry.buckets.iterator();\r\n        int doc = 0;\r\n        for (long i = 0, end = entry.docDeltas.size(); i < end; ++i) {\r\n            doc += docDeltaIterator.next();\r\n            final long bucket = buckets.next();\r\n            final long rebasedBucket = hash.find(bucket);\r\n            if (rebasedBucket != -1) {\r\n                if (needsScores) {\r\n                    if (docIt.docID() < doc) {\r\n                        docIt.advance(doc);\r\n                    }\r\n                    assert docIt.docID() == doc;\r\n                }\r\n                leafCollector.collect(doc, rebasedBucket);\r\n            }\r\n        }\r\n    }\r\n    collector.postCollection();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.segments.IndicesSegmentsRequestTests.testAllowNoIndex",
	"Comment": "by default indicesoptions setting indicessegmentsrequest should not throw exception when no index present",
	"Method": "void testAllowNoIndex(){\r\n    client().admin().indices().prepareDelete(\"test\").get();\r\n    IndicesSegmentResponse rsp = client().admin().indices().prepareSegments().get();\r\n    assertEquals(0, rsp.getIndices().size());\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase.testFromXContent",
	"Comment": "generic test that creates new aggregatorfactory from the testaggregatorfactory and checks both for equality and asserts equality onthe two queries.",
	"Method": "void testFromXContent(){\r\n    AF testAgg = createTestAggregatorFactory();\r\n    AggregatorFactories.Builder factoriesBuilder = AggregatorFactories.builder().skipResolveOrder().addPipelineAggregator(testAgg);\r\n    logger.info(\"Content string: {}\", factoriesBuilder);\r\n    XContentBuilder builder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));\r\n    if (randomBoolean()) {\r\n        builder.prettyPrint();\r\n    }\r\n    factoriesBuilder.toXContent(builder, ToXContent.EMPTY_PARAMS);\r\n    XContentBuilder shuffled = shuffleXContent(builder);\r\n    try (XContentParser parser = createParser(shuffled)) {\r\n        String contentString = factoriesBuilder.toString();\r\n        logger.info(\"Content string: {}\", contentString);\r\n        PipelineAggregationBuilder newAgg = parse(parser);\r\n        assertNotSame(newAgg, testAgg);\r\n        assertEquals(testAgg, newAgg);\r\n        assertEquals(testAgg.hashCode(), newAgg.hashCode());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.datafeed.DatafeedUpdate.equals",
	"Comment": "the lists of indices and types are compared for equality but they are notsorted first so this test could fail simply because the indices and typeslists are in different orders.",
	"Method": "boolean equals(Object other){\r\n    if (this == other) {\r\n        return true;\r\n    }\r\n    if (other instanceof DatafeedUpdate == false) {\r\n        return false;\r\n    }\r\n    DatafeedUpdate that = (DatafeedUpdate) other;\r\n    return Objects.equals(this.id, that.id) && Objects.equals(this.jobId, that.jobId) && Objects.equals(this.frequency, that.frequency) && Objects.equals(this.queryDelay, that.queryDelay) && Objects.equals(this.indices, that.indices) && Objects.equals(this.types, that.types) && Objects.equals(this.query, that.query) && Objects.equals(this.scrollSize, that.scrollSize) && Objects.equals(this.aggregations, that.aggregations) && Objects.equals(this.delayedDataCheckConfig, that.delayedDataCheckConfig) && Objects.equals(this.scriptFields, that.scriptFields) && Objects.equals(this.chunkingConfig, that.chunkingConfig);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.sampler.SamplerAggregatorTests.testSampler",
	"Comment": "uses the sampler aggregation to find the minimum value of a field out of the top 3 scoring documents in a search.",
	"Method": "void testSampler(){\r\n    TextFieldType textFieldType = new TextFieldType();\r\n    textFieldType.setIndexAnalyzer(new NamedAnalyzer(\"foo\", AnalyzerScope.GLOBAL, new StandardAnalyzer()));\r\n    MappedFieldType numericFieldType = new NumberFieldMapper.NumberFieldType(NumberFieldMapper.NumberType.LONG);\r\n    numericFieldType.setName(\"int\");\r\n    IndexWriterConfig indexWriterConfig = newIndexWriterConfig();\r\n    indexWriterConfig.setMaxBufferedDocs(100);\r\n    indexWriterConfig.setRAMBufferSizeMB(100);\r\n    try (Directory dir = newDirectory();\r\n        IndexWriter w = new IndexWriter(dir, indexWriterConfig)) {\r\n        for (long value : new long[] { 7, 3, -10, -6, 5, 50 }) {\r\n            Document doc = new Document();\r\n            StringBuilder text = new StringBuilder();\r\n            for (int i = 0; i < value; i++) {\r\n                text.append(\"good \");\r\n            }\r\n            doc.add(new Field(\"text\", text.toString(), textFieldType));\r\n            doc.add(new SortedNumericDocValuesField(\"int\", value));\r\n            w.addDocument(doc);\r\n        }\r\n        SamplerAggregationBuilder aggBuilder = new SamplerAggregationBuilder(\"sampler\").shardSize(3).subAggregation(new MinAggregationBuilder(\"min\").field(\"int\"));\r\n        try (IndexReader reader = DirectoryReader.open(w)) {\r\n            assertEquals(\"test expects a single segment\", 1, reader.leaves().size());\r\n            IndexSearcher searcher = new IndexSearcher(reader);\r\n            Sampler sampler = searchAndReduce(searcher, new TermQuery(new Term(\"text\", \"good\")), aggBuilder, textFieldType, numericFieldType);\r\n            Min min = sampler.getAggregations().get(\"min\");\r\n            assertEquals(5.0, min.getValue(), 0);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.script.FilterScript.getDoc",
	"Comment": "the doc lookup for the lucene segment this script was created for.",
	"Method": "Map<String, ScriptDocValues<?>> getDoc(){\r\n    return leafLookup.doc();\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.phrase.StupidBackoffModelTests.createMutation",
	"Comment": "mutate the given model so the returned smoothing model is different",
	"Method": "StupidBackoff createMutation(SmoothingModel input){\r\n    StupidBackoff original = (StupidBackoff) input;\r\n    return new StupidBackoff(original.getDiscount() + 0.1);\r\n}"
}, {
	"Path": "org.elasticsearch.transport.ConnectionProfile.buildDefaultConnectionProfile",
	"Comment": "builds a default connection profile based on the provided settings.",
	"Method": "ConnectionProfile buildDefaultConnectionProfile(Settings settings){\r\n    int connectionsPerNodeRecovery = TransportService.CONNECTIONS_PER_NODE_RECOVERY.get(settings);\r\n    int connectionsPerNodeBulk = TransportService.CONNECTIONS_PER_NODE_BULK.get(settings);\r\n    int connectionsPerNodeReg = TransportService.CONNECTIONS_PER_NODE_REG.get(settings);\r\n    int connectionsPerNodeState = TransportService.CONNECTIONS_PER_NODE_STATE.get(settings);\r\n    int connectionsPerNodePing = TransportService.CONNECTIONS_PER_NODE_PING.get(settings);\r\n    Builder builder = new Builder();\r\n    builder.setConnectTimeout(TransportService.TCP_CONNECT_TIMEOUT.get(settings));\r\n    builder.setHandshakeTimeout(TransportService.TCP_CONNECT_TIMEOUT.get(settings));\r\n    builder.setPingInterval(TcpTransport.PING_SCHEDULE.get(settings));\r\n    builder.setCompressionEnabled(Transport.TRANSPORT_TCP_COMPRESS.get(settings));\r\n    builder.addConnections(connectionsPerNodeBulk, TransportRequestOptions.Type.BULK);\r\n    builder.addConnections(connectionsPerNodePing, TransportRequestOptions.Type.PING);\r\n    builder.addConnections(DiscoveryNode.isMasterNode(settings) ? connectionsPerNodeState : 0, TransportRequestOptions.Type.STATE);\r\n    builder.addConnections(DiscoveryNode.isDataNode(settings) ? connectionsPerNodeRecovery : 0, TransportRequestOptions.Type.RECOVERY);\r\n    builder.addConnections(connectionsPerNodeReg, TransportRequestOptions.Type.REG);\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.config.Detector.extractAnalysisFields",
	"Comment": "returns a list with the byfieldname, overfieldname and partitionfieldname that are not null",
	"Method": "List<String> extractAnalysisFields(List<String> extractAnalysisFields){\r\n    List<String> analysisFields = Arrays.asList(getByFieldName(), getOverFieldName(), getPartitionFieldName());\r\n    return analysisFields.stream().filter(item -> item != null).collect(Collectors.toList());\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.assertConcreteMappingsOnAll",
	"Comment": "waits until mappings for the provided fields exist on all nodes. note, this waits for the currentstarted shards and checks for concrete mappings.",
	"Method": "void assertConcreteMappingsOnAll(String index,String type,String fieldNames){\r\n    Set<String> nodes = internalCluster().nodesInclude(index);\r\n    assertThat(nodes, Matchers.not(Matchers.emptyIterable()));\r\n    for (String node : nodes) {\r\n        IndicesService indicesService = internalCluster().getInstance(IndicesService.class, node);\r\n        IndexService indexService = indicesService.indexService(resolveIndex(index));\r\n        assertThat(\"index service doesn't exists on \" + node, indexService, notNullValue());\r\n        MapperService mapperService = indexService.mapperService();\r\n        for (String fieldName : fieldNames) {\r\n            MappedFieldType fieldType = mapperService.fullName(fieldName);\r\n            assertNotNull(\"field \" + fieldName + \" doesn't exists on \" + node, fieldType);\r\n        }\r\n    }\r\n    assertMappingOnMaster(index, type, fieldNames);\r\n}"
}, {
	"Path": "org.elasticsearch.test.geo.RandomShapeGenerator.xRandomRectangle",
	"Comment": "creates a small random rectangle by default to keep shape test performance at bay",
	"Method": "Rectangle xRandomRectangle(Random r,Point nearP,Rectangle bounds,boolean small,Rectangle xRandomRectangle,Random r,Point nearP,Rectangle xRandomRectangle,Random r,Point nearP,boolean small){\r\n    return xRandomRectangle(r, nearP, ctx.getWorldBounds(), small);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.nested.ReverseNestedAggregationBuilder.path",
	"Comment": "set the path to use for this nested aggregation. the path must matchthe path to a nested object in the mappings. if it is not specifiedthen this aggregation will go back to the root document.",
	"Method": "ReverseNestedAggregationBuilder path(String path,String path){\r\n    return path;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketEmptyAggregations",
	"Comment": "utility method to build empty aggregations of the sub aggregators.",
	"Method": "InternalAggregations bucketEmptyAggregations(){\r\n    final InternalAggregation[] aggregations = new InternalAggregation[subAggregators.length];\r\n    for (int i = 0; i < subAggregators.length; i++) {\r\n        aggregations[i] = subAggregators[i].buildEmptyAggregation();\r\n    }\r\n    return new InternalAggregations(Arrays.asList(aggregations));\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.GetTermVectorsIT.testSimpleTermVectorsWithGenerate",
	"Comment": "like testsimpletermvectors but we create fields with no term vectors",
	"Method": "void testSimpleTermVectorsWithGenerate(){\r\n    String[] fieldNames = new String[10];\r\n    for (int i = 0; i < fieldNames.length; i++) {\r\n        fieldNames[i] = \"field\" + String.valueOf(i);\r\n    }\r\n    XContentBuilder mapping = jsonBuilder().startObject().startObject(\"type1\").startObject(\"properties\");\r\n    XContentBuilder source = jsonBuilder().startObject();\r\n    for (String field : fieldNames) {\r\n        mapping.startObject(field).field(\"type\", \"text\").field(\"term_vector\", randomBoolean() ? \"with_positions_offsets_payloads\" : \"no\").field(\"analyzer\", \"tv_test\").endObject();\r\n        source.field(field, \"the quick brown fox jumps over the lazy dog\");\r\n    }\r\n    mapping.endObject().endObject().endObject();\r\n    source.endObject();\r\n    assertAcked(prepareCreate(\"test\").addMapping(\"type1\", mapping).setSettings(Settings.builder().put(indexSettings()).put(\"index.analysis.analyzer.tv_test.tokenizer\", \"standard\").putList(\"index.analysis.analyzer.tv_test.filter\", \"lowercase\")));\r\n    ensureGreen();\r\n    for (int i = 0; i < 10; i++) {\r\n        client().prepareIndex(\"test\", \"type1\", Integer.toString(i)).setSource(source).execute().actionGet();\r\n        refresh();\r\n    }\r\n    for (int i = 0; i < 10; i++) {\r\n        TermVectorsResponse response = client().prepareTermVectors(\"test\", \"type1\", Integer.toString(i)).setPayloads(true).setOffsets(true).setPositions(true).setSelectedFields(fieldNames).execute().actionGet();\r\n        assertThat(\"doc id: \" + i + \" doesn't exists but should\", response.isExists(), equalTo(true));\r\n        Fields fields = response.getFields();\r\n        assertThat(fields.size(), equalTo(fieldNames.length));\r\n        for (String fieldName : fieldNames) {\r\n            checkBrownFoxTermVector(fields, fieldName, false);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.results.Bucket.getRecords",
	"Comment": "get all the anomaly records associated with this bucket.the records are not part of the bucket document. they willonly be present when the bucket was retrieved and expandedto contain the associated records.",
	"Method": "List<AnomalyRecord> getRecords(){\r\n    return records;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.FieldSortBuilder.getNestedPath",
	"Comment": "returns the nested path if sorting occurs in a field that is inside anested object.",
	"Method": "String getNestedPath(){\r\n    return this.nestedPath;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.MasterDisruptionIT.testMasterNodeGCs",
	"Comment": "test that cluster recovers from a long gc on master that causes other nodes to elect a new one",
	"Method": "void testMasterNodeGCs(){\r\n    List<String> nodes = startCluster(3);\r\n    String oldMasterNode = internalCluster().getMasterName();\r\n    SingleNodeDisruption masterNodeDisruption = new IntermittentLongGCDisruption(random(), oldMasterNode, 100, 200, 30000, 60000);\r\n    internalCluster().setDisruptionScheme(masterNodeDisruption);\r\n    masterNodeDisruption.startDisrupting();\r\n    Set<String> oldNonMasterNodesSet = new HashSet(nodes);\r\n    oldNonMasterNodesSet.remove(oldMasterNode);\r\n    List<String> oldNonMasterNodes = new ArrayList(oldNonMasterNodesSet);\r\n    logger.info(\"waiting for nodes to de-elect master [{}]\", oldMasterNode);\r\n    for (String node : oldNonMasterNodesSet) {\r\n        assertDifferentMaster(node, oldMasterNode);\r\n    }\r\n    logger.info(\"waiting for nodes to elect a new master\");\r\n    ensureStableCluster(2, oldNonMasterNodes.get(0));\r\n    logger.info(\"waiting for any pinging to stop\");\r\n    assertDiscoveryCompleted(oldNonMasterNodes);\r\n    masterNodeDisruption.stopDisrupting();\r\n    final TimeValue waitTime = new TimeValue(DISRUPTION_HEALING_OVERHEAD.millis() + masterNodeDisruption.expectedTimeToHeal().millis());\r\n    ensureStableCluster(3, waitTime, false, oldNonMasterNodes.get(0));\r\n    String newMaster = internalCluster().getMasterName();\r\n    assertThat(newMaster, not(equalTo(oldMasterNode)));\r\n    assertMaster(newMaster, nodes);\r\n}"
}, {
	"Path": "org.elasticsearch.search.profile.ProfileResult.getTimeBreakdown",
	"Comment": "returns the timing breakdown for this particular query node",
	"Method": "Map<String, Long> getTimeBreakdown(){\r\n    return Collections.unmodifiableMap(timings);\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.AbstractSortTestCase.testFromXContent",
	"Comment": "test that creates new sort from a random test sort and checks both for equality",
	"Method": "void testFromXContent(){\r\n    for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {\r\n        T testItem = createTestItem();\r\n        XContentBuilder builder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));\r\n        if (randomBoolean()) {\r\n            builder.prettyPrint();\r\n        }\r\n        testItem.toXContent(builder, ToXContent.EMPTY_PARAMS);\r\n        XContentBuilder shuffled = shuffleXContent(builder);\r\n        try (XContentParser itemParser = createParser(shuffled)) {\r\n            itemParser.nextToken();\r\n            itemParser.nextToken();\r\n            String elementName = itemParser.currentName();\r\n            itemParser.nextToken();\r\n            T parsedItem = fromXContent(itemParser, elementName);\r\n            assertNotSame(testItem, parsedItem);\r\n            assertEquals(testItem, parsedItem);\r\n            assertEquals(testItem.hashCode(), parsedItem.hashCode());\r\n            assertWarnings(testItem);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.ensureYellow",
	"Comment": "ensures the cluster has a yellow state via the cluster health api.",
	"Method": "ClusterHealthStatus ensureYellow(String indices){\r\n    return ensureColor(ClusterHealthStatus.YELLOW, TimeValue.timeValueSeconds(30), false, indices);\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.ESRestTestCase.client",
	"Comment": "get the client used for ordinary api calls while writing a test",
	"Method": "RestClient client(){\r\n    return client;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.config.AnalysisConfig.termFields",
	"Comment": "return the list of term fields.these are the influencer fields, partition field,by field and over field of each detector.null and empty strings are filtered from theconfig.",
	"Method": "Set<String> termFields(SortedSet<String> termFields,List<Detector> detectors,List<String> influencers){\r\n    SortedSet<String> termFields = new TreeSet();\r\n    detectors.forEach(d -> termFields.addAll(d.getByOverPartitionTerms()));\r\n    for (String i : influencers) {\r\n        addIfNotNull(termFields, i);\r\n    }\r\n    termFields.remove(\"\");\r\n    return termFields;\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.SuggestBuilders.termSuggestion",
	"Comment": "creates a term suggestion lookup query with the provided field",
	"Method": "TermSuggestionBuilder termSuggestion(String fieldname){\r\n    return new TermSuggestionBuilder(fieldname);\r\n}"
}, {
	"Path": "org.elasticsearch.search.SearchPhaseResult.getShardIndex",
	"Comment": "returns the shard index in the context of the currently executing search request that isused for accounting on the coordinating node",
	"Method": "int getShardIndex(){\r\n    assert shardIndex != -1 : \"shardIndex is not set\";\r\n    return shardIndex;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.GeoDistanceSortBuilder.getNestedFilter",
	"Comment": "returns the nested filter that the nested objects should match with in order to be taken into accountfor sorting.",
	"Method": "QueryBuilder getNestedFilter(){\r\n    return this.nestedFilter;\r\n}"
}, {
	"Path": "org.elasticsearch.test.TestCluster.assertAfterTest",
	"Comment": "this method checks all the things that need to be checked after each test",
	"Method": "void assertAfterTest(){\r\n    ensureEstimatedStats();\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.sorts",
	"Comment": "gets the bytes representing the sort builders for this request.",
	"Method": "List<SortBuilder<?>> sorts(){\r\n    return sorts;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.process.autodetect.state.DataCounts.getOutOfOrderTimeStampCount",
	"Comment": "the number of records with a timestamp that isbefore the time of the latest record. records shouldbe in ascending chronological order",
	"Method": "long getOutOfOrderTimeStampCount(){\r\n    return outOfOrderTimeStampCount;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TcpTransport.consumeNetworkReads",
	"Comment": "consumes bytes that are available from network reads. this method returns the number of bytes consumedin this call.",
	"Method": "int consumeNetworkReads(TcpChannel channel,BytesReference bytesReference){\r\n    BytesReference message = decodeFrame(bytesReference);\r\n    if (message == null) {\r\n        return 0;\r\n    } else {\r\n        inboundMessage(channel, message);\r\n        return message.length() + BYTES_NEEDED_FOR_MESSAGE_SIZE;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.MasterDisruptionIT.testIsolateMasterAndVerifyClusterStateConsensus",
	"Comment": "this test isolates the master from rest of the cluster, waits for a new master to be elected, restores the partitionand verifies that all node agree on the new cluster state",
	"Method": "void testIsolateMasterAndVerifyClusterStateConsensus(){\r\n    final List<String> nodes = startCluster(3);\r\n    assertAcked(prepareCreate(\"test\").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1 + randomInt(2)).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, randomInt(2))));\r\n    ensureGreen();\r\n    String isolatedNode = internalCluster().getMasterName();\r\n    TwoPartitions partitions = isolateNode(isolatedNode);\r\n    NetworkDisruption networkDisruption = addRandomDisruptionType(partitions);\r\n    networkDisruption.startDisrupting();\r\n    String nonIsolatedNode = partitions.getMajoritySide().iterator().next();\r\n    ensureStableCluster(2, nonIsolatedNode);\r\n    assertNoMaster(isolatedNode, TimeValue.timeValueSeconds(40));\r\n    networkDisruption.stopDisrupting();\r\n    for (String node : nodes) {\r\n        ensureStableCluster(3, new TimeValue(DISRUPTION_HEALING_OVERHEAD.millis() + networkDisruption.expectedTimeToHeal().millis()), true, node);\r\n    }\r\n    logger.info(\"issue a reroute\");\r\n    assertAcked(client().admin().cluster().prepareReroute());\r\n    ensureGreen(\"test\");\r\n    assertBusy(() -> {\r\n        ClusterState state = null;\r\n        for (String node : nodes) {\r\n            ClusterState nodeState = getNodeClusterState(node);\r\n            if (state == null) {\r\n                state = nodeState;\r\n                continue;\r\n            }\r\n            try {\r\n                assertEquals(\"unequal versions\", state.version(), nodeState.version());\r\n                assertEquals(\"unequal node count\", state.nodes().getSize(), nodeState.nodes().getSize());\r\n                assertEquals(\"different masters \", state.nodes().getMasterNodeId(), nodeState.nodes().getMasterNodeId());\r\n                assertEquals(\"different meta data version\", state.metaData().version(), nodeState.metaData().version());\r\n                assertEquals(\"different routing\", state.routingTable().toString(), nodeState.routingTable().toString());\r\n            } catch (AssertionError t) {\r\n                fail(\"failed comparing cluster state: \" + t.getMessage() + \"\\n\" + \"--- cluster state of node [\" + nodes.get(0) + \"]: ---\\n\" + state + \"\\n--- cluster state [\" + node + \"]: ---\\n\" + nodeState);\r\n            }\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.ESRestTestCase.preserveTemplatesUponCompletion",
	"Comment": "controls whether or not to preserve templates upon completion of this test. the default implementation is to delete not preservetemplates.",
	"Method": "boolean preserveTemplatesUponCompletion(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractWireTestCase.assertSerialization",
	"Comment": "serialize the given instance and asserts that both are equal",
	"Method": "T assertSerialization(T testInstance,T assertSerialization,T testInstance,Version version){\r\n    T deserializedInstance = copyInstance(testInstance, version);\r\n    assertEqualInstances(testInstance, deserializedInstance);\r\n    return deserializedInstance;\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.beginSnapshot",
	"Comment": "starts snapshot.creates snapshot in repository and updates snapshot metadata record with list of shards that needs to be processed.",
	"Method": "void beginSnapshot(ClusterState clusterState,SnapshotsInProgress.Entry snapshot,boolean partial,CreateSnapshotListener userCreateSnapshotListener){\r\n    threadPool.executor(ThreadPool.Names.SNAPSHOT).execute(new AbstractRunnable() {\r\n        boolean snapshotCreated;\r\n        @Override\r\n        protected void doRun() {\r\n            Repository repository = repositoriesService.repository(snapshot.snapshot().getRepository());\r\n            MetaData metaData = clusterState.metaData();\r\n            if (!snapshot.includeGlobalState()) {\r\n                MetaData.Builder builder = MetaData.builder();\r\n                for (IndexId index : snapshot.indices()) {\r\n                    builder.put(metaData.index(index.getName()), false);\r\n                }\r\n                metaData = builder.build();\r\n            }\r\n            repository.initializeSnapshot(snapshot.snapshot().getSnapshotId(), snapshot.indices(), metaData);\r\n            snapshotCreated = true;\r\n            logger.info(\"snapshot [{}] started\", snapshot.snapshot());\r\n            if (snapshot.indices().isEmpty()) {\r\n                userCreateSnapshotListener.onResponse();\r\n                endSnapshot(snapshot);\r\n                return;\r\n            }\r\n            clusterService.submitStateUpdateTask(\"update_snapshot [\" + snapshot.snapshot() + \"]\", new ClusterStateUpdateTask() {\r\n                SnapshotsInProgress.Entry endSnapshot;\r\n                String failure;\r\n                @Override\r\n                public ClusterState execute(ClusterState currentState) {\r\n                    SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\r\n                    List<SnapshotsInProgress.Entry> entries = new ArrayList();\r\n                    for (SnapshotsInProgress.Entry entry : snapshots.entries()) {\r\n                        if (entry.snapshot().equals(snapshot.snapshot()) == false) {\r\n                            entries.add(entry);\r\n                            continue;\r\n                        }\r\n                        if (entry.state() != State.ABORTED) {\r\n                            ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards = shards(currentState, entry.indices());\r\n                            if (!partial) {\r\n                                Tuple<Set<String>, Set<String>> indicesWithMissingShards = indicesWithMissingShards(shards, currentState.metaData());\r\n                                Set<String> missing = indicesWithMissingShards.v1();\r\n                                Set<String> closed = indicesWithMissingShards.v2();\r\n                                if (missing.isEmpty() == false || closed.isEmpty() == false) {\r\n                                    endSnapshot = new SnapshotsInProgress.Entry(entry, State.FAILED, shards);\r\n                                    entries.add(endSnapshot);\r\n                                    final StringBuilder failureMessage = new StringBuilder();\r\n                                    if (missing.isEmpty() == false) {\r\n                                        failureMessage.append(\"Indices don't have primary shards \");\r\n                                        failureMessage.append(missing);\r\n                                    }\r\n                                    if (closed.isEmpty() == false) {\r\n                                        if (failureMessage.length() > 0) {\r\n                                            failureMessage.append(\"; \");\r\n                                        }\r\n                                        failureMessage.append(\"Indices are closed \");\r\n                                        failureMessage.append(closed);\r\n                                    }\r\n                                    failure = failureMessage.toString();\r\n                                    continue;\r\n                                }\r\n                            }\r\n                            SnapshotsInProgress.Entry updatedSnapshot = new SnapshotsInProgress.Entry(entry, State.STARTED, shards);\r\n                            entries.add(updatedSnapshot);\r\n                            if (completed(shards.values())) {\r\n                                endSnapshot = updatedSnapshot;\r\n                            }\r\n                        } else {\r\n                            assert entry.state() == State.ABORTED : \"expecting snapshot to be aborted during initialization\";\r\n                            failure = \"snapshot was aborted during initialization\";\r\n                            endSnapshot = entry;\r\n                            entries.add(endSnapshot);\r\n                        }\r\n                    }\r\n                    return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(Collections.unmodifiableList(entries))).build();\r\n                }\r\n                @Override\r\n                public void onFailure(String source, Exception e) {\r\n                    logger.warn(() -> new ParameterizedMessage(\"[{}] failed to create snapshot\", snapshot.snapshot().getSnapshotId()), e);\r\n                    removeSnapshotFromClusterState(snapshot.snapshot(), null, e, new CleanupAfterErrorListener(snapshot, true, userCreateSnapshotListener, e));\r\n                }\r\n                @Override\r\n                public void onNoLongerMaster(String source) {\r\n                    logger.warn(\"[{}] failed to create snapshot - no longer a master\", snapshot.snapshot().getSnapshotId());\r\n                    userCreateSnapshotListener.onFailure(new SnapshotException(snapshot.snapshot(), \"master changed during snapshot initialization\"));\r\n                }\r\n                @Override\r\n                public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\r\n                    userCreateSnapshotListener.onResponse();\r\n                    if (endSnapshot != null) {\r\n                        endSnapshot(endSnapshot, failure);\r\n                    }\r\n                }\r\n            });\r\n        }\r\n        @Override\r\n        public void onFailure(Exception e) {\r\n            logger.warn(() -> new ParameterizedMessage(\"failed to create snapshot [{}]\", snapshot.snapshot().getSnapshotId()), e);\r\n            removeSnapshotFromClusterState(snapshot.snapshot(), null, e, new CleanupAfterErrorListener(snapshot, snapshotCreated, userCreateSnapshotListener, e));\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.beginSnapshot",
	"Comment": "starts snapshot.creates snapshot in repository and updates snapshot metadata record with list of shards that needs to be processed.",
	"Method": "void beginSnapshot(ClusterState clusterState,SnapshotsInProgress.Entry snapshot,boolean partial,CreateSnapshotListener userCreateSnapshotListener){\r\n    Repository repository = repositoriesService.repository(snapshot.snapshot().getRepository());\r\n    MetaData metaData = clusterState.metaData();\r\n    if (!snapshot.includeGlobalState()) {\r\n        MetaData.Builder builder = MetaData.builder();\r\n        for (IndexId index : snapshot.indices()) {\r\n            builder.put(metaData.index(index.getName()), false);\r\n        }\r\n        metaData = builder.build();\r\n    }\r\n    repository.initializeSnapshot(snapshot.snapshot().getSnapshotId(), snapshot.indices(), metaData);\r\n    snapshotCreated = true;\r\n    logger.info(\"snapshot [{}] started\", snapshot.snapshot());\r\n    if (snapshot.indices().isEmpty()) {\r\n        userCreateSnapshotListener.onResponse();\r\n        endSnapshot(snapshot);\r\n        return;\r\n    }\r\n    clusterService.submitStateUpdateTask(\"update_snapshot [\" + snapshot.snapshot() + \"]\", new ClusterStateUpdateTask() {\r\n        SnapshotsInProgress.Entry endSnapshot;\r\n        String failure;\r\n        @Override\r\n        public ClusterState execute(ClusterState currentState) {\r\n            SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\r\n            List<SnapshotsInProgress.Entry> entries = new ArrayList();\r\n            for (SnapshotsInProgress.Entry entry : snapshots.entries()) {\r\n                if (entry.snapshot().equals(snapshot.snapshot()) == false) {\r\n                    entries.add(entry);\r\n                    continue;\r\n                }\r\n                if (entry.state() != State.ABORTED) {\r\n                    ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards = shards(currentState, entry.indices());\r\n                    if (!partial) {\r\n                        Tuple<Set<String>, Set<String>> indicesWithMissingShards = indicesWithMissingShards(shards, currentState.metaData());\r\n                        Set<String> missing = indicesWithMissingShards.v1();\r\n                        Set<String> closed = indicesWithMissingShards.v2();\r\n                        if (missing.isEmpty() == false || closed.isEmpty() == false) {\r\n                            endSnapshot = new SnapshotsInProgress.Entry(entry, State.FAILED, shards);\r\n                            entries.add(endSnapshot);\r\n                            final StringBuilder failureMessage = new StringBuilder();\r\n                            if (missing.isEmpty() == false) {\r\n                                failureMessage.append(\"Indices don't have primary shards \");\r\n                                failureMessage.append(missing);\r\n                            }\r\n                            if (closed.isEmpty() == false) {\r\n                                if (failureMessage.length() > 0) {\r\n                                    failureMessage.append(\"; \");\r\n                                }\r\n                                failureMessage.append(\"Indices are closed \");\r\n                                failureMessage.append(closed);\r\n                            }\r\n                            failure = failureMessage.toString();\r\n                            continue;\r\n                        }\r\n                    }\r\n                    SnapshotsInProgress.Entry updatedSnapshot = new SnapshotsInProgress.Entry(entry, State.STARTED, shards);\r\n                    entries.add(updatedSnapshot);\r\n                    if (completed(shards.values())) {\r\n                        endSnapshot = updatedSnapshot;\r\n                    }\r\n                } else {\r\n                    assert entry.state() == State.ABORTED : \"expecting snapshot to be aborted during initialization\";\r\n                    failure = \"snapshot was aborted during initialization\";\r\n                    endSnapshot = entry;\r\n                    entries.add(endSnapshot);\r\n                }\r\n            }\r\n            return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(Collections.unmodifiableList(entries))).build();\r\n        }\r\n        @Override\r\n        public void onFailure(String source, Exception e) {\r\n            logger.warn(() -> new ParameterizedMessage(\"[{}] failed to create snapshot\", snapshot.snapshot().getSnapshotId()), e);\r\n            removeSnapshotFromClusterState(snapshot.snapshot(), null, e, new CleanupAfterErrorListener(snapshot, true, userCreateSnapshotListener, e));\r\n        }\r\n        @Override\r\n        public void onNoLongerMaster(String source) {\r\n            logger.warn(\"[{}] failed to create snapshot - no longer a master\", snapshot.snapshot().getSnapshotId());\r\n            userCreateSnapshotListener.onFailure(new SnapshotException(snapshot.snapshot(), \"master changed during snapshot initialization\"));\r\n        }\r\n        @Override\r\n        public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\r\n            userCreateSnapshotListener.onResponse();\r\n            if (endSnapshot != null) {\r\n                endSnapshot(endSnapshot, failure);\r\n            }\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.beginSnapshot",
	"Comment": "starts snapshot.creates snapshot in repository and updates snapshot metadata record with list of shards that needs to be processed.",
	"Method": "void beginSnapshot(ClusterState clusterState,SnapshotsInProgress.Entry snapshot,boolean partial,CreateSnapshotListener userCreateSnapshotListener){\r\n    SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\r\n    List<SnapshotsInProgress.Entry> entries = new ArrayList();\r\n    for (SnapshotsInProgress.Entry entry : snapshots.entries()) {\r\n        if (entry.snapshot().equals(snapshot.snapshot()) == false) {\r\n            entries.add(entry);\r\n            continue;\r\n        }\r\n        if (entry.state() != State.ABORTED) {\r\n            ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards = shards(currentState, entry.indices());\r\n            if (!partial) {\r\n                Tuple<Set<String>, Set<String>> indicesWithMissingShards = indicesWithMissingShards(shards, currentState.metaData());\r\n                Set<String> missing = indicesWithMissingShards.v1();\r\n                Set<String> closed = indicesWithMissingShards.v2();\r\n                if (missing.isEmpty() == false || closed.isEmpty() == false) {\r\n                    endSnapshot = new SnapshotsInProgress.Entry(entry, State.FAILED, shards);\r\n                    entries.add(endSnapshot);\r\n                    final StringBuilder failureMessage = new StringBuilder();\r\n                    if (missing.isEmpty() == false) {\r\n                        failureMessage.append(\"Indices don't have primary shards \");\r\n                        failureMessage.append(missing);\r\n                    }\r\n                    if (closed.isEmpty() == false) {\r\n                        if (failureMessage.length() > 0) {\r\n                            failureMessage.append(\"; \");\r\n                        }\r\n                        failureMessage.append(\"Indices are closed \");\r\n                        failureMessage.append(closed);\r\n                    }\r\n                    failure = failureMessage.toString();\r\n                    continue;\r\n                }\r\n            }\r\n            SnapshotsInProgress.Entry updatedSnapshot = new SnapshotsInProgress.Entry(entry, State.STARTED, shards);\r\n            entries.add(updatedSnapshot);\r\n            if (completed(shards.values())) {\r\n                endSnapshot = updatedSnapshot;\r\n            }\r\n        } else {\r\n            assert entry.state() == State.ABORTED : \"expecting snapshot to be aborted during initialization\";\r\n            failure = \"snapshot was aborted during initialization\";\r\n            endSnapshot = entry;\r\n            entries.add(endSnapshot);\r\n        }\r\n    }\r\n    return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, new SnapshotsInProgress(Collections.unmodifiableList(entries))).build();\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.beginSnapshot",
	"Comment": "starts snapshot.creates snapshot in repository and updates snapshot metadata record with list of shards that needs to be processed.",
	"Method": "void beginSnapshot(ClusterState clusterState,SnapshotsInProgress.Entry snapshot,boolean partial,CreateSnapshotListener userCreateSnapshotListener){\r\n    logger.warn(() -> new ParameterizedMessage(\"[{}] failed to create snapshot\", snapshot.snapshot().getSnapshotId()), e);\r\n    removeSnapshotFromClusterState(snapshot.snapshot(), null, e, new CleanupAfterErrorListener(snapshot, true, userCreateSnapshotListener, e));\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.beginSnapshot",
	"Comment": "starts snapshot.creates snapshot in repository and updates snapshot metadata record with list of shards that needs to be processed.",
	"Method": "void beginSnapshot(ClusterState clusterState,SnapshotsInProgress.Entry snapshot,boolean partial,CreateSnapshotListener userCreateSnapshotListener){\r\n    logger.warn(\"[{}] failed to create snapshot - no longer a master\", snapshot.snapshot().getSnapshotId());\r\n    userCreateSnapshotListener.onFailure(new SnapshotException(snapshot.snapshot(), \"master changed during snapshot initialization\"));\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.beginSnapshot",
	"Comment": "starts snapshot.creates snapshot in repository and updates snapshot metadata record with list of shards that needs to be processed.",
	"Method": "void beginSnapshot(ClusterState clusterState,SnapshotsInProgress.Entry snapshot,boolean partial,CreateSnapshotListener userCreateSnapshotListener){\r\n    userCreateSnapshotListener.onResponse();\r\n    if (endSnapshot != null) {\r\n        endSnapshot(endSnapshot, failure);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.beginSnapshot",
	"Comment": "starts snapshot.creates snapshot in repository and updates snapshot metadata record with list of shards that needs to be processed.",
	"Method": "void beginSnapshot(ClusterState clusterState,SnapshotsInProgress.Entry snapshot,boolean partial,CreateSnapshotListener userCreateSnapshotListener){\r\n    logger.warn(() -> new ParameterizedMessage(\"failed to create snapshot [{}]\", snapshot.snapshot().getSnapshotId()), e);\r\n    removeSnapshotFromClusterState(snapshot.snapshot(), null, e, new CleanupAfterErrorListener(snapshot, snapshotCreated, userCreateSnapshotListener, e));\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractStreamableXContentTestCase.testFromXContent",
	"Comment": "generic test that creates new instance from the test instance and checksboth for equality and asserts equality on the two queries.",
	"Method": "void testFromXContent(){\r\n    AbstractXContentTestCase.testFromXContent(NUMBER_OF_TEST_RUNS, this::createTestInstance, supportsUnknownFields(), getShuffleFieldsExceptions(), getRandomFieldsExcludeFilter(), this::createParser, this::doParseInstance, this::assertEqualInstances, true, getToXContentParams());\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilderTests.copyBuilder",
	"Comment": "we use the streaming infra to create a copy of the builder provided as argument",
	"Method": "SearchSourceBuilder copyBuilder(SearchSourceBuilder original){\r\n    return ESTestCase.copyWriteable(original, namedWriteableRegistry, SearchSourceBuilder::new);\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.snapshots",
	"Comment": "returns a list of snapshots from repository sorted by snapshot creation date",
	"Method": "List<SnapshotInfo> snapshots(String repositoryName,List<SnapshotId> snapshotIds,Set<SnapshotId> incompatibleSnapshotIds,boolean ignoreUnavailable){\r\n    final Set<SnapshotInfo> snapshotSet = new HashSet();\r\n    final Set<SnapshotId> snapshotIdsToIterate = new HashSet(snapshotIds);\r\n    final List<SnapshotsInProgress.Entry> entries = currentSnapshots(repositoryName, snapshotIdsToIterate.stream().map(SnapshotId::getName).collect(Collectors.toList()));\r\n    for (SnapshotsInProgress.Entry entry : entries) {\r\n        snapshotSet.add(inProgressSnapshot(entry));\r\n        snapshotIdsToIterate.remove(entry.snapshot().getSnapshotId());\r\n    }\r\n    final Repository repository = repositoriesService.repository(repositoryName);\r\n    for (SnapshotId snapshotId : snapshotIdsToIterate) {\r\n        try {\r\n            if (incompatibleSnapshotIds.contains(snapshotId)) {\r\n                snapshotSet.add(SnapshotInfo.incompatible(snapshotId));\r\n            } else {\r\n                snapshotSet.add(repository.getSnapshotInfo(snapshotId));\r\n            }\r\n        } catch (Exception ex) {\r\n            if (ignoreUnavailable) {\r\n                logger.warn(() -> new ParameterizedMessage(\"failed to get snapshot [{}]\", snapshotId), ex);\r\n            } else {\r\n                throw new SnapshotException(repositoryName, snapshotId, \"Snapshot could not be read\", ex);\r\n            }\r\n        }\r\n    }\r\n    final ArrayList<SnapshotInfo> snapshotList = new ArrayList(snapshotSet);\r\n    CollectionUtil.timSort(snapshotList);\r\n    return Collections.unmodifiableList(snapshotList);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregationBuilder.interval",
	"Comment": "set the interval on this builder, and return the builder so that calls can be chained.",
	"Method": "double interval(HistogramAggregationBuilder interval,double interval){\r\n    if (interval <= 0) {\r\n        throw new IllegalArgumentException(\"[interval] must be >0 for histogram aggregation [\" + name + \"]\");\r\n    }\r\n    this.interval = interval;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.terms.TermsAggregationBuilder.minDocCount",
	"Comment": "set the minimum document count terms should have in order to appear inthe response.",
	"Method": "TermsAggregationBuilder minDocCount(long minDocCount,long minDocCount){\r\n    return bucketCountThresholds.getMinDocCount();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShardTestCase.reinitShard",
	"Comment": "takes an existing shard, closes it and starts a new initialing shard at the same location",
	"Method": "IndexShard reinitShard(IndexShard current,IndexingOperationListener listeners,IndexShard reinitShard,IndexShard current,ShardRouting routing,IndexingOperationListener listeners,IndexShard reinitShard,IndexShard current,ShardRouting routing,EngineFactory engineFactory,IndexingOperationListener listeners){\r\n    closeShards(current);\r\n    return newShard(routing, current.shardPath(), current.indexSettings().getIndexMetaData(), null, null, engineFactory, current.getGlobalCheckpointSyncer(), EMPTY_EVENT_LISTENER, listeners);\r\n}"
}, {
	"Path": "org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertSearchResponse",
	"Comment": "applies basic assertions on the searchresponse. this method checks if all shards were successful, ifany of the shards threw an exception and if the response is serializable.",
	"Method": "SearchResponse assertSearchResponse(SearchRequestBuilder request,SearchResponse assertSearchResponse,SearchResponse response){\r\n    assertNoFailures(response);\r\n    return response;\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequestTests.testSerializationOfEmptyRequestWorks",
	"Comment": "reindex makes use of index requests without a source so this needs to be handled",
	"Method": "void testSerializationOfEmptyRequestWorks(){\r\n    IndexRequest request = new IndexRequest(\"index\", \"type\");\r\n    assertNull(request.getContentType());\r\n    try (BytesStreamOutput out = new BytesStreamOutput()) {\r\n        request.writeTo(out);\r\n        try (StreamInput in = out.bytes().streamInput()) {\r\n            IndexRequest serialized = new IndexRequest();\r\n            serialized.readFrom(in);\r\n            assertNull(request.getContentType());\r\n            assertEquals(\"index\", request.index());\r\n            assertEquals(\"type\", request.type());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.EngineTestCase.getDocIds",
	"Comment": "gets a collection of tuples of docid, sequence number, and primary term of all live documents in the provided engine.",
	"Method": "List<DocIdSeqNoAndTerm> getDocIds(Engine engine,boolean refresh){\r\n    if (refresh) {\r\n        engine.refresh(\"test_get_doc_ids\");\r\n    }\r\n    try (Engine.Searcher searcher = engine.acquireSearcher(\"test_get_doc_ids\")) {\r\n        List<DocIdSeqNoAndTerm> docs = new ArrayList();\r\n        for (LeafReaderContext leafContext : searcher.reader().leaves()) {\r\n            LeafReader reader = leafContext.reader();\r\n            NumericDocValues seqNoDocValues = reader.getNumericDocValues(SeqNoFieldMapper.NAME);\r\n            NumericDocValues primaryTermDocValues = reader.getNumericDocValues(SeqNoFieldMapper.PRIMARY_TERM_NAME);\r\n            Bits liveDocs = reader.getLiveDocs();\r\n            for (int i = 0; i < reader.maxDoc(); i++) {\r\n                if (liveDocs == null || liveDocs.get(i)) {\r\n                    if (primaryTermDocValues.advanceExact(i) == false) {\r\n                        continue;\r\n                    }\r\n                    final long primaryTerm = primaryTermDocValues.longValue();\r\n                    Document uuid = reader.document(i, Collections.singleton(IdFieldMapper.NAME));\r\n                    BytesRef binaryID = uuid.getBinaryValue(IdFieldMapper.NAME);\r\n                    String id = Uid.decodeId(Arrays.copyOfRange(binaryID.bytes, binaryID.offset, binaryID.offset + binaryID.length));\r\n                    if (seqNoDocValues.advanceExact(i) == false) {\r\n                        throw new AssertionError(\"seqNoDocValues not found for doc[\" + i + \"] id[\" + id + \"]\");\r\n                    }\r\n                    final long seqNo = seqNoDocValues.longValue();\r\n                    docs.add(new DocIdSeqNoAndTerm(id, seqNo, primaryTerm));\r\n                }\r\n            }\r\n        }\r\n        docs.sort(Comparator.comparing(DocIdSeqNoAndTerm::getId).thenComparingLong(DocIdSeqNoAndTerm::getSeqNo).thenComparingLong(DocIdSeqNoAndTerm::getPrimaryTerm));\r\n        return docs;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.SuggestionBuilder.analyzer",
	"Comment": "sets the analyzer to analyse to suggest text with. defaults to the searchanalyzer of the suggest field.",
	"Method": "T analyzer(String analyzer,String analyzer){\r\n    return this.analyzer;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.stats.IndicesStatsTests.newIndicesStatsResponse",
	"Comment": "gives access to package private indicesstatsresponse constructor for test purpose.",
	"Method": "IndicesStatsResponse newIndicesStatsResponse(ShardStats[] shards,int totalShards,int successfulShards,int failedShards,List<DefaultShardOperationFailedException> shardFailures){\r\n    return new IndicesStatsResponse(shards, totalShards, successfulShards, failedShards, shardFailures);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificanceHeuristicTests.createInternalAggregations",
	"Comment": "create aggregations as they might come from three different shards and return as list.",
	"Method": "List<InternalAggregation> createInternalAggregations(){\r\n    SignificanceHeuristic significanceHeuristic = getRandomSignificanceheuristic();\r\n    TestAggFactory<?, ?> factory = randomBoolean() ? new StringTestAggFactory() : new LongTestAggFactory();\r\n    List<InternalAggregation> aggs = new ArrayList();\r\n    aggs.add(factory.createAggregation(significanceHeuristic, 4, 10, 1, (f, i) -> f.createBucket(4, 4, 5, 10, 0)));\r\n    aggs.add(factory.createAggregation(significanceHeuristic, 4, 10, 1, (f, i) -> f.createBucket(4, 4, 5, 10, 1)));\r\n    aggs.add(factory.createAggregation(significanceHeuristic, 8, 10, 2, (f, i) -> f.createBucket(4, 4, 5, 10, i)));\r\n    return aggs;\r\n}"
}, {
	"Path": "org.elasticsearch.search.SearchHit.toInnerXContent",
	"Comment": "public because we render hit as part of completion suggestion option",
	"Method": "XContentBuilder toInnerXContent(XContentBuilder builder,Params params){\r\n    List<DocumentField> metaFields = new ArrayList();\r\n    List<DocumentField> otherFields = new ArrayList();\r\n    if (fields != null && !fields.isEmpty()) {\r\n        for (DocumentField field : fields.values()) {\r\n            if (field.getValues().isEmpty()) {\r\n                continue;\r\n            }\r\n            if (field.isMetadataField()) {\r\n                metaFields.add(field);\r\n            } else {\r\n                otherFields.add(field);\r\n            }\r\n        }\r\n    }\r\n    if (getExplanation() != null && shard != null) {\r\n        builder.field(Fields._SHARD, shard.getShardId());\r\n        builder.field(Fields._NODE, shard.getNodeIdText());\r\n    }\r\n    if (index != null) {\r\n        builder.field(Fields._INDEX, RemoteClusterAware.buildRemoteIndexName(clusterAlias, index));\r\n    }\r\n    if (type != null) {\r\n        builder.field(Fields._TYPE, type);\r\n    }\r\n    if (id != null) {\r\n        builder.field(Fields._ID, id);\r\n    }\r\n    if (nestedIdentity != null) {\r\n        nestedIdentity.toXContent(builder, params);\r\n    }\r\n    if (version != -1) {\r\n        builder.field(Fields._VERSION, version);\r\n    }\r\n    if (Float.isNaN(score)) {\r\n        builder.nullField(Fields._SCORE);\r\n    } else {\r\n        builder.field(Fields._SCORE, score);\r\n    }\r\n    for (DocumentField field : metaFields) {\r\n        if (field.getName().equals(IgnoredFieldMapper.NAME)) {\r\n            builder.field(field.getName(), field.getValues());\r\n        } else {\r\n            builder.field(field.getName(), field.<Object>getValue());\r\n        }\r\n    }\r\n    if (source != null) {\r\n        XContentHelper.writeRawField(SourceFieldMapper.NAME, source, builder, params);\r\n    }\r\n    if (!otherFields.isEmpty()) {\r\n        builder.startObject(Fields.FIELDS);\r\n        for (DocumentField field : otherFields) {\r\n            field.toXContent(builder, params);\r\n        }\r\n        builder.endObject();\r\n    }\r\n    if (highlightFields != null && !highlightFields.isEmpty()) {\r\n        builder.startObject(Fields.HIGHLIGHT);\r\n        for (HighlightField field : highlightFields.values()) {\r\n            field.toXContent(builder, params);\r\n        }\r\n        builder.endObject();\r\n    }\r\n    sortValues.toXContent(builder, params);\r\n    if (matchedQueries.length > 0) {\r\n        builder.startArray(Fields.MATCHED_QUERIES);\r\n        for (String matchedFilter : matchedQueries) {\r\n            builder.value(matchedFilter);\r\n        }\r\n        builder.endArray();\r\n    }\r\n    if (getExplanation() != null) {\r\n        builder.field(Fields._EXPLANATION);\r\n        buildExplanation(builder, getExplanation());\r\n    }\r\n    if (innerHits != null) {\r\n        builder.startObject(Fields.INNER_HITS);\r\n        for (Map.Entry<String, SearchHits> entry : innerHits.entrySet()) {\r\n            builder.startObject(entry.getKey());\r\n            entry.getValue().toXContent(builder, params);\r\n            builder.endObject();\r\n        }\r\n        builder.endObject();\r\n    }\r\n    return builder;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds.parseAndValidate",
	"Comment": "parse the bounds and perform any delayed validation. returns the result of the parsing.",
	"Method": "ExtendedBounds parseAndValidate(String aggName,SearchContext context,DocValueFormat format){\r\n    Long min = this.min;\r\n    Long max = this.max;\r\n    assert format != null;\r\n    if (minAsStr != null) {\r\n        min = format.parseLong(minAsStr, false, context.getQueryShardContext()::nowInMillis);\r\n    }\r\n    if (maxAsStr != null) {\r\n        max = format.parseLong(maxAsStr, false, context.getQueryShardContext()::nowInMillis);\r\n    }\r\n    if (min != null && max != null && min.compareTo(max) > 0) {\r\n        throw new SearchParseException(context, \"[extended_bounds.min][\" + min + \"] cannot be greater than \" + \"[extended_bounds.max][\" + max + \"] for histogram aggregation [\" + aggName + \"]\", null);\r\n    }\r\n    return new ExtendedBounds(min, max, minAsStr, maxAsStr);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.BucketsAggregator.maxBucketOrd",
	"Comment": "return an upper bound of the maximum bucket ordinal seen so far.",
	"Method": "long maxBucketOrd(){\r\n    return docCounts.size();\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.completion.CompletionSuggestionBuilder.regex",
	"Comment": "sets a regular expression pattern for prefixes to provide completions for.",
	"Method": "CompletionSuggestionBuilder regex(String regex,CompletionSuggestionBuilder regex,String regex,RegexOptions regexOptions){\r\n    this.regex(regex);\r\n    this.regexOptions = regexOptions;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.forceMerge",
	"Comment": "waits for all relocations and force merge all indices in the cluster to 1 segment.",
	"Method": "ForceMergeResponse forceMerge(){\r\n    waitForRelocation();\r\n    ForceMergeResponse actionGet = client().admin().indices().prepareForceMerge().setMaxNumSegments(1).execute().actionGet();\r\n    assertNoFailures(actionGet);\r\n    return actionGet;\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotShardsService.currentSnapshotShards",
	"Comment": "returns status of shards that are snapshotted on the node and belong to the given snapshotthis method is executed on data node",
	"Method": "Map<ShardId, IndexShardSnapshotStatus> currentSnapshotShards(Snapshot snapshot){\r\n    return shardSnapshots.get(snapshot);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.RangeQueryBuilderTests.testToQueryUnmappedWithTimezone",
	"Comment": "specifying a timezone together with an unmapped field should throw an exception.",
	"Method": "void testToQueryUnmappedWithTimezone(){\r\n    RangeQueryBuilder query = new RangeQueryBuilder(\"bogus_field\");\r\n    query.from(1).to(10).timeZone(\"UTC\");\r\n    QueryShardException e = expectThrows(QueryShardException.class, () -> query.toQuery(createShardContext()));\r\n    assertThat(e.getMessage(), containsString(\"[range] time_zone can not be applied\"));\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.config.Job.getResultsIndexNameNoPrefix",
	"Comment": "private version of getresultsindexname so that a job can be built from anotherjob and pass index name validation",
	"Method": "String getResultsIndexNameNoPrefix(){\r\n    return resultsIndexName;\r\n}"
}, {
	"Path": "org.elasticsearch.search.profile.query.QueryProfilerIT.testNoProfile",
	"Comment": "this test makes sure no profile results are returned when profiling is disabled",
	"Method": "void testNoProfile(){\r\n    createIndex(\"test\");\r\n    ensureGreen();\r\n    int numDocs = randomIntBetween(100, 150);\r\n    IndexRequestBuilder[] docs = new IndexRequestBuilder[numDocs];\r\n    for (int i = 0; i < numDocs; i++) {\r\n        docs[i] = client().prepareIndex(\"test\", \"type1\", String.valueOf(i)).setSource(\"field1\", English.intToEnglish(i), \"field2\", i);\r\n    }\r\n    indexRandom(true, docs);\r\n    refresh();\r\n    QueryBuilder q = QueryBuilders.rangeQuery(\"field2\").from(0).to(5);\r\n    logger.info(\"Query: {}\", q);\r\n    SearchResponse resp = client().prepareSearch().setQuery(q).setProfile(false).execute().actionGet();\r\n    assertThat(\"Profile response element should be an empty map\", resp.getProfileResults().size(), equalTo(0));\r\n}"
}, {
	"Path": "org.elasticsearch.search.fetch.subphase.highlight.HighlightBuilderTests.testSerialization",
	"Comment": "test serialization and deserialization of the highlighter builder",
	"Method": "void testSerialization(){\r\n    for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {\r\n        HighlightBuilder original = randomHighlighterBuilder();\r\n        HighlightBuilder deserialized = serializedCopy(original);\r\n        assertEquals(deserialized, original);\r\n        assertEquals(deserialized.hashCode(), original.hashCode());\r\n        assertNotSame(deserialized, original);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.MovAvgPipelineAggregationBuilder.model",
	"Comment": "gets a movavgmodel for the moving average. the model is used todefine what type of moving average you want to use on the series",
	"Method": "MovAvgPipelineAggregationBuilder model(MovAvgModel model,MovAvgModel model){\r\n    return model;\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.TaskManager.waitForTaskCompletion",
	"Comment": "blocks the calling thread, waiting for the task to vanish from the taskmanager.",
	"Method": "void waitForTaskCompletion(Task task,long untilInNanos){\r\n    while (System.nanoTime() - untilInNanos < 0) {\r\n        if (getTask(task.getId()) == null) {\r\n            return;\r\n        }\r\n        try {\r\n            Thread.sleep(WAIT_FOR_COMPLETION_POLL.millis());\r\n        } catch (InterruptedException e) {\r\n            throw new ElasticsearchException(\"Interrupted waiting for completion of [{}]\", e, task);\r\n        }\r\n    }\r\n    throw new ElasticsearchTimeoutException(\"Timed out waiting for completion of [{}]\", task);\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.ensureAtMostNumDataNodes",
	"Comment": "ensures that at most n are up and running.if less nodes that n are running this methodwill not start any additional nodes.",
	"Method": "void ensureAtMostNumDataNodes(int n){\r\n    int size = numDataNodes();\r\n    if (size <= n) {\r\n        return;\r\n    }\r\n    final Stream<NodeAndClient> collection = n == 0 ? nodes.values().stream() : nodes.values().stream().filter(new DataNodePredicate().and(new MasterNodePredicate(getMasterName()).negate()));\r\n    final Iterator<NodeAndClient> values = collection.iterator();\r\n    logger.info(\"changing cluster size from {} data nodes to {}\", size, n);\r\n    Set<NodeAndClient> nodesToRemove = new HashSet();\r\n    int numNodesAndClients = 0;\r\n    while (values.hasNext() && numNodesAndClients++ < size - n) {\r\n        NodeAndClient next = values.next();\r\n        nodesToRemove.add(next);\r\n    }\r\n    stopNodesAndClients(nodesToRemove);\r\n    if (!nodesToRemove.isEmpty() && size() > 0) {\r\n        validateClusterFormed();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.license.XPackLicenseState.isStatsAndHealthAllowed",
	"Comment": "indicates whether the stats and health api calls should be allowed. if a license is expired and past the graceperiod then we deny these calls.",
	"Method": "boolean isStatsAndHealthAllowed(){\r\n    return status.active;\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.TaskManager.removeBan",
	"Comment": "removes the ban for the specified parent task.this method is called when a previously banned task finally cancelled",
	"Method": "void removeBan(TaskId parentTaskId){\r\n    logger.trace(\"removing ban for the parent task {}\", parentTaskId);\r\n    banedParents.remove(parentTaskId);\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotUtils.filterIndices",
	"Comment": "filters out list of available indices based on the list of selected indices.",
	"Method": "List<String> filterIndices(List<String> availableIndices,String[] selectedIndices,IndicesOptions indicesOptions){\r\n    if (IndexNameExpressionResolver.isAllIndices(Arrays.asList(selectedIndices))) {\r\n        return availableIndices;\r\n    }\r\n    Set<String> result = null;\r\n    for (int i = 0; i < selectedIndices.length; i++) {\r\n        String indexOrPattern = selectedIndices[i];\r\n        boolean add = true;\r\n        if (!indexOrPattern.isEmpty()) {\r\n            if (availableIndices.contains(indexOrPattern)) {\r\n                if (result == null) {\r\n                    result = new HashSet();\r\n                }\r\n                result.add(indexOrPattern);\r\n                continue;\r\n            }\r\n            if (indexOrPattern.charAt(0) == '+') {\r\n                add = true;\r\n                indexOrPattern = indexOrPattern.substring(1);\r\n                if (i == 0) {\r\n                    result = new HashSet();\r\n                }\r\n            } else if (indexOrPattern.charAt(0) == '-') {\r\n                if (i == 0) {\r\n                    result = new HashSet(availableIndices);\r\n                }\r\n                add = false;\r\n                indexOrPattern = indexOrPattern.substring(1);\r\n            }\r\n        }\r\n        if (indexOrPattern.isEmpty() || !Regex.isSimpleMatchPattern(indexOrPattern)) {\r\n            if (!availableIndices.contains(indexOrPattern)) {\r\n                if (!indicesOptions.ignoreUnavailable()) {\r\n                    throw new IndexNotFoundException(indexOrPattern);\r\n                } else {\r\n                    if (result == null) {\r\n                        result = new HashSet(availableIndices.subList(0, i));\r\n                    }\r\n                }\r\n            } else {\r\n                if (result != null) {\r\n                    if (add) {\r\n                        result.add(indexOrPattern);\r\n                    } else {\r\n                        result.remove(indexOrPattern);\r\n                    }\r\n                }\r\n            }\r\n            continue;\r\n        }\r\n        if (result == null) {\r\n            result = new HashSet(availableIndices.subList(0, i));\r\n        }\r\n        boolean found = false;\r\n        for (String index : availableIndices) {\r\n            if (Regex.simpleMatch(indexOrPattern, index)) {\r\n                found = true;\r\n                if (add) {\r\n                    result.add(index);\r\n                } else {\r\n                    result.remove(index);\r\n                }\r\n            }\r\n        }\r\n        if (!found && !indicesOptions.allowNoIndices()) {\r\n            throw new IndexNotFoundException(indexOrPattern);\r\n        }\r\n    }\r\n    if (result == null) {\r\n        return Collections.unmodifiableList(new ArrayList(Arrays.asList(selectedIndices)));\r\n    }\r\n    return Collections.unmodifiableList(new ArrayList(result));\r\n}"
}, {
	"Path": "org.elasticsearch.action.get.GetResponseTests.testFromXContentWithRandomFields",
	"Comment": "this test adds random fields and objects to the xcontent rendered out toensure we can parse it back to be forward compatible with additions tothe xcontent",
	"Method": "void testFromXContentWithRandomFields(){\r\n    doFromXContentTestWithRandomFields(true);\r\n}"
}, {
	"Path": "org.elasticsearch.rest.action.search.RestSearchAction.parseSearchSource",
	"Comment": "parses the rest request on top of the searchsourcebuilder, preservingvalues that are not overridden by the rest request.",
	"Method": "void parseSearchSource(SearchSourceBuilder searchSourceBuilder,RestRequest request,IntConsumer setSize){\r\n    QueryBuilder queryBuilder = RestActions.urlParamsToQueryBuilder(request);\r\n    if (queryBuilder != null) {\r\n        searchSourceBuilder.query(queryBuilder);\r\n    }\r\n    int from = request.paramAsInt(\"from\", -1);\r\n    if (from != -1) {\r\n        searchSourceBuilder.from(from);\r\n    }\r\n    int size = request.paramAsInt(\"size\", -1);\r\n    if (size != -1) {\r\n        setSize.accept(size);\r\n    }\r\n    if (request.hasParam(\"explain\")) {\r\n        searchSourceBuilder.explain(request.paramAsBoolean(\"explain\", null));\r\n    }\r\n    if (request.hasParam(\"version\")) {\r\n        searchSourceBuilder.version(request.paramAsBoolean(\"version\", null));\r\n    }\r\n    if (request.hasParam(\"timeout\")) {\r\n        searchSourceBuilder.timeout(request.paramAsTime(\"timeout\", null));\r\n    }\r\n    if (request.hasParam(\"terminate_after\")) {\r\n        int terminateAfter = request.paramAsInt(\"terminate_after\", SearchContext.DEFAULT_TERMINATE_AFTER);\r\n        if (terminateAfter < 0) {\r\n            throw new IllegalArgumentException(\"terminateAfter must be > 0\");\r\n        } else if (terminateAfter > 0) {\r\n            searchSourceBuilder.terminateAfter(terminateAfter);\r\n        }\r\n    }\r\n    StoredFieldsContext storedFieldsContext = StoredFieldsContext.fromRestRequest(SearchSourceBuilder.STORED_FIELDS_FIELD.getPreferredName(), request);\r\n    if (storedFieldsContext != null) {\r\n        searchSourceBuilder.storedFields(storedFieldsContext);\r\n    }\r\n    String sDocValueFields = request.param(\"docvalue_fields\");\r\n    if (sDocValueFields != null) {\r\n        if (Strings.hasText(sDocValueFields)) {\r\n            String[] sFields = Strings.splitStringByCommaToArray(sDocValueFields);\r\n            for (String field : sFields) {\r\n                searchSourceBuilder.docValueField(field, null);\r\n            }\r\n        }\r\n    }\r\n    FetchSourceContext fetchSourceContext = FetchSourceContext.parseFromRestRequest(request);\r\n    if (fetchSourceContext != null) {\r\n        searchSourceBuilder.fetchSource(fetchSourceContext);\r\n    }\r\n    if (request.hasParam(\"track_scores\")) {\r\n        searchSourceBuilder.trackScores(request.paramAsBoolean(\"track_scores\", false));\r\n    }\r\n    if (request.hasParam(\"track_total_hits\")) {\r\n        searchSourceBuilder.trackTotalHits(request.paramAsBoolean(\"track_total_hits\", true));\r\n    }\r\n    String sSorts = request.param(\"sort\");\r\n    if (sSorts != null) {\r\n        String[] sorts = Strings.splitStringByCommaToArray(sSorts);\r\n        for (String sort : sorts) {\r\n            int delimiter = sort.lastIndexOf(\":\");\r\n            if (delimiter != -1) {\r\n                String sortField = sort.substring(0, delimiter);\r\n                String reverse = sort.substring(delimiter + 1);\r\n                if (\"asc\".equals(reverse)) {\r\n                    searchSourceBuilder.sort(sortField, SortOrder.ASC);\r\n                } else if (\"desc\".equals(reverse)) {\r\n                    searchSourceBuilder.sort(sortField, SortOrder.DESC);\r\n                }\r\n            } else {\r\n                searchSourceBuilder.sort(sort);\r\n            }\r\n        }\r\n    }\r\n    String sStats = request.param(\"stats\");\r\n    if (sStats != null) {\r\n        searchSourceBuilder.stats(Arrays.asList(Strings.splitStringByCommaToArray(sStats)));\r\n    }\r\n    String suggestField = request.param(\"suggest_field\");\r\n    if (suggestField != null) {\r\n        String suggestText = request.param(\"suggest_text\", request.param(\"q\"));\r\n        int suggestSize = request.paramAsInt(\"suggest_size\", 5);\r\n        String suggestMode = request.param(\"suggest_mode\");\r\n        searchSourceBuilder.suggest(new SuggestBuilder().addSuggestion(suggestField, termSuggestion(suggestField).text(suggestText).size(suggestSize).suggestMode(SuggestMode.resolve(suggestMode))));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateResponseTests.testFromXContentWithRandomFields",
	"Comment": "this test adds random fields and objects to the xcontent rendered out toensure we can parse it back to be forward compatible with additions tothe xcontent",
	"Method": "void testFromXContentWithRandomFields(){\r\n    doFromXContentTestWithRandomFields(true);\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.FieldSortBuilderTests.testMultiValueMode",
	"Comment": "test that the sort builder mode gets transferred correctly to the sortfield",
	"Method": "void testMultiValueMode(){\r\n    QueryShardContext shardContextMock = createMockShardContext();\r\n    FieldSortBuilder sortBuilder = new FieldSortBuilder(\"value\").sortMode(SortMode.MIN);\r\n    SortField sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField, instanceOf(SortedNumericSortField.class));\r\n    SortedNumericSortField numericSortField = (SortedNumericSortField) sortField;\r\n    assertEquals(SortedNumericSelector.Type.MIN, numericSortField.getSelector());\r\n    sortBuilder = new FieldSortBuilder(\"value\").sortMode(SortMode.MAX);\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField, instanceOf(SortedNumericSortField.class));\r\n    numericSortField = (SortedNumericSortField) sortField;\r\n    assertEquals(SortedNumericSelector.Type.MAX, numericSortField.getSelector());\r\n    sortBuilder = new FieldSortBuilder(\"value\").sortMode(SortMode.SUM);\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    XFieldComparatorSource comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertEquals(MultiValueMode.SUM, comparatorSource.sortMode());\r\n    sortBuilder = new FieldSortBuilder(\"value\").sortMode(SortMode.AVG);\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertEquals(MultiValueMode.AVG, comparatorSource.sortMode());\r\n    sortBuilder = new FieldSortBuilder(\"value\").sortMode(SortMode.MEDIAN);\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    assertEquals(MultiValueMode.MEDIAN, comparatorSource.sortMode());\r\n    sortBuilder = new FieldSortBuilder(\"value\");\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField, instanceOf(SortedNumericSortField.class));\r\n    numericSortField = (SortedNumericSortField) sortField;\r\n    assertEquals(SortedNumericSelector.Type.MIN, numericSortField.getSelector());\r\n    sortBuilder = new FieldSortBuilder(\"value\").order(SortOrder.DESC);\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField, instanceOf(SortedNumericSortField.class));\r\n    numericSortField = (SortedNumericSortField) sortField;\r\n    assertEquals(SortedNumericSelector.Type.MAX, numericSortField.getSelector());\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.RandomDocumentPicks.addRandomField",
	"Comment": "adds a random non existing field to the provided document and associates itwith the provided value. the field will be added at a random position within the document,not necessarily at the top level using a leaf field name.",
	"Method": "String addRandomField(Random random,IngestDocument ingestDocument,Object value){\r\n    String fieldName;\r\n    do {\r\n        fieldName = randomFieldName(random);\r\n    } while (canAddField(fieldName, ingestDocument) == false);\r\n    ingestDocument.setFieldValue(fieldName, value);\r\n    return fieldName;\r\n}"
}, {
	"Path": "org.elasticsearch.test.VersionUtils.splitByMinor",
	"Comment": "split the given versions into sub lists grouped by minor version",
	"Method": "List<List<Version>> splitByMinor(List<Version> versions){\r\n    Map<Integer, List<Version>> byMinor = versions.stream().collect(Collectors.groupingBy(v -> (int) v.minor));\r\n    return byMinor.entrySet().stream().sorted(Map.Entry.comparingByKey()).map(Map.Entry::getValue).collect(Collectors.toList());\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.Aggregations.get",
	"Comment": "returns the aggregation that is associated with the specified name.",
	"Method": "A get(String name){\r\n    return (A) asMap().get(name);\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.monitoring.exporter.MonitoringTemplateUtils.indexName",
	"Comment": "get the index name given a specific date format, a monitored system and a timestamp.",
	"Method": "String indexName(DateTimeFormatter formatter,MonitoredSystem system,long timestamp){\r\n    return \".monitoring-\" + system.getSystem() + \"-\" + TEMPLATE_VERSION + \"-\" + formatter.print(timestamp);\r\n}"
}, {
	"Path": "org.elasticsearch.threadpool.ThreadPool.terminate",
	"Comment": "returns true if the given pool was terminated successfully. if the termination timed out,the service is null this method will return false.",
	"Method": "boolean terminate(ExecutorService service,long timeout,TimeUnit timeUnit,boolean terminate,ThreadPool pool,long timeout,TimeUnit timeUnit){\r\n    if (pool != null) {\r\n        try {\r\n            pool.shutdown();\r\n            if (awaitTermination(pool, timeout, timeUnit)) {\r\n                return true;\r\n            }\r\n            pool.shutdownNow();\r\n            return awaitTermination(pool, timeout, timeUnit);\r\n        } finally {\r\n            IOUtils.closeWhileHandlingException(pool);\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.ScriptSortBuilder.setNestedPath",
	"Comment": "sets the nested path if sorting occurs on a field that is inside a nested object. for sorting by script thisneeds to be specified.",
	"Method": "ScriptSortBuilder setNestedPath(String nestedPath){\r\n    if (this.nestedSort != null) {\r\n        throw new IllegalArgumentException(\"Setting both nested_path/nested_filter and nested not allowed\");\r\n    }\r\n    this.nestedPath = nestedPath;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.WorkerBulkByScrollTaskStateTests.testDelayAndRethrottle",
	"Comment": "furiously rethrottles a delayed request to make sure that we never run it twice.",
	"Method": "void testDelayAndRethrottle(){\r\n    List<Throwable> errors = new CopyOnWriteArrayList();\r\n    AtomicBoolean done = new AtomicBoolean();\r\n    int threads = between(1, 10);\r\n    CyclicBarrier waitForShutdown = new CyclicBarrier(threads);\r\n    float originalRequestsPerSecond = (float) randomDoubleBetween(1, 10000, true);\r\n    workerState.rethrottle(originalRequestsPerSecond);\r\n    TimeValue maxDelay = timeValueSeconds(between(1, 5));\r\n    assertThat(maxDelay.nanos(), greaterThanOrEqualTo(0L));\r\n    int batchSizeForMaxDelay = (int) (maxDelay.seconds() * originalRequestsPerSecond);\r\n    ThreadPool threadPool = new TestThreadPool(getTestName()) {\r\n        @Override\r\n        public ScheduledFuture<?> schedule(TimeValue delay, String name, Runnable command) {\r\n            assertThat(delay.nanos(), both(greaterThanOrEqualTo(0L)).and(lessThanOrEqualTo(maxDelay.nanos())));\r\n            return super.schedule(delay, name, command);\r\n        }\r\n    };\r\n    try {\r\n        workerState.delayPrepareBulkRequest(threadPool, timeValueNanos(System.nanoTime()), batchSizeForMaxDelay, new AbstractRunnable() {\r\n            @Override\r\n            protected void doRun() throws Exception {\r\n                boolean oldValue = done.getAndSet(true);\r\n                if (oldValue) {\r\n                    throw new RuntimeException(\"Ran twice oh no!\");\r\n                }\r\n            }\r\n            @Override\r\n            public void onFailure(Exception e) {\r\n                errors.add(e);\r\n            }\r\n        });\r\n        Runnable test = () -> {\r\n            try {\r\n                int rethrottles = 0;\r\n                while (false == done.get()) {\r\n                    float requestsPerSecond = (float) randomDoubleBetween(0, originalRequestsPerSecond * 2, true);\r\n                    workerState.rethrottle(requestsPerSecond);\r\n                    rethrottles += 1;\r\n                }\r\n                logger.info(\"Rethrottled [{}] times\", rethrottles);\r\n                waitForShutdown.await();\r\n            } catch (Exception e) {\r\n                errors.add(e);\r\n            }\r\n        };\r\n        for (int i = 1; i < threads; i++) {\r\n            threadPool.generic().execute(test);\r\n        }\r\n        test.run();\r\n    } finally {\r\n        threadPool.shutdown();\r\n        threadPool.awaitTermination(10, TimeUnit.SECONDS);\r\n    }\r\n    assertThat(errors, empty());\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.WorkerBulkByScrollTaskStateTests.testDelayAndRethrottle",
	"Comment": "furiously rethrottles a delayed request to make sure that we never run it twice.",
	"Method": "void testDelayAndRethrottle(){\r\n    assertThat(delay.nanos(), both(greaterThanOrEqualTo(0L)).and(lessThanOrEqualTo(maxDelay.nanos())));\r\n    return super.schedule(delay, name, command);\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.WorkerBulkByScrollTaskStateTests.testDelayAndRethrottle",
	"Comment": "furiously rethrottles a delayed request to make sure that we never run it twice.",
	"Method": "void testDelayAndRethrottle(){\r\n    boolean oldValue = done.getAndSet(true);\r\n    if (oldValue) {\r\n        throw new RuntimeException(\"Ran twice oh no!\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.WorkerBulkByScrollTaskStateTests.testDelayAndRethrottle",
	"Comment": "furiously rethrottles a delayed request to make sure that we never run it twice.",
	"Method": "void testDelayAndRethrottle(){\r\n    errors.add(e);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.BasePipelineAggregationTestCase.testSerialization",
	"Comment": "test serialization and deserialization of the test aggregatorfactory.",
	"Method": "void testSerialization(){\r\n    AF testAgg = createTestAggregatorFactory();\r\n    try (BytesStreamOutput output = new BytesStreamOutput()) {\r\n        output.writeNamedWriteable(testAgg);\r\n        try (StreamInput in = new NamedWriteableAwareStreamInput(output.bytes().streamInput(), namedWriteableRegistry)) {\r\n            PipelineAggregationBuilder deserializedQuery = in.readNamedWriteable(PipelineAggregationBuilder.class);\r\n            assertEquals(deserializedQuery, testAgg);\r\n            assertEquals(deserializedQuery.hashCode(), testAgg.hashCode());\r\n            assertNotSame(deserializedQuery, testAgg);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractSerializingTestCase.testFromXContent",
	"Comment": "generic test that creates new instance from the test instance and checksboth for equality and asserts equality on the two instances.",
	"Method": "void testFromXContent(){\r\n    AbstractXContentTestCase.testFromXContent(NUMBER_OF_TEST_RUNS, this::createTestInstance, supportsUnknownFields(), getShuffleFieldsExceptions(), getRandomFieldsExcludeFilter(), this::createParser, this::doParseInstance, this::assertEqualInstances, assertToXContentEquivalence(), getToXContentParams());\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificantTextAggregationBuilder.shardMinDocCount",
	"Comment": "set the minimum document count terms should have on the shard in order toappear in the response.",
	"Method": "SignificantTextAggregationBuilder shardMinDocCount(long shardMinDocCount){\r\n    if (shardMinDocCount < 0) {\r\n        throw new IllegalArgumentException(\"[shardMinDocCount] must be greater than or equal to 0. Found [\" + shardMinDocCount + \"] in [\" + name + \"]\");\r\n    }\r\n    bucketCountThresholds.setShardMinDocCount(shardMinDocCount);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.RemoteClusterService.getRemoteClusterClient",
	"Comment": "returns a client to the remote cluster if the given cluster alias exists.",
	"Method": "Client getRemoteClusterClient(ThreadPool threadPool,String clusterAlias){\r\n    if (transportService.getRemoteClusterService().getRemoteClusterNames().contains(clusterAlias) == false) {\r\n        throw new IllegalArgumentException(\"unknown cluster alias [\" + clusterAlias + \"]\");\r\n    }\r\n    return new RemoteClusterAwareClient(settings, threadPool, transportService, clusterAlias);\r\n}"
}, {
	"Path": "org.elasticsearch.transport.RemoteClusterConnection.ensureConnected",
	"Comment": "ensures that this cluster is connected. if the cluster is connected this operationwill invoke the listener immediately.",
	"Method": "void ensureConnected(ActionListener<Void> voidActionListener){\r\n    if (connectedNodes.size() == 0) {\r\n        connectHandler.connect(voidActionListener);\r\n    } else {\r\n        voidActionListener.onResponse(null);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.MinIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(min(\"foo\").field(\"d\").script(new Script(ScriptType.INLINE, AggregationTestScriptsPlugin.NAME, \"_value - 1\", emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(min(\"foo\").field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.range.IpRangeAggregationBuilder.getRanges",
	"Comment": "get the current list or ranges that are configured on this aggregation.",
	"Method": "List<Range> getRanges(){\r\n    return Collections.unmodifiableList(ranges);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.DateHistogramIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=date\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"d\", date(1, 1)), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"d\", date(2, 1)));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    Map<String, Object> params = new HashMap();\r\n    params.put(\"fieldname\", \"d\");\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(dateHistogram(\"histo\").field(\"d\").script(new Script(ScriptType.INLINE, \"mockscript\", DateScriptMocksPlugin.LONG_PLUS_ONE_MONTH, params)).dateHistogramInterval(DateHistogramInterval.MONTH)).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(dateHistogram(\"histo\").field(\"d\").dateHistogramInterval(DateHistogramInterval.MONTH)).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.datafeed.DatafeedConfig.equals",
	"Comment": "the lists of indices and types are compared for equality but they are notsorted first so this test could fail simply because the indices and typeslists are in different orders.",
	"Method": "boolean equals(Object other){\r\n    if (this == other) {\r\n        return true;\r\n    }\r\n    if (other instanceof DatafeedConfig == false) {\r\n        return false;\r\n    }\r\n    DatafeedConfig that = (DatafeedConfig) other;\r\n    return Objects.equals(this.id, that.id) && Objects.equals(this.jobId, that.jobId) && Objects.equals(this.frequency, that.frequency) && Objects.equals(this.queryDelay, that.queryDelay) && Objects.equals(this.indices, that.indices) && Objects.equals(this.types, that.types) && Objects.equals(this.query, that.query) && Objects.equals(this.scrollSize, that.scrollSize) && Objects.equals(this.aggregations, that.aggregations) && Objects.equals(this.scriptFields, that.scriptFields) && Objects.equals(this.chunkingConfig, that.chunkingConfig) && Objects.equals(this.headers, that.headers) && Objects.equals(this.delayedDataCheckConfig, that.delayedDataCheckConfig);\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.FieldSortBuilder.sortMode",
	"Comment": "returns what values to pick in the case a document contains multiplevalues for the targeted sort field.",
	"Method": "FieldSortBuilder sortMode(SortMode sortMode,SortMode sortMode){\r\n    return this.sortMode;\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.isRepositoryInUse",
	"Comment": "checks if a repository is currently in use by one of the snapshots",
	"Method": "boolean isRepositoryInUse(ClusterState clusterState,String repository){\r\n    SnapshotsInProgress snapshots = clusterState.custom(SnapshotsInProgress.TYPE);\r\n    if (snapshots != null) {\r\n        for (SnapshotsInProgress.Entry snapshot : snapshots.entries()) {\r\n            if (repository.equals(snapshot.snapshot().getRepository())) {\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    SnapshotDeletionsInProgress deletionsInProgress = clusterState.custom(SnapshotDeletionsInProgress.TYPE);\r\n    if (deletionsInProgress != null) {\r\n        for (SnapshotDeletionsInProgress.Entry entry : deletionsInProgress.getEntries()) {\r\n            if (entry.getSnapshot().getRepository().equals(repository)) {\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.CompressibleBytesOutputStream.materializeBytes",
	"Comment": "this method ensures that compression is complete and returns the underlying bytes.",
	"Method": "BytesReference materializeBytes(){\r\n    if (shouldCompress) {\r\n        stream.close();\r\n    }\r\n    return bytesStreamOutput.bytes();\r\n}"
}, {
	"Path": "org.elasticsearch.search.profile.SearchProfileShardResults.buildShardResults",
	"Comment": "helper method to convert profiler into internalprofileshardresults, whichcan be serialized to other nodes, emitted as json, etc.",
	"Method": "ProfileShardResult buildShardResults(Profilers profilers){\r\n    List<QueryProfiler> queryProfilers = profilers.getQueryProfilers();\r\n    AggregationProfiler aggProfiler = profilers.getAggregationProfiler();\r\n    List<QueryProfileShardResult> queryResults = new ArrayList(queryProfilers.size());\r\n    for (QueryProfiler queryProfiler : queryProfilers) {\r\n        QueryProfileShardResult result = new QueryProfileShardResult(queryProfiler.getTree(), queryProfiler.getRewriteTime(), queryProfiler.getCollector());\r\n        queryResults.add(result);\r\n    }\r\n    AggregationProfileShardResult aggResults = new AggregationProfileShardResult(aggProfiler.getTree());\r\n    return new ProfileShardResult(queryResults, aggResults);\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.AbstractTermVectorsTestCase.generateTestDocs",
	"Comment": "generate test documentsthe returned documents are already indexed.",
	"Method": "TestDoc[] generateTestDocs(String index,TestFieldSetting[] fieldSettings){\r\n    String[] fieldContentOptions = new String[] { \"Generating a random permutation of a sequence (such as when shuffling cards).\", \"Selecting a random sample of a population (important in statistical sampling).\", \"Allocating experimental units via random assignment to a treatment or control condition.\", \"Generating random numbers: see Random number generation.\", \"Transforming a data stream (such as when using a scrambler in telecommunications).\" };\r\n    String[] contentArray = new String[fieldSettings.length];\r\n    Map<String, Object> docSource = new HashMap();\r\n    int totalShards = getNumShards(index).numPrimaries;\r\n    TestDoc[] testDocs = new TestDoc[totalShards];\r\n    for (int i = 0; i < totalShards; i++) {\r\n        docSource.clear();\r\n        for (int j = 0; j < contentArray.length; j++) {\r\n            contentArray[j] = fieldContentOptions[randomInt(fieldContentOptions.length - 1)];\r\n            docSource.put(fieldSettings[j].name, contentArray[j]);\r\n        }\r\n        final String id = routingKeyForShard(index, i);\r\n        TestDoc doc = new TestDoc(id, fieldSettings, contentArray.clone());\r\n        index(doc.index, doc.type, doc.id, docSource);\r\n        testDocs[i] = doc;\r\n    }\r\n    refresh();\r\n    return testDocs;\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.nodesInclude",
	"Comment": "returns a set of nodes that have at least one shard of the given index.",
	"Method": "Set<String> nodesInclude(String index){\r\n    if (clusterService().state().routingTable().hasIndex(index)) {\r\n        List<ShardRouting> allShards = clusterService().state().routingTable().allShards(index);\r\n        DiscoveryNodes discoveryNodes = clusterService().state().getNodes();\r\n        Set<String> nodes = new HashSet();\r\n        for (ShardRouting shardRouting : allShards) {\r\n            if (shardRouting.assignedToNode()) {\r\n                DiscoveryNode discoveryNode = discoveryNodes.get(shardRouting.currentNodeId());\r\n                nodes.add(discoveryNode.getName());\r\n            }\r\n        }\r\n        return nodes;\r\n    }\r\n    return Collections.emptySet();\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.MinAggregator.getPointReaderOrNull",
	"Comment": "returns a converter for point values if early termination is applicable tothe context or null otherwise.",
	"Method": "Function<byte[], Number> getPointReaderOrNull(SearchContext context,Aggregator parent,ValuesSourceConfig<ValuesSource.Numeric> config){\r\n    if (context.query() != null && context.query().getClass() != MatchAllDocsQuery.class) {\r\n        return null;\r\n    }\r\n    if (parent != null) {\r\n        return null;\r\n    }\r\n    if (config.fieldContext() != null && config.script() == null) {\r\n        MappedFieldType fieldType = config.fieldContext().fieldType();\r\n        if (fieldType == null || fieldType.indexOptions() == IndexOptions.NONE) {\r\n            return null;\r\n        }\r\n        Function<byte[], Number> converter = null;\r\n        if (fieldType instanceof NumberFieldMapper.NumberFieldType) {\r\n            converter = ((NumberFieldMapper.NumberFieldType) fieldType)::parsePoint;\r\n        } else if (fieldType.getClass() == DateFieldMapper.DateFieldType.class) {\r\n            converter = (in) -> LongPoint.decodeDimension(in, 0);\r\n        }\r\n        return converter;\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificantTextAggregationBuilder.filterDuplicateText",
	"Comment": "control if duplicate paragraphs of text should try be filtered from thestatistical text analysis. can improve results but slows down analysis.default is false.",
	"Method": "SignificantTextAggregationBuilder filterDuplicateText(boolean filterDuplicateText){\r\n    this.filterDuplicateText = filterDuplicateText;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.TaskAwareRequest.createTask",
	"Comment": "returns the task object that should be used to keep track of the processing of the request.",
	"Method": "Task createTask(long id,String type,String action,TaskId parentTaskId,Map<String, String> headers){\r\n    return new Task(id, type, action, getDescription(), parentTaskId, headers);\r\n}"
}, {
	"Path": "org.elasticsearch.search.profile.ProfileResultTests.testFromXContentWithRandomFields",
	"Comment": "this test adds random fields and objects to the xcontent rendered out to ensure we can parse itback to be forward compatible with additions to the xcontent",
	"Method": "void testFromXContentWithRandomFields(){\r\n    doFromXContentTestWithRandomFields(true);\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.config.AnalysisLimits.getCategorizationExamplesLimit",
	"Comment": "gets the limit to the number of examples that are stored per category",
	"Method": "Long getCategorizationExamplesLimit(){\r\n    return categorizationExamplesLimit;\r\n}"
}, {
	"Path": "org.elasticsearch.common.cache.CacheTests.testCount",
	"Comment": "cache some entries, randomly invalidate some of them, then check that the number of cached entries is correct",
	"Method": "void testCount(){\r\n    Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();\r\n    int count = 0;\r\n    for (int i = 0; i < numberOfEntries; i++) {\r\n        count++;\r\n        cache.put(i, Integer.toString(i));\r\n    }\r\n    for (int i = 0; i < numberOfEntries; i++) {\r\n        if (rarely()) {\r\n            count--;\r\n            cache.invalidate(i);\r\n        }\r\n    }\r\n    assertEquals(count, cache.count());\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationAllPermitsAcquisitionTests.transportChannel",
	"Comment": "transport channel that is needed for replica operation testing.",
	"Method": "TransportChannel transportChannel(PlainActionFuture<Response> listener){\r\n    return new TransportChannel() {\r\n        @Override\r\n        public String getProfileName() {\r\n            return \"\";\r\n        }\r\n        @Override\r\n        public void sendResponse(TransportResponse response) throws IOException {\r\n            listener.onResponse(((Response) response));\r\n        }\r\n        @Override\r\n        public void sendResponse(Exception exception) throws IOException {\r\n            listener.onFailure(exception);\r\n        }\r\n        @Override\r\n        public String getChannelType() {\r\n            return \"replica_test\";\r\n        }\r\n    };\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationAllPermitsAcquisitionTests.transportChannel",
	"Comment": "transport channel that is needed for replica operation testing.",
	"Method": "TransportChannel transportChannel(PlainActionFuture<Response> listener){\r\n    return \"\";\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationAllPermitsAcquisitionTests.transportChannel",
	"Comment": "transport channel that is needed for replica operation testing.",
	"Method": "TransportChannel transportChannel(PlainActionFuture<Response> listener){\r\n    listener.onResponse(((Response) response));\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationAllPermitsAcquisitionTests.transportChannel",
	"Comment": "transport channel that is needed for replica operation testing.",
	"Method": "TransportChannel transportChannel(PlainActionFuture<Response> listener){\r\n    listener.onFailure(exception);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationAllPermitsAcquisitionTests.transportChannel",
	"Comment": "transport channel that is needed for replica operation testing.",
	"Method": "TransportChannel transportChannel(PlainActionFuture<Response> listener){\r\n    return \"replica_test\";\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.config.AnalysisLimits.getModelMemoryLimit",
	"Comment": "maximum size of the model in mb before the anomaly detectorwill drop new samples to prevent the model using any morememory.",
	"Method": "Long getModelMemoryLimit(){\r\n    return modelMemoryLimit;\r\n}"
}, {
	"Path": "org.elasticsearch.search.searchafter.SearchAfterBuilderTests.randomJsonSearchFromBuilder",
	"Comment": "this little trick ensure that equals and hashcode are the same when using the xcontent serialization.",
	"Method": "SearchAfterBuilder randomJsonSearchFromBuilder(){\r\n    int numSearchAfter = randomIntBetween(1, 10);\r\n    XContentBuilder jsonBuilder = XContentFactory.jsonBuilder();\r\n    jsonBuilder.startObject();\r\n    jsonBuilder.startArray(\"search_after\");\r\n    for (int i = 0; i < numSearchAfter; i++) {\r\n        int branch = randomInt(9);\r\n        switch(branch) {\r\n            case 0:\r\n                jsonBuilder.value(randomInt());\r\n                break;\r\n            case 1:\r\n                jsonBuilder.value(randomFloat());\r\n                break;\r\n            case 2:\r\n                jsonBuilder.value(randomLong());\r\n                break;\r\n            case 3:\r\n                jsonBuilder.value(randomDouble());\r\n                break;\r\n            case 4:\r\n                jsonBuilder.value(randomAlphaOfLengthBetween(5, 20));\r\n                break;\r\n            case 5:\r\n                jsonBuilder.value(randomBoolean());\r\n                break;\r\n            case 6:\r\n                jsonBuilder.value(randomByte());\r\n                break;\r\n            case 7:\r\n                jsonBuilder.value(randomShort());\r\n                break;\r\n            case 8:\r\n                jsonBuilder.value(new Text(randomAlphaOfLengthBetween(5, 20)));\r\n                break;\r\n            case 9:\r\n                jsonBuilder.nullValue();\r\n                break;\r\n        }\r\n    }\r\n    jsonBuilder.endArray();\r\n    jsonBuilder.endObject();\r\n    try (XContentParser parser = createParser(JsonXContent.jsonXContent, BytesReference.bytes(jsonBuilder))) {\r\n        parser.nextToken();\r\n        parser.nextToken();\r\n        parser.nextToken();\r\n        return SearchAfterBuilder.fromXContent(parser);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.TestCluster.wipeIndices",
	"Comment": "deletes the given indices from the tests cluster. if no index name is passed to this methodall indices are removed.",
	"Method": "void wipeIndices(String indices){\r\n    assert indices != null && indices.length > 0;\r\n    if (size() > 0) {\r\n        try {\r\n            assertAcked(client().admin().indices().prepareDelete(indices));\r\n        } catch (IndexNotFoundException e) {\r\n        } catch (IllegalArgumentException e) {\r\n            if (\"_all\".equals(indices[0])) {\r\n                ClusterStateResponse clusterStateResponse = client().admin().cluster().prepareState().execute().actionGet();\r\n                ObjectArrayList<String> concreteIndices = new ObjectArrayList();\r\n                for (IndexMetaData indexMetaData : clusterStateResponse.getState().metaData()) {\r\n                    concreteIndices.add(indexMetaData.getIndex().getName());\r\n                }\r\n                if (!concreteIndices.isEmpty()) {\r\n                    assertAcked(client().admin().indices().prepareDelete(concreteIndices.toArray(String.class)));\r\n                }\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.rest.action.search.RestSearchAction.parseSearchRequest",
	"Comment": "parses the rest request on top of the searchrequest, preserving values that are not overridden by the rest request.",
	"Method": "void parseSearchRequest(SearchRequest searchRequest,RestRequest request,XContentParser requestContentParser,IntConsumer setSize){\r\n    if (searchRequest.source() == null) {\r\n        searchRequest.source(new SearchSourceBuilder());\r\n    }\r\n    searchRequest.indices(Strings.splitStringByCommaToArray(request.param(\"index\")));\r\n    if (requestContentParser != null) {\r\n        searchRequest.source().parseXContent(requestContentParser, true);\r\n    }\r\n    final int batchedReduceSize = request.paramAsInt(\"batched_reduce_size\", searchRequest.getBatchedReduceSize());\r\n    searchRequest.setBatchedReduceSize(batchedReduceSize);\r\n    searchRequest.setPreFilterShardSize(request.paramAsInt(\"pre_filter_shard_size\", searchRequest.getPreFilterShardSize()));\r\n    if (request.hasParam(\"max_concurrent_shard_requests\")) {\r\n        final int maxConcurrentShardRequests = request.paramAsInt(\"max_concurrent_shard_requests\", searchRequest.getMaxConcurrentShardRequests());\r\n        searchRequest.setMaxConcurrentShardRequests(maxConcurrentShardRequests);\r\n    }\r\n    if (request.hasParam(\"allow_partial_search_results\")) {\r\n        searchRequest.allowPartialSearchResults(request.paramAsBoolean(\"allow_partial_search_results\", null));\r\n    }\r\n    String searchType = request.param(\"search_type\");\r\n    if (\"query_and_fetch\".equals(searchType) || \"dfs_query_and_fetch\".equals(searchType)) {\r\n        throw new IllegalArgumentException(\"Unsupported search type [\" + searchType + \"]\");\r\n    } else {\r\n        searchRequest.searchType(searchType);\r\n    }\r\n    parseSearchSource(searchRequest.source(), request, setSize);\r\n    searchRequest.requestCache(request.paramAsBoolean(\"request_cache\", null));\r\n    String scroll = request.param(\"scroll\");\r\n    if (scroll != null) {\r\n        searchRequest.scroll(new Scroll(parseTimeValue(scroll, null, \"scroll\")));\r\n    }\r\n    if (request.hasParam(\"type\")) {\r\n        deprecationLogger.deprecated(TYPES_DEPRECATION_MESSAGE);\r\n        searchRequest.types(Strings.splitStringByCommaToArray(request.param(\"type\")));\r\n    }\r\n    searchRequest.routing(request.param(\"routing\"));\r\n    searchRequest.preference(request.param(\"preference\"));\r\n    searchRequest.indicesOptions(IndicesOptions.fromRequest(request, searchRequest.indicesOptions()));\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.randomTimeZone",
	"Comment": "generate a random timezone from the ones available in java.util",
	"Method": "TimeZone randomTimeZone(){\r\n    return TimeZone.getTimeZone(randomFrom(JAVA_TIMEZONE_IDS));\r\n}"
}, {
	"Path": "org.elasticsearch.license.XPackLicenseState.isMonitoringAllowed",
	"Comment": "monitoring is always available as long as there is a valid license",
	"Method": "boolean isMonitoringAllowed(){\r\n    return status.active;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.heuristics.PercentageScore.getScore",
	"Comment": "indicates the significance of a term in a sample by determining what percentageof all occurrences of a term are found in the sample.",
	"Method": "double getScore(long subsetFreq,long subsetSize,long supersetFreq,long supersetSize){\r\n    checkFrequencyValidity(subsetFreq, subsetSize, supersetFreq, supersetSize, \"PercentageScore\");\r\n    if (supersetFreq == 0) {\r\n        return 0;\r\n    }\r\n    return (double) subsetFreq / (double) supersetFreq;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.randomRepoPath",
	"Comment": "returns path to a random directory that can be used to create a temporary file system repo",
	"Method": "Path randomRepoPath(Path randomRepoPath,Settings settings){\r\n    Environment environment = TestEnvironment.newEnvironment(settings);\r\n    Path[] repoFiles = environment.repoFiles();\r\n    assert repoFiles.length > 0;\r\n    Path path;\r\n    do {\r\n        path = repoFiles[0].resolve(randomAlphaOfLength(10));\r\n    } while (Files.exists(path));\r\n    return path;\r\n}"
}, {
	"Path": "org.elasticsearch.search.query.QueryCollectorContext.createQueryCollectorWithProfiler",
	"Comment": "creates the collector tree from the provided collectors and wraps each collector with a profiler",
	"Method": "InternalProfileCollector createQueryCollectorWithProfiler(List<QueryCollectorContext> collectors){\r\n    InternalProfileCollector collector = null;\r\n    for (QueryCollectorContext ctx : collectors) {\r\n        collector = ctx.createWithProfiler(collector);\r\n    }\r\n    return collector;\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.currentSnapshots",
	"Comment": "returns status of the currently running snapshotsthis method is executed on master node",
	"Method": "List<SnapshotInfo> currentSnapshots(String repositoryName,List<SnapshotsInProgress.Entry> currentSnapshots,String repository,List<String> snapshots){\r\n    SnapshotsInProgress snapshotsInProgress = clusterService.state().custom(SnapshotsInProgress.TYPE);\r\n    if (snapshotsInProgress == null || snapshotsInProgress.entries().isEmpty()) {\r\n        return Collections.emptyList();\r\n    }\r\n    if (\"_all\".equals(repository)) {\r\n        return snapshotsInProgress.entries();\r\n    }\r\n    if (snapshotsInProgress.entries().size() == 1) {\r\n        SnapshotsInProgress.Entry entry = snapshotsInProgress.entries().get(0);\r\n        if (entry.snapshot().getRepository().equals(repository) == false) {\r\n            return Collections.emptyList();\r\n        }\r\n        if (snapshots.isEmpty() == false) {\r\n            for (String snapshot : snapshots) {\r\n                if (entry.snapshot().getSnapshotId().getName().equals(snapshot)) {\r\n                    return snapshotsInProgress.entries();\r\n                }\r\n            }\r\n            return Collections.emptyList();\r\n        } else {\r\n            return snapshotsInProgress.entries();\r\n        }\r\n    }\r\n    List<SnapshotsInProgress.Entry> builder = new ArrayList();\r\n    for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\r\n        if (entry.snapshot().getRepository().equals(repository) == false) {\r\n            continue;\r\n        }\r\n        if (snapshots.isEmpty() == false) {\r\n            for (String snapshot : snapshots) {\r\n                if (entry.snapshot().getSnapshotId().getName().equals(snapshot)) {\r\n                    builder.add(entry);\r\n                    break;\r\n                }\r\n            }\r\n        } else {\r\n            builder.add(entry);\r\n        }\r\n    }\r\n    return Collections.unmodifiableList(builder);\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.ccr.action.FollowStatsIT.testStatsWhenNoPersistentTasksMetaDataExists",
	"Comment": "previously we would throw a nullpointerexception when there was no persistent tasks metadata in the cluster state. this testsmaintains that we do not make this mistake again.",
	"Method": "void testStatsWhenNoPersistentTasksMetaDataExists(){\r\n    final ClusterStateResponse response = client().admin().cluster().state(new ClusterStateRequest()).actionGet();\r\n    assertNull(response.getState().metaData().custom(PersistentTasksCustomMetaData.TYPE));\r\n    final AtomicBoolean onResponse = new AtomicBoolean();\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    client().execute(FollowStatsAction.INSTANCE, new FollowStatsAction.StatsRequest(), new ActionListener<FollowStatsAction.StatsResponses>() {\r\n        @Override\r\n        public void onResponse(final FollowStatsAction.StatsResponses statsResponses) {\r\n            try {\r\n                assertThat(statsResponses.getTaskFailures(), empty());\r\n                assertThat(statsResponses.getNodeFailures(), empty());\r\n                onResponse.set(true);\r\n            } finally {\r\n                latch.countDown();\r\n            }\r\n        }\r\n        @Override\r\n        public void onFailure(final Exception e) {\r\n            try {\r\n                fail(e.toString());\r\n            } finally {\r\n                latch.countDown();\r\n            }\r\n        }\r\n    });\r\n    latch.await();\r\n    assertTrue(onResponse.get());\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.ccr.action.FollowStatsIT.testStatsWhenNoPersistentTasksMetaDataExists",
	"Comment": "previously we would throw a nullpointerexception when there was no persistent tasks metadata in the cluster state. this testsmaintains that we do not make this mistake again.",
	"Method": "void testStatsWhenNoPersistentTasksMetaDataExists(){\r\n    try {\r\n        assertThat(statsResponses.getTaskFailures(), empty());\r\n        assertThat(statsResponses.getNodeFailures(), empty());\r\n        onResponse.set(true);\r\n    } finally {\r\n        latch.countDown();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.ccr.action.FollowStatsIT.testStatsWhenNoPersistentTasksMetaDataExists",
	"Comment": "previously we would throw a nullpointerexception when there was no persistent tasks metadata in the cluster state. this testsmaintains that we do not make this mistake again.",
	"Method": "void testStatsWhenNoPersistentTasksMetaDataExists(){\r\n    try {\r\n        fail(e.toString());\r\n    } finally {\r\n        latch.countDown();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.SortBuilders.geoDistanceSort",
	"Comment": "constructs a new distance based sort on a geo point like field.",
	"Method": "GeoDistanceSortBuilder geoDistanceSort(String fieldName,double lat,double lon,GeoDistanceSortBuilder geoDistanceSort,String fieldName,GeoPoint points,GeoDistanceSortBuilder geoDistanceSort,String fieldName,String geohashes){\r\n    return new GeoDistanceSortBuilder(fieldName, geohashes);\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.NetworkAddressTests.forge",
	"Comment": "creates address without any lookups. hostname can be null, for missing",
	"Method": "InetAddress forge(String hostname,String address){\r\n    byte[] bytes = InetAddress.getByName(address).getAddress();\r\n    return InetAddress.getByAddress(hostname, bytes);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.DateRangeIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"date\", \"type=date\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(jsonBuilder().startObject().timeField(\"date\", date(1, 1)).endObject()), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(jsonBuilder().startObject().timeField(\"date\", date(2, 1)).endObject()));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    Map<String, Object> params = new HashMap();\r\n    params.put(\"fieldname\", \"date\");\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(dateRange(\"foo\").field(\"date\").script(new Script(ScriptType.INLINE, \"mockscript\", DateScriptMocksPlugin.DOUBLE_PLUS_ONE_MONTH, params)).addRange(new DateTime(2012, 1, 1, 0, 0, 0, 0, DateTimeZone.UTC), new DateTime(2013, 1, 1, 0, 0, 0, 0, DateTimeZone.UTC))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(dateRange(\"foo\").field(\"date\").addRange(new DateTime(2012, 1, 1, 0, 0, 0, 0, DateTimeZone.UTC), new DateTime(2013, 1, 1, 0, 0, 0, 0, DateTimeZone.UTC))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.ClusterDisruptionIT.testSendingShardFailure",
	"Comment": "simulate handling of sending shard failure during an isolation",
	"Method": "void testSendingShardFailure(){\r\n    List<String> nodes = startCluster(3);\r\n    String masterNode = internalCluster().getMasterName();\r\n    List<String> nonMasterNodes = nodes.stream().filter(node -> !node.equals(masterNode)).collect(Collectors.toList());\r\n    String nonMasterNode = randomFrom(nonMasterNodes);\r\n    assertAcked(prepareCreate(\"test\").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 3).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 2)));\r\n    ensureGreen();\r\n    String nonMasterNodeId = internalCluster().clusterService(nonMasterNode).localNode().getId();\r\n    ShardRouting failedShard = randomFrom(clusterService().state().getRoutingNodes().node(nonMasterNodeId).shardsWithState(ShardRoutingState.STARTED));\r\n    ShardStateAction service = internalCluster().getInstance(ShardStateAction.class, nonMasterNode);\r\n    CountDownLatch latch = new CountDownLatch(1);\r\n    AtomicBoolean success = new AtomicBoolean();\r\n    String isolatedNode = randomBoolean() ? masterNode : nonMasterNode;\r\n    TwoPartitions partitions = isolateNode(isolatedNode);\r\n    NetworkLinkDisruptionType disruptionType = new NetworkDisconnect();\r\n    NetworkDisruption networkDisruption = new NetworkDisruption(partitions, disruptionType);\r\n    setDisruptionScheme(networkDisruption);\r\n    networkDisruption.startDisrupting();\r\n    service.localShardFailed(failedShard, \"simulated\", new CorruptIndexException(\"simulated\", (String) null), new ShardStateAction.Listener() {\r\n        @Override\r\n        public void onSuccess() {\r\n            success.set(true);\r\n            latch.countDown();\r\n        }\r\n        @Override\r\n        public void onFailure(Exception e) {\r\n            success.set(false);\r\n            latch.countDown();\r\n            assert false;\r\n        }\r\n    });\r\n    if (isolatedNode.equals(nonMasterNode)) {\r\n        assertNoMaster(nonMasterNode);\r\n    } else {\r\n        ensureStableCluster(2, nonMasterNode);\r\n    }\r\n    networkDisruption.removeAndEnsureHealthy(internalCluster());\r\n    ensureStableCluster(3);\r\n    latch.await();\r\n    assertTrue(success.get());\r\n    List<ShardRouting> shards = clusterService().state().getRoutingTable().allShards(\"test\");\r\n    for (ShardRouting shard : shards) {\r\n        assertThat(shard.allocationId(), not(equalTo(failedShard.allocationId())));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.ClusterDisruptionIT.testSendingShardFailure",
	"Comment": "simulate handling of sending shard failure during an isolation",
	"Method": "void testSendingShardFailure(){\r\n    success.set(true);\r\n    latch.countDown();\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.ClusterDisruptionIT.testSendingShardFailure",
	"Comment": "simulate handling of sending shard failure during an isolation",
	"Method": "void testSendingShardFailure(){\r\n    success.set(false);\r\n    latch.countDown();\r\n    assert false;\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.phrase.SmoothingModelTestCase.testFromXContent",
	"Comment": "test that creates new smoothing model from a random test smoothing model and checks both for equality",
	"Method": "void testFromXContent(){\r\n    SmoothingModel testModel = createTestModel();\r\n    XContentBuilder contentBuilder = XContentFactory.contentBuilder(randomFrom(XContentType.values()));\r\n    if (randomBoolean()) {\r\n        contentBuilder.prettyPrint();\r\n    }\r\n    contentBuilder.startObject();\r\n    testModel.innerToXContent(contentBuilder, ToXContent.EMPTY_PARAMS);\r\n    contentBuilder.endObject();\r\n    try (XContentParser parser = createParser(shuffleXContent(contentBuilder))) {\r\n        parser.nextToken();\r\n        SmoothingModel parsedModel = fromXContent(parser);\r\n        assertNotSame(testModel, parsedModel);\r\n        assertEquals(testModel, parsedModel);\r\n        assertEquals(testModel.hashCode(), parsedModel.hashCode());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.fetch.subphase.highlight.AbstractHighlighterBuilder.highlightQuery",
	"Comment": "sets a query to be used for highlighting instead of the search query.",
	"Method": "HB highlightQuery(QueryBuilder highlightQuery,QueryBuilder highlightQuery){\r\n    return this.highlightQuery;\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.RandomDocumentPicks.randomIngestDocument",
	"Comment": "generates a document that holds random metadata and the document provided as a map argument",
	"Method": "IngestDocument randomIngestDocument(Random random,IngestDocument randomIngestDocument,Random random,Map<String, Object> source){\r\n    String index = randomString(random);\r\n    String type = randomString(random);\r\n    String id = randomString(random);\r\n    String routing = null;\r\n    Long version = randomNonNegtiveLong(random);\r\n    VersionType versionType = RandomPicks.randomFrom(random, new VersionType[] { VersionType.INTERNAL, VersionType.EXTERNAL, VersionType.EXTERNAL_GTE });\r\n    if (random.nextBoolean()) {\r\n        routing = randomString(random);\r\n    }\r\n    return new IngestDocument(index, type, id, routing, version, versionType, source);\r\n}"
}, {
	"Path": "org.elasticsearch.update.UpdateNoopIT.testTotallyEmpty",
	"Comment": "totally empty requests are noop if and only if detect noops is true andits true by default.",
	"Method": "void testTotallyEmpty(){\r\n    updateAndCheckSource(1, XContentFactory.jsonBuilder().startObject().field(\"f\", \"foo\").startObject(\"m\").field(\"mf1\", \"foo\").field(\"mf2\", \"baz\").endObject().endObject());\r\n    update(true, 1, XContentFactory.jsonBuilder().startObject().endObject());\r\n    update(false, 2, XContentFactory.jsonBuilder().startObject().endObject());\r\n    update(null, 2, XContentFactory.jsonBuilder().startObject().endObject());\r\n}"
}, {
	"Path": "org.elasticsearch.test.VersionUtils.moveLastToUnreleased",
	"Comment": "move the last version of the last minor in versions to the unreleased versions",
	"Method": "Version moveLastToUnreleased(List<List<Version>> versions,List<Version> unreleasedVersions){\r\n    List<Version> lastMinor = new ArrayList(versions.get(versions.size() - 1));\r\n    Version lastVersion = lastMinor.remove(lastMinor.size() - 1);\r\n    if (lastMinor.isEmpty()) {\r\n        versions.remove(versions.size() - 1);\r\n    } else {\r\n        versions.set(versions.size() - 1, lastMinor);\r\n    }\r\n    unreleasedVersions.add(lastVersion);\r\n    return lastVersion;\r\n}"
}, {
	"Path": "org.elasticsearch.license.XPackLicenseState.isLogstashAllowed",
	"Comment": "logstash is allowed as long as there is an active license of type trial, standard, gold or platinum",
	"Method": "boolean isLogstashAllowed(){\r\n    Status localStatus = status;\r\n    return localStatus.active && (isBasic(localStatus.mode) == false);\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.completion.FuzzyOptions.getFuzzyPrefixLength",
	"Comment": "returns the minimum length of the input prefix required to apply any edits",
	"Method": "int getFuzzyPrefixLength(){\r\n    return fuzzyPrefixLength;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomTemplates",
	"Comment": "randomly adds, deletes or updates index templates in the metadata",
	"Method": "MetaData randomTemplates(MetaData metaData){\r\n    return randomParts(metaData, \"template\", new RandomPart<IndexTemplateMetaData>() {\r\n        @Override\r\n        public ImmutableOpenMap<String, IndexTemplateMetaData> parts(MetaData metaData) {\r\n            return metaData.templates();\r\n        }\r\n        @Override\r\n        public MetaData.Builder put(MetaData.Builder builder, IndexTemplateMetaData part) {\r\n            return builder.put(part);\r\n        }\r\n        @Override\r\n        public MetaData.Builder remove(MetaData.Builder builder, String name) {\r\n            return builder.removeTemplate(name);\r\n        }\r\n        @Override\r\n        public IndexTemplateMetaData randomCreate(String name) {\r\n            IndexTemplateMetaData.Builder builder = IndexTemplateMetaData.builder(name);\r\n            builder.order(randomInt(1000)).patterns(Collections.singletonList(randomName(\"temp\"))).settings(randomSettings(Settings.EMPTY));\r\n            int aliasCount = randomIntBetween(0, 10);\r\n            for (int i = 0; i < aliasCount; i++) {\r\n                builder.putAlias(randomAlias());\r\n            }\r\n            return builder.build();\r\n        }\r\n        @Override\r\n        public IndexTemplateMetaData randomChange(IndexTemplateMetaData part) {\r\n            IndexTemplateMetaData.Builder builder = new IndexTemplateMetaData.Builder(part);\r\n            builder.order(randomInt(1000));\r\n            return builder.build();\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomTemplates",
	"Comment": "randomly adds, deletes or updates index templates in the metadata",
	"Method": "MetaData randomTemplates(MetaData metaData){\r\n    return metaData.templates();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomTemplates",
	"Comment": "randomly adds, deletes or updates index templates in the metadata",
	"Method": "MetaData randomTemplates(MetaData metaData){\r\n    return builder.put(part);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomTemplates",
	"Comment": "randomly adds, deletes or updates index templates in the metadata",
	"Method": "MetaData randomTemplates(MetaData metaData){\r\n    return builder.removeTemplate(name);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomTemplates",
	"Comment": "randomly adds, deletes or updates index templates in the metadata",
	"Method": "MetaData randomTemplates(MetaData metaData){\r\n    IndexTemplateMetaData.Builder builder = IndexTemplateMetaData.builder(name);\r\n    builder.order(randomInt(1000)).patterns(Collections.singletonList(randomName(\"temp\"))).settings(randomSettings(Settings.EMPTY));\r\n    int aliasCount = randomIntBetween(0, 10);\r\n    for (int i = 0; i < aliasCount; i++) {\r\n        builder.putAlias(randomAlias());\r\n    }\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateDiffIT.randomTemplates",
	"Comment": "randomly adds, deletes or updates index templates in the metadata",
	"Method": "MetaData randomTemplates(MetaData metaData){\r\n    IndexTemplateMetaData.Builder builder = new IndexTemplateMetaData.Builder(part);\r\n    builder.order(randomInt(1000));\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilderTests.createRandomFunctionScoreBuilder",
	"Comment": "creates a random function score query using only constructor params. the caller is responsible for randomizing fields set outside ofthe constructor.",
	"Method": "FunctionScoreQueryBuilder createRandomFunctionScoreBuilder(){\r\n    switch(randomIntBetween(0, 3)) {\r\n        case 0:\r\n            FilterFunctionBuilder[] functions = new FilterFunctionBuilder[randomIntBetween(0, 3)];\r\n            for (int i = 0; i < functions.length; i++) {\r\n                functions[i] = new FilterFunctionBuilder(RandomQueryBuilder.createQuery(random()), randomScoreFunction());\r\n            }\r\n            if (randomBoolean()) {\r\n                return new FunctionScoreQueryBuilder(RandomQueryBuilder.createQuery(random()), functions);\r\n            }\r\n            return new FunctionScoreQueryBuilder(functions);\r\n        case 1:\r\n            return new FunctionScoreQueryBuilder(randomScoreFunction());\r\n        case 2:\r\n            return new FunctionScoreQueryBuilder(RandomQueryBuilder.createQuery(random()), randomScoreFunction());\r\n        case 3:\r\n            return new FunctionScoreQueryBuilder(RandomQueryBuilder.createQuery(random()));\r\n        default:\r\n            throw new UnsupportedOperationException();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESSingleNodeTestCase.ensureGreen",
	"Comment": "ensures the cluster has a green state via the cluster health api. this method will also wait for relocations.it is useful to ensure that all action on the cluster have finished and all shards that were currently relocatingare now allocated and started.",
	"Method": "ClusterHealthStatus ensureGreen(String indices,ClusterHealthStatus ensureGreen,TimeValue timeout,String indices){\r\n    ClusterHealthResponse actionGet = client().admin().cluster().health(Requests.clusterHealthRequest(indices).timeout(timeout).waitForGreenStatus().waitForEvents(Priority.LANGUID).waitForNoRelocatingShards(true)).actionGet();\r\n    if (actionGet.isTimedOut()) {\r\n        logger.info(\"ensureGreen timed out, cluster state:\\n{}\\n{}\", client().admin().cluster().prepareState().get().getState(), client().admin().cluster().preparePendingClusterTasks().get());\r\n        assertThat(\"timed out waiting for green state\", actionGet.isTimedOut(), equalTo(false));\r\n    }\r\n    assertThat(actionGet.getStatus(), equalTo(ClusterHealthStatus.GREEN));\r\n    logger.debug(\"indices {} are green\", indices.length == 0 ? \"[_all]\" : indices);\r\n    return actionGet.getStatus();\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.ExtendedStatsIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(extendedStats(\"foo\").field(\"d\").script(new Script(ScriptType.INLINE, AggregationTestScriptsPlugin.NAME, \"_value + 1\", Collections.emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(extendedStats(\"foo\").field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractStreamableXContentTestCase.getShuffleFieldsExceptions",
	"Comment": "fields that have to be ignored when shuffling as part of testfromxcontent",
	"Method": "String[] getShuffleFieldsExceptions(){\r\n    return Strings.EMPTY_ARRAY;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.waitForRelocation",
	"Comment": "waits for all relocating shards to become active and the cluster has reached the given health statususing the cluster health api.",
	"Method": "ClusterHealthStatus waitForRelocation(ClusterHealthStatus waitForRelocation,ClusterHealthStatus status){\r\n    ClusterHealthRequest request = Requests.clusterHealthRequest().waitForNoRelocatingShards(true);\r\n    if (status != null) {\r\n        request.waitForStatus(status);\r\n    }\r\n    ClusterHealthResponse actionGet = client().admin().cluster().health(request).actionGet();\r\n    if (actionGet.isTimedOut()) {\r\n        logger.info(\"waitForRelocation timed out (status={}), cluster state:\\n{}\\n{}\", status, client().admin().cluster().prepareState().get().getState(), client().admin().cluster().preparePendingClusterTasks().get());\r\n        assertThat(\"timed out waiting for relocation\", actionGet.isTimedOut(), equalTo(false));\r\n    }\r\n    if (status != null) {\r\n        assertThat(actionGet.getStatus(), equalTo(status));\r\n    }\r\n    return actionGet.getStatus();\r\n}"
}, {
	"Path": "org.elasticsearch.license.LicenseVerifier.verifyLicense",
	"Comment": "verifies the license content with the signature using the packagedpublic key",
	"Method": "boolean verifyLicense(License license,byte[] publicKeyData,boolean verifyLicense,License license){\r\n    final byte[] publicKeyBytes;\r\n    try (InputStream is = LicenseVerifier.class.getResourceAsStream(\"/public.key\")) {\r\n        ByteArrayOutputStream out = new ByteArrayOutputStream();\r\n        Streams.copy(is, out);\r\n        publicKeyBytes = out.toByteArray();\r\n    } catch (IOException ex) {\r\n        throw new IllegalStateException(ex);\r\n    }\r\n    return verifyLicense(license, publicKeyBytes);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.support.AggregationPath.validate",
	"Comment": "validates this path over the given aggregator as a point of reference.",
	"Method": "void validate(Aggregator root){\r\n    Aggregator aggregator = root;\r\n    for (int i = 0; i < pathElements.size(); i++) {\r\n        String name = pathElements.get(i).name;\r\n        aggregator = ProfilingAggregator.unwrap(aggregator.subAggregator(name));\r\n        if (aggregator == null) {\r\n            throw new AggregationExecutionException(\"Invalid aggregator order path [\" + this + \"]. The \" + \"provided aggregation [\" + name + \"] either does not exist, or is a pipeline aggregation \" + \"and cannot be used to sort the buckets.\");\r\n        }\r\n        if (i < pathElements.size() - 1) {\r\n            if (!(aggregator instanceof SingleBucketAggregator)) {\r\n                throw new AggregationExecutionException(\"Invalid aggregation order path [\" + this + \"]. Buckets can only be sorted on a sub-aggregator path \" + \"that is built out of zero or more single-bucket aggregations within the path and a final \" + \"single-bucket or a metrics aggregation at the path end. Sub-path [\" + subPath(0, i + 1) + \"] points to non single-bucket aggregation\");\r\n            }\r\n            if (pathElements.get(i).key != null) {\r\n                throw new AggregationExecutionException(\"Invalid aggregation order path [\" + this + \"]. Buckets can only be sorted on a sub-aggregator path \" + \"that is built out of zero or more single-bucket aggregations within the path and a \" + \"final single-bucket or a metrics aggregation at the path end. Sub-path [\" + subPath(0, i + 1) + \"] points to non single-bucket aggregation\");\r\n            }\r\n        }\r\n    }\r\n    boolean singleBucket = aggregator instanceof SingleBucketAggregator;\r\n    if (!singleBucket && !(aggregator instanceof NumericMetricsAggregator)) {\r\n        throw new AggregationExecutionException(\"Invalid aggregation order path [\" + this + \"]. Buckets can only be sorted on a sub-aggregator path \" + \"that is built out of zero or more single-bucket aggregations within the path and a final \" + \"single-bucket or a metrics aggregation at the path end.\");\r\n    }\r\n    AggregationPath.PathElement lastToken = lastPathElement();\r\n    if (singleBucket) {\r\n        if (lastToken.key != null && !\"doc_count\".equals(lastToken.key)) {\r\n            throw new AggregationExecutionException(\"Invalid aggregation order path [\" + this + \"]. Ordering on a single-bucket aggregation can only be done on its doc_count. \" + \"Either drop the key (a la \\\"\" + lastToken.name + \"\\\") or change it to \\\"doc_count\\\" (a la \\\"\" + lastToken.name + \".doc_count\\\")\");\r\n        }\r\n        return;\r\n    }\r\n    if (aggregator instanceof NumericMetricsAggregator.SingleValue) {\r\n        if (lastToken.key != null && !\"value\".equals(lastToken.key)) {\r\n            throw new AggregationExecutionException(\"Invalid aggregation order path [\" + this + \"]. Ordering on a single-value metrics aggregation can only be done on its value. \" + \"Either drop the key (a la \\\"\" + lastToken.name + \"\\\") or change it to \\\"value\\\" (a la \\\"\" + lastToken.name + \".value\\\")\");\r\n        }\r\n        return;\r\n    }\r\n    if (lastToken.key == null) {\r\n        throw new AggregationExecutionException(\"Invalid aggregation order path [\" + this + \"]. When ordering on a multi-value metrics aggregation a metric name must be specified\");\r\n    }\r\n    if (!((NumericMetricsAggregator.MultiValue) aggregator).hasMetric(lastToken.key)) {\r\n        throw new AggregationExecutionException(\"Invalid aggregation order path [\" + this + \"]. Unknown metric name [\" + lastToken.key + \"] on multi-value metrics aggregation [\" + lastToken.name + \"]\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.DateHistogramOffsetIT.testSingleValueWithOffsetMinDocCount",
	"Comment": "set offset so day buckets start at 6am. index first 12 hours for two days, with one day gap.",
	"Method": "void testSingleValueWithOffsetMinDocCount(){\r\n    prepareIndex(date(\"2014-03-11T00:00:00+00:00\"), 12, 1, 0);\r\n    prepareIndex(date(\"2014-03-14T00:00:00+00:00\"), 12, 1, 13);\r\n    SearchResponse response = client().prepareSearch(\"idx2\").setQuery(matchAllQuery()).addAggregation(dateHistogram(\"date_histo\").field(\"date\").offset(\"6h\").minDocCount(0).format(DATE_FORMAT).dateHistogramInterval(DateHistogramInterval.DAY)).execute().actionGet();\r\n    assertThat(response.getHits().getTotalHits(), equalTo(24L));\r\n    Histogram histo = response.getAggregations().get(\"date_histo\");\r\n    List<? extends Histogram.Bucket> buckets = histo.getBuckets();\r\n    assertThat(buckets.size(), equalTo(5));\r\n    checkBucketFor(buckets.get(0), new DateTime(2014, 3, 10, 6, 0, DateTimeZone.UTC), 6L);\r\n    checkBucketFor(buckets.get(1), new DateTime(2014, 3, 11, 6, 0, DateTimeZone.UTC), 6L);\r\n    checkBucketFor(buckets.get(2), new DateTime(2014, 3, 12, 6, 0, DateTimeZone.UTC), 0L);\r\n    checkBucketFor(buckets.get(3), new DateTime(2014, 3, 13, 6, 0, DateTimeZone.UTC), 6L);\r\n    checkBucketFor(buckets.get(4), new DateTime(2014, 3, 14, 6, 0, DateTimeZone.UTC), 6L);\r\n}"
}, {
	"Path": "org.elasticsearch.transport.TcpTransport.messageReceived",
	"Comment": "this method handles the message receive part for both request and responses",
	"Method": "void messageReceived(BytesReference reference,TcpChannel channel){\r\n    String profileName = channel.getProfile();\r\n    InetSocketAddress remoteAddress = channel.getRemoteAddress();\r\n    int messageLengthBytes = reference.length();\r\n    final int totalMessageSize = messageLengthBytes + TcpHeader.MARKER_BYTES_SIZE + TcpHeader.MESSAGE_LENGTH_SIZE;\r\n    readBytesMetric.inc(totalMessageSize);\r\n    boolean hasMessageBytesToRead = (totalMessageSize - TcpHeader.HEADER_SIZE) > 0;\r\n    StreamInput streamIn = reference.streamInput();\r\n    boolean success = false;\r\n    try (ThreadContext.StoredContext tCtx = threadPool.getThreadContext().stashContext()) {\r\n        long requestId = streamIn.readLong();\r\n        byte status = streamIn.readByte();\r\n        Version version = Version.fromId(streamIn.readInt());\r\n        if (TransportStatus.isCompress(status) && hasMessageBytesToRead && streamIn.available() > 0) {\r\n            Compressor compressor;\r\n            try {\r\n                final int bytesConsumed = TcpHeader.REQUEST_ID_SIZE + TcpHeader.STATUS_SIZE + TcpHeader.VERSION_ID_SIZE;\r\n                compressor = CompressorFactory.compressor(reference.slice(bytesConsumed, reference.length() - bytesConsumed));\r\n            } catch (NotCompressedException ex) {\r\n                int maxToRead = Math.min(reference.length(), 10);\r\n                StringBuilder sb = new StringBuilder(\"stream marked as compressed, but no compressor found, first [\").append(maxToRead).append(\"] content bytes out of [\").append(reference.length()).append(\"] readable bytes with message size [\").append(messageLengthBytes).append(\"] \").append(\"] are [\");\r\n                for (int i = 0; i < maxToRead; i++) {\r\n                    sb.append(reference.get(i)).append(\",\");\r\n                }\r\n                sb.append(\"]\");\r\n                throw new IllegalStateException(sb.toString());\r\n            }\r\n            streamIn = compressor.streamInput(streamIn);\r\n        }\r\n        final boolean isHandshake = TransportStatus.isHandshake(status);\r\n        ensureVersionCompatibility(version, this.version, isHandshake);\r\n        streamIn = new NamedWriteableAwareStreamInput(streamIn, namedWriteableRegistry);\r\n        streamIn.setVersion(version);\r\n        threadPool.getThreadContext().readHeaders(streamIn);\r\n        threadPool.getThreadContext().putTransient(\"_remote_address\", remoteAddress);\r\n        if (TransportStatus.isRequest(status)) {\r\n            handleRequest(channel, profileName, streamIn, requestId, messageLengthBytes, version, remoteAddress, status);\r\n        } else {\r\n            final TransportResponseHandler<?> handler;\r\n            if (isHandshake) {\r\n                handler = handshaker.removeHandlerForHandshake(requestId);\r\n            } else {\r\n                TransportResponseHandler<? extends TransportResponse> theHandler = responseHandlers.onResponseReceived(requestId, messageListener);\r\n                if (theHandler == null && TransportStatus.isError(status)) {\r\n                    handler = handshaker.removeHandlerForHandshake(requestId);\r\n                } else {\r\n                    handler = theHandler;\r\n                }\r\n            }\r\n            if (handler != null) {\r\n                if (TransportStatus.isError(status)) {\r\n                    handlerResponseError(streamIn, handler);\r\n                } else {\r\n                    handleResponse(remoteAddress, streamIn, handler);\r\n                }\r\n                final int nextByte = streamIn.read();\r\n                if (nextByte != -1) {\r\n                    throw new IllegalStateException(\"Message not fully read (response) for requestId [\" + requestId + \"], handler [\" + handler + \"], error [\" + TransportStatus.isError(status) + \"]; resetting\");\r\n                }\r\n            }\r\n        }\r\n        success = true;\r\n    } finally {\r\n        if (success) {\r\n            IOUtils.close(streamIn);\r\n        } else {\r\n            IOUtils.closeWhileHandlingException(streamIn);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.Version.isCompatible",
	"Comment": "returns true iff both version are compatible. otherwise false",
	"Method": "boolean isCompatible(Version version){\r\n    boolean compatible = onOrAfter(version.minimumCompatibilityVersion()) && version.onOrAfter(minimumCompatibilityVersion());\r\n    assert compatible == false || Math.max(major, version.major) - Math.min(major, version.major) <= 1;\r\n    return compatible;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.GeoDistanceSortBuilder.fieldName",
	"Comment": "returns the geo point like field the distance based sort operates on.",
	"Method": "String fieldName(){\r\n    return this.fieldName;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.HyperLogLogPlusPlus.precisionFromThreshold",
	"Comment": "compute the required precision so that count distinct entrieswould be counted with linear counting.",
	"Method": "int precisionFromThreshold(long count){\r\n    final long hashTableEntries = (long) Math.ceil(count / MAX_LOAD_FACTOR);\r\n    int precision = PackedInts.bitsRequired(hashTableEntries * Integer.BYTES);\r\n    precision = Math.max(precision, MIN_PRECISION);\r\n    precision = Math.min(precision, MAX_PRECISION);\r\n    return precision;\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.CorruptedFileIT.testCorruptionOnNetworkLayer",
	"Comment": "tests corruption that happens on the network layer and that the primary does not get affected by corruption that happens on the wayto the replica. the file on disk stays uncorrupted",
	"Method": "void testCorruptionOnNetworkLayer(){\r\n    int numDocs = scaledRandomIntBetween(100, 1000);\r\n    internalCluster().ensureAtLeastNumDataNodes(2);\r\n    if (cluster().numDataNodes() < 3) {\r\n        internalCluster().startNode(Settings.builder().put(Node.NODE_DATA_SETTING.getKey(), true).put(Node.NODE_MASTER_SETTING.getKey(), false));\r\n    }\r\n    NodesStatsResponse nodeStats = client().admin().cluster().prepareNodesStats().get();\r\n    List<NodeStats> dataNodeStats = new ArrayList();\r\n    for (NodeStats stat : nodeStats.getNodes()) {\r\n        if (stat.getNode().isDataNode()) {\r\n            dataNodeStats.add(stat);\r\n        }\r\n    }\r\n    assertThat(dataNodeStats.size(), greaterThanOrEqualTo(2));\r\n    Collections.shuffle(dataNodeStats, random());\r\n    NodeStats primariesNode = dataNodeStats.get(0);\r\n    NodeStats unluckyNode = dataNodeStats.get(1);\r\n    assertAcked(prepareCreate(\"test\").setSettings(// don't go crazy here it must recovery fast\r\n    Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, \"0\").put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, // This does corrupt files on the replica, so we can't check:\r\n    between(1, 4)).put(MockFSIndexStore.INDEX_CHECK_INDEX_ON_CLOSE_SETTING.getKey(), false).put(\"index.routing.allocation.include._name\", primariesNode.getNode().getName()).put(EnableAllocationDecider.INDEX_ROUTING_REBALANCE_ENABLE_SETTING.getKey(), EnableAllocationDecider.Rebalance.NONE)));\r\n    ensureGreen();\r\n    IndexRequestBuilder[] builders = new IndexRequestBuilder[numDocs];\r\n    for (int i = 0; i < builders.length; i++) {\r\n        builders[i] = client().prepareIndex(\"test\", \"type\").setSource(\"field\", \"value\");\r\n    }\r\n    indexRandom(true, builders);\r\n    ensureGreen();\r\n    assertAllSuccessful(client().admin().indices().prepareFlush().setForce(true).execute().actionGet());\r\n    SearchResponse countResponse = client().prepareSearch().setSize(0).get();\r\n    assertHitCount(countResponse, numDocs);\r\n    final boolean truncate = randomBoolean();\r\n    for (NodeStats dataNode : dataNodeStats) {\r\n        MockTransportService mockTransportService = ((MockTransportService) internalCluster().getInstance(TransportService.class, dataNode.getNode().getName()));\r\n        mockTransportService.addSendBehavior(internalCluster().getInstance(TransportService.class, unluckyNode.getNode().getName()), (connection, requestId, action, request, options) -> {\r\n            if (action.equals(PeerRecoveryTargetService.Actions.FILE_CHUNK)) {\r\n                RecoveryFileChunkRequest req = (RecoveryFileChunkRequest) request;\r\n                if (truncate && req.length() > 1) {\r\n                    BytesRef bytesRef = req.content().toBytesRef();\r\n                    BytesArray array = new BytesArray(bytesRef.bytes, bytesRef.offset, (int) req.length() - 1);\r\n                    request = new RecoveryFileChunkRequest(req.recoveryId(), req.shardId(), req.metadata(), req.position(), array, req.lastChunk(), req.totalTranslogOps(), req.sourceThrottleTimeInNanos());\r\n                } else {\r\n                    assert req.content().toBytesRef().bytes == req.content().toBytesRef().bytes : \"no internal reference!!\";\r\n                    final byte[] array = req.content().toBytesRef().bytes;\r\n                    int i = randomIntBetween(0, req.content().length() - 1);\r\n                    array[i] = (byte) ~array[i];\r\n                }\r\n            }\r\n            connection.sendRequest(requestId, action, request, options);\r\n        });\r\n    }\r\n    Settings build = Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, \"1\").put(\"index.routing.allocation.include._name\", \"*\").build();\r\n    client().admin().indices().prepareUpdateSettings(\"test\").setSettings(build).get();\r\n    client().admin().cluster().prepareReroute().get();\r\n    ClusterHealthResponse actionGet = client().admin().cluster().health(Requests.clusterHealthRequest(\"test\").waitForGreenStatus()).actionGet();\r\n    if (actionGet.isTimedOut()) {\r\n        logger.info(\"ensureGreen timed out, cluster state:\\n{}\\n{}\", client().admin().cluster().prepareState().get().getState(), client().admin().cluster().preparePendingClusterTasks().get());\r\n        assertThat(\"timed out waiting for green state\", actionGet.isTimedOut(), equalTo(false));\r\n    }\r\n    ClusterStateResponse clusterStateResponse = client().admin().cluster().prepareState().get();\r\n    for (IndexShardRoutingTable table : clusterStateResponse.getState().getRoutingTable().index(\"test\")) {\r\n        for (ShardRouting routing : table) {\r\n            if (unluckyNode.getNode().getId().equals(routing.currentNodeId())) {\r\n                assertThat(routing.state(), not(equalTo(ShardRoutingState.STARTED)));\r\n                assertThat(routing.state(), not(equalTo(ShardRoutingState.RELOCATING)));\r\n            }\r\n        }\r\n    }\r\n    final int numIterations = scaledRandomIntBetween(5, 20);\r\n    for (int i = 0; i < numIterations; i++) {\r\n        SearchResponse response = client().prepareSearch().setSize(numDocs).get();\r\n        assertHitCount(response, numDocs);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.graph.action.GraphExploreRequestBuilder.setRouting",
	"Comment": "the routing values to control the shards that the action will be executed on.",
	"Method": "GraphExploreRequestBuilder setRouting(String routing,GraphExploreRequestBuilder setRouting,String routing){\r\n    request.routing(routing);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.restspec.ClientYamlSuiteRestApi.getFinalPaths",
	"Comment": "finds the best matching rest path given the current parameters and replacesplaceholders with their corresponding values received as arguments",
	"Method": "ClientYamlSuiteRestPath[] getFinalPaths(Map<String, String> pathParams){\r\n    List<ClientYamlSuiteRestPath> matchingRestPaths = findMatchingRestPaths(pathParams.keySet());\r\n    if (matchingRestPaths == null || matchingRestPaths.isEmpty()) {\r\n        throw new IllegalArgumentException(\"unable to find matching rest path for api [\" + name + \"] and path params \" + pathParams);\r\n    }\r\n    ClientYamlSuiteRestPath[] restPaths = new ClientYamlSuiteRestPath[matchingRestPaths.size()];\r\n    for (int i = 0; i < matchingRestPaths.size(); i++) {\r\n        ClientYamlSuiteRestPath restPath = matchingRestPaths.get(i);\r\n        restPaths[i] = restPath.replacePlaceholders(pathParams);\r\n    }\r\n    return restPaths;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.InternalOrder.isCountDesc",
	"Comment": "determine if the ordering strategy is sorting on bucket count descending.",
	"Method": "boolean isCountDesc(BucketOrder order){\r\n    return isOrder(order, COUNT_DESC);\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.ReplicaShardAllocatorTests.testThrottleWhenAllocatingToMatchingNode",
	"Comment": "tests when the node to allocate to due to matching is being throttled, we move the shard to ignoredto wait till throttling on it is done.",
	"Method": "void testThrottleWhenAllocatingToMatchingNode(){\r\n    RoutingAllocation allocation = onePrimaryOnNode1And1Replica(new AllocationDeciders(Arrays.asList(new TestAllocateDecision(Decision.YES), new SameShardAllocationDecider(Settings.EMPTY, new ClusterSettings(Settings.EMPTY, ClusterSettings.BUILT_IN_CLUSTER_SETTINGS)), new AllocationDecider() {\r\n        @Override\r\n        public Decision canAllocate(ShardRouting shardRouting, RoutingNode node, RoutingAllocation allocation) {\r\n            if (node.node().equals(node2)) {\r\n                return Decision.THROTTLE;\r\n            }\r\n            return Decision.YES;\r\n        }\r\n    })));\r\n    testAllocator.addData(node1, \"MATCH\", new StoreFileMetaData(\"file1\", 10, \"MATCH_CHECKSUM\", MIN_SUPPORTED_LUCENE_VERSION)).addData(node2, \"MATCH\", new StoreFileMetaData(\"file1\", 10, \"MATCH_CHECKSUM\", MIN_SUPPORTED_LUCENE_VERSION));\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().get(0).shardId(), equalTo(shardId));\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.ReplicaShardAllocatorTests.testThrottleWhenAllocatingToMatchingNode",
	"Comment": "tests when the node to allocate to due to matching is being throttled, we move the shard to ignoredto wait till throttling on it is done.",
	"Method": "void testThrottleWhenAllocatingToMatchingNode(){\r\n    if (node.node().equals(node2)) {\r\n        return Decision.THROTTLE;\r\n    }\r\n    return Decision.YES;\r\n}"
}, {
	"Path": "org.elasticsearch.common.cache.CacheTests.testInvalidateWithValue",
	"Comment": "randomly invalidate some cached entries, then check that a lookup for each of those and only those keys is null",
	"Method": "void testInvalidateWithValue(){\r\n    Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();\r\n    for (int i = 0; i < numberOfEntries; i++) {\r\n        cache.put(i, Integer.toString(i));\r\n    }\r\n    Set<Integer> keys = new HashSet();\r\n    for (Integer key : cache.keys()) {\r\n        if (rarely()) {\r\n            if (randomBoolean()) {\r\n                cache.invalidate(key, key.toString());\r\n                keys.add(key);\r\n            } else {\r\n                cache.invalidate(key, Integer.toString(key + randomIntBetween(2, 10)));\r\n            }\r\n        }\r\n    }\r\n    for (int i = 0; i < numberOfEntries; i++) {\r\n        if (keys.contains(i)) {\r\n            assertNull(cache.get(i));\r\n        } else {\r\n            assertNotNull(cache.get(i));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.histogram.AutoDateHistogramAggregationBuilder.buildRoundings",
	"Comment": "build roundings, computed dynamically as roundings are time zone dependent.the current implementation probably should not be invoked in a tight loop.",
	"Method": "RoundingInfo[] buildRoundings(DateTimeZone timeZone){\r\n    RoundingInfo[] roundings = new RoundingInfo[6];\r\n    roundings[0] = new RoundingInfo(createRounding(DateTimeUnit.SECOND_OF_MINUTE, timeZone), 1000L, \"s\", 1, 5, 10, 30);\r\n    roundings[1] = new RoundingInfo(createRounding(DateTimeUnit.MINUTES_OF_HOUR, timeZone), 60 * 1000L, \"m\", 1, 5, 10, 30);\r\n    roundings[2] = new RoundingInfo(createRounding(DateTimeUnit.HOUR_OF_DAY, timeZone), 60 * 60 * 1000L, \"h\", 1, 3, 12);\r\n    roundings[3] = new RoundingInfo(createRounding(DateTimeUnit.DAY_OF_MONTH, timeZone), 24 * 60 * 60 * 1000L, \"d\", 1, 7);\r\n    roundings[4] = new RoundingInfo(createRounding(DateTimeUnit.MONTH_OF_YEAR, timeZone), 30 * 24 * 60 * 60 * 1000L, \"M\", 1, 3);\r\n    roundings[5] = new RoundingInfo(createRounding(DateTimeUnit.YEAR_OF_CENTURY, timeZone), 365 * 24 * 60 * 60 * 1000L, \"y\", 1, 5, 10, 20, 50, 100);\r\n    return roundings;\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.RestoreService.isRepositoryInUse",
	"Comment": "checks if a repository is currently in use by one of the snapshots",
	"Method": "boolean isRepositoryInUse(ClusterState clusterState,String repository){\r\n    RestoreInProgress snapshots = clusterState.custom(RestoreInProgress.TYPE);\r\n    if (snapshots != null) {\r\n        for (RestoreInProgress.Entry snapshot : snapshots.entries()) {\r\n            if (repository.equals(snapshot.snapshot().getRepository())) {\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.removeFinishedSnapshotFromClusterState",
	"Comment": "removes a finished snapshot from the cluster state.this can happen if the previousmaster node processed a cluster state update that marked the snapshot as finished,but the previous master node died before removing the snapshot in progress from thecluster state.it is then the responsibility of the new master node to end thesnapshot and remove it from the cluster state.",
	"Method": "void removeFinishedSnapshotFromClusterState(ClusterChangedEvent event){\r\n    if (event.localNodeMaster() && !event.previousState().nodes().isLocalNodeElectedMaster()) {\r\n        SnapshotsInProgress snapshotsInProgress = event.state().custom(SnapshotsInProgress.TYPE);\r\n        if (snapshotsInProgress != null && !snapshotsInProgress.entries().isEmpty()) {\r\n            for (SnapshotsInProgress.Entry entry : snapshotsInProgress.entries()) {\r\n                if (entry.state().completed()) {\r\n                    endSnapshot(entry);\r\n                }\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESSingleNodeTestCase.nodeSettings",
	"Comment": "additional settings to add when creating the node. also allows overriding the default settings.",
	"Method": "Settings nodeSettings(){\r\n    return Settings.EMPTY;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.FieldSortBuilderTests.testBuildNested",
	"Comment": "test that the sort builder nested object gets created in the sortfield",
	"Method": "void testBuildNested(){\r\n    QueryShardContext shardContextMock = createMockShardContext();\r\n    FieldSortBuilder sortBuilder = new FieldSortBuilder(\"fieldName\").setNestedSort(new NestedSortBuilder(\"path\").setFilter(QueryBuilders.termQuery(MAPPED_STRING_FIELDNAME, \"value\")));\r\n    SortField sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    XFieldComparatorSource comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    Nested nested = comparatorSource.nested();\r\n    assertNotNull(nested);\r\n    assertEquals(new TermQuery(new Term(MAPPED_STRING_FIELDNAME, \"value\")), nested.getInnerQuery());\r\n    sortBuilder = new FieldSortBuilder(\"fieldName\").setNestedPath(\"path\");\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    nested = comparatorSource.nested();\r\n    assertNotNull(nested);\r\n    assertEquals(new TermQuery(new Term(TypeFieldMapper.NAME, \"__path\")), nested.getInnerQuery());\r\n    sortBuilder = new FieldSortBuilder(\"fieldName\").setNestedPath(\"path\").setNestedFilter(QueryBuilders.termQuery(MAPPED_STRING_FIELDNAME, \"value\"));\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    nested = comparatorSource.nested();\r\n    assertNotNull(nested);\r\n    assertEquals(new TermQuery(new Term(MAPPED_STRING_FIELDNAME, \"value\")), nested.getInnerQuery());\r\n    sortBuilder = new FieldSortBuilder(\"fieldName\").setNestedFilter(QueryBuilders.termQuery(MAPPED_STRING_FIELDNAME, \"value\"));\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField, instanceOf(SortedNumericSortField.class));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.significant.SignificanceHeuristicTests.testBuilderAndParser",
	"Comment": "2. the parser does not swallow parameters after a significance heuristic was defined",
	"Method": "void testBuilderAndParser(){\r\n    SearchModule searchModule = new SearchModule(Settings.EMPTY, false, emptyList());\r\n    ParseFieldRegistry<SignificanceHeuristicParser> heuristicParserMapper = searchModule.getSignificanceHeuristicParserRegistry();\r\n    assertTrue(parseFromString(heuristicParserMapper, \"\\\"jlh\\\":{}\") instanceof JLHScore);\r\n    assertTrue(parseFromString(heuristicParserMapper, \"\\\"gnd\\\":{}\") instanceof GND);\r\n    boolean includeNegatives = randomBoolean();\r\n    boolean backgroundIsSuperset = randomBoolean();\r\n    String mutual = \"\\\"mutual_information\\\":{\\\"include_negatives\\\": \" + includeNegatives + \", \\\"background_is_superset\\\":\" + backgroundIsSuperset + \"}\";\r\n    assertEquals(new MutualInformation(includeNegatives, backgroundIsSuperset), parseFromString(heuristicParserMapper, mutual));\r\n    String chiSquare = \"\\\"chi_square\\\":{\\\"include_negatives\\\": \" + includeNegatives + \", \\\"background_is_superset\\\":\" + backgroundIsSuperset + \"}\";\r\n    assertEquals(new ChiSquare(includeNegatives, backgroundIsSuperset), parseFromString(heuristicParserMapper, chiSquare));\r\n    assertThat(parseFromBuilder(heuristicParserMapper, new JLHScore()), instanceOf(JLHScore.class));\r\n    assertThat(parseFromBuilder(heuristicParserMapper, new GND(backgroundIsSuperset)), instanceOf(GND.class));\r\n    assertEquals(new MutualInformation(includeNegatives, backgroundIsSuperset), parseFromBuilder(heuristicParserMapper, new MutualInformation(includeNegatives, backgroundIsSuperset)));\r\n    assertEquals(new ChiSquare(includeNegatives, backgroundIsSuperset), parseFromBuilder(heuristicParserMapper, new ChiSquare(includeNegatives, backgroundIsSuperset)));\r\n    String faultyHeuristicdefinition = \"\\\"mutual_information\\\":{\\\"include_negatives\\\": false, \\\"some_unknown_field\\\": false}\";\r\n    String expectedError = \"unknown field [some_unknown_field]\";\r\n    checkParseException(heuristicParserMapper, faultyHeuristicdefinition, expectedError);\r\n    faultyHeuristicdefinition = \"\\\"chi_square\\\":{\\\"unknown_field\\\": true}\";\r\n    expectedError = \"unknown field [unknown_field]\";\r\n    checkParseException(heuristicParserMapper, faultyHeuristicdefinition, expectedError);\r\n    faultyHeuristicdefinition = \"\\\"jlh\\\":{\\\"unknown_field\\\": true}\";\r\n    expectedError = \"expected an empty object, but found \";\r\n    checkParseException(heuristicParserMapper, faultyHeuristicdefinition, expectedError);\r\n    faultyHeuristicdefinition = \"\\\"gnd\\\":{\\\"unknown_field\\\": true}\";\r\n    expectedError = \"unknown field [unknown_field]\";\r\n    checkParseException(heuristicParserMapper, faultyHeuristicdefinition, expectedError);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBoundsTests.unparsed",
	"Comment": "convert an extended bounds in parsed for into one in unparsed form.",
	"Method": "ExtendedBounds unparsed(ExtendedBounds template){\r\n    FormatDateTimeFormatter formatter = Joda.forPattern(\"dateOptionalTime\");\r\n    String minAsStr = template.getMin() == null ? null : formatter.printer().print(new Instant(template.getMin()));\r\n    String maxAsStr = template.getMax() == null ? null : formatter.printer().print(new Instant(template.getMax()));\r\n    return new ExtendedBounds(minAsStr, maxAsStr);\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.term.TermSuggestionBuilderTests.randomSuggestionBuilder",
	"Comment": "creates random suggestion builder, renders it to xcontent and back to new instance that should be equal to original",
	"Method": "TermSuggestionBuilder randomSuggestionBuilder(){\r\n    return randomTermSuggestionBuilder();\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractSerializingTestCase.getRandomFieldsExcludeFilter",
	"Comment": "returns a predicate that given the field name indicates whether the field has to be excluded from random fields insertion or not",
	"Method": "Predicate<String> getRandomFieldsExcludeFilter(){\r\n    return field -> false;\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.ESRestTestCase.restClientSettings",
	"Comment": "used to obtain settings for the rest client that is used to send rest requests.",
	"Method": "Settings restClientSettings(){\r\n    Settings.Builder builder = Settings.builder();\r\n    if (System.getProperty(\"tests.rest.client_path_prefix\") != null) {\r\n        builder.put(CLIENT_PATH_PREFIX, System.getProperty(\"tests.rest.client_path_prefix\"));\r\n    }\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.sort",
	"Comment": "adds a sort against the given field name and the sort ordering.",
	"Method": "SearchSourceBuilder sort(String name,SortOrder order,SearchSourceBuilder sort,String name,SearchSourceBuilder sort,SortBuilder<?> sort){\r\n    if (sorts == null) {\r\n        sorts = new ArrayList();\r\n    }\r\n    sorts.add(sort);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.SuggestBuilders.phraseSuggestion",
	"Comment": "creates a phrase suggestion lookup query with the provided field",
	"Method": "PhraseSuggestionBuilder phraseSuggestion(String fieldname){\r\n    return new PhraseSuggestionBuilder(fieldname);\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractXContentTestCase.getRandomFieldsExcludeFilter",
	"Comment": "returns a predicate that given the field name indicates whether the field has to be excluded from random fields insertion or not",
	"Method": "Predicate<String> getRandomFieldsExcludeFilter(){\r\n    return field -> false;\r\n}"
}, {
	"Path": "org.elasticsearch.search.MockSearchService.removeActiveContext",
	"Comment": "clear an active search context from the list of tracked contexts. package private for testing.",
	"Method": "void removeActiveContext(SearchContext context){\r\n    ACTIVE_SEARCH_CONTEXTS.remove(context);\r\n}"
}, {
	"Path": "org.elasticsearch.common.RoundingTests.nastyDate",
	"Comment": "to be even more nasty, go to a transition in the selected time zone.in one third of the cases stay there, otherwise go half a unit back or forth",
	"Method": "long nastyDate(long initialDate,ZoneId timezone,long unitMillis){\r\n    ZoneOffsetTransition transition = timezone.getRules().nextTransition(Instant.ofEpochMilli(initialDate));\r\n    long date = initialDate;\r\n    if (transition != null) {\r\n        date = transition.getInstant().toEpochMilli();\r\n    }\r\n    if (randomBoolean()) {\r\n        return date + (randomLong() % unitMillis);\r\n    } else {\r\n        return date;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.pipeline.DateDerivativeIT.testSingleValuedFieldNormalised_timeZone_CET_DstStart",
	"Comment": "do a derivative on a date histogram with time zone cet at dst start",
	"Method": "void testSingleValuedFieldNormalised_timeZone_CET_DstStart(){\r\n    createIndex(IDX_DST_START);\r\n    List<IndexRequestBuilder> builders = new ArrayList();\r\n    DateTimeZone timezone = DateTimeZone.forID(\"CET\");\r\n    addNTimes(1, IDX_DST_START, new DateTime(\"2012-03-24T01:00:00\", timezone), builders);\r\n    addNTimes(2, IDX_DST_START, new DateTime(\"2012-03-25T01:00:00\", timezone), builders);\r\n    addNTimes(3, IDX_DST_START, new DateTime(\"2012-03-26T01:00:00\", timezone), builders);\r\n    addNTimes(4, IDX_DST_START, new DateTime(\"2012-03-27T01:00:00\", timezone), builders);\r\n    indexRandom(true, builders);\r\n    ensureSearchable();\r\n    SearchResponse response = client().prepareSearch(IDX_DST_START).addAggregation(dateHistogram(\"histo\").field(\"date\").dateHistogramInterval(DateHistogramInterval.DAY).timeZone(timezone).minDocCount(0).subAggregation(derivative(\"deriv\", \"_count\").unit(DateHistogramInterval.HOUR))).execute().actionGet();\r\n    assertSearchResponse(response);\r\n    Histogram deriv = response.getAggregations().get(\"histo\");\r\n    assertThat(deriv, notNullValue());\r\n    assertThat(deriv.getName(), equalTo(\"histo\"));\r\n    List<? extends Bucket> buckets = deriv.getBuckets();\r\n    assertThat(buckets.size(), equalTo(4));\r\n    assertBucket(buckets.get(0), new DateTime(\"2012-03-24\", timezone).toDateTime(DateTimeZone.UTC), 1L, nullValue(), null, null);\r\n    assertBucket(buckets.get(1), new DateTime(\"2012-03-25\", timezone).toDateTime(DateTimeZone.UTC), 2L, notNullValue(), 1d, 1d / 24d);\r\n    assertBucket(buckets.get(2), new DateTime(\"2012-03-26\", timezone).toDateTime(DateTimeZone.UTC), 3L, notNullValue(), 1d, 1d / 23d);\r\n    assertBucket(buckets.get(3), new DateTime(\"2012-03-27\", timezone).toDateTime(DateTimeZone.UTC), 4L, notNullValue(), 1d, 1d / 24d);\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.forceUnigrams",
	"Comment": "if set to true the phrase suggester will fail if the analyzer onlyproduces ngrams. the default it true.",
	"Method": "PhraseSuggestionBuilder forceUnigrams(boolean forceUnigrams,Boolean forceUnigrams){\r\n    return this.forceUnigrams;\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.TaskAwareRequest.getDescription",
	"Comment": "returns optional description of the request to be displayed by the task manager",
	"Method": "String getDescription(){\r\n    return \"\";\r\n}"
}, {
	"Path": "org.elasticsearch.search.Scroll.keepAlive",
	"Comment": "how long the resources will be kept open to support the scroll request.",
	"Method": "TimeValue keepAlive(){\r\n    return keepAlive;\r\n}"
}, {
	"Path": "org.elasticsearch.search.SearchHit.getClusterAlias",
	"Comment": "returns the cluster alias this hit comes from or null if it comes from a local cluster",
	"Method": "String getClusterAlias(){\r\n    return clusterAlias;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.IndicesRequestCacheIT.testCacheAggs",
	"Comment": "one of the primary purposes of the query cache is to cache aggs results",
	"Method": "void testCacheAggs(){\r\n    Client client = client();\r\n    assertAcked(client.admin().indices().prepareCreate(\"index\").addMapping(\"type\", \"f\", \"type=date\").setSettings(Settings.builder().put(IndicesRequestCache.INDEX_CACHE_REQUEST_ENABLED_SETTING.getKey(), true)).get());\r\n    indexRandom(true, client.prepareIndex(\"index\", \"type\").setSource(\"f\", \"2014-03-10T00:00:00.000Z\"), client.prepareIndex(\"index\", \"type\").setSource(\"f\", \"2014-05-13T00:00:00.000Z\"));\r\n    ensureSearchable(\"index\");\r\n    final SearchResponse r1 = client.prepareSearch(\"index\").setSize(0).setSearchType(SearchType.QUERY_THEN_FETCH).addAggregation(dateHistogram(\"histo\").field(\"f\").timeZone(DateTimeZone.forID(\"+01:00\")).minDocCount(0).dateHistogramInterval(DateHistogramInterval.MONTH)).get();\r\n    assertSearchResponse(r1);\r\n    assertThat(client.admin().indices().prepareStats(\"index\").setRequestCache(true).get().getTotal().getRequestCache().getMemorySizeInBytes(), greaterThan(0L));\r\n    for (int i = 0; i < 10; ++i) {\r\n        final SearchResponse r2 = client.prepareSearch(\"index\").setSize(0).setSearchType(SearchType.QUERY_THEN_FETCH).addAggregation(dateHistogram(\"histo\").field(\"f\").timeZone(DateTimeZone.forID(\"+01:00\")).minDocCount(0).dateHistogramInterval(DateHistogramInterval.MONTH)).get();\r\n        assertSearchResponse(r2);\r\n        Histogram h1 = r1.getAggregations().get(\"histo\");\r\n        Histogram h2 = r2.getAggregations().get(\"histo\");\r\n        final List<? extends Bucket> buckets1 = h1.getBuckets();\r\n        final List<? extends Bucket> buckets2 = h2.getBuckets();\r\n        assertEquals(buckets1.size(), buckets2.size());\r\n        for (int j = 0; j < buckets1.size(); ++j) {\r\n            final Bucket b1 = buckets1.get(j);\r\n            final Bucket b2 = buckets2.get(j);\r\n            assertEquals(b1.getKey(), b2.getKey());\r\n            assertEquals(b1.getDocCount(), b2.getDocCount());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.InternalAggregation.readSize",
	"Comment": "read a size under the assumption that a value of 0 means unlimited.",
	"Method": "int readSize(StreamInput in){\r\n    final int size = in.readVInt();\r\n    return size == 0 ? Integer.MAX_VALUE : size;\r\n}"
}, {
	"Path": "org.elasticsearch.test.transport.MockTransportService.addConnectBehavior",
	"Comment": "adds a new connect behavior that is used for creating connections with the given delegate address.",
	"Method": "boolean addConnectBehavior(TransportService transportService,StubbableTransport.OpenConnectionBehavior connectBehavior,boolean addConnectBehavior,TransportAddress transportAddress,StubbableTransport.OpenConnectionBehavior connectBehavior){\r\n    return transport().addConnectBehavior(transportAddress, connectBehavior);\r\n}"
}, {
	"Path": "org.elasticsearch.protocol.xpack.XPackUsageResponse.getUsages",
	"Comment": "return a map from feature name to usage information for that feature.",
	"Method": "Map<String, Map<String, Object>> getUsages(){\r\n    return usages;\r\n}"
}, {
	"Path": "org.elasticsearch.action.delete.DeleteResponseTests.testFromXContentWithRandomFields",
	"Comment": "this test adds random fields and objects to the xcontent rendered out toensure we can parse it back to be forward compatible with additions tothe xcontent",
	"Method": "void testFromXContentWithRandomFields(){\r\n    doFromXContentTestWithRandomFields(true);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.histogram.InternalAutoDateHistogramTests.testGetAppropriateRoundingUsesCorrectIntervals",
	"Comment": "this test was added to reproduce a bug where getappropriaterounding was only ever using the first innerintervalspassed in, instead of using the interval associated with the loop.",
	"Method": "void testGetAppropriateRoundingUsesCorrectIntervals(){\r\n    RoundingInfo[] roundings = new RoundingInfo[6];\r\n    DateTimeZone timeZone = DateTimeZone.UTC;\r\n    roundings[0] = new RoundingInfo(createRounding(DateTimeUnit.SECOND_OF_MINUTE, timeZone), 1000L, \"s\", 1000);\r\n    roundings[1] = new RoundingInfo(createRounding(DateTimeUnit.MINUTES_OF_HOUR, timeZone), 60 * 1000L, \"m\", 1, 5, 10, 30);\r\n    roundings[2] = new RoundingInfo(createRounding(DateTimeUnit.HOUR_OF_DAY, timeZone), 60 * 60 * 1000L, \"h\", 1, 3, 12);\r\n    OffsetDateTime timestamp = Instant.parse(\"2018-01-01T00:00:01.000Z\").atOffset(ZoneOffset.UTC);\r\n    int result = InternalAutoDateHistogram.getAppropriateRounding(timestamp.toEpochSecond() * 1000, timestamp.plusDays(1).toEpochSecond() * 1000, 0, roundings, 25);\r\n    assertThat(result, equalTo(2));\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.ReplicaShardAllocatorTests.testNoAsyncFetchData",
	"Comment": "verifies that when we are still fetching data in an async manner, the replica shard moves to ignore unassigned.",
	"Method": "void testNoAsyncFetchData(){\r\n    RoutingAllocation allocation = onePrimaryOnNode1And1Replica(yesAllocationDeciders());\r\n    testAllocator.clean();\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodes().unassigned().ignored().size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().get(0).shardId(), equalTo(shardId));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.composite.CompositeValuesCollectorQueue.getLeafCollector",
	"Comment": "creates the collector that will visit the composite buckets of the matching documents.if forceleadsourcevalue is not null, the leading source will use this valuefor each document.the provided collector in is called on each composite bucket.",
	"Method": "LeafBucketCollector getLeafCollector(LeafReaderContext context,LeafBucketCollector in,LeafBucketCollector getLeafCollector,Comparable<?> forceLeadSourceValue,LeafReaderContext context,LeafBucketCollector in){\r\n    int last = arrays.length - 1;\r\n    LeafBucketCollector collector = in;\r\n    while (last > 0) {\r\n        collector = arrays[last--].getLeafCollector(context, collector);\r\n    }\r\n    if (forceLeadSourceValue != null) {\r\n        collector = arrays[last].getLeafCollector(forceLeadSourceValue, context, collector);\r\n    } else {\r\n        collector = arrays[last].getLeafCollector(context, collector);\r\n    }\r\n    return collector;\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.FieldSortBuilderTests.testBuildSortFieldMissingValue",
	"Comment": "test that missing values get transferred correctly to the sortfield",
	"Method": "void testBuildSortFieldMissingValue(){\r\n    QueryShardContext shardContextMock = createMockShardContext();\r\n    FieldSortBuilder fieldSortBuilder = new FieldSortBuilder(\"value\").missing(\"_first\");\r\n    SortField sortField = fieldSortBuilder.build(shardContextMock).field;\r\n    SortedNumericSortField expectedSortField = new SortedNumericSortField(\"value\", SortField.Type.DOUBLE);\r\n    expectedSortField.setMissingValue(Double.NEGATIVE_INFINITY);\r\n    assertEquals(expectedSortField, sortField);\r\n    fieldSortBuilder = new FieldSortBuilder(\"value\").missing(\"_last\");\r\n    sortField = fieldSortBuilder.build(shardContextMock).field;\r\n    expectedSortField = new SortedNumericSortField(\"value\", SortField.Type.DOUBLE);\r\n    expectedSortField.setMissingValue(Double.POSITIVE_INFINITY);\r\n    assertEquals(expectedSortField, sortField);\r\n    Double randomDouble = randomDouble();\r\n    fieldSortBuilder = new FieldSortBuilder(\"value\").missing(randomDouble);\r\n    sortField = fieldSortBuilder.build(shardContextMock).field;\r\n    expectedSortField = new SortedNumericSortField(\"value\", SortField.Type.DOUBLE);\r\n    expectedSortField.setMissingValue(randomDouble);\r\n    assertEquals(expectedSortField, sortField);\r\n    fieldSortBuilder = new FieldSortBuilder(\"value\").missing(randomDouble.toString());\r\n    sortField = fieldSortBuilder.build(shardContextMock).field;\r\n    expectedSortField = new SortedNumericSortField(\"value\", SortField.Type.DOUBLE);\r\n    expectedSortField.setMissingValue(randomDouble);\r\n    assertEquals(expectedSortField, sortField);\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESTestCase.randomZone",
	"Comment": "generate a random timezone from the ones available in java.time",
	"Method": "ZoneId randomZone(){\r\n    return ZoneId.of(randomFrom(JAVA_ZONE_IDS));\r\n}"
}, {
	"Path": "org.elasticsearch.test.InternalTestCluster.assertSameDocIdsOnShards",
	"Comment": "asserts that all shards with the same shardid should have document ids.",
	"Method": "void assertSameDocIdsOnShards(){\r\n    assertBusy(() -> {\r\n        ClusterState state = client().admin().cluster().prepareState().get().getState();\r\n        for (ObjectObjectCursor<String, IndexRoutingTable> indexRoutingTable : state.routingTable().indicesRouting()) {\r\n            for (IntObjectCursor<IndexShardRoutingTable> indexShardRoutingTable : indexRoutingTable.value.shards()) {\r\n                ShardRouting primaryShardRouting = indexShardRoutingTable.value.primaryShard();\r\n                if (primaryShardRouting == null || primaryShardRouting.assignedToNode() == false) {\r\n                    continue;\r\n                }\r\n                DiscoveryNode primaryNode = state.nodes().get(primaryShardRouting.currentNodeId());\r\n                IndexShard primaryShard = getInstance(IndicesService.class, primaryNode.getName()).indexServiceSafe(primaryShardRouting.index()).getShard(primaryShardRouting.id());\r\n                final List<DocIdSeqNoAndTerm> docsOnPrimary;\r\n                try {\r\n                    docsOnPrimary = IndexShardTestCase.getDocIdAndSeqNos(primaryShard);\r\n                } catch (AlreadyClosedException ex) {\r\n                    continue;\r\n                }\r\n                for (ShardRouting replicaShardRouting : indexShardRoutingTable.value.replicaShards()) {\r\n                    if (replicaShardRouting.assignedToNode() == false) {\r\n                        continue;\r\n                    }\r\n                    DiscoveryNode replicaNode = state.nodes().get(replicaShardRouting.currentNodeId());\r\n                    IndexShard replicaShard = getInstance(IndicesService.class, replicaNode.getName()).indexServiceSafe(replicaShardRouting.index()).getShard(replicaShardRouting.id());\r\n                    final List<DocIdSeqNoAndTerm> docsOnReplica;\r\n                    try {\r\n                        docsOnReplica = IndexShardTestCase.getDocIdAndSeqNos(replicaShard);\r\n                    } catch (AlreadyClosedException ex) {\r\n                        continue;\r\n                    }\r\n                    assertThat(\"out of sync shards: primary=[\" + primaryShardRouting + \"] num_docs_on_primary=[\" + docsOnPrimary.size() + \"] vs replica=[\" + replicaShardRouting + \"] num_docs_on_replica=[\" + docsOnReplica.size() + \"]\", docsOnReplica, equalTo(docsOnPrimary));\r\n                }\r\n            }\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.NodeJoinControllerTests.cloneNode",
	"Comment": "creates an object clone of node, so it will be a different object instance",
	"Method": "DiscoveryNode cloneNode(DiscoveryNode node){\r\n    return new DiscoveryNode(node.getName(), node.getId(), node.getEphemeralId(), node.getHostName(), node.getHostAddress(), node.getAddress(), node.getAttributes(), node.getRoles(), node.getVersion());\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.terminateAfter",
	"Comment": "gets the number of documents to terminate after collecting.",
	"Method": "SearchSourceBuilder terminateAfter(int terminateAfter,int terminateAfter){\r\n    return terminateAfter;\r\n}"
}, {
	"Path": "org.elasticsearch.test.DiffableTestUtils.testDiffableSerialization",
	"Comment": "tests making random changes to an object, calculating diffs for these changes, sending thisdiffs over the wire and appling these diffs on the other side.",
	"Method": "void testDiffableSerialization(Supplier<T> testInstance,Function<T, T> modifier,NamedWriteableRegistry namedWriteableRegistry,Reader<T> reader,Reader<Diff<T>> diffReader){\r\n    T remoteInstance = testInstance.get();\r\n    T localInstance = assertSerialization(remoteInstance, namedWriteableRegistry, reader);\r\n    for (int runs = 0; runs < NUMBER_OF_DIFF_TEST_RUNS; runs++) {\r\n        T remoteChanges = modifier.apply(remoteInstance);\r\n        Diff<T> remoteDiffs = remoteChanges.diff(remoteInstance);\r\n        Diff<T> localDiffs = copyInstance(remoteDiffs, namedWriteableRegistry, diffReader);\r\n        localInstance = assertDiffApplication(remoteChanges, localInstance, localDiffs);\r\n        remoteInstance = remoteChanges;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.rest.MethodHandlers.getValidMethods",
	"Comment": "return a set of all valid http methods for the particular path",
	"Method": "Set<RestRequest.Method> getValidMethods(){\r\n    return methodHandlers.keySet();\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.completion.CompletionSuggestionBuilder.prefix",
	"Comment": "sets the prefix to provide completions for.the prefix gets analyzed by the suggest analyzer.",
	"Method": "CompletionSuggestionBuilder prefix(String prefix,CompletionSuggestionBuilder prefix,String prefix,Fuzziness fuzziness,CompletionSuggestionBuilder prefix,String prefix,FuzzyOptions fuzzyOptions){\r\n    super.prefix(prefix);\r\n    this.fuzzyOptions = fuzzyOptions;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.transportClientPlugins",
	"Comment": "returns a collection of plugins that should be loaded when creating a transport client.",
	"Method": "Collection<Class<? extends Plugin>> transportClientPlugins(){\r\n    return Collections.emptyList();\r\n}"
}, {
	"Path": "org.elasticsearch.test.BackgroundIndexer.setMinFieldSize",
	"Comment": "the minimum size in code points of a payload field in the indexed documents",
	"Method": "void setMinFieldSize(int fieldSize){\r\n    minFieldSize = fieldSize;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.ScriptedMetricIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    Script mapScript = new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, \"state['count'] = 1\", Collections.emptyMap());\r\n    Script combineScript = new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, \"no-op aggregation\", Collections.emptyMap());\r\n    Script reduceScript = new Script(ScriptType.INLINE, CustomScriptPlugin.NAME, \"no-op list aggregation\", Collections.emptyMap());\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(scriptedMetric(\"foo\").mapScript(mapScript).combineScript(combineScript).reduceScript(reduceScript)).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShardTests.testRecoverFromStoreWithNoOps",
	"Comment": "this test just verifies that we fill up local checkpoint up to max seen seqid on primary recovery",
	"Method": "void testRecoverFromStoreWithNoOps(){\r\n    final IndexShard shard = newStartedShard(true);\r\n    indexDoc(shard, \"_doc\", \"0\");\r\n    indexDoc(shard, \"_doc\", \"1\");\r\n    final IndexShard otherShard = newStartedShard(false);\r\n    updateMappings(otherShard, shard.indexSettings().getIndexMetaData());\r\n    SourceToParse sourceToParse = SourceToParse.source(shard.shardId().getIndexName(), \"_doc\", \"1\", new BytesArray(\"{}\"), XContentType.JSON);\r\n    otherShard.applyIndexOperationOnReplica(1, 1, IndexRequest.UNSET_AUTO_GENERATED_TIMESTAMP, false, sourceToParse);\r\n    final ShardRouting primaryShardRouting = shard.routingEntry();\r\n    IndexShard newShard = reinitShard(otherShard, ShardRoutingHelper.initWithSameId(primaryShardRouting, RecoverySource.ExistingStoreRecoverySource.INSTANCE));\r\n    DiscoveryNode localNode = new DiscoveryNode(\"foo\", buildNewFakeTransportAddress(), emptyMap(), emptySet(), Version.CURRENT);\r\n    newShard.markAsRecovering(\"store\", new RecoveryState(newShard.routingEntry(), localNode, null));\r\n    assertTrue(newShard.recoverFromStore());\r\n    assertEquals(1, newShard.recoveryState().getTranslog().recoveredOperations());\r\n    assertEquals(1, newShard.recoveryState().getTranslog().totalOperations());\r\n    assertEquals(1, newShard.recoveryState().getTranslog().totalOperationsOnStart());\r\n    assertEquals(100.0f, newShard.recoveryState().getTranslog().recoveredPercent(), 0.01f);\r\n    try (Translog.Snapshot snapshot = getTranslog(newShard).newSnapshot()) {\r\n        Translog.Operation operation;\r\n        int numNoops = 0;\r\n        while ((operation = snapshot.next()) != null) {\r\n            if (operation.opType() == Translog.Operation.Type.NO_OP) {\r\n                numNoops++;\r\n                assertEquals(newShard.getPendingPrimaryTerm(), operation.primaryTerm());\r\n                assertEquals(0, operation.seqNo());\r\n            }\r\n        }\r\n        assertEquals(1, numNoops);\r\n    }\r\n    IndexShardTestCase.updateRoutingEntry(newShard, newShard.routingEntry().moveToStarted());\r\n    assertDocCount(newShard, 1);\r\n    assertDocCount(shard, 2);\r\n    for (int i = 0; i < 2; i++) {\r\n        newShard = reinitShard(newShard, ShardRoutingHelper.initWithSameId(primaryShardRouting, RecoverySource.ExistingStoreRecoverySource.INSTANCE));\r\n        newShard.markAsRecovering(\"store\", new RecoveryState(newShard.routingEntry(), localNode, null));\r\n        assertTrue(newShard.recoverFromStore());\r\n        try (Translog.Snapshot snapshot = getTranslog(newShard).newSnapshot()) {\r\n            assertThat(snapshot.totalOperations(), equalTo(2));\r\n        }\r\n    }\r\n    closeShards(newShard, shard);\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.yaml.ClientYamlTestResponse.getWarningHeaders",
	"Comment": "get a list of all of the values of all warning headers returned in the response.",
	"Method": "List<String> getWarningHeaders(){\r\n    List<String> warningHeaders = new ArrayList();\r\n    for (Header header : response.getHeaders()) {\r\n        if (header.getName().equals(\"Warning\")) {\r\n            warningHeaders.add(header.getValue());\r\n        }\r\n    }\r\n    return warningHeaders;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.RemoteClusterService.getConnection",
	"Comment": "returns a connection to the given node on the given remote cluster",
	"Method": "Transport.Connection getConnection(DiscoveryNode node,String cluster,Transport.Connection getConnection,String cluster){\r\n    return getRemoteClusterConnection(cluster).getConnection();\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.removePluginCustoms",
	"Comment": "remove any customs except for customs that we know all clients understand.",
	"Method": "ClusterState removePluginCustoms(ClusterState clusterState){\r\n    final ClusterState.Builder builder = ClusterState.builder(clusterState);\r\n    clusterState.customs().keysIt().forEachRemaining(key -> {\r\n        if (SAFE_CUSTOMS.contains(key) == false) {\r\n            builder.removeCustom(key);\r\n        }\r\n    });\r\n    final MetaData.Builder mdBuilder = MetaData.builder(clusterState.metaData());\r\n    clusterState.metaData().customs().keysIt().forEachRemaining(key -> {\r\n        if (SAFE_METADATA_CUSTOMS.contains(key) == false) {\r\n            mdBuilder.removeCustom(key);\r\n        }\r\n    });\r\n    builder.metaData(mdBuilder);\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.config.Job.getCreateTime",
	"Comment": "the job creation time. this name is preferred when serialising to therest api.",
	"Method": "Date getCreateTime(){\r\n    return createTime;\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.phrase.DirectCandidateGeneratorBuilder.minDocFreq",
	"Comment": "sets a minimal threshold in number of documents a suggested termshould appear in. this can be specified as an absolute number or as arelative percentage of number of documents. this can improve qualityby only suggesting high frequency terms. defaults to 0f and is notenabled. if a value higher than 1 is specified then the number cannotbe fractional.",
	"Method": "DirectCandidateGeneratorBuilder minDocFreq(float minDocFreq,Float minDocFreq){\r\n    return minDocFreq;\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.ReplicaShardAllocatorTests.testAsyncFetchOnAnythingButIndexCreation",
	"Comment": "verifies that for anything but index creation, fetch data ends up being called, since we need to go and tryand find a better copy for the shard.",
	"Method": "void testAsyncFetchOnAnythingButIndexCreation(){\r\n    UnassignedInfo.Reason reason = RandomPicks.randomFrom(random(), EnumSet.complementOf(EnumSet.of(UnassignedInfo.Reason.INDEX_CREATED)));\r\n    RoutingAllocation allocation = onePrimaryOnNode1And1Replica(yesAllocationDeciders(), Settings.EMPTY, reason);\r\n    testAllocator.clean();\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(\"failed with reason \" + reason, testAllocator.getFetchDataCalledAndClean(), equalTo(true));\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocatorTests.testShardLockObtainFailedExceptionPreferOtherValidCopies",
	"Comment": "tests that when one node returns a shardlockobtainfailedexception and another properly loads the store, it willselect the second node as target",
	"Method": "void testShardLockObtainFailedExceptionPreferOtherValidCopies(){\r\n    String allocId1 = randomAlphaOfLength(10);\r\n    String allocId2 = randomAlphaOfLength(10);\r\n    final RoutingAllocation allocation = routingAllocationWithOnePrimaryNoReplicas(yesAllocationDeciders(), CLUSTER_RECOVERED, allocId1, allocId2);\r\n    testAllocator.addData(node1, allocId1, randomBoolean(), new ShardLockObtainFailedException(shardId, \"test\"));\r\n    testAllocator.addData(node2, allocId2, randomBoolean(), null);\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodesChanged(), equalTo(true));\r\n    assertThat(allocation.routingNodes().unassigned().ignored().isEmpty(), equalTo(true));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).size(), equalTo(1));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).currentNodeId(), equalTo(node2.getId()));\r\n    assertThat(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).get(0).allocationId().getId(), equalTo(allocId2));\r\n    assertClusterHealthStatus(allocation, ClusterHealthStatus.YELLOW);\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.TaskManager.setBan",
	"Comment": "bans all tasks with the specified parent task from execution, cancels all tasks that are currently executing.this method is called when a parent task that has children is cancelled.",
	"Method": "void setBan(TaskId parentTaskId,String reason){\r\n    logger.trace(\"setting ban for the parent task {} {}\", parentTaskId, reason);\r\n    synchronized (banedParents) {\r\n        if (lastDiscoveryNodes.nodeExists(parentTaskId.getNodeId())) {\r\n            banedParents.put(parentTaskId, reason);\r\n        }\r\n    }\r\n    for (Map.Entry<Long, CancellableTaskHolder> taskEntry : cancellableTasks.entrySet()) {\r\n        CancellableTaskHolder holder = taskEntry.getValue();\r\n        if (holder.hasParent(parentTaskId)) {\r\n            holder.cancel(reason);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.profile.SearchProfileShardResultsTests.testFromXContentWithRandomFields",
	"Comment": "this test adds random fields and objects to the xcontent rendered out to ensure we can parse itback to be forward compatible with additions to the xcontent",
	"Method": "void testFromXContentWithRandomFields(){\r\n    doFromXContentTestWithRandomFields(true);\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.job.config.Job.earliestValidTimestamp",
	"Comment": "returns the timestamp before which data is not accepted by the job.this is the latest record timestamp minus the job latency.",
	"Method": "long earliestValidTimestamp(DataCounts dataCounts){\r\n    long currentTime = 0;\r\n    Date latestRecordTimestamp = dataCounts.getLatestRecordTimeStamp();\r\n    if (latestRecordTimestamp != null) {\r\n        TimeValue latency = analysisConfig.getLatency();\r\n        long latencyMillis = latency == null ? 0 : latency.millis();\r\n        currentTime = latestRecordTimestamp.getTime() - latencyMillis;\r\n    }\r\n    return currentTime;\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.trackTotalHits",
	"Comment": "indicates if the total hit count for the query should be tracked.",
	"Method": "boolean trackTotalHits(SearchSourceBuilder trackTotalHits,boolean trackTotalHits){\r\n    this.trackTotalHits = trackTotalHits;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.DocumentParserTests.createObjectMapper",
	"Comment": "creates an object mapper, which is about 100x harder than it should be....",
	"Method": "ObjectMapper createObjectMapper(MapperService mapperService,String name){\r\n    IndexMetaData build = IndexMetaData.builder(\"\").settings(Settings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, Version.CURRENT)).numberOfShards(1).numberOfReplicas(0).build();\r\n    IndexSettings settings = new IndexSettings(build, Settings.EMPTY);\r\n    ParseContext context = new ParseContext.InternalParseContext(settings, mapperService.documentMapperParser(), mapperService.documentMapper(\"type\"), null, null);\r\n    String[] nameParts = name.split(\"\\\\.\");\r\n    for (int i = 0; i < nameParts.length - 1; ++i) {\r\n        context.path().add(nameParts[i]);\r\n    }\r\n    Mapper.Builder builder = new ObjectMapper.Builder(nameParts[nameParts.length - 1]).enabled(true);\r\n    Mapper.BuilderContext builderContext = new Mapper.BuilderContext(context.indexSettings().getSettings(), context.path());\r\n    return (ObjectMapper) builder.build(builderContext);\r\n}"
}, {
	"Path": "org.elasticsearch.search.DefaultSearchContext.sourceRequested",
	"Comment": "a shortcut function to see whether there is a fetchsourcecontext and it says the source is requested.",
	"Method": "boolean sourceRequested(){\r\n    return fetchSourceContext != null && fetchSourceContext.fetchSource();\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.completion.CompletionSuggestionBuilder.skipDuplicates",
	"Comment": "returns whether duplicate suggestions should be filtered out.",
	"Method": "boolean skipDuplicates(CompletionSuggestionBuilder skipDuplicates,boolean skipDuplicates){\r\n    this.skipDuplicates = skipDuplicates;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.MlTasks.jobTaskId",
	"Comment": "namespaces the task ids for jobs.a datafeed id can be used as a job id, because they are stored separately in cluster state.",
	"Method": "String jobTaskId(String jobId){\r\n    return \"job-\" + jobId;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ClientHelper.clientWithOrigin",
	"Comment": "returns a client that will always set the appropriate origin and ensure the proper context is restored by listeners",
	"Method": "Client clientWithOrigin(Client client,String origin){\r\n    return new OriginSettingClient(client, origin);\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.GeoDistanceSortBuilderTests.testBuildNested",
	"Comment": "test that the sort builder nested object gets created in the sortfield",
	"Method": "void testBuildNested(){\r\n    QueryShardContext shardContextMock = createMockShardContext();\r\n    GeoDistanceSortBuilder sortBuilder = new GeoDistanceSortBuilder(\"fieldName\", 1.0, 1.0).setNestedSort(new NestedSortBuilder(\"path\").setFilter(QueryBuilders.matchAllQuery()));\r\n    SortField sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    XFieldComparatorSource comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    Nested nested = comparatorSource.nested();\r\n    assertNotNull(nested);\r\n    assertEquals(new MatchAllDocsQuery(), nested.getInnerQuery());\r\n    sortBuilder = new GeoDistanceSortBuilder(\"fieldName\", 1.0, 1.0).setNestedPath(\"path\");\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    nested = comparatorSource.nested();\r\n    assertNotNull(nested);\r\n    assertEquals(new TermQuery(new Term(TypeFieldMapper.NAME, \"__path\")), nested.getInnerQuery());\r\n    sortBuilder = new GeoDistanceSortBuilder(\"fieldName\", 1.0, 1.0).setNestedPath(\"path\").setNestedFilter(QueryBuilders.matchAllQuery());\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField.getComparatorSource(), instanceOf(XFieldComparatorSource.class));\r\n    comparatorSource = (XFieldComparatorSource) sortField.getComparatorSource();\r\n    nested = comparatorSource.nested();\r\n    assertNotNull(nested);\r\n    assertEquals(new MatchAllDocsQuery(), nested.getInnerQuery());\r\n    sortBuilder = new GeoDistanceSortBuilder(\"fieldName\", 1.0, 1.0).setNestedFilter(QueryBuilders.termQuery(\"fieldName\", \"value\"));\r\n    sortField = sortBuilder.build(shardContextMock).field;\r\n    assertThat(sortField, instanceOf(SortField.class));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.terms.TermsAggregationBuilder.shardMinDocCount",
	"Comment": "returns the minimum document count required per term, per shard",
	"Method": "TermsAggregationBuilder shardMinDocCount(long shardMinDocCount,long shardMinDocCount){\r\n    return bucketCountThresholds.getShardMinDocCount();\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocatorTests.testDontForceAllocateOnThrottleDecision",
	"Comment": "tests that when the nodes with prior copies of the given shard return a throttle decision,then we do not force allocate to that node but instead throttle.",
	"Method": "void testDontForceAllocateOnThrottleDecision(){\r\n    testAllocator.addData(node1, \"allocId1\", randomBoolean());\r\n    AllocationDeciders deciders = new AllocationDeciders(// force allocating the shard, we still THROTTLE due to the decision from TestAllocateDecision\r\n    Arrays.asList(new TestAllocateDecision(Decision.THROTTLE), getNoDeciderThatAllowsForceAllocate()));\r\n    RoutingAllocation allocation = routingAllocationWithOnePrimaryNoReplicas(deciders, CLUSTER_RECOVERED, \"allocId1\");\r\n    testAllocator.allocateUnassigned(allocation);\r\n    assertThat(allocation.routingNodesChanged(), equalTo(true));\r\n    List<ShardRouting> ignored = allocation.routingNodes().unassigned().ignored();\r\n    assertEquals(ignored.size(), 1);\r\n    assertEquals(ignored.get(0).unassignedInfo().getLastAllocationStatus(), AllocationStatus.DECIDERS_THROTTLED);\r\n    assertTrue(allocation.routingNodes().shardsWithState(ShardRoutingState.INITIALIZING).isEmpty());\r\n}"
}, {
	"Path": "org.elasticsearch.common.time.JavaDateMathParserTests.testImplicitRounding",
	"Comment": "implicit rounding happening when parts of the date are not specified",
	"Method": "void testImplicitRounding(){\r\n    assertDateMathEquals(\"2014-11-18\", \"2014-11-18\", 0, false, null);\r\n    assertDateMathEquals(\"2014-11-18\", \"2014-11-18T23:59:59.999Z\", 0, true, null);\r\n    assertDateMathEquals(\"2014-11-18T09:20\", \"2014-11-18T09:20\", 0, false, null);\r\n    assertDateMathEquals(\"2014-11-18T09:20\", \"2014-11-18T09:20:59.999Z\", 0, true, null);\r\n    assertDateMathEquals(\"2014-11-18\", \"2014-11-17T23:00:00.000Z\", 0, false, ZoneId.of(\"CET\"));\r\n    assertDateMathEquals(\"2014-11-18\", \"2014-11-18T22:59:59.999Z\", 0, true, ZoneId.of(\"CET\"));\r\n    assertDateMathEquals(\"2014-11-18T09:20\", \"2014-11-18T08:20:00.000Z\", 0, false, ZoneId.of(\"CET\"));\r\n    assertDateMathEquals(\"2014-11-18T09:20\", \"2014-11-18T08:20:59.999Z\", 0, true, ZoneId.of(\"CET\"));\r\n    DateFormatter formatter = DateFormatters.forPattern(\"yyyy-MM-ddXXX\");\r\n    JavaDateMathParser parser = new JavaDateMathParser(formatter);\r\n    long time = parser.parse(\"2011-10-09+01:00\", () -> 0, false, (ZoneId) null);\r\n    assertEquals(this.parser.parse(\"2011-10-09T00:00:00.000+01:00\", () -> 0), time);\r\n    time = parser.parse(\"2011-10-09+01:00\", () -> 0, true, (ZoneId) null);\r\n    assertEquals(this.parser.parse(\"2011-10-09T23:59:59.999+01:00\", () -> 0), time);\r\n}"
}, {
	"Path": "org.elasticsearch.BuildTests.testJarMetadata",
	"Comment": "asking for the jar metadata should not throw exception in tests, no matter how configured",
	"Method": "void testJarMetadata(){\r\n    URL url = Build.getElasticsearchCodeSourceLocation();\r\n    try (InputStream ignored = FileSystemUtils.openFileURLStream(url)) {\r\n    }\r\n    assertNotNull(Build.CURRENT.date());\r\n    assertNotNull(Build.CURRENT.shortHash());\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.nodePlugins",
	"Comment": "returns a collection of plugins that should be loaded on each node.",
	"Method": "Collection<Class<? extends Plugin>> nodePlugins(){\r\n    return Collections.emptyList();\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.timeout",
	"Comment": "gets the timeout to control how long search is allowed to take.",
	"Method": "SearchSourceBuilder timeout(TimeValue timeout,TimeValue timeout){\r\n    return timeout;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.ShardSearchFailureTests.testFromXContentWithRandomFields",
	"Comment": "this test adds random fields and objects to the xcontent rendered out toensure we can parse it back to be forward compatible with additions tothe xcontent",
	"Method": "void testFromXContentWithRandomFields(){\r\n    doFromXContentTestWithRandomFields(true);\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.AbstractSuggestionBuilderTestCase.testSerialization",
	"Comment": "test serialization and deserialization of the suggestion builder",
	"Method": "void testSerialization(){\r\n    for (int runs = 0; runs < NUMBER_OF_TESTBUILDERS; runs++) {\r\n        SB original = randomTestBuilder();\r\n        SB deserialized = copy(original);\r\n        assertEquals(deserialized, original);\r\n        assertEquals(deserialized.hashCode(), original.hashCode());\r\n        assertNotSame(deserialized, original);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.rest.RestRequest.getHeaders",
	"Comment": "get all of the headers and values associated with the headers. modifications of this map are not supported.",
	"Method": "Map<String, List<String>> getHeaders(){\r\n    return headers;\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.TaskManager.getCancellableTasks",
	"Comment": "returns the list of currently running tasks on the node that can be cancelled",
	"Method": "Map<Long, CancellableTask> getCancellableTasks(){\r\n    HashMap<Long, CancellableTask> taskHashMap = new HashMap();\r\n    for (CancellableTaskHolder holder : cancellableTasks.values()) {\r\n        taskHashMap.put(holder.getTask().getId(), holder.getTask());\r\n    }\r\n    return Collections.unmodifiableMap(taskHashMap);\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.TDigestPercentilesIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(percentiles(\"foo\").field(\"d\").percentiles(50.0).script(new Script(ScriptType.INLINE, AggregationTestScriptsPlugin.NAME, \"_value - 1\", emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(percentiles(\"foo\").field(\"d\").percentiles(50.0)).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.bucket.histogram.DateHistogramAggregationBuilder.extendedBounds",
	"Comment": "set extended bounds on this histogram, so that buckets would also be generated on intervals that did not match any documents.",
	"Method": "ExtendedBounds extendedBounds(DateHistogramAggregationBuilder extendedBounds,ExtendedBounds extendedBounds){\r\n    if (extendedBounds == null) {\r\n        throw new IllegalArgumentException(\"[extendedBounds] must not be null: [\" + name + \"]\");\r\n    }\r\n    this.extendedBounds = extendedBounds;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.RemoteClusterConnection.updateSkipUnavailable",
	"Comment": "updates the skipunavailable flag that can be dynamically set for each remote cluster",
	"Method": "void updateSkipUnavailable(boolean skipUnavailable){\r\n    this.skipUnavailable = skipUnavailable;\r\n}"
}, {
	"Path": "org.elasticsearch.common.cache.CacheTests.testInvalidateAll",
	"Comment": "invalidate all cached entries, then check that the cache is empty",
	"Method": "void testInvalidateAll(){\r\n    Cache<Integer, String> cache = CacheBuilder.<Integer, String>builder().build();\r\n    for (int i = 0; i < numberOfEntries; i++) {\r\n        cache.put(i, Integer.toString(i));\r\n    }\r\n    cache.invalidateAll();\r\n    assertEquals(0, cache.count());\r\n    assertEquals(0, cache.weight());\r\n}"
}, {
	"Path": "org.elasticsearch.test.rest.ESRestTestCase.getStrictDeprecationMode",
	"Comment": "whether the used rest client should return any response containing atleast one warning header as a failure.",
	"Method": "boolean getStrictDeprecationMode(){\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.ClusterStateCreationUtils.stateWithAssignedPrimariesAndOneReplica",
	"Comment": "creates cluster state with several shards and one replica and all shards started.",
	"Method": "ClusterState stateWithAssignedPrimariesAndOneReplica(String index,int numberOfShards){\r\n    int numberOfNodes = 2;\r\n    DiscoveryNodes.Builder discoBuilder = DiscoveryNodes.builder();\r\n    for (int i = 0; i < numberOfNodes + 1; i++) {\r\n        final DiscoveryNode node = newNode(i);\r\n        discoBuilder = discoBuilder.add(node);\r\n    }\r\n    discoBuilder.localNodeId(newNode(0).getId());\r\n    discoBuilder.masterNodeId(newNode(1).getId());\r\n    IndexMetaData indexMetaData = IndexMetaData.builder(index).settings(Settings.builder().put(SETTING_VERSION_CREATED, Version.CURRENT).put(SETTING_NUMBER_OF_SHARDS, numberOfShards).put(SETTING_NUMBER_OF_REPLICAS, 1).put(SETTING_CREATION_DATE, System.currentTimeMillis())).build();\r\n    ClusterState.Builder state = ClusterState.builder(new ClusterName(\"test\"));\r\n    state.nodes(discoBuilder);\r\n    state.metaData(MetaData.builder().put(indexMetaData, false).generateClusterUuidIfNeeded());\r\n    IndexRoutingTable.Builder indexRoutingTableBuilder = IndexRoutingTable.builder(indexMetaData.getIndex());\r\n    for (int i = 0; i < numberOfShards; i++) {\r\n        RoutingTable.Builder routing = new RoutingTable.Builder();\r\n        routing.addAsNew(indexMetaData);\r\n        final ShardId shardId = new ShardId(index, \"_na_\", i);\r\n        IndexShardRoutingTable.Builder indexShardRoutingBuilder = new IndexShardRoutingTable.Builder(shardId);\r\n        indexShardRoutingBuilder.addShard(TestShardRouting.newShardRouting(index, i, newNode(0).getId(), null, true, ShardRoutingState.STARTED));\r\n        indexShardRoutingBuilder.addShard(TestShardRouting.newShardRouting(index, i, newNode(1).getId(), null, false, ShardRoutingState.STARTED));\r\n        indexRoutingTableBuilder.addIndexShard(indexShardRoutingBuilder.build());\r\n    }\r\n    state.routingTable(RoutingTable.builder().add(indexRoutingTableBuilder.build()).build());\r\n    return state.build();\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.ccr.CcrLicenseChecker.hasPrivilegesToFollowIndices",
	"Comment": "check if the user executing the current action has privileges to follow the specified indices on the cluster specified by the leaderclient. the specified callback will be invoked with null if the user has the necessary privileges to follow the specified indices,otherwise the callback will be invoked with an exception outlining the authorization error.",
	"Method": "void hasPrivilegesToFollowIndices(Client remoteClient,String[] indices,Consumer<Exception> handler){\r\n    Objects.requireNonNull(remoteClient, \"remoteClient\");\r\n    Objects.requireNonNull(indices, \"indices\");\r\n    if (indices.length == 0) {\r\n        throw new IllegalArgumentException(\"indices must not be empty\");\r\n    }\r\n    Objects.requireNonNull(handler, \"handler\");\r\n    if (isAuthAllowed.getAsBoolean() == false) {\r\n        handler.accept(null);\r\n        return;\r\n    }\r\n    ThreadContext threadContext = remoteClient.threadPool().getThreadContext();\r\n    SecurityContext securityContext = new SecurityContext(Settings.EMPTY, threadContext);\r\n    String username = securityContext.getUser().principal();\r\n    RoleDescriptor.IndicesPrivileges privileges = RoleDescriptor.IndicesPrivileges.builder().indices(indices).privileges(IndicesStatsAction.NAME, ShardChangesAction.NAME).build();\r\n    HasPrivilegesRequest request = new HasPrivilegesRequest();\r\n    request.username(username);\r\n    request.clusterPrivileges(Strings.EMPTY_ARRAY);\r\n    request.indexPrivileges(privileges);\r\n    request.applicationPrivileges(new RoleDescriptor.ApplicationResourcePrivileges[0]);\r\n    CheckedConsumer<HasPrivilegesResponse, Exception> responseHandler = response -> {\r\n        if (response.isCompleteMatch()) {\r\n            handler.accept(null);\r\n        } else {\r\n            StringBuilder message = new StringBuilder(\"insufficient privileges to follow\");\r\n            message.append(indices.length == 1 ? \" index \" : \" indices \");\r\n            message.append(Arrays.toString(indices));\r\n            HasPrivilegesResponse.ResourcePrivileges resourcePrivileges = response.getIndexPrivileges().get(0);\r\n            for (Map.Entry<String, Boolean> entry : resourcePrivileges.getPrivileges().entrySet()) {\r\n                if (entry.getValue() == false) {\r\n                    message.append(\", privilege for action [\");\r\n                    message.append(entry.getKey());\r\n                    message.append(\"] is missing\");\r\n                }\r\n            }\r\n            handler.accept(Exceptions.authorizationError(message.toString()));\r\n        }\r\n    };\r\n    remoteClient.execute(HasPrivilegesAction.INSTANCE, request, ActionListener.wrap(responseHandler, handler));\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.DelayedAllocationIT.testDelayedAllocationTimesOut",
	"Comment": "with a very small delay timeout, verify that it expires and we get to green eventhough the node hosting the shard is not coming back.",
	"Method": "void testDelayedAllocationTimesOut(){\r\n    internalCluster().startNodes(3);\r\n    prepareCreate(\"test\").setSettings(Settings.builder().put(IndexMetaData.SETTING_NUMBER_OF_SHARDS, 1).put(IndexMetaData.SETTING_NUMBER_OF_REPLICAS, 1).put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), TimeValue.timeValueMillis(100))).get();\r\n    ensureGreen(\"test\");\r\n    indexRandomData();\r\n    internalCluster().stopRandomNode(InternalTestCluster.nameFilter(findNodeWithShard()));\r\n    ensureGreen(\"test\");\r\n    internalCluster().startNode();\r\n    assertAcked(client().admin().indices().prepareUpdateSettings(\"test\").setSettings(Settings.builder().put(UnassignedInfo.INDEX_DELAYED_NODE_LEFT_TIMEOUT_SETTING.getKey(), TimeValue.timeValueMillis(100))).get());\r\n    internalCluster().stopRandomNode(InternalTestCluster.nameFilter(findNodeWithShard()));\r\n    ensureGreen(\"test\");\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.BootstrapForTesting.addClassCodebase",
	"Comment": "add the codebase url of the given classname to the codebases map, if the class exists.",
	"Method": "void addClassCodebase(Map<String, URL> codebases,String name,String classname){\r\n    try {\r\n        Class<?> clazz = BootstrapForTesting.class.getClassLoader().loadClass(classname);\r\n        URL location = clazz.getProtectionDomain().getCodeSource().getLocation();\r\n        if (location.toString().endsWith(\".jar\") == false) {\r\n            if (codebases.put(name, location) != null) {\r\n                throw new IllegalStateException(\"Already added \" + name + \" codebase for testing\");\r\n            }\r\n        }\r\n    } catch (ClassNotFoundException e) {\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.ml.datafeed.DatafeedJobValidator.validate",
	"Comment": "validates a datafeedconfig in relation to the job it refers to",
	"Method": "void validate(DatafeedConfig datafeedConfig,Job job){\r\n    AnalysisConfig analysisConfig = job.getAnalysisConfig();\r\n    if (analysisConfig.getLatency() != null && analysisConfig.getLatency().seconds() > 0) {\r\n        throw ExceptionsHelper.badRequestException(Messages.getMessage(Messages.DATAFEED_DOES_NOT_SUPPORT_JOB_WITH_LATENCY));\r\n    }\r\n    if (datafeedConfig.hasAggregations()) {\r\n        checkSummaryCountFieldNameIsSet(analysisConfig);\r\n        checkValidHistogramInterval(datafeedConfig, analysisConfig);\r\n        checkFrequencyIsMultipleOfHistogramInterval(datafeedConfig);\r\n    }\r\n    DelayedDataCheckConfig delayedDataCheckConfig = datafeedConfig.getDelayedDataCheckConfig();\r\n    TimeValue bucketSpan = analysisConfig.getBucketSpan();\r\n    if (delayedDataCheckConfig.isEnabled()) {\r\n        checkValidDelayedDataCheckConfig(bucketSpan, delayedDataCheckConfig);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.test.RandomObjects.addFields",
	"Comment": "randomly adds fields, objects, or arrays to the provided builder. the maximum depth is 5.",
	"Method": "void addFields(Random random,XContentBuilder builder,int minNumFields,int currentDepth){\r\n    int numFields = randomIntBetween(random, minNumFields, 5);\r\n    for (int i = 0; i < numFields; i++) {\r\n        if (currentDepth < 5 && random.nextInt(100) >= 70) {\r\n            if (random.nextBoolean()) {\r\n                builder.startObject(RandomStrings.randomAsciiLettersOfLengthBetween(random, 6, 10));\r\n                addFields(random, builder, minNumFields, currentDepth + 1);\r\n                builder.endObject();\r\n            } else {\r\n                builder.startArray(RandomStrings.randomAsciiLettersOfLengthBetween(random, 6, 10));\r\n                int numElements = randomIntBetween(random, 1, 5);\r\n                boolean object = random.nextBoolean();\r\n                int dataType = -1;\r\n                if (object == false) {\r\n                    dataType = randomDataType(random);\r\n                }\r\n                for (int j = 0; j < numElements; j++) {\r\n                    if (object) {\r\n                        builder.startObject();\r\n                        addFields(random, builder, minNumFields, 5);\r\n                        builder.endObject();\r\n                    } else {\r\n                        builder.value(randomFieldValue(random, dataType));\r\n                    }\r\n                }\r\n                builder.endArray();\r\n            }\r\n        } else {\r\n            builder.field(RandomStrings.randomAsciiLettersOfLengthBetween(random, 6, 10), randomFieldValue(random, randomDataType(random)));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.GeoUtilTests.parsePrecision",
	"Comment": "invokes geoutils.parseprecision parser on the value generated by tokengeneratorthe supplied tokengenerator should generate a single field that contains the precision inone of the supported formats or malformed precision value if error handling is tested. themethod return the parsed value or throws an exception, if precision value is malformed.",
	"Method": "int parsePrecision(CheckedConsumer<XContentBuilder, IOException> tokenGenerator){\r\n    XContentBuilder builder = jsonBuilder().startObject();\r\n    tokenGenerator.accept(builder);\r\n    builder.endObject();\r\n    try (XContentParser parser = createParser(JsonXContent.jsonXContent, BytesReference.bytes(builder))) {\r\n        assertEquals(XContentParser.Token.START_OBJECT, parser.nextToken());\r\n        assertEquals(XContentParser.Token.FIELD_NAME, parser.nextToken());\r\n        assertTrue(parser.nextToken().isValue());\r\n        int precision = GeoUtils.parsePrecision(parser);\r\n        assertEquals(XContentParser.Token.END_OBJECT, parser.nextToken());\r\n        assertNull(parser.nextToken());\r\n        return precision;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.support.MultiValuesSourceAggregationBuilder.serializeTargetValueType",
	"Comment": "should this builder serialize its targetvaluetype? defaults to false. all subclasses that override this to trueshould use the three argument read constructor rather than the four argument version.",
	"Method": "boolean serializeTargetValueType(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.transport.ConnectionManager.connectToNode",
	"Comment": "connects to a node with the given connection profile. if the node is already connected this method has no effect.once a successful is established, it can be validated before being exposed.",
	"Method": "void connectToNode(DiscoveryNode node,ConnectionProfile connectionProfile,CheckedBiConsumer<Transport.Connection, ConnectionProfile, IOException> connectionValidator){\r\n    ConnectionProfile resolvedProfile = ConnectionProfile.resolveConnectionProfile(connectionProfile, defaultProfile);\r\n    if (node == null) {\r\n        throw new ConnectTransportException(null, \"can't connect to a null node\");\r\n    }\r\n    closeLock.readLock().lock();\r\n    try {\r\n        ensureOpen();\r\n        try (Releasable ignored = connectionLock.acquire(node.getId())) {\r\n            Transport.Connection connection = connectedNodes.get(node);\r\n            if (connection != null) {\r\n                return;\r\n            }\r\n            boolean success = false;\r\n            try {\r\n                connection = internalOpenConnection(node, resolvedProfile);\r\n                connectionValidator.accept(connection, resolvedProfile);\r\n                connectedNodes.put(node, connection);\r\n                if (logger.isDebugEnabled()) {\r\n                    logger.debug(\"connected to node [{}]\", node);\r\n                }\r\n                try {\r\n                    connectionListener.onNodeConnected(node);\r\n                } finally {\r\n                    final Transport.Connection finalConnection = connection;\r\n                    connection.addCloseListener(ActionListener.wrap(() -> {\r\n                        connectedNodes.remove(node, finalConnection);\r\n                        connectionListener.onNodeDisconnected(node);\r\n                    }));\r\n                }\r\n                if (connection.isClosed()) {\r\n                    throw new NodeNotConnectedException(node, \"connection concurrently closed\");\r\n                }\r\n                success = true;\r\n            } catch (ConnectTransportException e) {\r\n                throw e;\r\n            } catch (Exception e) {\r\n                throw new ConnectTransportException(node, \"general node connection failure\", e);\r\n            } finally {\r\n                if (success == false) {\r\n                    logger.trace(() -> new ParameterizedMessage(\"failed to connect to [{}], cleaning dangling connections\", node));\r\n                    IOUtils.closeWhileHandlingException(connection);\r\n                }\r\n            }\r\n        }\r\n    } finally {\r\n        closeLock.readLock().unlock();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.rest.RestRequest.consumedParams",
	"Comment": "returns a list of parameters that have been consumed. this method returns a copy, callersare free to modify the returned list.",
	"Method": "List<String> consumedParams(){\r\n    return consumedParams.stream().collect(Collectors.toList());\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.AggregatorFactories.createSubAggregators",
	"Comment": "create all aggregators so that they can be consumed with multiplebuckets.",
	"Method": "Aggregator[] createSubAggregators(Aggregator parent){\r\n    Aggregator[] aggregators = new Aggregator[countAggregators()];\r\n    for (int i = 0; i < factories.length; ++i) {\r\n        final boolean collectsFromSingleBucket = false;\r\n        Aggregator factory = factories[i].create(parent, collectsFromSingleBucket);\r\n        Profilers profilers = factory.context().getProfilers();\r\n        if (profilers != null) {\r\n            factory = new ProfilingAggregator(factory, profilers.getAggregationProfiler());\r\n        }\r\n        aggregators[i] = factory;\r\n    }\r\n    return aggregators;\r\n}"
}, {
	"Path": "org.elasticsearch.search.query.QuerySearchResult.consumeTopDocs",
	"Comment": "returns and nulls out the top docs for this search results. this allows to free up memory once the top docs are consumed.",
	"Method": "TopDocsAndMaxScore consumeTopDocs(){\r\n    TopDocsAndMaxScore topDocsAndMaxScore = this.topDocsAndMaxScore;\r\n    if (topDocsAndMaxScore == null) {\r\n        throw new IllegalStateException(\"topDocs already consumed\");\r\n    }\r\n    this.topDocsAndMaxScore = null;\r\n    return topDocsAndMaxScore;\r\n}"
}, {
	"Path": "org.elasticsearch.search.query.QuerySearchResult.consumeAggs",
	"Comment": "returns and nulls out the aggregation for this search results. this allows to free up memory once the aggregation is consumed.",
	"Method": "Aggregations consumeAggs(){\r\n    if (aggregations == null) {\r\n        throw new IllegalStateException(\"aggs already consumed\");\r\n    }\r\n    Aggregations aggs = aggregations;\r\n    aggregations = null;\r\n    return aggs;\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngineTests.testConcurrentWritesAndCommits",
	"Comment": "and ensuring that the commit points contain the correct sequence number data",
	"Method": "void testConcurrentWritesAndCommits(){\r\n    List<Engine.IndexCommitRef> commits = new ArrayList();\r\n    try (Store store = createStore();\r\n        InternalEngine engine = createEngine(config(defaultSettings, store, createTempDir(), newMergePolicy(), null))) {\r\n        final int numIndexingThreads = scaledRandomIntBetween(2, 4);\r\n        final int numDocsPerThread = randomIntBetween(500, 1000);\r\n        final CyclicBarrier barrier = new CyclicBarrier(numIndexingThreads + 1);\r\n        final List<Thread> indexingThreads = new ArrayList();\r\n        final CountDownLatch doneLatch = new CountDownLatch(numIndexingThreads);\r\n        for (int threadNum = 0; threadNum < numIndexingThreads; threadNum++) {\r\n            final int threadIdx = threadNum;\r\n            Thread indexingThread = new Thread(() -> {\r\n                try {\r\n                    barrier.await();\r\n                    for (int i = 0; i < numDocsPerThread; i++) {\r\n                        final String id = \"thread\" + threadIdx + \"#\" + i;\r\n                        ParsedDocument doc = testParsedDocument(id, null, testDocument(), B_1, null);\r\n                        engine.index(indexForDoc(doc));\r\n                    }\r\n                } catch (Exception e) {\r\n                    throw new RuntimeException(e);\r\n                } finally {\r\n                    doneLatch.countDown();\r\n                }\r\n            });\r\n            indexingThreads.add(indexingThread);\r\n        }\r\n        for (Thread thread : indexingThreads) {\r\n            thread.start();\r\n        }\r\n        barrier.await();\r\n        int commitLimit = randomIntBetween(10, 20);\r\n        long sleepTime = 1;\r\n        boolean doneIndexing;\r\n        do {\r\n            doneIndexing = doneLatch.await(sleepTime, TimeUnit.MILLISECONDS);\r\n            commits.add(engine.acquireLastIndexCommit(true));\r\n            if (commits.size() > commitLimit) {\r\n                IOUtils.close(commits.remove(randomIntBetween(0, commits.size() - 1)));\r\n                sleepTime = sleepTime * 2;\r\n            }\r\n        } while (doneIndexing == false);\r\n        long prevLocalCheckpoint = SequenceNumbers.NO_OPS_PERFORMED;\r\n        long prevMaxSeqNo = SequenceNumbers.NO_OPS_PERFORMED;\r\n        for (Engine.IndexCommitRef commitRef : commits) {\r\n            final IndexCommit commit = commitRef.getIndexCommit();\r\n            Map<String, String> userData = commit.getUserData();\r\n            long localCheckpoint = userData.containsKey(SequenceNumbers.LOCAL_CHECKPOINT_KEY) ? Long.parseLong(userData.get(SequenceNumbers.LOCAL_CHECKPOINT_KEY)) : SequenceNumbers.NO_OPS_PERFORMED;\r\n            long maxSeqNo = userData.containsKey(SequenceNumbers.MAX_SEQ_NO) ? Long.parseLong(userData.get(SequenceNumbers.MAX_SEQ_NO)) : SequenceNumbers.UNASSIGNED_SEQ_NO;\r\n            assertThat(localCheckpoint, greaterThanOrEqualTo(prevLocalCheckpoint));\r\n            assertThat(maxSeqNo, greaterThanOrEqualTo(prevMaxSeqNo));\r\n            try (IndexReader reader = DirectoryReader.open(commit)) {\r\n                Long highest = getHighestSeqNo(reader);\r\n                final long highestSeqNo;\r\n                if (highest != null) {\r\n                    highestSeqNo = highest.longValue();\r\n                } else {\r\n                    highestSeqNo = SequenceNumbers.NO_OPS_PERFORMED;\r\n                }\r\n                assertThat(highestSeqNo, greaterThanOrEqualTo(localCheckpoint));\r\n                assertThat(highestSeqNo, lessThanOrEqualTo(maxSeqNo));\r\n                FixedBitSet seqNosBitSet = getSeqNosSet(reader, highestSeqNo);\r\n                for (int i = 0; i <= localCheckpoint; i++) {\r\n                    assertTrue(\"local checkpoint [\" + localCheckpoint + \"], _seq_no [\" + i + \"] should be indexed\", seqNosBitSet.get(i));\r\n                }\r\n            }\r\n            prevLocalCheckpoint = localCheckpoint;\r\n            prevMaxSeqNo = maxSeqNo;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.rest.RestController.checkErrorTraceParameter",
	"Comment": "checks the request parameters against enabled settings for error trace support",
	"Method": "boolean checkErrorTraceParameter(RestRequest request,RestChannel channel){\r\n    if (request.paramAsBoolean(\"error_trace\", false) && channel.detailedErrorsEnabled() == false) {\r\n        return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.completion.FuzzyOptions.isTranspositions",
	"Comment": "returns if transpositions option is setif transpositions is set, then swapping one character for another counts as one edit instead of two.",
	"Method": "boolean isTranspositions(){\r\n    return transpositions;\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractWireTestCase.testEqualsAndHashcode",
	"Comment": "tests that the equals and hashcode methods are consistent and copiedversions of the instance have are equal.",
	"Method": "void testEqualsAndHashcode(){\r\n    for (int runs = 0; runs < NUMBER_OF_TEST_RUNS; runs++) {\r\n        EqualsHashCodeTestUtils.checkEqualsAndHashCode(createTestInstance(), this::copyInstance, this::mutateInstance);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.phrase.PhraseSuggestionBuilder.highlight",
	"Comment": "setup highlighting for suggestions.if this is called a highlight fieldis returned with suggestions wrapping changed tokens with pretag and posttag.",
	"Method": "PhraseSuggestionBuilder highlight(String preTag,String postTag){\r\n    if ((preTag == null) != (postTag == null)) {\r\n        throw new IllegalArgumentException(\"Pre and post tag must both be null or both not be null.\");\r\n    }\r\n    this.preTag = preTag;\r\n    this.postTag = postTag;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.term.TermSuggestionBuilder.minWordLength",
	"Comment": "get the minimum length of a text term to be corrected setting.",
	"Method": "TermSuggestionBuilder minWordLength(int minWordLength,int minWordLength){\r\n    return minWordLength;\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.createSnapshot",
	"Comment": "initializes the snapshotting process.this method is used by clients to start snapshot. it makes sure that there is no snapshots are currently running andcreates a snapshot record in cluster state metadata.",
	"Method": "void createSnapshot(SnapshotRequest request,CreateSnapshotListener listener){\r\n    final String repositoryName = request.repositoryName;\r\n    final String snapshotName = request.snapshotName;\r\n    validate(repositoryName, snapshotName);\r\n    final SnapshotId snapshotId = new SnapshotId(snapshotName, UUIDs.randomBase64UUID());\r\n    final RepositoryData repositoryData = repositoriesService.repository(repositoryName).getRepositoryData();\r\n    clusterService.submitStateUpdateTask(request.cause(), new ClusterStateUpdateTask() {\r\n        private SnapshotsInProgress.Entry newSnapshot = null;\r\n        @Override\r\n        public ClusterState execute(ClusterState currentState) {\r\n            validate(request, currentState);\r\n            SnapshotDeletionsInProgress deletionsInProgress = currentState.custom(SnapshotDeletionsInProgress.TYPE);\r\n            if (deletionsInProgress != null && deletionsInProgress.hasDeletionsInProgress()) {\r\n                throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, \"cannot snapshot while a snapshot deletion is in-progress\");\r\n            }\r\n            SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\r\n            if (snapshots == null || snapshots.entries().isEmpty()) {\r\n                List<String> indices = Arrays.asList(indexNameExpressionResolver.concreteIndexNames(currentState, request.indicesOptions(), request.indices()));\r\n                logger.trace(\"[{}][{}] creating snapshot for indices [{}]\", repositoryName, snapshotName, indices);\r\n                List<IndexId> snapshotIndices = repositoryData.resolveNewIndices(indices);\r\n                newSnapshot = new SnapshotsInProgress.Entry(new Snapshot(repositoryName, snapshotId), request.includeGlobalState(), request.partial(), State.INIT, snapshotIndices, System.currentTimeMillis(), repositoryData.getGenId(), null);\r\n                snapshots = new SnapshotsInProgress(newSnapshot);\r\n            } else {\r\n                throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, \" a snapshot is already running\");\r\n            }\r\n            return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, snapshots).build();\r\n        }\r\n        @Override\r\n        public void onFailure(String source, Exception e) {\r\n            logger.warn(() -> new ParameterizedMessage(\"[{}][{}] failed to create snapshot\", repositoryName, snapshotName), e);\r\n            newSnapshot = null;\r\n            listener.onFailure(e);\r\n        }\r\n        @Override\r\n        public void clusterStateProcessed(String source, ClusterState oldState, final ClusterState newState) {\r\n            if (newSnapshot != null) {\r\n                beginSnapshot(newState, newSnapshot, request.partial(), listener);\r\n            }\r\n        }\r\n        @Override\r\n        public TimeValue timeout() {\r\n            return request.masterNodeTimeout();\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.createSnapshot",
	"Comment": "initializes the snapshotting process.this method is used by clients to start snapshot. it makes sure that there is no snapshots are currently running andcreates a snapshot record in cluster state metadata.",
	"Method": "void createSnapshot(SnapshotRequest request,CreateSnapshotListener listener){\r\n    validate(request, currentState);\r\n    SnapshotDeletionsInProgress deletionsInProgress = currentState.custom(SnapshotDeletionsInProgress.TYPE);\r\n    if (deletionsInProgress != null && deletionsInProgress.hasDeletionsInProgress()) {\r\n        throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, \"cannot snapshot while a snapshot deletion is in-progress\");\r\n    }\r\n    SnapshotsInProgress snapshots = currentState.custom(SnapshotsInProgress.TYPE);\r\n    if (snapshots == null || snapshots.entries().isEmpty()) {\r\n        List<String> indices = Arrays.asList(indexNameExpressionResolver.concreteIndexNames(currentState, request.indicesOptions(), request.indices()));\r\n        logger.trace(\"[{}][{}] creating snapshot for indices [{}]\", repositoryName, snapshotName, indices);\r\n        List<IndexId> snapshotIndices = repositoryData.resolveNewIndices(indices);\r\n        newSnapshot = new SnapshotsInProgress.Entry(new Snapshot(repositoryName, snapshotId), request.includeGlobalState(), request.partial(), State.INIT, snapshotIndices, System.currentTimeMillis(), repositoryData.getGenId(), null);\r\n        snapshots = new SnapshotsInProgress(newSnapshot);\r\n    } else {\r\n        throw new ConcurrentSnapshotExecutionException(repositoryName, snapshotName, \" a snapshot is already running\");\r\n    }\r\n    return ClusterState.builder(currentState).putCustom(SnapshotsInProgress.TYPE, snapshots).build();\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.createSnapshot",
	"Comment": "initializes the snapshotting process.this method is used by clients to start snapshot. it makes sure that there is no snapshots are currently running andcreates a snapshot record in cluster state metadata.",
	"Method": "void createSnapshot(SnapshotRequest request,CreateSnapshotListener listener){\r\n    logger.warn(() -> new ParameterizedMessage(\"[{}][{}] failed to create snapshot\", repositoryName, snapshotName), e);\r\n    newSnapshot = null;\r\n    listener.onFailure(e);\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.createSnapshot",
	"Comment": "initializes the snapshotting process.this method is used by clients to start snapshot. it makes sure that there is no snapshots are currently running andcreates a snapshot record in cluster state metadata.",
	"Method": "void createSnapshot(SnapshotRequest request,CreateSnapshotListener listener){\r\n    if (newSnapshot != null) {\r\n        beginSnapshot(newState, newSnapshot, request.partial(), listener);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.createSnapshot",
	"Comment": "initializes the snapshotting process.this method is used by clients to start snapshot. it makes sure that there is no snapshots are currently running andcreates a snapshot record in cluster state metadata.",
	"Method": "void createSnapshot(SnapshotRequest request,CreateSnapshotListener listener){\r\n    return request.masterNodeTimeout();\r\n}"
}, {
	"Path": "org.elasticsearch.script.ScriptService.setMaxCompilationRate",
	"Comment": "this configures the maximum script compilations per five minute window.",
	"Method": "void setMaxCompilationRate(Tuple<Integer, TimeValue> newRate){\r\n    this.rate = newRate;\r\n    this.scriptsPerTimeWindow = rate.v1();\r\n    this.compilesAllowedPerNano = ((double) rate.v1()) / newRate.v2().nanos();\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT.testDeleteSnapshotWithCorruptedGlobalState",
	"Comment": "tests that a snapshot with a corrupted global state file can still be deleted",
	"Method": "void testDeleteSnapshotWithCorruptedGlobalState(){\r\n    final Path repo = randomRepoPath();\r\n    assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\").setType(\"fs\").setSettings(Settings.builder().put(\"location\", repo).put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\r\n    createIndex(\"test-idx-1\", \"test-idx-2\");\r\n    indexRandom(true, client().prepareIndex(\"test-idx-1\", \"_doc\").setSource(\"foo\", \"bar\"), client().prepareIndex(\"test-idx-2\", \"_doc\").setSource(\"foo\", \"bar\"), client().prepareIndex(\"test-idx-2\", \"_doc\").setSource(\"foo\", \"bar\"));\r\n    flushAndRefresh(\"test-idx-1\", \"test-idx-2\");\r\n    CreateSnapshotResponse createSnapshotResponse = client().admin().cluster().prepareCreateSnapshot(\"test-repo\", \"test-snap\").setIncludeGlobalState(true).setWaitForCompletion(true).get();\r\n    SnapshotInfo snapshotInfo = createSnapshotResponse.getSnapshotInfo();\r\n    assertThat(snapshotInfo.successfulShards(), greaterThan(0));\r\n    assertThat(snapshotInfo.successfulShards(), equalTo(snapshotInfo.totalShards()));\r\n    final Path globalStatePath = repo.resolve(\"meta-\" + snapshotInfo.snapshotId().getUUID() + \".dat\");\r\n    if (randomBoolean()) {\r\n        IOUtils.deleteFilesIgnoringExceptions(globalStatePath);\r\n    } else {\r\n        try (SeekableByteChannel outChan = Files.newByteChannel(globalStatePath, StandardOpenOption.WRITE)) {\r\n            outChan.truncate(randomInt(10));\r\n        }\r\n    }\r\n    List<SnapshotInfo> snapshotInfos = client().admin().cluster().prepareGetSnapshots(\"test-repo\").get().getSnapshots();\r\n    assertThat(snapshotInfos.size(), equalTo(1));\r\n    assertThat(snapshotInfos.get(0).state(), equalTo(SnapshotState.SUCCESS));\r\n    assertThat(snapshotInfos.get(0).snapshotId().getName(), equalTo(\"test-snap\"));\r\n    SnapshotsStatusResponse snapshotStatusResponse = client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get();\r\n    assertThat(snapshotStatusResponse.getSnapshots(), hasSize(1));\r\n    assertThat(snapshotStatusResponse.getSnapshots().get(0).getSnapshot().getSnapshotId().getName(), equalTo(\"test-snap\"));\r\n    assertAcked(client().admin().cluster().prepareDeleteSnapshot(\"test-repo\", \"test-snap\").get());\r\n    assertThrows(client().admin().cluster().prepareGetSnapshots(\"test-repo\").addSnapshots(\"test-snap\"), SnapshotMissingException.class);\r\n    assertThrows(client().admin().cluster().prepareSnapshotStatus(\"test-repo\").addSnapshots(\"test-snap\"), SnapshotMissingException.class);\r\n    createSnapshotResponse = client().admin().cluster().prepareCreateSnapshot(\"test-repo\", \"test-snap\").setIncludeGlobalState(true).setWaitForCompletion(true).get();\r\n    snapshotInfo = createSnapshotResponse.getSnapshotInfo();\r\n    assertThat(snapshotInfo.successfulShards(), greaterThan(0));\r\n    assertThat(snapshotInfo.successfulShards(), equalTo(snapshotInfo.totalShards()));\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.metrics.ValueCountIT.testDontCacheScripts",
	"Comment": "make sure that a request using a script does not get cached and a requestnot using a script does get cached.",
	"Method": "void testDontCacheScripts(){\r\n    assertAcked(prepareCreate(\"cache_test_idx\").addMapping(\"type\", \"d\", \"type=long\").setSettings(Settings.builder().put(\"requests.cache.enable\", true).put(\"number_of_shards\", 1).put(\"number_of_replicas\", 1)).get());\r\n    indexRandom(true, client().prepareIndex(\"cache_test_idx\", \"type\", \"1\").setSource(\"s\", 1), client().prepareIndex(\"cache_test_idx\", \"type\", \"2\").setSource(\"s\", 2));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    SearchResponse r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(count(\"foo\").field(\"d\").script(new Script(ScriptType.INLINE, METRIC_SCRIPT_ENGINE, VALUE_FIELD_SCRIPT, Collections.emptyMap()))).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(0L));\r\n    r = client().prepareSearch(\"cache_test_idx\").setSize(0).addAggregation(count(\"foo\").field(\"d\")).get();\r\n    assertSearchResponse(r);\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getHitCount(), equalTo(0L));\r\n    assertThat(client().admin().indices().prepareStats(\"cache_test_idx\").setRequestCache(true).get().getTotal().getRequestCache().getMissCount(), equalTo(1L));\r\n}"
}, {
	"Path": "org.elasticsearch.search.sort.GeoDistanceSortBuilder.setNestedPath",
	"Comment": "sets the nested path if sorting occurs on a field that is inside a nested object. by default when sorting on afield inside a nested object, the nearest upper nested object is selected as nested path.",
	"Method": "GeoDistanceSortBuilder setNestedPath(String nestedPath){\r\n    if (this.nestedSort != null) {\r\n        throw new IllegalArgumentException(\"Setting both nested_path/nested_filter and nested not allowed\");\r\n    }\r\n    this.nestedPath = nestedPath;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.refresh",
	"Comment": "waits for relocations and refreshes all indices in the cluster.",
	"Method": "RefreshResponse refresh(String indices){\r\n    waitForRelocation();\r\n    RefreshResponse actionGet = client().admin().indices().prepareRefresh(indices).execute().actionGet();\r\n    assertNoFailures(actionGet);\r\n    return actionGet;\r\n}"
}, {
	"Path": "org.elasticsearch.tasks.CancellableTask.cancelOnParentLeaving",
	"Comment": "returns true if this task should be automatically cancelled if the coordinating node thatrequested this task left the cluster.",
	"Method": "boolean cancelOnParentLeaving(){\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.test.BackgroundIndexer.setMaxFieldSize",
	"Comment": "the minimum size in code points of a payload field in the indexed documents",
	"Method": "void setMaxFieldSize(int fieldSize){\r\n    maxFieldSize = fieldSize;\r\n}"
}, {
	"Path": "org.elasticsearch.search.fetch.StoredFieldsContext.fieldNames",
	"Comment": "gets the field names to load and return as part of the search request.",
	"Method": "List<String> fieldNames(){\r\n    return fieldNames;\r\n}"
}, {
	"Path": "org.elasticsearch.test.ESIntegTestCase.transportClientSettings",
	"Comment": "this method is used to obtain additional settings for clients created by the internal cluster.these settings will be applied on the client in addition to some randomized settings defined inthe cluster. these settings will also override any other settings the internal cluster mightadd by default.",
	"Method": "Settings transportClientSettings(){\r\n    return Settings.EMPTY;\r\n}"
}, {
	"Path": "org.elasticsearch.search.profile.AbstractInternalProfileTree.getTree",
	"Comment": "after the query has been run and profiled, we need to merge the flat timing mapwith the dependency graph to build a data structure that mirrors the originalquery tree",
	"Method": "List<ProfileResult> getTree(){\r\n    ArrayList<ProfileResult> results = new ArrayList(5);\r\n    for (Integer root : roots) {\r\n        results.add(doGetTree(root));\r\n    }\r\n    return results;\r\n}"
}, {
	"Path": "org.elasticsearch.script.TermsSetQueryScript.getDoc",
	"Comment": "the doc lookup for the lucene segment this script was created for.",
	"Method": "Map<String, ScriptDocValues<?>> getDoc(){\r\n    return leafLookup.doc();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.service.MasterServiceTests.testClusterStateTaskListenerThrowingExceptionIsOkay",
	"Comment": "test that a listener throwing an exception while handling anotification does not prevent publication notification to theexecutor",
	"Method": "void testClusterStateTaskListenerThrowingExceptionIsOkay(){\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    AtomicBoolean published = new AtomicBoolean();\r\n    masterService.submitStateUpdateTask(\"testClusterStateTaskListenerThrowingExceptionIsOkay\", new Object(), ClusterStateTaskConfig.build(Priority.NORMAL), new ClusterStateTaskExecutor<Object>() {\r\n        @Override\r\n        public ClusterTasksResult<Object> execute(ClusterState currentState, List<Object> tasks) throws Exception {\r\n            ClusterState newClusterState = ClusterState.builder(currentState).build();\r\n            return ClusterTasksResult.builder().successes(tasks).build(newClusterState);\r\n        }\r\n        @Override\r\n        public void clusterStatePublished(ClusterChangedEvent clusterChangedEvent) {\r\n            published.set(true);\r\n            latch.countDown();\r\n        }\r\n    }, new ClusterStateTaskListener() {\r\n        @Override\r\n        public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\r\n            throw new IllegalStateException(source);\r\n        }\r\n        @Override\r\n        public void onFailure(String source, Exception e) {\r\n        }\r\n    });\r\n    latch.await();\r\n    assertTrue(published.get());\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.service.MasterServiceTests.testClusterStateTaskListenerThrowingExceptionIsOkay",
	"Comment": "test that a listener throwing an exception while handling anotification does not prevent publication notification to theexecutor",
	"Method": "void testClusterStateTaskListenerThrowingExceptionIsOkay(){\r\n    ClusterState newClusterState = ClusterState.builder(currentState).build();\r\n    return ClusterTasksResult.builder().successes(tasks).build(newClusterState);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.service.MasterServiceTests.testClusterStateTaskListenerThrowingExceptionIsOkay",
	"Comment": "test that a listener throwing an exception while handling anotification does not prevent publication notification to theexecutor",
	"Method": "void testClusterStateTaskListenerThrowingExceptionIsOkay(){\r\n    published.set(true);\r\n    latch.countDown();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.service.MasterServiceTests.testClusterStateTaskListenerThrowingExceptionIsOkay",
	"Comment": "test that a listener throwing an exception while handling anotification does not prevent publication notification to theexecutor",
	"Method": "void testClusterStateTaskListenerThrowingExceptionIsOkay(){\r\n    throw new IllegalStateException(source);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.service.MasterServiceTests.testClusterStateTaskListenerThrowingExceptionIsOkay",
	"Comment": "test that a listener throwing an exception while handling anotification does not prevent publication notification to theexecutor",
	"Method": "void testClusterStateTaskListenerThrowingExceptionIsOkay(){\r\n}"
}, {
	"Path": "org.elasticsearch.search.builder.SearchSourceBuilder.indexBoosts",
	"Comment": "gets the boost a specific indices or aliases will receive when the query isexecuted against them.",
	"Method": "List<IndexBoost> indexBoosts(){\r\n    return indexBoosts;\r\n}"
}, {
	"Path": "org.elasticsearch.search.suggest.completion.context.ContextMapping.validateContextPaths",
	"Comment": "verifies that all field paths specified in contexts point to the fields with correct mappings",
	"Method": "void validateContextPaths(Version indexVersionCreated,List<FieldMapper> fieldMappers,Function<String, MappedFieldType> fieldResolver){\r\n    for (FieldMapper fieldMapper : fieldMappers) {\r\n        if (CompletionFieldMapper.CONTENT_TYPE.equals(fieldMapper.typeName())) {\r\n            CompletionFieldMapper.CompletionFieldType fieldType = ((CompletionFieldMapper) fieldMapper).fieldType();\r\n            if (fieldType.hasContextMappings()) {\r\n                for (ContextMapping context : fieldType.getContextMappings()) {\r\n                    context.validateReferences(indexVersionCreated, fieldResolver);\r\n                }\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.snapshots.SnapshotsService.shards",
	"Comment": "calculates the list of shards that should be included into the current snapshot",
	"Method": "ImmutableOpenMap<ShardId, SnapshotsInProgress.ShardSnapshotStatus> shards(ClusterState clusterState,List<IndexId> indices){\r\n    ImmutableOpenMap.Builder<ShardId, SnapshotsInProgress.ShardSnapshotStatus> builder = ImmutableOpenMap.builder();\r\n    MetaData metaData = clusterState.metaData();\r\n    for (IndexId index : indices) {\r\n        final String indexName = index.getName();\r\n        IndexMetaData indexMetaData = metaData.index(indexName);\r\n        if (indexMetaData == null) {\r\n            builder.put(new ShardId(indexName, IndexMetaData.INDEX_UUID_NA_VALUE, 0), new SnapshotsInProgress.ShardSnapshotStatus(null, State.MISSING, \"missing index\"));\r\n        } else if (indexMetaData.getState() == IndexMetaData.State.CLOSE) {\r\n            for (int i = 0; i < indexMetaData.getNumberOfShards(); i++) {\r\n                ShardId shardId = new ShardId(indexMetaData.getIndex(), i);\r\n                builder.put(shardId, new SnapshotsInProgress.ShardSnapshotStatus(null, State.MISSING, \"index is closed\"));\r\n            }\r\n        } else {\r\n            IndexRoutingTable indexRoutingTable = clusterState.getRoutingTable().index(indexName);\r\n            for (int i = 0; i < indexMetaData.getNumberOfShards(); i++) {\r\n                ShardId shardId = new ShardId(indexMetaData.getIndex(), i);\r\n                if (indexRoutingTable != null) {\r\n                    ShardRouting primary = indexRoutingTable.shard(i).primaryShard();\r\n                    if (primary == null || !primary.assignedToNode()) {\r\n                        builder.put(shardId, new SnapshotsInProgress.ShardSnapshotStatus(null, State.MISSING, \"primary shard is not allocated\"));\r\n                    } else if (primary.relocating() || primary.initializing()) {\r\n                        builder.put(shardId, new SnapshotsInProgress.ShardSnapshotStatus(primary.currentNodeId(), State.WAITING));\r\n                    } else if (!primary.started()) {\r\n                        builder.put(shardId, new SnapshotsInProgress.ShardSnapshotStatus(primary.currentNodeId(), State.MISSING, \"primary shard hasn't been started yet\"));\r\n                    } else {\r\n                        builder.put(shardId, new SnapshotsInProgress.ShardSnapshotStatus(primary.currentNodeId()));\r\n                    }\r\n                } else {\r\n                    builder.put(shardId, new SnapshotsInProgress.ShardSnapshotStatus(null, State.MISSING, \"missing routing table\"));\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.transport.ConnectionProfile.buildSingleChannelProfile",
	"Comment": "builds a connection profile that is dedicated to a single channel type. allows passing connection andhandshake timeouts and compression settings.",
	"Method": "ConnectionProfile buildSingleChannelProfile(TransportRequestOptions.Type channelType,ConnectionProfile buildSingleChannelProfile,TransportRequestOptions.Type channelType,boolean compressionEnabled,ConnectionProfile buildSingleChannelProfile,TransportRequestOptions.Type channelType,TimeValue connectTimeout,TimeValue handshakeTimeout,ConnectionProfile buildSingleChannelProfile,TransportRequestOptions.Type channelType,TimeValue connectTimeout,TimeValue handshakeTimeout,TimeValue pingInterval,Boolean compressionEnabled){\r\n    Builder builder = new Builder();\r\n    builder.addConnections(1, channelType);\r\n    final EnumSet<TransportRequestOptions.Type> otherTypes = EnumSet.allOf(TransportRequestOptions.Type.class);\r\n    otherTypes.remove(channelType);\r\n    builder.addConnections(0, otherTypes.toArray(new TransportRequestOptions.Type[0]));\r\n    if (connectTimeout != null) {\r\n        builder.setConnectTimeout(connectTimeout);\r\n    }\r\n    if (handshakeTimeout != null) {\r\n        builder.setHandshakeTimeout(handshakeTimeout);\r\n    }\r\n    if (pingInterval != null) {\r\n        builder.setPingInterval(pingInterval);\r\n    }\r\n    if (compressionEnabled != null) {\r\n        builder.setCompressionEnabled(compressionEnabled);\r\n    }\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.test.AbstractBuilderTestCase.createUniqueRandomName",
	"Comment": "make sure query names are unique by suffixing them with increasing counter",
	"Method": "String createUniqueRandomName(){\r\n    String queryName = randomAlphaOfLengthBetween(1, 10) + queryNameId;\r\n    queryNameId++;\r\n    return queryName;\r\n}"
}, {
	"Path": "org.elasticsearch.test.BackgroundIndexer.getIds",
	"Comment": "returns the id set of all documents indexed by this indexer run",
	"Method": "Set<String> getIds(){\r\n    return this.ids;\r\n}"
}, {
	"Path": "org.elasticsearch.xpack.core.monitoring.client.MonitoringClient.prepareMonitoringBulk",
	"Comment": "creates a request builder that bulk index monitoring documents.",
	"Method": "MonitoringBulkRequestBuilder prepareMonitoringBulk(){\r\n    return new MonitoringBulkRequestBuilder(client);\r\n}"
}, {
	"Path": "org.elasticsearch.license.ExpirationCallback.delay",
	"Comment": "calculates the delay for the next trigger time. when now is in avalid time bracket with respect to expirationdate, the delay is 0.when now is before the time bracket, than delay to the start of thetime bracket and when now is passed the valid time bracket, the delayis null",
	"Method": "TimeValue delay(long expirationDate,long now){\r\n    final TimeValue delay;\r\n    switch(orientation) {\r\n        case PRE:\r\n            if (expirationDate >= now) {\r\n                long preExpiryDuration = expirationDate - now;\r\n                if (preExpiryDuration > max) {\r\n                    delay = TimeValue.timeValueMillis(preExpiryDuration - max);\r\n                } else if (preExpiryDuration <= max && preExpiryDuration >= min) {\r\n                    delay = TimeValue.timeValueMillis(0);\r\n                } else {\r\n                    delay = null;\r\n                }\r\n            } else {\r\n                delay = null;\r\n            }\r\n            break;\r\n        case POST:\r\n            if (expirationDate >= now) {\r\n                delay = TimeValue.timeValueMillis(expirationDate - now + min);\r\n            } else {\r\n                long expiredDuration = now - expirationDate;\r\n                if (expiredDuration < min) {\r\n                    delay = TimeValue.timeValueMillis(min - expiredDuration);\r\n                } else if (expiredDuration >= min && expiredDuration <= max) {\r\n                    delay = TimeValue.timeValueMillis(0);\r\n                } else {\r\n                    delay = null;\r\n                }\r\n            }\r\n            break;\r\n        default:\r\n            throw new IllegalStateException(\"orientation [\" + orientation + \"] unknown\");\r\n    }\r\n    return delay;\r\n}"
}, {
	"Path": "org.elasticsearch.search.aggregations.InternalAggregation.writeSize",
	"Comment": "write a size under the assumption that a value of 0 means unlimited.",
	"Method": "void writeSize(int size,StreamOutput out){\r\n    if (size == Integer.MAX_VALUE) {\r\n        size = 0;\r\n    }\r\n    out.writeVInt(size);\r\n}"
}]