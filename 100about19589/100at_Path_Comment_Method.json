[{
	"Path": "org.elasticsearch.common.lucene.search.XMoreLikeThis.setMinTermFreq",
	"Comment": "sets the frequency below which terms will be ignored in the source doc.",
	"Method": "void setMinTermFreq(int minTermFreq){\r\n    this.minTermFreq = minTermFreq;\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.rnn.TopNGramRecord.countTree",
	"Comment": "adds the tree and all its subtrees to the appropriatepriorityqueues for each predicted class.",
	"Method": "void countTree(Tree tree){\r\n    Tree simplified = simplifyTree(tree);\r\n    for (int i = 0; i < numClasses; ++i) {\r\n        countTreeHelper(simplified, i, classToNGrams.get(i));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.GeoPointFieldMapper.parseGeoPointStringIgnoringMalformed",
	"Comment": "parses geopoint represented as a string and ignores malformed geopoints if needed",
	"Method": "void parseGeoPointStringIgnoringMalformed(ParseContext context,GeoPoint sparse){\r\n    try {\r\n        parse(context, sparse.resetFromString(context.parser().text(), ignoreZValue.value()));\r\n    } catch (ElasticsearchParseException e) {\r\n        if (ignoreMalformed.value() == false) {\r\n            throw e;\r\n        }\r\n        context.addIgnoredField(fieldType.name());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractBulkByScrollRequest.getTimeout",
	"Comment": "timeout to wait for the shards on to be available for each bulk request?",
	"Method": "TimeValue getTimeout(){\r\n    return timeout;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVectorTable.getProtoBuilder",
	"Comment": "returns the proto builder object for this feature factor. recursively constructs protos for all the concatvectors in factortable.",
	"Method": "ConcatVectorTableProto.ConcatVectorTable.Builder getProtoBuilder(){\r\n    ConcatVectorTableProto.ConcatVectorTable.Builder b = ConcatVectorTableProto.ConcatVectorTable.newBuilder();\r\n    for (int n : getDimensions()) {\r\n        b.addDimensionSize(n);\r\n    }\r\n    for (int[] assignment : this) {\r\n        b.addFactorTable(getAssignmentValue(assignment).get().getProtoBuilder());\r\n    }\r\n    return b;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.BaseRemoteProxy.getNewInstance",
	"Comment": "takes a registration request and return the remoteproxy associated to it. it can be any classextending remoteproxy.",
	"Method": "T getNewInstance(RegistrationRequest request,GridRegistry registry){\r\n    try {\r\n        String proxyClass = request.getConfiguration().proxy;\r\n        if (proxyClass == null) {\r\n            log.fine(\"No proxy class. Using default\");\r\n            proxyClass = BaseRemoteProxy.class.getCanonicalName();\r\n        }\r\n        Class<?> clazz = Class.forName(proxyClass);\r\n        log.fine(\"Using class \" + clazz.getName());\r\n        Object[] args = new Object[] { request, registry };\r\n        Class<?>[] argsClass = new Class[] { RegistrationRequest.class, GridRegistry.class };\r\n        Constructor<?> c = clazz.getConstructor(argsClass);\r\n        Object proxy = c.newInstance(args);\r\n        if (proxy instanceof RemoteProxy) {\r\n            ((RemoteProxy) proxy).setupTimeoutListener();\r\n            return (T) proxy;\r\n        }\r\n        throw new InvalidParameterException(\"Error: \" + proxy.getClass() + \" isn't a remote proxy\");\r\n    } catch (InvocationTargetException e) {\r\n        throw new InvalidParameterException(\"Error: \" + e.getTargetException().getMessage());\r\n    } catch (Exception e) {\r\n        throw new InvalidParameterException(\"Error: \" + e.getMessage());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexResponse.parseXContentFields",
	"Comment": "parse the current token and update the parsing context appropriately.",
	"Method": "void parseXContentFields(XContentParser parser,Builder context){\r\n    DocWriteResponse.parseInnerToXContent(parser, context);\r\n}"
}, {
	"Path": "android.text.SpanSet.recycle",
	"Comment": "removes all internal references to the spans to avoid memory leaks.",
	"Method": "void recycle(){\r\n    for (int i = 0; i < numberOfSpans; i++) {\r\n        spans[i] = null;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractBulkByScrollRequestBuilder.setMaxRetries",
	"Comment": "total number of retries attempted for rejections. there is no way to ask for unlimited retries.",
	"Method": "Self setMaxRetries(int maxRetries){\r\n    request.setMaxRetries(maxRetries);\r\n    return self();\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.Hook.equals",
	"Comment": "hooks are equal if they have same state, substate, head, tag, start,and end.",
	"Method": "boolean equals(Object o){\r\n    if (this == o) {\r\n        return true;\r\n    }\r\n    if (o instanceof Hook) {\r\n        Hook e = (Hook) o;\r\n        if (state == e.state && subState == e.subState && head == e.head && tag == e.tag && start == e.start && end == e.end) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.ExpandSearchPhase.isCollapseRequest",
	"Comment": "returns true iff the search request has inner hits and needs field collapsing",
	"Method": "boolean isCollapseRequest(){\r\n    final SearchRequest searchRequest = context.getRequest();\r\n    return searchRequest.source() != null && searchRequest.source().collapse() != null && searchRequest.source().collapse().getInnerHits().isEmpty() == false;\r\n}"
}, {
	"Path": "org.elasticsearch.action.get.GetRequest.version",
	"Comment": "sets the version, which will cause the get operation to only be performed if a matchingversion exists and no changes happened on the doc since then.",
	"Method": "long version(GetRequest version,long version){\r\n    this.version = version;\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.TregexPoweredTreebankParserParams.display",
	"Comment": "output a description of the current annotation configuration tostandard error.",
	"Method": "void display(){\r\n    for (String feature : features) System.err.printf(\"%s \", feature);\r\n    log.info();\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.DanglingIndicesState.findNewDanglingIndices",
	"Comment": "finds new dangling indices by iterating over the indices and trying to find indicesthat have state on disk, but are not part of the provided meta data, or not detectedas dangled already.",
	"Method": "Map<Index, IndexMetaData> findNewDanglingIndices(MetaData metaData){\r\n    final Set<String> excludeIndexPathIds = new HashSet(metaData.indices().size() + danglingIndices.size());\r\n    for (ObjectCursor<IndexMetaData> cursor : metaData.indices().values()) {\r\n        excludeIndexPathIds.add(cursor.value.getIndex().getUUID());\r\n    }\r\n    excludeIndexPathIds.addAll(danglingIndices.keySet().stream().map(Index::getUUID).collect(Collectors.toList()));\r\n    try {\r\n        final List<IndexMetaData> indexMetaDataList = metaStateService.loadIndicesStates(excludeIndexPathIds::contains);\r\n        Map<Index, IndexMetaData> newIndices = new HashMap(indexMetaDataList.size());\r\n        final IndexGraveyard graveyard = metaData.indexGraveyard();\r\n        for (IndexMetaData indexMetaData : indexMetaDataList) {\r\n            if (metaData.hasIndex(indexMetaData.getIndex().getName())) {\r\n                logger.warn(\"[{}] can not be imported as a dangling index, as index with same name already exists in cluster metadata\", indexMetaData.getIndex());\r\n            } else if (graveyard.containsIndex(indexMetaData.getIndex())) {\r\n                logger.warn(\"[{}] can not be imported as a dangling index, as an index with the same name and UUID exist in the \" + \"index tombstones.  This situation is likely caused by copying over the data directory for an index \" + \"that was previously deleted.\", indexMetaData.getIndex());\r\n            } else {\r\n                logger.info(\"[{}] dangling index exists on local file system, but not in cluster metadata, \" + \"auto import to cluster state\", indexMetaData.getIndex());\r\n                newIndices.put(indexMetaData.getIndex(), indexMetaData);\r\n            }\r\n        }\r\n        return newIndices;\r\n    } catch (IOException e) {\r\n        logger.warn(\"failed to list dangling indices\", e);\r\n        return emptyMap();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.ReaderIteratorFactory.addAll",
	"Comment": "adds all objects in collection c to the underlying collection ofinput sources.",
	"Method": "boolean addAll(Collection<?> c){\r\n    return this.c.addAll(c);\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.AtomicArray.toArray",
	"Comment": "copies the content of the underlying atomic array to a normal one.",
	"Method": "E[] toArray(E[] a){\r\n    if (a.length != array.length()) {\r\n        throw new ElasticsearchGenerationException(\"AtomicArrays can only be copied to arrays of the same size\");\r\n    }\r\n    for (int i = 0; i < array.length(); i++) {\r\n        a[i] = array.get(i);\r\n    }\r\n    return a;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequest.routing",
	"Comment": "the routing values to control the shards that the search will be executed on.",
	"Method": "String routing(SearchRequest routing,String routing,SearchRequest routing,String routings){\r\n    this.routing = Strings.arrayToCommaDelimitedString(routings);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogHeader.getPrimaryTerm",
	"Comment": "returns the primary term stored in this translog header.all operations in a translog file are expected to have their primary terms at most this term.",
	"Method": "long getPrimaryTerm(){\r\n    return primaryTerm;\r\n}"
}, {
	"Path": "org.elasticsearch.action.explain.ExplainRequestBuilder.setParent",
	"Comment": "simple sets the routing. since the parent is only used to get to the right shard.",
	"Method": "ExplainRequestBuilder setParent(String parent){\r\n    request().parent(parent);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.WordTag.equals",
	"Comment": "a wordtag is equal only to another wordtag with the same word and tag values.",
	"Method": "boolean equals(Object o){\r\n    if (this == o)\r\n        return true;\r\n    if (!(o instanceof WordTag))\r\n        return false;\r\n    final WordTag wordTag = (WordTag) o;\r\n    if (tag != null ? !tag.equals(wordTag.tag) : wordTag.tag != null)\r\n        return false;\r\n    if (word != null ? !word.equals(wordTag.word) : wordTag.word != null)\r\n        return false;\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel",
	"Comment": "reads one object from the given objectinputstream, which isassumed to be a lexicalizedparser.throws a classcastexceptionif this is not true.the stream is not closed.",
	"Method": "LexicalizedParser loadModel(LexicalizedParser loadModel,Options op,String extraFlags,LexicalizedParser loadModel,String parserFileOrUrl,String extraFlags,LexicalizedParser loadModel,String parserFileOrUrl,List<String> extraFlags,LexicalizedParser loadModel,String parserFileOrUrl,Options op,String extraFlags,LexicalizedParser loadModel,ObjectInputStream ois){\r\n    try {\r\n        Object o = ois.readObject();\r\n        if (o instanceof LexicalizedParser) {\r\n            return (LexicalizedParser) o;\r\n        }\r\n        throw new ClassCastException(\"Wanted LexicalizedParser, got \" + o.getClass());\r\n    } catch (IOException e) {\r\n        throw new RuntimeIOException(e);\r\n    } catch (ClassNotFoundException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.getNumIndexVersionsLookups",
	"Comment": "returns the number of times a version was looked up either from the index.note this is only available if assertions are enabled",
	"Method": "long getNumIndexVersionsLookups(){\r\n    return numIndexVersionsLookups.count();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequestBuilder.setOrder",
	"Comment": "sets the order of this template if more than one template matches.",
	"Method": "PutIndexTemplateRequestBuilder setOrder(int order){\r\n    request.order(order);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.markAsRecovering",
	"Comment": "marks the shard as recovering based on a recovery state, fails with exception is recovering is not allowed to be set.",
	"Method": "IndexShardState markAsRecovering(String reason,RecoveryState recoveryState){\r\n    synchronized (mutex) {\r\n        if (state == IndexShardState.CLOSED) {\r\n            throw new IndexShardClosedException(shardId);\r\n        }\r\n        if (state == IndexShardState.STARTED) {\r\n            throw new IndexShardStartedException(shardId);\r\n        }\r\n        if (state == IndexShardState.RECOVERING) {\r\n            throw new IndexShardRecoveringException(shardId);\r\n        }\r\n        if (state == IndexShardState.POST_RECOVERY) {\r\n            throw new IndexShardRecoveringException(shardId);\r\n        }\r\n        this.recoveryState = recoveryState;\r\n        return changeState(IndexShardState.RECOVERING, reason);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.command.AllocationCommand.getMessage",
	"Comment": "returns any feedback the command wants to provide for logging. this message should be appropriate to expose to the user after thecommand has been applied",
	"Method": "Optional<String> getMessage(){\r\n    return Optional.empty();\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDocument.printState",
	"Comment": "for internal debugging purposes only.prints the state of the given basicdocument to stderr.",
	"Method": "void printState(BasicDocument<L> bd){\r\n    log.info(\"BasicDocument:\");\r\n    log.info(\"\\tTitle: \" + bd.title());\r\n    log.info(\"\\tLabels: \" + bd.labels());\r\n    log.info(\"\\tOriginalText: \" + bd.originalText());\r\n    log.info(\"\\tWords: \" + bd);\r\n    log.info();\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.getHighlightMaxAnalyzedOffset",
	"Comment": "returns the maximum number of chars that will be analyzed in a highlight request",
	"Method": "int getHighlightMaxAnalyzedOffset(){\r\n    return this.maxAnalyzedOffset;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryShardContext.getIndexSettings",
	"Comment": "returns the index settings for this context. this might return null if thecontext has not index scope.",
	"Method": "IndexSettings getIndexSettings(){\r\n    return indexSettings;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.util.RelationTriple.allTokens",
	"Comment": "returns all the tokens in the extraction, in the order subject then relation then object.",
	"Method": "List<CoreLabel> allTokens(){\r\n    List<CoreLabel> allTokens = new ArrayList();\r\n    allTokens.addAll(canonicalSubject);\r\n    allTokens.addAll(relation);\r\n    allTokens.addAll(canonicalObject);\r\n    return allTokens;\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.ParseFieldRegistry.lookup",
	"Comment": "lookup a value from the registry by name while checking that the name matches the parsefield.",
	"Method": "T lookup(String name,XContentLocation xContentLocation,DeprecationHandler deprecationHandler){\r\n    T value = lookupReturningNullIfNotFound(name, deprecationHandler);\r\n    if (value == null) {\r\n        throw new ParsingException(xContentLocation, \"no [\" + registryName + \"] registered for [\" + name + \"]\");\r\n    }\r\n    return value;\r\n}"
}, {
	"Path": "org.elasticsearch.index.flush.FlushStats.getPeriodic",
	"Comment": "the number of flushes that were periodically triggered when translog exceeded the flush threshold.",
	"Method": "long getPeriodic(){\r\n    return periodic;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.StatusServletTests.testHubGetAllConfig",
	"Comment": "when no param is specified, a call to the hub api returns all the config params the hubcurrently uses.",
	"Method": "void testHubGetAllConfig(){\r\n    String url = hubApi.toExternalForm();\r\n    HttpRequest r = new HttpRequest(GET, url);\r\n    Map<String, Object> j = ImmutableMap.of(\"configuration\", ImmutableList.of());\r\n    r.setContent(new Json().toJson(j).getBytes(UTF_8));\r\n    HttpResponse response = client.execute(r);\r\n    assertEquals(200, response.getStatus());\r\n    Map<String, Object> o = extractObject(response);\r\n    assertTrue((boolean) o.get(\"success\"));\r\n    assertEquals(\"org.openqa.grid.internal.utils.DefaultCapabilityMatcher\", o.get(\"capabilityMatcher\"));\r\n    assertNull(o.get(\"prioritizer\"));\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.LocalCheckpointTracker.getBitSetKey",
	"Comment": "return the bit set for the provided sequence number, possibly allocating a new set if needed.",
	"Method": "long getBitSetKey(long seqNo){\r\n    return seqNo / BIT_SET_SIZE;\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.Store.decRef",
	"Comment": "decreases the refcount of this store instance. if the refcount drops to 0, then thisstore is closed.",
	"Method": "void decRef(){\r\n    refCounter.decRef();\r\n}"
}, {
	"Path": "org.json.ParsingTest.testParsingNumbersThatAreBestRepresentedAsLongs",
	"Comment": "unfortunately the original implementation attempts to figure out whatjava number type best suits an input value.",
	"Method": "void testParsingNumbersThatAreBestRepresentedAsLongs(){\r\n    assertParsed(9223372036854775807L, \"9223372036854775807\");\r\n    assertParsed(9223372036854775806L, \"9223372036854775806\");\r\n    assertParsed(-9223372036854775808L, \"-9223372036854775808\");\r\n    assertParsed(-9223372036854775807L, \"-9223372036854775807\");\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.WriteRequest.setRefreshPolicy",
	"Comment": "parse the refresh policy from a string, only modifying it if the string is non null. convenient to use with request parsing.",
	"Method": "R setRefreshPolicy(RefreshPolicy refreshPolicy,R setRefreshPolicy,String refreshPolicy){\r\n    if (refreshPolicy != null) {\r\n        setRefreshPolicy(RefreshPolicy.parse(refreshPolicy));\r\n    }\r\n    return (R) this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ack.CreateIndexClusterStateUpdateResponse.isShardsAcknowledged",
	"Comment": "returns whether the requisite number of shard copies started before the completion of the operation.",
	"Method": "boolean isShardsAcknowledged(){\r\n    return shardsAcknowledged;\r\n}"
}, {
	"Path": "edu.stanford.nlp.maxent.iis.LambdaSolve.fExpected",
	"Comment": "computes the expected value of a feature for the current model.",
	"Method": "double fExpected(Feature f){\r\n    double s = 0.0;\r\n    for (int i = 0; i < f.len(); i++) {\r\n        int x = f.getX(i);\r\n        int y = f.getY(i);\r\n        s += p.data.ptildeX(x) * pcond(y, x) * f.getVal(i);\r\n    }\r\n    return s;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.readGlobalCheckpoint",
	"Comment": "reads the sequence numbers global checkpoint from the translog checkpoint.this ensures that the transloguuid from this translog matches with the provided transloguuid.",
	"Method": "long readGlobalCheckpoint(Path location,String expectedTranslogUUID){\r\n    final Checkpoint checkpoint = readCheckpoint(location, expectedTranslogUUID);\r\n    return checkpoint.globalCheckpoint;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.ShardRouting.relocatingNodeId",
	"Comment": "the relocating node id the shard is either relocating to or relocating from.",
	"Method": "String relocatingNodeId(){\r\n    return this.relocatingNodeId;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Settings.get",
	"Comment": "returns the setting value associated with the setting key. if it does not exists,returns the default value provided.",
	"Method": "String get(String setting,String get,String setting,String defaultValue,String get,String setting,String defaultValue,boolean isList,String get,String key,Object get,Object key){\r\n    Object value = settings.get(setting);\r\n    if (value != null) {\r\n        if (value instanceof List) {\r\n            if (isList == false) {\r\n                throw new IllegalArgumentException(\"Found list type value for setting [\" + setting + \"] but but did not expect a list for it.\");\r\n            }\r\n        } else if (isList) {\r\n            throw new IllegalArgumentException(\"Expected list type value for setting [\" + setting + \"] but found [\" + value.getClass() + ']');\r\n        }\r\n        return toString(value);\r\n    } else {\r\n        return defaultValue;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.CreateClauseDataset.countDatums",
	"Comment": "count the number of extractions in the given dataset. that is, the sum count of the pair spansfor each sentence.",
	"Method": "int countDatums(List<Pair<CoreMap, Collection<Pair<Span, Span>>>> data){\r\n    int count = 0;\r\n    for (Pair<CoreMap, Collection<Pair<Span, Span>>> datum : data) {\r\n        count += datum.second.size();\r\n    }\r\n    return count;\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.LocalCheckpointTracker.contains",
	"Comment": "checks if the given sequence number was marked as completed in this tracker.",
	"Method": "boolean contains(long seqNo){\r\n    assert seqNo >= 0 : \"invalid seq_no=\" + seqNo;\r\n    if (seqNo >= nextSeqNo) {\r\n        return false;\r\n    }\r\n    if (seqNo <= checkpoint) {\r\n        return true;\r\n    }\r\n    final long bitSetKey = getBitSetKey(seqNo);\r\n    final CountedBitSet bitSet;\r\n    synchronized (this) {\r\n        bitSet = processedSeqNo.get(bitSetKey);\r\n    }\r\n    return bitSet != null && bitSet.get(seqNoToBitSetOffset(seqNo));\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.arabic.IBMArabicEscaper.escapeString",
	"Comment": "escapes a word. this method willmap a word to the null string.",
	"Method": "String escapeString(String word){\r\n    String firstStage = stripAnnotationsAndClassing(word);\r\n    String secondStage = ATBTreeUtils.escape(firstStage);\r\n    if (secondStage.isEmpty()) {\r\n        return firstStage;\r\n    } else if (!firstStage.equals(secondStage)) {\r\n        return secondStage;\r\n    }\r\n    String thirdStage = lexMapper.map(null, secondStage);\r\n    if (thirdStage.isEmpty()) {\r\n        return secondStage;\r\n    }\r\n    return thirdStage;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.LeaderBulkByScrollTaskState.onSliceFailure",
	"Comment": "record a failure from a slice and respond to the listener if the request is finished.",
	"Method": "void onSliceFailure(ActionListener<BulkByScrollResponse> listener,int sliceId,Exception e){\r\n    results.setOnce(sliceId, new Result(sliceId, e));\r\n    recordSliceCompletionAndRespondIfAllDone(listener);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams.diskTreebank",
	"Comment": "returns a disktreebank with a negrapenntokenizer and anegrapenntreenormalizer",
	"Method": "DiskTreebank diskTreebank(){\r\n    return new DiskTreebank(treeReaderFactory(), inputEncoding);\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.shouldPeriodicallyFlush",
	"Comment": "tests whether or not the engine should be flushed periodically.this test is based on the current size of the translog compared to the configured flush threshold size.",
	"Method": "boolean shouldPeriodicallyFlush(){\r\n    final Engine engine = getEngineOrNull();\r\n    if (engine != null) {\r\n        try {\r\n            return engine.shouldPeriodicallyFlush();\r\n        } catch (final AlreadyClosedException e) {\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.utils.SelfRegisteringRemote.sendRegistrationRequest",
	"Comment": "sends 1 registration request, bypassing the retry logic and the proxy already registered check.use only for testing.",
	"Method": "void sendRegistrationRequest(){\r\n    registerToHub(false);\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.ZenDiscovery.validateIncomingState",
	"Comment": "does simple sanity check of the incoming cluster state. throws an exception on rejections.",
	"Method": "void validateIncomingState(Logger logger,ClusterState incomingState,ClusterState lastState){\r\n    final ClusterName incomingClusterName = incomingState.getClusterName();\r\n    if (!incomingClusterName.equals(lastState.getClusterName())) {\r\n        logger.warn(\"received cluster state from [{}] which is also master but with a different cluster name [{}]\", incomingState.nodes().getMasterNode(), incomingClusterName);\r\n        throw new IllegalStateException(\"received state from a node that is not part of the cluster\");\r\n    }\r\n    if (lastState.nodes().getLocalNode().equals(incomingState.nodes().getLocalNode()) == false) {\r\n        logger.warn(\"received a cluster state from [{}] and not part of the cluster, should not happen\", incomingState.nodes().getMasterNode());\r\n        throw new IllegalStateException(\"received state with a local node that does not match the current local node\");\r\n    }\r\n    if (shouldIgnoreOrRejectNewClusterState(logger, lastState, incomingState)) {\r\n        String message = String.format(Locale.ROOT, \"rejecting cluster state version [%d] uuid [%s] received from [%s]\", incomingState.version(), incomingState.stateUUID(), incomingState.nodes().getMasterNodeId());\r\n        logger.warn(message);\r\n        throw new IllegalStateException(message);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.IndexShardRoutingTable.onlyNodeSelectorActiveInitializingShardsIt",
	"Comment": "returns shards based on nodeattributes givensuch as node name , node attribute, node ipsupports node specifications in cluster api",
	"Method": "ShardIterator onlyNodeSelectorActiveInitializingShardsIt(String nodeAttributes,DiscoveryNodes discoveryNodes,ShardIterator onlyNodeSelectorActiveInitializingShardsIt,String[] nodeAttributes,DiscoveryNodes discoveryNodes){\r\n    ArrayList<ShardRouting> ordered = new ArrayList(activeShards.size() + allInitializingShards.size());\r\n    Set<String> selectedNodes = Sets.newHashSet(discoveryNodes.resolveNodes(nodeAttributes));\r\n    int seed = shuffler.nextSeed();\r\n    for (ShardRouting shardRouting : shuffler.shuffle(activeShards, seed)) {\r\n        if (selectedNodes.contains(shardRouting.currentNodeId())) {\r\n            ordered.add(shardRouting);\r\n        }\r\n    }\r\n    for (ShardRouting shardRouting : shuffler.shuffle(allInitializingShards, seed)) {\r\n        if (selectedNodes.contains(shardRouting.currentNodeId())) {\r\n            ordered.add(shardRouting);\r\n        }\r\n    }\r\n    if (ordered.isEmpty()) {\r\n        final String message = String.format(Locale.ROOT, \"no data nodes with %s [%s] found for shard: %s\", nodeAttributes.length == 1 ? \"criteria\" : \"criterion\", String.join(\",\", nodeAttributes), shardId());\r\n        throw new IllegalArgumentException(message);\r\n    }\r\n    return new PlainShardIterator(shardId, ordered);\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.KeyStoreWrapper.keystorePath",
	"Comment": "returns a path representing the es keystore in the given config dir.",
	"Method": "Path keystorePath(Path configDir){\r\n    return configDir.resolve(KEYSTORE_FILENAME);\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.newUpdater",
	"Comment": "build the updater responsible for validating new values, logging the newvalue, and eventually setting the value where it belongs.",
	"Method": "AbstractScopedSettings.SettingUpdater<T> newUpdater(Consumer<T> consumer,Logger logger,AbstractScopedSettings.SettingUpdater<T> newUpdater,Consumer<T> consumer,Logger logger,Consumer<T> validator,AbstractScopedSettings.SettingUpdater<Settings> newUpdater,Consumer<Settings> consumer,Logger logger,Consumer<Settings> validator){\r\n    if (isDynamic()) {\r\n        return new Updater(consumer, logger, validator);\r\n    } else {\r\n        throw new IllegalStateException(\"setting [\" + getKey() + \"] is not dynamic\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.ConstructorInjectorStore.get",
	"Comment": "returns a new complete constructor injector with injection listeners registered.",
	"Method": "ConstructorInjector<T> get(TypeLiteral<T> key,Errors errors){\r\n    return (ConstructorInjector<T>) cache.get(key, errors);\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.BootstrapInfo.isNativesAvailable",
	"Comment": "returns true if we successfully loaded native libraries.if this returns false, then native operations such as lockingmemory did not work.",
	"Method": "boolean isNativesAvailable(){\r\n    return Natives.JNA_AVAILABLE;\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.DanglingIndicesState.allocateDanglingIndices",
	"Comment": "allocates the provided list of the dangled indices by sending them to the master nodefor allocation.",
	"Method": "void allocateDanglingIndices(){\r\n    if (danglingIndices.isEmpty()) {\r\n        return;\r\n    }\r\n    try {\r\n        allocateDangledIndices.allocateDangled(Collections.unmodifiableCollection(new ArrayList(danglingIndices.values())), new LocalAllocateDangledIndices.Listener() {\r\n            @Override\r\n            public void onResponse(LocalAllocateDangledIndices.AllocateDangledResponse response) {\r\n                logger.trace(\"allocated dangled\");\r\n            }\r\n            @Override\r\n            public void onFailure(Throwable e) {\r\n                logger.info(\"failed to send allocated dangled\", e);\r\n            }\r\n        });\r\n    } catch (Exception e) {\r\n        logger.warn(\"failed to send allocate dangled\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.DanglingIndicesState.allocateDanglingIndices",
	"Comment": "allocates the provided list of the dangled indices by sending them to the master nodefor allocation.",
	"Method": "void allocateDanglingIndices(){\r\n    logger.trace(\"allocated dangled\");\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.DanglingIndicesState.allocateDanglingIndices",
	"Comment": "allocates the provided list of the dangled indices by sending them to the master nodefor allocation.",
	"Method": "void allocateDanglingIndices(){\r\n    logger.info(\"failed to send allocated dangled\", e);\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.RecordIterator.determineNumFields",
	"Comment": "a static convenience method that tells you how many fields are in thefirst line of the specified file, using the default whitespacedelimiter.",
	"Method": "int determineNumFields(String filename,String delim,int determineNumFields,String filename){\r\n    return determineNumFields(filename, WHITESPACE);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.get.GetSnapshotsRequest.verbose",
	"Comment": "returns whether the request will return a verbose response.",
	"Method": "GetSnapshotsRequest verbose(boolean verbose,boolean verbose){\r\n    return verbose;\r\n}"
}, {
	"Path": "org.elasticsearch.index.snapshots.blobstore.SnapshotFiles.containPhysicalIndexFile",
	"Comment": "returns true if this snapshot contains a file with a given original name",
	"Method": "boolean containPhysicalIndexFile(String physicalName){\r\n    return findPhysicalIndexFile(physicalName) != null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.QueueResizingEsThreadPoolExecutor.getTaskExecutionEWMA",
	"Comment": "returns the exponentially weighted moving average of the task execution time",
	"Method": "double getTaskExecutionEWMA(){\r\n    return executionEWMA.getAverage();\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.arabic.process.ArabicSegmenterFeatureFactory.getCliqueFeatures",
	"Comment": "extracts all the features from the input data at a certain index.",
	"Method": "Collection<String> getCliqueFeatures(PaddedList<IN> cInfo,int loc,Clique clique){\r\n    Collection<String> features = Generics.newHashSet();\r\n    if (clique == cliqueC) {\r\n        addAllInterningAndSuffixing(features, featuresC(cInfo, loc), \"C\");\r\n    } else if (clique == cliqueCpC) {\r\n        addAllInterningAndSuffixing(features, featuresCpC(cInfo, loc), \"CpC\");\r\n    } else if (clique == cliqueCp2C) {\r\n        addAllInterningAndSuffixing(features, featuresCp2C(cInfo, loc), \"Cp2C\");\r\n    } else if (clique == cliqueCp3C) {\r\n        addAllInterningAndSuffixing(features, featuresCp3C(cInfo, loc), \"Cp3C\");\r\n    }\r\n    String domain = cInfo.get(loc).get(CoreAnnotations.DomainAnnotation.class);\r\n    if (domain != null) {\r\n        Collection<String> domainFeatures = Generics.newHashSet();\r\n        for (String feature : features) {\r\n            domainFeatures.add(feature + DOMAIN_MARKER + domain);\r\n        }\r\n        features.addAll(domainFeatures);\r\n    }\r\n    return features;\r\n}"
}, {
	"Path": "org.elasticsearch.env.NodeEnvironment.assertCanWrite",
	"Comment": "this is a best effort to ensure that we actually have write permissions to write in all our data directories.this prevents disasters if nodes are started under the wrong username etc.",
	"Method": "void assertCanWrite(){\r\n    for (Path path : nodeDataPaths()) {\r\n        tryWriteTempFile(path);\r\n    }\r\n    for (String indexFolderName : this.availableIndexFolders()) {\r\n        for (Path indexPath : this.resolveIndexFolder(indexFolderName)) {\r\n            Path indexStatePath = indexPath.resolve(MetaDataStateFormat.STATE_DIR_NAME);\r\n            tryWriteTempFile(indexStatePath);\r\n            tryWriteTempFile(indexPath);\r\n            try (DirectoryStream<Path> stream = Files.newDirectoryStream(indexPath)) {\r\n                for (Path shardPath : stream) {\r\n                    String fileName = shardPath.getFileName().toString();\r\n                    if (Files.isDirectory(shardPath) && fileName.chars().allMatch(Character::isDigit)) {\r\n                        Path indexDir = shardPath.resolve(ShardPath.INDEX_FOLDER_NAME);\r\n                        Path statePath = shardPath.resolve(MetaDataStateFormat.STATE_DIR_NAME);\r\n                        Path translogDir = shardPath.resolve(ShardPath.TRANSLOG_FOLDER_NAME);\r\n                        tryWriteTempFile(indexDir);\r\n                        tryWriteTempFile(translogDir);\r\n                        tryWriteTempFile(statePath);\r\n                        tryWriteTempFile(shardPath);\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.node.DiscoveryNode.isIngestNode",
	"Comment": "returns a boolean that tells whether this an ingest node or not",
	"Method": "boolean isIngestNode(Settings settings,boolean isIngestNode){\r\n    return roles.contains(Role.INGEST);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.service.ClusterApplierService.assertNotCalledFromClusterStateApplier",
	"Comment": "asserts that the current stack trace does not involve a cluster state applier",
	"Method": "boolean assertNotCalledFromClusterStateApplier(String reason){\r\n    if (Thread.currentThread().getName().contains(CLUSTER_UPDATE_THREAD_NAME)) {\r\n        for (StackTraceElement element : Thread.currentThread().getStackTrace()) {\r\n            final String className = element.getClassName();\r\n            final String methodName = element.getMethodName();\r\n            if (className.equals(ClusterStateObserver.class.getName())) {\r\n                return true;\r\n            } else if (className.equals(ClusterApplierService.class.getName()) && methodName.equals(\"callClusterStateAppliers\")) {\r\n                throw new AssertionError(\"should not be called by a cluster state applier. reason [\" + reason + \"]\");\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.arabic.process.ArabicSegmenter.evaluate",
	"Comment": "evaluate accuracy when the input is gold segmented textsegmentationmarkers and morphological analyses. in other words, the evaluation file has thesame format as the training data.",
	"Method": "void evaluate(PrintWriter pwOut){\r\n    log.info(\"Starting evaluation...\");\r\n    boolean hasSegmentationMarkers = true;\r\n    boolean hasTags = true;\r\n    DocumentReaderAndWriter<CoreLabel> docReader = new ArabicDocumentReaderAndWriter(hasSegmentationMarkers, hasTags, hasDomainLabels, domain, tf);\r\n    ObjectBank<List<CoreLabel>> lines = classifier.makeObjectBankFromFile(flags.testFile, docReader);\r\n    PrintWriter tedEvalGoldTree = null, tedEvalParseTree = null;\r\n    PrintWriter tedEvalGoldSeg = null, tedEvalParseSeg = null;\r\n    if (tedEvalPrefix != null) {\r\n        try {\r\n            tedEvalGoldTree = new PrintWriter(tedEvalPrefix + \"_gold.ftree\");\r\n            tedEvalGoldSeg = new PrintWriter(tedEvalPrefix + \"_gold.segmentation\");\r\n            tedEvalParseTree = new PrintWriter(tedEvalPrefix + \"_parse.ftree\");\r\n            tedEvalParseSeg = new PrintWriter(tedEvalPrefix + \"_parse.segmentation\");\r\n        } catch (FileNotFoundException e) {\r\n            System.err.printf(\"%s: %s%n\", ArabicSegmenter.class.getName(), e.getMessage());\r\n        }\r\n    }\r\n    Counter<String> labelTotal = new ClassicCounter();\r\n    Counter<String> labelCorrect = new ClassicCounter();\r\n    int total = 0;\r\n    int correct = 0;\r\n    for (List<CoreLabel> line : lines) {\r\n        final String[] inputTokens = tedEvalSanitize(IOBUtils.IOBToString(line).replaceAll(\":\", \"#pm#\")).split(\" \");\r\n        final String[] goldTokens = tedEvalSanitize(IOBUtils.IOBToString(line, \":\")).split(\" \");\r\n        line = classifier.classify(line);\r\n        final String[] parseTokens = tedEvalSanitize(IOBUtils.IOBToString(line, \":\")).split(\" \");\r\n        for (CoreLabel label : line) {\r\n            String observation = label.get(CoreAnnotations.CharAnnotation.class);\r\n            if (!observation.equals(IOBUtils.getBoundaryCharacter())) {\r\n                total++;\r\n                String hypothesis = label.get(CoreAnnotations.AnswerAnnotation.class);\r\n                String reference = label.get(CoreAnnotations.GoldAnswerAnnotation.class);\r\n                labelTotal.incrementCount(reference);\r\n                if (hypothesis.equals(reference)) {\r\n                    correct++;\r\n                    labelCorrect.incrementCount(reference);\r\n                }\r\n            }\r\n        }\r\n        if (tedEvalParseSeg != null) {\r\n            tedEvalGoldTree.printf(\"(root\");\r\n            tedEvalParseTree.printf(\"(root\");\r\n            int safeLength = inputTokens.length;\r\n            if (inputTokens.length != goldTokens.length) {\r\n                log.info(\"In generating TEDEval files: Input and gold do not have the same number of tokens\");\r\n                log.info(\"    (ignoring any extras)\");\r\n                log.info(\"  input: \" + Arrays.toString(inputTokens));\r\n                log.info(\"  gold: \" + Arrays.toString(goldTokens));\r\n                safeLength = Math.min(inputTokens.length, goldTokens.length);\r\n            }\r\n            if (inputTokens.length != parseTokens.length) {\r\n                log.info(\"In generating TEDEval files: Input and parse do not have the same number of tokens\");\r\n                log.info(\"    (ignoring any extras)\");\r\n                log.info(\"  input: \" + Arrays.toString(inputTokens));\r\n                log.info(\"  parse: \" + Arrays.toString(parseTokens));\r\n                safeLength = Math.min(inputTokens.length, parseTokens.length);\r\n            }\r\n            for (int i = 0; i < safeLength; i++) {\r\n                for (String segment : goldTokens[i].split(\":\")) tedEvalGoldTree.printf(\" (seg %s)\", segment);\r\n                tedEvalGoldSeg.printf(\"%s\\t%s%n\", inputTokens[i], goldTokens[i]);\r\n                for (String segment : parseTokens[i].split(\":\")) tedEvalParseTree.printf(\" (seg %s)\", segment);\r\n                tedEvalParseSeg.printf(\"%s\\t%s%n\", inputTokens[i], parseTokens[i]);\r\n            }\r\n            tedEvalGoldTree.printf(\")%n\");\r\n            tedEvalGoldSeg.println();\r\n            tedEvalParseTree.printf(\")%n\");\r\n            tedEvalParseSeg.println();\r\n        }\r\n    }\r\n    double accuracy = ((double) correct) / ((double) total);\r\n    accuracy *= 100.0;\r\n    pwOut.println(\"EVALUATION RESULTS\");\r\n    pwOut.printf(\"#datums:\\t%d%n\", total);\r\n    pwOut.printf(\"#correct:\\t%d%n\", correct);\r\n    pwOut.printf(\"accuracy:\\t%.2f%n\", accuracy);\r\n    pwOut.println(\"==================\");\r\n    pwOut.println(\"PER LABEL ACCURACIES\");\r\n    for (String refLabel : labelTotal.keySet()) {\r\n        double nTotal = labelTotal.getCount(refLabel);\r\n        double nCorrect = labelCorrect.getCount(refLabel);\r\n        double acc = (nCorrect / nTotal) * 100.0;\r\n        pwOut.printf(\" %s\\t%.2f%n\", refLabel, acc);\r\n    }\r\n    if (tedEvalParseSeg != null) {\r\n        tedEvalGoldTree.close();\r\n        tedEvalGoldSeg.close();\r\n        tedEvalParseTree.close();\r\n        tedEvalParseSeg.close();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogHeader.write",
	"Comment": "writes this header with the latest format into the file channel",
	"Method": "void write(FileChannel channel){\r\n    @SuppressWarnings({ \"IOResourceOpenedButNotSafelyClosed\", \"resource\" })\r\n    final BufferedChecksumStreamOutput out = new BufferedChecksumStreamOutput(new OutputStreamStreamOutput(java.nio.channels.Channels.newOutputStream(channel)));\r\n    CodecUtil.writeHeader(new OutputStreamDataOutput(out), TRANSLOG_CODEC, CURRENT_VERSION);\r\n    final BytesRef uuid = new BytesRef(translogUUID);\r\n    out.writeInt(uuid.length);\r\n    out.writeBytes(uuid.bytes, uuid.offset, uuid.length);\r\n    out.writeLong(primaryTerm);\r\n    out.writeInt((int) out.getChecksum());\r\n    out.flush();\r\n    channel.force(true);\r\n    assert channel.position() == headerSizeInBytes : \"Header is not fully written; header size [\" + headerSizeInBytes + \"], channel position [\" + channel.position() + \"]\";\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkPrimaryExecutionContext.markAsRequiringMappingUpdate",
	"Comment": "indicates that the current operation can not be completed and needs to wait for a new mapping from the master",
	"Method": "void markAsRequiringMappingUpdate(){\r\n    assert assertInvariants(ItemProcessingState.TRANSLATED);\r\n    currentItemState = ItemProcessingState.WAIT_FOR_MAPPING_UPDATE;\r\n    requestToExecute = null;\r\n    assert assertInvariants(ItemProcessingState.WAIT_FOR_MAPPING_UPDATE);\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.getMinGenerationForSeqNo",
	"Comment": "gets the minimum generation that could contain any sequence number after the specified sequence number, or the current generation ifthere is no generation that could any such sequence number.",
	"Method": "TranslogGeneration getMinGenerationForSeqNo(long seqNo){\r\n    try (ReleasableLock ignored = readLock.acquire()) {\r\n        long minTranslogFileGeneration = this.currentFileGeneration();\r\n        for (final TranslogReader reader : readers) {\r\n            if (seqNo <= reader.getCheckpoint().maxSeqNo) {\r\n                minTranslogFileGeneration = Math.min(minTranslogFileGeneration, reader.getGeneration());\r\n            }\r\n        }\r\n        return new TranslogGeneration(translogUUID, minTranslogFileGeneration);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.AllocationId.newTargetRelocation",
	"Comment": "creates a new allocation id for the target initializing shard that is the resultof a relocation.",
	"Method": "AllocationId newTargetRelocation(AllocationId allocationId){\r\n    assert allocationId.getRelocationId() != null;\r\n    return new AllocationId(allocationId.getRelocationId(), allocationId.getId());\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.CoreLabel.setWord",
	"Comment": "set the word value for the label.also, clears the lemma, sincethat may have changed if the word changed.",
	"Method": "void setWord(String word){\r\n    String originalWord = get(CoreAnnotations.TextAnnotation.class);\r\n    set(CoreAnnotations.TextAnnotation.class, word);\r\n    if (word != null && !word.equals(originalWord) && containsKey(CoreAnnotations.LemmaAnnotation.class)) {\r\n        remove(CoreAnnotations.LemmaAnnotation.class);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.BulkByScrollTask.taskInfoGivenSubtaskInfo",
	"Comment": "build the status for this task given a snapshot of the information of running slices. this is only supported if the task isset as a leader for slice subtasks",
	"Method": "TaskInfo taskInfoGivenSubtaskInfo(String localNodeId,List<TaskInfo> sliceInfo){\r\n    if (isLeader() == false) {\r\n        throw new IllegalStateException(\"This task is not set to be a leader of other slice subtasks\");\r\n    }\r\n    List<BulkByScrollTask.StatusOrException> sliceStatuses = Arrays.asList(new BulkByScrollTask.StatusOrException[leaderState.getSlices()]);\r\n    for (TaskInfo t : sliceInfo) {\r\n        BulkByScrollTask.Status status = (BulkByScrollTask.Status) t.getStatus();\r\n        sliceStatuses.set(status.getSliceId(), new BulkByScrollTask.StatusOrException(status));\r\n    }\r\n    Status status = leaderState.getStatus(sliceStatuses);\r\n    return taskInfo(localNodeId, getDescription(), status);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.SequenceMatcher.findAllNonOverlapping",
	"Comment": "applies the matcher and returns all non overlapping matches",
	"Method": "Iterable<SequenceMatchResult<T>> findAllNonOverlapping(){\r\n    Iterator<SequenceMatchResult<T>> iter = new Iterator<SequenceMatchResult<T>>() {\r\n        SequenceMatchResult<T> next;\r\n        private SequenceMatchResult<T> getNext() {\r\n            boolean found = find();\r\n            if (found) {\r\n                return toBasicSequenceMatchResult();\r\n            } else {\r\n                return null;\r\n            }\r\n        }\r\n        @Override\r\n        public boolean hasNext() {\r\n            if (next == null) {\r\n                next = getNext();\r\n                return (next != null);\r\n            } else {\r\n                return true;\r\n            }\r\n        }\r\n        @Override\r\n        public SequenceMatchResult<T> next() {\r\n            if (!hasNext()) {\r\n                throw new NoSuchElementException();\r\n            }\r\n            SequenceMatchResult<T> res = next;\r\n            next = null;\r\n            return res;\r\n        }\r\n        public void remove() {\r\n            throw new UnsupportedOperationException();\r\n        }\r\n    };\r\n    return new IterableIterator(iter);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.SequenceMatcher.findAllNonOverlapping",
	"Comment": "applies the matcher and returns all non overlapping matches",
	"Method": "Iterable<SequenceMatchResult<T>> findAllNonOverlapping(){\r\n    boolean found = find();\r\n    if (found) {\r\n        return toBasicSequenceMatchResult();\r\n    } else {\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.SequenceMatcher.findAllNonOverlapping",
	"Comment": "applies the matcher and returns all non overlapping matches",
	"Method": "Iterable<SequenceMatchResult<T>> findAllNonOverlapping(){\r\n    if (next == null) {\r\n        next = getNext();\r\n        return (next != null);\r\n    } else {\r\n        return true;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.SequenceMatcher.findAllNonOverlapping",
	"Comment": "applies the matcher and returns all non overlapping matches",
	"Method": "Iterable<SequenceMatchResult<T>> findAllNonOverlapping(){\r\n    if (!hasNext()) {\r\n        throw new NoSuchElementException();\r\n    }\r\n    SequenceMatchResult<T> res = next;\r\n    next = null;\r\n    return res;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.SequenceMatcher.findAllNonOverlapping",
	"Comment": "applies the matcher and returns all non overlapping matches",
	"Method": "Iterable<SequenceMatchResult<T>> findAllNonOverlapping(){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.MembersInjectorStore.createWithListeners",
	"Comment": "creates a new members injector and attaches both injection listeners and method aspects.",
	"Method": "MembersInjectorImpl<T> createWithListeners(TypeLiteral<T> type,Errors errors){\r\n    int numErrorsBefore = errors.size();\r\n    Set<InjectionPoint> injectionPoints;\r\n    try {\r\n        injectionPoints = InjectionPoint.forInstanceMethodsAndFields(type);\r\n    } catch (ConfigurationException e) {\r\n        errors.merge(e.getErrorMessages());\r\n        injectionPoints = e.getPartialValue();\r\n    }\r\n    List<SingleMemberInjector> injectors = getInjectors(injectionPoints, errors);\r\n    errors.throwIfNewErrors(numErrorsBefore);\r\n    EncounterImpl<T> encounter = new EncounterImpl(errors, injector.lookups);\r\n    for (TypeListenerBinding typeListener : typeListenerBindings) {\r\n        if (typeListener.getTypeMatcher().matches(type)) {\r\n            try {\r\n                typeListener.getListener().hear(type, encounter);\r\n            } catch (RuntimeException e) {\r\n                errors.errorNotifyingTypeListener(typeListener, type, e);\r\n            }\r\n        }\r\n    }\r\n    encounter.invalidate();\r\n    errors.throwIfNewErrors(numErrorsBefore);\r\n    return new MembersInjectorImpl(injector, type, encounter, injectors);\r\n}"
}, {
	"Path": "org.elasticsearch.ExceptionsHelper.rethrowAndSuppress",
	"Comment": "rethrows the first exception in the list and adds all remaining to the suppressed list.if the given list is empty no exception is thrown",
	"Method": "void rethrowAndSuppress(List<T> exceptions){\r\n    T main = null;\r\n    for (T ex : exceptions) {\r\n        main = useOrSuppress(main, ex);\r\n    }\r\n    if (main != null) {\r\n        throw main;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.collect.ImmutableOpenMap.containsKey",
	"Comment": "returns true if this container has an association to a value forthe given key.",
	"Method": "boolean containsKey(KType key,boolean containsKey,KType key){\r\n    return map.containsKey(key);\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.getDefaultRaw",
	"Comment": "returns the default value string representation for this setting.",
	"Method": "String getDefaultRaw(Settings settings){\r\n    return defaultValue.apply(settings);\r\n}"
}, {
	"Path": "org.elasticsearch.action.get.GetRequestBuilder.setVersion",
	"Comment": "sets the version, which will cause the get operation to only be performed if a matchingversion exists and no changes happened on the doc since then.",
	"Method": "GetRequestBuilder setVersion(long version){\r\n    request.version(version);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.MoveDecision.rebalance",
	"Comment": "creates a decision for whether to move the shard to a different node to form a better cluster balance.",
	"Method": "MoveDecision rebalance(Decision canRebalanceDecision,AllocationDecision allocationDecision,DiscoveryNode assignedNode,int currentNodeRanking,List<NodeAllocationResult> nodeDecisions){\r\n    return new MoveDecision(null, canRebalanceDecision, allocationDecision, assignedNode, nodeDecisions, currentNodeRanking);\r\n}"
}, {
	"Path": "org.elasticsearch.indices.recovery.PeerRecoveryTargetService.getStoreMetadataSnapshot",
	"Comment": "obtains a snapshot of the store metadata for the recovery target.",
	"Method": "Store.MetadataSnapshot getStoreMetadataSnapshot(RecoveryTarget recoveryTarget){\r\n    try {\r\n        return recoveryTarget.indexShard().snapshotStoreMetadata();\r\n    } catch (final org.apache.lucene.index.IndexNotFoundException e) {\r\n        logger.trace(\"{} shard folder empty, recovering all files\", recoveryTarget);\r\n        return Store.MetadataSnapshot.EMPTY;\r\n    } catch (final IOException e) {\r\n        logger.warn(\"error while listing local files, recovering as if there are none\", e);\r\n        return Store.MetadataSnapshot.EMPTY;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.resetEngineToGlobalCheckpoint",
	"Comment": "rollback the current engine to the safe commit, then replay local translog up to the global checkpoint.",
	"Method": "void resetEngineToGlobalCheckpoint(){\r\n    assert getActiveOperationsCount() == 0 : \"Ongoing writes [\" + getActiveOperations() + \"]\";\r\n    sync();\r\n    final long globalCheckpoint = getGlobalCheckpoint();\r\n    final Engine newEngine;\r\n    synchronized (mutex) {\r\n        verifyNotClosed();\r\n        IOUtils.close(currentEngineReference.getAndSet(null));\r\n        trimUnsafeCommits();\r\n        newEngine = createNewEngine(newEngineConfig());\r\n        active.set(true);\r\n    }\r\n    newEngine.advanceMaxSeqNoOfUpdatesOrDeletes(globalCheckpoint);\r\n    final Engine.TranslogRecoveryRunner translogRunner = (engine, snapshot) -> runTranslogRecovery(engine, snapshot, Engine.Operation.Origin.LOCAL_RESET, () -> {\r\n    });\r\n    newEngine.recoverFromTranslog(translogRunner, globalCheckpoint);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.ReplicationOperation.checkActiveShardCount",
	"Comment": "checks whether we can perform a write based on the required active shard count setting.returnsif ok to proceed, or a string describing the reason to stop",
	"Method": "String checkActiveShardCount(){\r\n    final ShardId shardId = primary.routingEntry().shardId();\r\n    final ActiveShardCount waitForActiveShards = request.waitForActiveShards();\r\n    if (waitForActiveShards == ActiveShardCount.NONE) {\r\n        return null;\r\n    }\r\n    final IndexShardRoutingTable shardRoutingTable = primary.getReplicationGroup().getRoutingTable();\r\n    if (waitForActiveShards.enoughShardsActive(shardRoutingTable)) {\r\n        return null;\r\n    } else {\r\n        final String resolvedShards = waitForActiveShards == ActiveShardCount.ALL ? Integer.toString(shardRoutingTable.shards().size()) : waitForActiveShards.toString();\r\n        logger.trace(\"[{}] not enough active copies to meet shard count of [{}] (have {}, needed {}), scheduling a retry. op [{}], \" + \"request [{}]\", shardId, waitForActiveShards, shardRoutingTable.activeShards().size(), resolvedShards, opType, request);\r\n        return \"Not enough active copies to meet shard count of [\" + waitForActiveShards + \"] (have \" + shardRoutingTable.activeShards().size() + \", needed \" + resolvedShards + \").\";\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.analysis.TokenFilterFactory.getChainAwareTokenFilterFactory",
	"Comment": "rewrite the tokenfilterfactory to take into account the preceding analysis chain, or referto other tokenfilterfactories",
	"Method": "TokenFilterFactory getChainAwareTokenFilterFactory(TokenizerFactory tokenizer,List<CharFilterFactory> charFilters,List<TokenFilterFactory> previousTokenFilters,Function<String, TokenFilterFactory> allFilters){\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.util.RelationTriple.relationTokenSpan",
	"Comment": "get a representative span for the relation expressed by this triple.this is a bit more complicated than the subject and object spans, as the relationspan is occasionally discontinuous.if this is the case, this method returns the largest contiguous chunk.if the relation span is empty, return the object span.",
	"Method": "Pair<Integer, Integer> relationTokenSpan(){\r\n    if (relation.isEmpty()) {\r\n        return objectTokenSpan();\r\n    } else if (relation.size() == 1) {\r\n        return Pair.makePair(relation.get(0).index() - 1, relation.get(0).index());\r\n    } else {\r\n        int longestChunk = 0;\r\n        int longestChunkStart = 0;\r\n        int thisChunk = 1;\r\n        int thisChunkStart = 0;\r\n        for (int i = 1; i < relation.size(); ++i) {\r\n            CoreLabel token = relation.get(i);\r\n            CoreLabel lastToken = relation.get(i - 1);\r\n            if (lastToken.index() + 1 == token.index()) {\r\n                thisChunk += 1;\r\n            } else if (lastToken.index() + 2 == token.index()) {\r\n                thisChunk += 2;\r\n            } else {\r\n                if (thisChunk > longestChunk) {\r\n                    longestChunk = thisChunk;\r\n                    longestChunkStart = thisChunkStart;\r\n                }\r\n                thisChunkStart = i;\r\n                thisChunk = 1;\r\n            }\r\n        }\r\n        if (thisChunk > longestChunk) {\r\n            longestChunk = thisChunk;\r\n            longestChunkStart = thisChunkStart;\r\n        }\r\n        return Pair.makePair(relation.get(longestChunkStart).index() - 1, relation.get(longestChunkStart).index() - 1 + longestChunk);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.io.FileSystemUtils.isAccessibleDirectory",
	"Comment": "check that a directory exists, is a directory and is readableby the current user",
	"Method": "boolean isAccessibleDirectory(Path directory,Logger logger){\r\n    assert directory != null && logger != null;\r\n    if (!Files.exists(directory)) {\r\n        logger.debug(\"[{}] directory does not exist.\", directory.toAbsolutePath());\r\n        return false;\r\n    }\r\n    if (!Files.isDirectory(directory)) {\r\n        logger.debug(\"[{}] should be a directory but is not.\", directory.toAbsolutePath());\r\n        return false;\r\n    }\r\n    if (!Files.isReadable(directory)) {\r\n        logger.debug(\"[{}] directory is not readable.\", directory.toAbsolutePath());\r\n        return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.ArrayMath.safeMean",
	"Comment": "returns the mean of a vector of doubles.any values which are nan orinfinite are ignored.if the vector is empty, 0.0 is returned.",
	"Method": "double safeMean(double[] v){\r\n    double[] u = filterNaNAndInfinite(v);\r\n    if (numRows(u) == 0)\r\n        return 0.0;\r\n    return mean(u);\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.AbstractScopedSettings.isFinalSetting",
	"Comment": "returns true if the setting for the given key is final. otherwise false.",
	"Method": "boolean isFinalSetting(String key){\r\n    final Setting<?> setting = get(key);\r\n    return setting != null && setting.isFinal();\r\n}"
}, {
	"Path": "org.openqa.selenium.remote.server.DefaultDriverProvider.canCreateDriverInstanceFor",
	"Comment": "checks that the browser name set in the provided capabilities matches the browser nameset in the desired capabilities.",
	"Method": "boolean canCreateDriverInstanceFor(Capabilities capabilities){\r\n    return this.capabilities.getBrowserName().equals(capabilities.getBrowserName());\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.Security.getCodebaseJarMap",
	"Comment": "return a map from codebase name to codebase url of jar codebases used by es core.",
	"Method": "Map<String, URL> getCodebaseJarMap(Set<URL> urls){\r\n    Map<String, URL> codebases = new LinkedHashMap();\r\n    for (URL url : urls) {\r\n        try {\r\n            String fileName = PathUtils.get(url.toURI()).getFileName().toString();\r\n            if (fileName.endsWith(\".jar\") == false) {\r\n                continue;\r\n            }\r\n            codebases.put(fileName, url);\r\n        } catch (URISyntaxException e) {\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n    return codebases;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.DiskThresholdMonitor.warnAboutDiskIfNeeded",
	"Comment": "warn about the given disk usage if the low or high watermark has been passed",
	"Method": "void warnAboutDiskIfNeeded(DiskUsage usage){\r\n    if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdFloodStage().getBytes()) {\r\n        logger.warn(\"flood stage disk watermark [{}] exceeded on {}, all indices on this node will be marked read-only\", diskThresholdSettings.getFreeBytesThresholdFloodStage(), usage);\r\n    } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdHigh().getBytes()) {\r\n        logger.warn(\"high disk watermark [{}] exceeded on {}, shards will be relocated away from this node\", diskThresholdSettings.getFreeBytesThresholdHigh(), usage);\r\n    } else if (usage.getFreeBytes() < diskThresholdSettings.getFreeBytesThresholdLow().getBytes()) {\r\n        logger.info(\"low disk watermark [{}] exceeded on {}, replicas will not be assigned to this node\", diskThresholdSettings.getFreeBytesThresholdLow(), usage);\r\n    }\r\n    if (usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdFloodStage()) {\r\n        logger.warn(\"flood stage disk watermark [{}] exceeded on {}, all indices on this node will be marked read-only\", Strings.format1Decimals(100.0 - diskThresholdSettings.getFreeDiskThresholdFloodStage(), \"%\"), usage);\r\n    } else if (usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdHigh()) {\r\n        logger.warn(\"high disk watermark [{}] exceeded on {}, shards will be relocated away from this node\", Strings.format1Decimals(100.0 - diskThresholdSettings.getFreeDiskThresholdHigh(), \"%\"), usage);\r\n    } else if (usage.getFreeDiskAsPercentage() < diskThresholdSettings.getFreeDiskThresholdLow()) {\r\n        logger.info(\"low disk watermark [{}] exceeded on {}, replicas will not be assigned to this node\", Strings.format1Decimals(100.0 - diskThresholdSettings.getFreeDiskThresholdLow(), \"%\"), usage);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.PluginsService.addSortedBundle",
	"Comment": "add the given bundle to the sorted bundles, first adding dependencies",
	"Method": "void addSortedBundle(Bundle bundle,Map<String, Bundle> bundles,LinkedHashSet<Bundle> sortedBundles,LinkedHashSet<String> dependencyStack){\r\n    String name = bundle.plugin.getName();\r\n    if (dependencyStack.contains(name)) {\r\n        StringBuilder msg = new StringBuilder(\"Cycle found in plugin dependencies: \");\r\n        dependencyStack.forEach(s -> {\r\n            msg.append(s);\r\n            msg.append(\" -> \");\r\n        });\r\n        msg.append(name);\r\n        throw new IllegalStateException(msg.toString());\r\n    }\r\n    if (sortedBundles.contains(bundle)) {\r\n        return;\r\n    }\r\n    dependencyStack.add(name);\r\n    for (String dependency : bundle.plugin.getExtendedPlugins()) {\r\n        Bundle depBundle = bundles.get(dependency);\r\n        if (depBundle == null) {\r\n            throw new IllegalArgumentException(\"Missing plugin [\" + dependency + \"], dependency of [\" + name + \"]\");\r\n        }\r\n        addSortedBundle(depBundle, bundles, sortedBundles, dependencyStack);\r\n        assert sortedBundles.contains(depBundle);\r\n    }\r\n    dependencyStack.remove(name);\r\n    sortedBundles.add(bundle);\r\n}"
}, {
	"Path": "org.elasticsearch.indices.IndicesService.indexService",
	"Comment": "returns an indexservice for the specified index if exists otherwise returns null.",
	"Method": "IndexService indexService(Index index){\r\n    return indices.get(index.getUUID());\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequestBuilder.addAlias",
	"Comment": "adds an alias that will be added when the index template gets created.",
	"Method": "PutIndexTemplateRequestBuilder addAlias(Alias alias){\r\n    request.alias(alias);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterState.supersedes",
	"Comment": "a cluster state supersedes another state if they are from the same master and the version of this state is higher than that of theother state.in essence that means that all the changes from the other cluster state are also reflected by the current one",
	"Method": "boolean supersedes(ClusterState other){\r\n    return this.nodes().getMasterNodeId() != null && this.nodes().getMasterNodeId().equals(other.nodes().getMasterNodeId()) && this.version() > other.version();\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.DocumentReader.getReader",
	"Comment": "returns the reader for the text input source of this documentreader.",
	"Method": "Reader getReader(Reader getReader,String text,Reader getReader,File file,Reader getReader,URL url,Reader getReader,InputStream in){\r\n    return (new InputStreamReader(in));\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.totalOperationsByMinGen",
	"Comment": "returns the number of operations in the translog files at least the given generation",
	"Method": "int totalOperationsByMinGen(long minGeneration){\r\n    try (ReleasableLock ignored = readLock.acquire()) {\r\n        ensureOpen();\r\n        return Stream.concat(readers.stream(), Stream.of(current)).filter(r -> r.getGeneration() >= minGeneration).mapToInt(BaseTranslogReader::totalOperations).sum();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.SimpleQueryStringBuilder.flags",
	"Comment": "specify the enabled features of the simplequerystring. defaults to all ifnone are specified.",
	"Method": "SimpleQueryStringBuilder flags(SimpleQueryStringFlag flags,SimpleQueryStringBuilder flags,int flags,int flags){\r\n    return this.flags;\r\n}"
}, {
	"Path": "org.openqa.selenium.JavascriptEnabledDriverTest.testShouldBeAbleToClickIfEvenSomethingHorribleHappens",
	"Comment": "if the click handler throws an exception, the firefox driver freezes. this is suboptimal.",
	"Method": "void testShouldBeAbleToClickIfEvenSomethingHorribleHappens(){\r\n    driver.get(pages.javascriptPage);\r\n    driver.findElement(By.id(\"error\")).click();\r\n    String text = driver.findElement(By.id(\"error\")).getText();\r\n    assertThat(text).isNotNull();\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.getNumDocUpdates",
	"Comment": "returns the number of documents have been updated since this engine was opened.this count does not include the updates from the existing segments before opening engine.",
	"Method": "long getNumDocUpdates(){\r\n    return numDocUpdates.count();\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.idsQuery",
	"Comment": "constructs a query that will match only specific ids within types.",
	"Method": "IdsQueryBuilder idsQuery(IdsQueryBuilder idsQuery,String types){\r\n    return new IdsQueryBuilder().types(types);\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.XMoreLikeThis.setMaxDocFreqPct",
	"Comment": "set the maximum percentage in which words may still appear. words that appearin more than this many percent of all docs will be ignored.",
	"Method": "void setMaxDocFreqPct(int maxPercentage){\r\n    this.maxDocFreq = maxPercentage * ir.numDocs() / 100;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.ArabicUnknownWordModel.getSignatureIndex",
	"Comment": "returns the index of the signature of the word numbered wordindex, wherethe signature is the string representation of unknown word features.",
	"Method": "int getSignatureIndex(int index,int sentencePosition,String word){\r\n    String uwSig = getSignature(word, sentencePosition);\r\n    int sig = wordIndex.addToIndex(uwSig);\r\n    return sig;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.AbstractPagedHashMap.capacity",
	"Comment": "return the number of allocated slots to store this hash table.",
	"Method": "long capacity(){\r\n    return mask + 1;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.nndep.ArcStandard.makeTransitions",
	"Comment": "generate all possible transitions which this parsing system cantake for any given configuration.",
	"Method": "List<String> makeTransitions(List<String> labels){\r\n    List<String> moves = new ArrayList();\r\n    for (String label : labels) {\r\n        moves.add(\"L(\" + label + ')');\r\n    }\r\n    for (String label : labels) {\r\n        moves.add(\"R(\" + label + ')');\r\n    }\r\n    moves.add(\"S\");\r\n    return moves;\r\n}"
}, {
	"Path": "org.elasticsearch.common.cache.CacheBuilder.setExpireAfterWrite",
	"Comment": "sets the amount of time before an entry in the cache expires after it was written.",
	"Method": "CacheBuilder<K, V> setExpireAfterWrite(TimeValue expireAfterWrite){\r\n    Objects.requireNonNull(expireAfterWrite);\r\n    final long expireAfterWriteNanos = expireAfterWrite.getNanos();\r\n    if (expireAfterWriteNanos <= 0) {\r\n        throw new IllegalArgumentException(\"expireAfterWrite <= 0\");\r\n    }\r\n    this.expireAfterWriteNanos = expireAfterWriteNanos;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.ElasticsearchException.ids",
	"Comment": "returns an array of all registered handle ids. these are the ids for every registeredexception.",
	"Method": "int[] ids(){\r\n    return Arrays.stream(ElasticsearchExceptionHandle.values()).mapToInt(h -> h.id).toArray();\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.Polarity.isDownwards",
	"Comment": "ignoring exclusion, determine if this word has downward polarity.",
	"Method": "boolean isDownwards(){\r\n    return projectLexicalRelation(NaturalLogicRelation.FORWARD_ENTAILMENT) == NaturalLogicRelation.REVERSE_ENTAILMENT && projectLexicalRelation(NaturalLogicRelation.REVERSE_ENTAILMENT) == NaturalLogicRelation.FORWARD_ENTAILMENT;\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.getGenerationThresholdSize",
	"Comment": "returns the generation threshold size. as sequence numbers can cause multiple generations tobe preserved for rollback purposes, we want to keep the size of individual generations fromgrowing too large to avoid excessive disk space consumption. therefore, the translog isautomatically rolled to a new generation when the current generation exceeds this generationthreshold size.",
	"Method": "ByteSizeValue getGenerationThresholdSize(){\r\n    return generationThresholdSize;\r\n}"
}, {
	"Path": "org.openqa.grid.web.servlet.handler.RequestHandler.forwardNewSessionRequestAndUpdateRegistry",
	"Comment": "forward the new session request to the testsession that has been assigned, and parse theresponse to extract and return the external key assigned by the remote.",
	"Method": "void forwardNewSessionRequestAndUpdateRegistry(TestSession session){\r\n    try (NewSessionPayload payload = NewSessionPayload.create(ImmutableMap.of(\"desiredCapabilities\", session.getRequestedCapabilities()))) {\r\n        StringBuilder json = new StringBuilder();\r\n        payload.writeTo(json);\r\n        request.setBody(json.toString());\r\n        session.forward(getRequest(), getResponse(), true);\r\n    } catch (IOException e) {\r\n        throw new NewSessionException(\"Error forwarding the request \" + e.getMessage(), e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cli.EnvironmentAwareCommand.putSystemPropertyIfSettingIsMissing",
	"Comment": "ensure the given setting exists, reading it from system properties if not already set.",
	"Method": "void putSystemPropertyIfSettingIsMissing(Map<String, String> settings,String setting,String key){\r\n    final String value = System.getProperty(key);\r\n    if (value != null) {\r\n        if (settings.containsKey(setting)) {\r\n            final String message = String.format(Locale.ROOT, \"duplicate setting [%s] found via command-line [%s] and system property [%s]\", setting, settings.get(setting), value);\r\n            throw new IllegalArgumentException(message);\r\n        } else {\r\n            settings.put(setting, value);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.match",
	"Comment": "returns true iff the given key matches the settings key or if this setting is a group setting if thegiven key is part of the settings group.",
	"Method": "boolean match(String toTest,boolean match,String key,boolean match,String key,boolean match,String toTest,boolean match,String toTest,boolean match,String key){\r\n    return key.match(toTest);\r\n}"
}, {
	"Path": "org.elasticsearch.indices.recovery.RecoveryTarget.retryCopy",
	"Comment": "returns a fresh recovery target to retry recovery from the same source node onto the same shard and using the same listener.",
	"Method": "RecoveryTarget retryCopy(){\r\n    return new RecoveryTarget(indexShard, sourceNode, listener, ensureClusterStateVersionCallback);\r\n}"
}, {
	"Path": "org.apache.dubbo.rpc.RpcContext.getResponse",
	"Comment": "get the response object of the underlying rpc protocol, e.g. httpservletresponse",
	"Method": "Object getResponse(T getResponse,Class<T> clazz){\r\n    return (response != null && clazz.isAssignableFrom(response.getClass())) ? (T) response : null;\r\n}"
}, {
	"Path": "java.io.EmulatedFieldsForDumping.emulatedFields",
	"Comment": "return the actual emulatedfields instance used by the receiver. we havethe actual work in a separate class so that the code can be shared. thereceiver has to be of a subclass of putfield.",
	"Method": "EmulatedFields emulatedFields(){\r\n    return emulatedFields;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequestBuilder.setAllShards",
	"Comment": "indicates whether the query should be validated on all shards",
	"Method": "ValidateQueryRequestBuilder setAllShards(boolean rewrite){\r\n    request.allShards(rewrite);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.common.ParserGrammar.parse",
	"Comment": "parses the list of hasword.if the parse fails for some reason,an x tree is returned instead of barfing.",
	"Method": "Tree parse(String sentence,Tree parse,List<? extends HasWord> words){\r\n    List<? extends HasWord> tokens = tokenize(sentence);\r\n    if (getOp().testOptions.preTag) {\r\n        Function<List<? extends HasWord>, List<TaggedWord>> tagger = loadTagger();\r\n        tokens = tagger.apply(tokens);\r\n    }\r\n    return parse(tokens);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequest.indices",
	"Comment": "returns list of indices that should be restored from snapshot",
	"Method": "RestoreSnapshotRequest indices(String indices,RestoreSnapshotRequest indices,List<String> indices,String[] indices){\r\n    return indices;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.DocumentReader.getTokenizerFactory",
	"Comment": "returns the tokenizer used to chop up text into words for the documents.",
	"Method": "TokenizerFactory<? extends HasWord> getTokenizerFactory(){\r\n    return (tokenizerFactory);\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.get",
	"Comment": "returns the value for this setting but falls back to the second provided settings object",
	"Method": "T get(Settings settings,T get,Settings settings,boolean validate,T get,Settings primary,Settings secondary,T get,Settings settings,Settings get,Settings settings){\r\n    if (exists(primary)) {\r\n        return get(primary);\r\n    }\r\n    if (exists(secondary)) {\r\n        return get(secondary);\r\n    }\r\n    if (fallbackSetting == null) {\r\n        return get(primary);\r\n    }\r\n    if (fallbackSetting.exists(primary)) {\r\n        return fallbackSetting.get(primary);\r\n    }\r\n    return fallbackSetting.get(secondary);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.LiveVersionMap.acquireLock",
	"Comment": "acquires a releaseable lock for the given uid. all underlock methods requirethis lock to be hold by the caller otherwise the visibility guarantees of this versionmap are broken. we assert on this lock to be hold when calling these methods.",
	"Method": "Releasable acquireLock(BytesRef uid){\r\n    return keyedLock.acquire(uid);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.PlainListenableActionFuture.newListenableFuture",
	"Comment": "this method returns a listenable future. the listeners will be called on completion of the future.the listeners will be executed by the same thread that completes the future.",
	"Method": "PlainListenableActionFuture<T> newListenableFuture(){\r\n    return new PlainListenableActionFuture();\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.metrics.UnlabeledAttachmentEval.makeObjects",
	"Comment": "build the set of dependencies for evaluation.this set excludesall dependencies for which the argument is a punctuation tag.",
	"Method": "Set<?> makeObjects(Tree tree){\r\n    if (tree == null) {\r\n        log.info(\"Warning: null tree\");\r\n        return Generics.newHashSet();\r\n    }\r\n    if (headFinder != null) {\r\n        tree.percolateHeads(headFinder);\r\n    }\r\n    Set<Dependency<Label, Label, Object>> deps = tree.dependencies(punctRejectFilter);\r\n    return deps;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.WorkerBulkByScrollTaskState.delayPrepareBulkRequest",
	"Comment": "schedule preparebulkrequestrunnable to run after some delay. this is where throttling plugs into reindexing so the request can berescheduled over and over again.",
	"Method": "void delayPrepareBulkRequest(ThreadPool threadPool,TimeValue lastBatchStartTime,int lastBatchSize,AbstractRunnable prepareBulkRequestRunnable){\r\n    synchronized (delayedPrepareBulkRequestReference) {\r\n        TimeValue delay = throttleWaitTime(lastBatchStartTime, timeValueNanos(System.nanoTime()), lastBatchSize);\r\n        logger.debug(\"[{}]: preparing bulk request for [{}]\", task.getId(), delay);\r\n        try {\r\n            delayedPrepareBulkRequestReference.set(new DelayedPrepareBulkRequest(threadPool, getRequestsPerSecond(), delay, new RunOnce(prepareBulkRequestRunnable)));\r\n        } catch (EsRejectedExecutionException e) {\r\n            prepareBulkRequestRunnable.onRejection(e);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoriesService.applyClusterState",
	"Comment": "checks if new repositories appeared in or disappeared from cluster metadata and updates current list ofrepositories accordingly.",
	"Method": "void applyClusterState(ClusterChangedEvent event){\r\n    try {\r\n        RepositoriesMetaData oldMetaData = event.previousState().getMetaData().custom(RepositoriesMetaData.TYPE);\r\n        RepositoriesMetaData newMetaData = event.state().getMetaData().custom(RepositoriesMetaData.TYPE);\r\n        if ((oldMetaData == null && newMetaData == null) || (oldMetaData != null && oldMetaData.equals(newMetaData))) {\r\n            return;\r\n        }\r\n        logger.trace(\"processing new index repositories for state version [{}]\", event.state().version());\r\n        Map<String, Repository> survivors = new HashMap();\r\n        for (Map.Entry<String, Repository> entry : repositories.entrySet()) {\r\n            if (newMetaData == null || newMetaData.repository(entry.getKey()) == null) {\r\n                logger.debug(\"unregistering repository [{}]\", entry.getKey());\r\n                closeRepository(entry.getValue());\r\n            } else {\r\n                survivors.put(entry.getKey(), entry.getValue());\r\n            }\r\n        }\r\n        Map<String, Repository> builder = new HashMap();\r\n        if (newMetaData != null) {\r\n            for (RepositoryMetaData repositoryMetaData : newMetaData.repositories()) {\r\n                Repository repository = survivors.get(repositoryMetaData.name());\r\n                if (repository != null) {\r\n                    RepositoryMetaData previousMetadata = repository.getMetadata();\r\n                    if (previousMetadata.type().equals(repositoryMetaData.type()) == false || previousMetadata.settings().equals(repositoryMetaData.settings()) == false) {\r\n                        logger.debug(\"updating repository [{}]\", repositoryMetaData.name());\r\n                        closeRepository(repository);\r\n                        repository = null;\r\n                        try {\r\n                            repository = createRepository(repositoryMetaData);\r\n                        } catch (RepositoryException ex) {\r\n                            logger.warn(() -> new ParameterizedMessage(\"failed to change repository [{}]\", repositoryMetaData.name()), ex);\r\n                        }\r\n                    }\r\n                } else {\r\n                    try {\r\n                        repository = createRepository(repositoryMetaData);\r\n                    } catch (RepositoryException ex) {\r\n                        logger.warn(() -> new ParameterizedMessage(\"failed to create repository [{}]\", repositoryMetaData.name()), ex);\r\n                    }\r\n                }\r\n                if (repository != null) {\r\n                    logger.debug(\"registering repository [{}]\", repositoryMetaData.name());\r\n                    builder.put(repositoryMetaData.name(), repository);\r\n                }\r\n            }\r\n        }\r\n        repositories = Collections.unmodifiableMap(builder);\r\n    } catch (Exception ex) {\r\n        logger.warn(\"failure updating cluster state \", ex);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.logging.DeprecationLogger.assertWarningValue",
	"Comment": "assert that the specified string has the warning value equal to the provided warning value.",
	"Method": "boolean assertWarningValue(String s,String warningValue){\r\n    final Matcher matcher = WARNING_HEADER_PATTERN.matcher(s);\r\n    final boolean matches = matcher.matches();\r\n    assert matches;\r\n    return matcher.group(1).equals(warningValue);\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.ObjectBank.removeAll",
	"Comment": "unsupported operation.if you wish to remove data sources,remove, do so in the underlying readeriteratorfactory.",
	"Method": "boolean removeAll(Collection<?> c){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.updateGlobalCheckpointForShard",
	"Comment": "update the local knowledge of the global checkpoint for the specified allocation id.",
	"Method": "void updateGlobalCheckpointForShard(String allocationId,long globalCheckpoint){\r\n    assert assertPrimaryMode();\r\n    verifyNotClosed();\r\n    replicationTracker.updateGlobalCheckpointForShard(allocationId, globalCheckpoint);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.settings.put.UpdateSettingsRequest.isPreserveExisting",
	"Comment": "returns true iff the settings update should only add but not update settings. if the setting already existsit should not be overwritten by this update. the default is false",
	"Method": "boolean isPreserveExisting(){\r\n    return preserveExisting;\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.BindingProcessor.isOkayDuplicate",
	"Comment": "we tolerate duplicate bindings only if one exposes the other.",
	"Method": "boolean isOkayDuplicate(Binding<?> original,BindingImpl<?> binding){\r\n    if (original instanceof ExposedBindingImpl) {\r\n        ExposedBindingImpl<?> exposed = (ExposedBindingImpl<?>) original;\r\n        InjectorImpl exposedFrom = (InjectorImpl) exposed.getPrivateElements().getInjector();\r\n        return (exposedFrom == binding.getInjector());\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.TransportMultiSearchAction.executeSearch",
	"Comment": "executes a single request from the queue of requests. when a request finishes, another request is taken from the queue. when arequest is executed, a permit is taken on the specified semaphore, and released as each request completes.",
	"Method": "void executeSearch(Queue<SearchRequestSlot> requests,AtomicArray<MultiSearchResponse.Item> responses,AtomicInteger responseCounter,ActionListener<MultiSearchResponse> listener,long relativeStartTime){\r\n    SearchRequestSlot request = requests.poll();\r\n    if (request == null) {\r\n        return;\r\n    }\r\n    final Thread thread = Thread.currentThread();\r\n    client.search(request.request, new ActionListener<SearchResponse>() {\r\n        @Override\r\n        public void onResponse(final SearchResponse searchResponse) {\r\n            handleResponse(request.responseSlot, new MultiSearchResponse.Item(searchResponse, null));\r\n        }\r\n        @Override\r\n        public void onFailure(final Exception e) {\r\n            handleResponse(request.responseSlot, new MultiSearchResponse.Item(null, e));\r\n        }\r\n        private void handleResponse(final int responseSlot, final MultiSearchResponse.Item item) {\r\n            responses.set(responseSlot, item);\r\n            if (responseCounter.decrementAndGet() == 0) {\r\n                assert requests.isEmpty();\r\n                finish();\r\n            } else {\r\n                if (thread == Thread.currentThread()) {\r\n                    threadPool.generic().execute(() -> executeSearch(requests, responses, responseCounter, listener, relativeStartTime));\r\n                } else {\r\n                    executeSearch(requests, responses, responseCounter, listener, relativeStartTime);\r\n                }\r\n            }\r\n        }\r\n        private void finish() {\r\n            listener.onResponse(new MultiSearchResponse(responses.toArray(new MultiSearchResponse.Item[responses.length()]), buildTookInMillis()));\r\n        }\r\n        private long buildTookInMillis() {\r\n            return TimeUnit.NANOSECONDS.toMillis(relativeTimeProvider.getAsLong() - relativeStartTime);\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.TransportMultiSearchAction.executeSearch",
	"Comment": "executes a single request from the queue of requests. when a request finishes, another request is taken from the queue. when arequest is executed, a permit is taken on the specified semaphore, and released as each request completes.",
	"Method": "void executeSearch(Queue<SearchRequestSlot> requests,AtomicArray<MultiSearchResponse.Item> responses,AtomicInteger responseCounter,ActionListener<MultiSearchResponse> listener,long relativeStartTime){\r\n    handleResponse(request.responseSlot, new MultiSearchResponse.Item(searchResponse, null));\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.TransportMultiSearchAction.executeSearch",
	"Comment": "executes a single request from the queue of requests. when a request finishes, another request is taken from the queue. when arequest is executed, a permit is taken on the specified semaphore, and released as each request completes.",
	"Method": "void executeSearch(Queue<SearchRequestSlot> requests,AtomicArray<MultiSearchResponse.Item> responses,AtomicInteger responseCounter,ActionListener<MultiSearchResponse> listener,long relativeStartTime){\r\n    handleResponse(request.responseSlot, new MultiSearchResponse.Item(null, e));\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.TransportMultiSearchAction.executeSearch",
	"Comment": "executes a single request from the queue of requests. when a request finishes, another request is taken from the queue. when arequest is executed, a permit is taken on the specified semaphore, and released as each request completes.",
	"Method": "void executeSearch(Queue<SearchRequestSlot> requests,AtomicArray<MultiSearchResponse.Item> responses,AtomicInteger responseCounter,ActionListener<MultiSearchResponse> listener,long relativeStartTime){\r\n    responses.set(responseSlot, item);\r\n    if (responseCounter.decrementAndGet() == 0) {\r\n        assert requests.isEmpty();\r\n        finish();\r\n    } else {\r\n        if (thread == Thread.currentThread()) {\r\n            threadPool.generic().execute(() -> executeSearch(requests, responses, responseCounter, listener, relativeStartTime));\r\n        } else {\r\n            executeSearch(requests, responses, responseCounter, listener, relativeStartTime);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.TransportMultiSearchAction.executeSearch",
	"Comment": "executes a single request from the queue of requests. when a request finishes, another request is taken from the queue. when arequest is executed, a permit is taken on the specified semaphore, and released as each request completes.",
	"Method": "void executeSearch(Queue<SearchRequestSlot> requests,AtomicArray<MultiSearchResponse.Item> responses,AtomicInteger responseCounter,ActionListener<MultiSearchResponse> listener,long relativeStartTime){\r\n    listener.onResponse(new MultiSearchResponse(responses.toArray(new MultiSearchResponse.Item[responses.length()]), buildTookInMillis()));\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.TransportMultiSearchAction.executeSearch",
	"Comment": "executes a single request from the queue of requests. when a request finishes, another request is taken from the queue. when arequest is executed, a permit is taken on the specified semaphore, and released as each request completes.",
	"Method": "void executeSearch(Queue<SearchRequestSlot> requests,AtomicArray<MultiSearchResponse.Item> responses,AtomicInteger responseCounter,ActionListener<MultiSearchResponse> listener,long relativeStartTime){\r\n    return TimeUnit.NANOSECONDS.toMillis(relativeTimeProvider.getAsLong() - relativeStartTime);\r\n}"
}, {
	"Path": "android.util.SparseBooleanArray.get",
	"Comment": "gets the boolean mapped from the specified key, or the specified valueif no such mapping has been made.",
	"Method": "boolean get(int key,boolean get,int key,boolean valueIfKeyNotFound){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i < 0) {\r\n        return valueIfKeyNotFound;\r\n    } else {\r\n        return mValues[i];\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.LocalCheckpointTracker.markSeqNoAsCompleted",
	"Comment": "marks the processing of the provided sequence number as completed as updates the checkpoint if possible.",
	"Method": "void markSeqNoAsCompleted(long seqNo){\r\n    if (seqNo >= nextSeqNo) {\r\n        nextSeqNo = seqNo + 1;\r\n    }\r\n    if (seqNo <= checkpoint) {\r\n        return;\r\n    }\r\n    final CountedBitSet bitSet = getBitSetForSeqNo(seqNo);\r\n    final int offset = seqNoToBitSetOffset(seqNo);\r\n    bitSet.set(offset);\r\n    if (seqNo == checkpoint + 1) {\r\n        updateCheckpoint();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.regexpQuery",
	"Comment": "a query that matches documents containing terms with a specified regular expression.",
	"Method": "RegexpQueryBuilder regexpQuery(String name,String regexp){\r\n    return new RegexpQueryBuilder(name, regexp);\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.waitForLocalCheckpointToAdvance",
	"Comment": "wait for the local checkpoint to advance to the global checkpoint.",
	"Method": "void waitForLocalCheckpointToAdvance(){\r\n    this.wait();\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.BulkByScrollTask.getWorkerState",
	"Comment": "returns the object that manages sending search requests. throws illegalstateexception if this task is not set to be aworker task.",
	"Method": "WorkerBulkByScrollTaskState getWorkerState(){\r\n    if (!isWorker()) {\r\n        throw new IllegalStateException(\"This task is not set to be a worker\");\r\n    }\r\n    return workerState;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.routingEntry",
	"Comment": "returns the latest cluster routing entry received with this shard.",
	"Method": "ShardRouting routingEntry(){\r\n    return this.shardRouting;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.transferAnnotations",
	"Comment": "transfer from src to dst all annotations generated bu sutime and numbernormalizer",
	"Method": "void transferAnnotations(CoreLabel src,CoreLabel dst){\r\n    if (src.containsKey(CoreAnnotations.NumericCompositeValueAnnotation.class)) {\r\n        dst.set(CoreAnnotations.NumericCompositeValueAnnotation.class, src.get(CoreAnnotations.NumericCompositeValueAnnotation.class));\r\n    }\r\n    if (src.containsKey(CoreAnnotations.NumericCompositeTypeAnnotation.class))\r\n        dst.set(CoreAnnotations.NumericCompositeTypeAnnotation.class, src.get(CoreAnnotations.NumericCompositeTypeAnnotation.class));\r\n    if (src.containsKey(TimeAnnotations.TimexAnnotation.class))\r\n        dst.set(TimeAnnotations.TimexAnnotation.class, src.get(TimeAnnotations.TimexAnnotation.class));\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.SegmentsStats.getTermsMemoryInBytes",
	"Comment": "estimation of the terms dictionary memory usage by a segment.",
	"Method": "long getTermsMemoryInBytes(){\r\n    return this.termsMemoryInBytes;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.inference.CliqueTree.marginalizeMessage",
	"Comment": "this is a key step in message passing. when we are calculating a message, we want to marginalize out all variablesnot relevant to the recipient of the message. this function does that.",
	"Method": "TableFactor marginalizeMessage(TableFactor message,int[] relevant,MarginalizationMethod marginalize){\r\n    TableFactor result = message;\r\n    for (int i : message.neighborIndices) {\r\n        boolean contains = false;\r\n        for (int j : relevant) {\r\n            if (i == j) {\r\n                contains = true;\r\n                break;\r\n            }\r\n        }\r\n        if (!contains) {\r\n            switch(marginalize) {\r\n                case SUM:\r\n                    result = result.sumOut(i);\r\n                    break;\r\n                case MAX:\r\n                    result = result.maxOut(i);\r\n                    break;\r\n            }\r\n        }\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.openqa.grid.web.utils.ExtraServletUtil.createServlet",
	"Comment": "reflexion to create the servlet based on the class name. returns null if the class cannot beinstantiated.",
	"Method": "Class<? extends Servlet> createServlet(String className){\r\n    try {\r\n        return Class.forName(className).asSubclass(Servlet.class);\r\n    } catch (ClassNotFoundException e) {\r\n        log.warning(\"The specified class : \" + className + \" cannot be instantiated \" + e.getMessage());\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryRewriteContext.nowInMillis",
	"Comment": "returns the time in milliseconds that is shared across all resources involved. even across shards and nodes.",
	"Method": "long nowInMillis(){\r\n    return nowInMillis.getAsLong();\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.ListenableFuture.addListener",
	"Comment": "adds a listener to this future. if the future has not yet completed, the listener will benotified of a response or exception in a runnable submitted to the executorservice provided.if the future has completed, the listener will be notified immediately without forking toa different thread.",
	"Method": "void addListener(ActionListener<V> listener,ExecutorService executor,ThreadContext threadContext){\r\n    if (done) {\r\n        notifyListener(listener, EsExecutors.newDirectExecutorService());\r\n    } else {\r\n        final boolean run;\r\n        synchronized (this) {\r\n            if (done) {\r\n                run = true;\r\n            } else {\r\n                listeners.add(new Tuple(ContextPreservingActionListener.wrapPreservingContext(listener, threadContext), executor));\r\n                run = false;\r\n            }\r\n        }\r\n        if (run) {\r\n            notifyListener(listener, EsExecutors.newDirectExecutorService());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoShapeQueryBuilder.indexedShapeIndex",
	"Comment": "sets the name of the index where the indexed shape can be found",
	"Method": "GeoShapeQueryBuilder indexedShapeIndex(String indexedShapeIndex,String indexedShapeIndex){\r\n    return indexedShapeIndex;\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoriesService.registerRepository",
	"Comment": "creates a new repository and adds it to the list of registered repositories.if a repository with the same name but different types or settings already exists, it will be closed andreplaced with the new repository. if a repository with the same name exists but it has the same type and settingsthe new repository is ignored.",
	"Method": "void registerRepository(RegisterRepositoryRequest request,ActionListener<ClusterStateUpdateResponse> listener,boolean registerRepository,RepositoryMetaData repositoryMetaData){\r\n    Repository previous = repositories.get(repositoryMetaData.name());\r\n    if (previous != null) {\r\n        RepositoryMetaData previousMetadata = previous.getMetadata();\r\n        if (previousMetadata.equals(repositoryMetaData)) {\r\n            return false;\r\n        }\r\n    }\r\n    Repository newRepo = createRepository(repositoryMetaData);\r\n    if (previous != null) {\r\n        closeRepository(previous);\r\n    }\r\n    Map<String, Repository> newRepositories = new HashMap(repositories);\r\n    newRepositories.put(repositoryMetaData.name(), newRepo);\r\n    repositories = newRepositories;\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.AbstractTreebankParserParams.setGenerateOriginalDependencies",
	"Comment": "for languages that have implementations of theoriginal stanford dependencies and universaldependencies, this parameter is used to decide whichimplementation should be used.",
	"Method": "void setGenerateOriginalDependencies(boolean originalDependencies){\r\n    this.generateOriginalDependencies = originalDependencies;\r\n    if (this.tlp != null) {\r\n        this.tlp.setGenerateOriginalDependencies(originalDependencies);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.KeyedLock.isHeldByCurrentThread",
	"Comment": "returns true iff the caller thread holds the lock for the given key",
	"Method": "boolean isHeldByCurrentThread(T key){\r\n    KeyLock lock = map.get(key);\r\n    if (lock == null) {\r\n        return false;\r\n    }\r\n    return lock.isHeldByCurrentThread();\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.MultiWordStringMatcher.putSpacesAroundTargetString",
	"Comment": "finds target string in text and put spaces around it so it will be matched with we match against tokens.",
	"Method": "String putSpacesAroundTargetString(String text,String targetString){\r\n    return markTargetString(text, targetString, \" \", \" \", true);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.EngineConfig.getPrimaryTermSupplier",
	"Comment": "returns a supplier that supplies the latest primary term value of the associated shard.",
	"Method": "LongSupplier getPrimaryTermSupplier(){\r\n    return primaryTermSupplier;\r\n}"
}, {
	"Path": "edu.stanford.nlp.maxent.iis.LambdaSolve.getDerivativesLossDomination",
	"Comment": "using the arrays calculated when computing the loss, it should not betoo hard to get the derivatives.",
	"Method": "double[] getDerivativesLossDomination(){\r\n    double[] drvs = new double[lambda.length];\r\n    for (int fNo = 0; fNo < drvs.length; fNo++) {\r\n        Feature f = p.functions.get(fNo);\r\n        for (int index = 0, length = f.len(); index < length; index++) {\r\n            int x = f.getX(index);\r\n            int y = f.getY(index);\r\n            double val = f.getVal(index);\r\n            if (zlambda[x] == 0) {\r\n                continue;\r\n            }\r\n            double mult = val * p.data.ptildeX(x) * p.data.getNumber() * (1 / zlambda[x]);\r\n            double weight = 1;\r\n            if (weightRanks) {\r\n                weight = p.data.values[x][y];\r\n            }\r\n            drvs[fNo] += mult * sub[x][y];\r\n            drvs[fNo] -= mult * weight * (sum[x][y] - 1) / sum[x][y];\r\n        }\r\n    }\r\n    return drvs;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.NodesFaultDetection.updateNodesAndPing",
	"Comment": "make sure that nodes in clusterstate are pinged. any pinging to nodes which are notpart of the cluster will be stopped",
	"Method": "void updateNodesAndPing(ClusterState clusterState){\r\n    for (DiscoveryNode monitoredNode : nodesFD.keySet()) {\r\n        if (!clusterState.nodes().nodeExists(monitoredNode)) {\r\n            nodesFD.remove(monitoredNode);\r\n        }\r\n    }\r\n    for (DiscoveryNode node : clusterState.nodes()) {\r\n        if (node.equals(localNode)) {\r\n            continue;\r\n        }\r\n        if (!nodesFD.containsKey(node)) {\r\n            NodeFD fd = new NodeFD(node);\r\n            nodesFD.put(node, fd);\r\n            threadPool.schedule(TimeValue.timeValueMillis(0), ThreadPool.Names.SAME, fd);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.shouldRollGeneration",
	"Comment": "tests whether or not the translog generation should be rolled to a new generation. this testis based on the size of the current generation compared to the configured generationthreshold size.",
	"Method": "boolean shouldRollGeneration(){\r\n    final long size = this.current.sizeInBytes();\r\n    final long threshold = this.indexSettings.getGenerationThresholdSize().getBytes();\r\n    return size > threshold;\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.builders.ShapeBuilder.intersection",
	"Comment": "calculate the intersection of a line segment and a vertical dateline.",
	"Method": "double intersection(Coordinate p1,Coordinate p2,double dateline,Coordinate intersection,double position){\r\n    if (p1.x == p2.x && p1.x != dateline) {\r\n        return Double.NaN;\r\n    } else if (p1.x == p2.x && p1.x == dateline) {\r\n        return 1.0;\r\n    } else {\r\n        final double t = (dateline - p1.x) / (p2.x - p1.x);\r\n        if (t > 1 || t <= 0) {\r\n            return Double.NaN;\r\n        } else {\r\n            return t;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequestBuilder.setTerminateAfter",
	"Comment": "an optional document count, upon collecting which the searchquery will early terminate",
	"Method": "SearchRequestBuilder setTerminateAfter(int terminateAfter){\r\n    sourceBuilder().terminateAfter(terminateAfter);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.NodeJoinController.failContextIfNeeded",
	"Comment": "utility method to fail the given election context under the cluster state thread",
	"Method": "void failContextIfNeeded(ElectionContext context,String reason){\r\n    if (electionContext == context) {\r\n        stopElectionContext(reason);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.french.process.FrenchLexer.yytext",
	"Comment": "returns the text matched by the current regular expression.",
	"Method": "String yytext(){\r\n    return new String(zzBuffer, zzStartRead, zzMarkedPos - zzStartRead);\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.spanish.pipeline.MultiWordPreprocessor.getContainingPhrase",
	"Comment": "get a string representation of the immediate phrase which contains the given node.",
	"Method": "String getContainingPhrase(Tree t,Tree parent){\r\n    if (parent == null)\r\n        return null;\r\n    List<Label> phraseYield = parent.yield();\r\n    StringBuilder containingPhrase = new StringBuilder();\r\n    for (Label l : phraseYield) containingPhrase.append(l.value()).append(\" \");\r\n    return containingPhrase.toString().substring(0, containingPhrase.length() - 1);\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.DocumentParser.createUpdate",
	"Comment": "build an update for the parent which will contain the given mapper and any intermediate fields.",
	"Method": "ObjectMapper createUpdate(ObjectMapper parent,String[] nameParts,int i,Mapper mapper){\r\n    List<ObjectMapper> parentMappers = new ArrayList();\r\n    ObjectMapper previousIntermediate = parent;\r\n    for (; i < nameParts.length - 1; ++i) {\r\n        Mapper intermediate = previousIntermediate.getMapper(nameParts[i]);\r\n        assert intermediate != null : \"Field \" + previousIntermediate.name() + \" does not have a subfield \" + nameParts[i];\r\n        assert intermediate instanceof ObjectMapper;\r\n        parentMappers.add((ObjectMapper) intermediate);\r\n        previousIntermediate = (ObjectMapper) intermediate;\r\n    }\r\n    if (parentMappers.isEmpty() == false) {\r\n        addToLastMapper(parentMappers, mapper, false);\r\n        popMappers(parentMappers, 1, false);\r\n        mapper = parentMappers.get(0);\r\n    }\r\n    return parent.mappingUpdate(mapper);\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.Security.configure",
	"Comment": "initializes securitymanager for the environmentcan only happen once!",
	"Method": "void configure(Environment environment,boolean filterBadDefaults){\r\n    Map<String, URL> codebases = getCodebaseJarMap(JarHell.parseClassPath());\r\n    Policy.setPolicy(new ESPolicy(codebases, createPermissions(environment), getPluginPermissions(environment), filterBadDefaults));\r\n    final String[] classesThatCanExit = new String[] { ElasticsearchUncaughtExceptionHandler.PrivilegedHaltAction.class.getName().replace(\"$\", \"\\\\$\"), Command.class.getName() };\r\n    System.setSecurityManager(new SecureSM(classesThatCanExit));\r\n    selfTest();\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.spanish.pipeline.MultiWordPreprocessor.updateTagger",
	"Comment": "source training data for a unigram tagger from the given tree.",
	"Method": "void updateTagger(TwoDimensionalCounter<String, String> tagger,Tree t){\r\n    List<CoreLabel> yield = t.taggedLabeledYield();\r\n    for (CoreLabel cl : yield) {\r\n        if (cl.tag().equals(SpanishTreeNormalizer.MW_TAG))\r\n            continue;\r\n        tagger.incrementCount(cl.word(), cl.tag());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.fieldcaps.FieldCapabilities.nonAggregatableIndices",
	"Comment": "the list of indices where this field is not aggregatable,or null if the field is aggregatable in all indices.",
	"Method": "String[] nonAggregatableIndices(){\r\n    return nonAggregatableIndices;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.UpdateByQueryRequest.setIndicesOptions",
	"Comment": "set the indicesoptions for controlling unavailable indices",
	"Method": "UpdateByQueryRequest setIndicesOptions(IndicesOptions indicesOptions){\r\n    getSearchRequest().indicesOptions(indicesOptions);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequest.includeAliases",
	"Comment": "returns true if aliases should be restored from this snapshot",
	"Method": "RestoreSnapshotRequest includeAliases(boolean includeAliases,boolean includeAliases){\r\n    return includeAliases;\r\n}"
}, {
	"Path": "org.elasticsearch.common.logging.LogConfigurator.configureWithoutConfig",
	"Comment": "configure logging without reading a log4j2.properties file, effectively configuring thestatus logger and all loggers to the console.",
	"Method": "void configureWithoutConfig(Settings settings){\r\n    Objects.requireNonNull(settings);\r\n    configureStatusLogger();\r\n    configureLoggerLevels(settings);\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.isDeprecated",
	"Comment": "returns true if this setting is deprecated, otherwise false",
	"Method": "boolean isDeprecated(){\r\n    return properties.contains(Property.Deprecated);\r\n}"
}, {
	"Path": "org.elasticsearch.common.regex.Regex.simpleMatchToAutomaton",
	"Comment": "return an automaton that matches the union of the provided patterns.",
	"Method": "Automaton simpleMatchToAutomaton(String pattern,Automaton simpleMatchToAutomaton,String patterns){\r\n    if (patterns.length < 1) {\r\n        throw new IllegalArgumentException(\"There must be at least one pattern, zero given\");\r\n    }\r\n    List<Automaton> automata = new ArrayList();\r\n    for (String pattern : patterns) {\r\n        automata.add(simpleMatchToAutomaton(pattern));\r\n    }\r\n    return Operations.union(automata);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.TokenSequencePattern.getMatcher",
	"Comment": "returns a tokensequencematcher that can be used to match this patternagainst the specified list of tokens.",
	"Method": "TokenSequenceMatcher getMatcher(List<? extends CoreMap> tokens){\r\n    return new TokenSequenceMatcher(this, tokens);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.IndexRoutingTable.allPrimaryShardsActive",
	"Comment": "returns true if all shards are primary and active. otherwise false.",
	"Method": "boolean allPrimaryShardsActive(){\r\n    return primaryShardsActive() == shards().size();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterState.wasReadFromDiff",
	"Comment": "used for testing and logging to determine how this cluster state was send over the wire",
	"Method": "boolean wasReadFromDiff(){\r\n    return wasReadFromDiff;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.ui.ParserPanel.startProgressMonitor",
	"Comment": "initializes the progress bar with the status text, and the expectednumber of seconds the process will take, and starts the timer.",
	"Method": "void startProgressMonitor(String text,int maxCount){\r\n    if (glassPane == null) {\r\n        if (getRootPane() != null) {\r\n            glassPane = getRootPane().getGlassPane();\r\n            glassPane.addMouseListener(new MouseAdapter() {\r\n                @Override\r\n                public void mouseClicked(MouseEvent evt) {\r\n                    Toolkit.getDefaultToolkit().beep();\r\n                }\r\n            });\r\n        }\r\n    }\r\n    if (glassPane != null) {\r\n        glassPane.setVisible(true);\r\n    }\r\n    statusLabel.setText(text);\r\n    progressBar.setMaximum(maxCount);\r\n    progressBar.setValue(0);\r\n    count = 0;\r\n    timer.start();\r\n    progressBar.setVisible(true);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.ui.ParserPanel.startProgressMonitor",
	"Comment": "initializes the progress bar with the status text, and the expectednumber of seconds the process will take, and starts the timer.",
	"Method": "void startProgressMonitor(String text,int maxCount){\r\n    Toolkit.getDefaultToolkit().beep();\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.isDynamic",
	"Comment": "returns true if this setting is dynamically updateable, otherwise false",
	"Method": "boolean isDynamic(){\r\n    return properties.contains(Property.Dynamic);\r\n}"
}, {
	"Path": "org.elasticsearch.index.fielddata.ScriptDocValues.add",
	"Comment": "throw meaningful exceptions if someone tries to modify the scriptdocvalues.",
	"Method": "void add(int index,T element){\r\n    throw new UnsupportedOperationException(\"doc values are unmodifiable\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.nndep.DependencyParser.predict",
	"Comment": "determine the dependency parse of the given sentence using the loaded model.you must first load a parser before calling this method.",
	"Method": "GrammaticalStructure predict(CoreMap sentence,GrammaticalStructure predict,List<? extends HasWord> sentence){\r\n    CoreLabel sentenceLabel = new CoreLabel();\r\n    List<CoreLabel> tokens = new ArrayList();\r\n    int i = 1;\r\n    for (HasWord wd : sentence) {\r\n        CoreLabel label;\r\n        if (wd instanceof CoreLabel) {\r\n            label = (CoreLabel) wd;\r\n            if (label.tag() == null)\r\n                throw new IllegalArgumentException(\"Parser requires words \" + \"with part-of-speech tag annotations\");\r\n        } else {\r\n            label = new CoreLabel();\r\n            label.setValue(wd.word());\r\n            label.setWord(wd.word());\r\n            if (!(wd instanceof HasTag))\r\n                throw new IllegalArgumentException(\"Parser requires words \" + \"with part-of-speech tag annotations\");\r\n            label.setTag(((HasTag) wd).tag());\r\n        }\r\n        label.setIndex(i);\r\n        i++;\r\n        tokens.add(label);\r\n    }\r\n    sentenceLabel.set(CoreAnnotations.TokensAnnotation.class, tokens);\r\n    return predict(sentenceLabel);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.block.ClusterBlock.disableStatePersistence",
	"Comment": "should global state persistence be disabled when this block is present. note,only relevant for global blocks.",
	"Method": "boolean disableStatePersistence(){\r\n    return this.disableStatePersistence;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.SingleObjectCache.getOrRefresh",
	"Comment": "returns the currently cached object and potentially refreshes the cache before returning.",
	"Method": "T getOrRefresh(){\r\n    if (needsRefresh()) {\r\n        if (refreshLock.tryLock()) {\r\n            try {\r\n                if (needsRefresh()) {\r\n                    cached = refresh();\r\n                    assert cached != null;\r\n                    lastRefreshTimestamp = System.currentTimeMillis();\r\n                }\r\n            } finally {\r\n                refreshLock.unlock();\r\n            }\r\n        }\r\n    }\r\n    assert cached != null;\r\n    return cached;\r\n}"
}, {
	"Path": "java.io.DataOutputStream.flush",
	"Comment": "flushes this stream to ensure all pending data is sent out to the targetstream. this implementation then also flushes the target stream.",
	"Method": "void flush(){\r\n    super.flush();\r\n}"
}, {
	"Path": "org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder.getNumMultiValuesDocs",
	"Comment": "returns the number distinct of document ids associated with two or more values.",
	"Method": "int getNumMultiValuesDocs(){\r\n    return numMultiValuedDocs;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MatchQueryBuilder.lenient",
	"Comment": "gets leniency setting that controls if format based failures will be ignored.",
	"Method": "MatchQueryBuilder lenient(boolean lenient,boolean lenient){\r\n    return this.lenient;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.upgrade.post.UpgradeResponse.versions",
	"Comment": "returns the highest upgrade version of the node that performed metadata upgrade and thethe version of the oldest lucene segment for each index that was upgraded.",
	"Method": "Map<String, Tuple<Version, String>> versions(){\r\n    return versions;\r\n}"
}, {
	"Path": "org.elasticsearch.common.collect.CopyOnWriteHashMap.copyAndRemove",
	"Comment": "remove the given key from this map. the current hash table is not modified.",
	"Method": "CopyOnWriteHashMap<K, V> copyAndRemove(Object key){\r\n    if (key == null) {\r\n        throw new IllegalArgumentException(\"null keys are not supported\");\r\n    }\r\n    final int hash = key.hashCode();\r\n    final InnerNode<K, V> newRoot = root.remove(key, hash);\r\n    if (root == newRoot) {\r\n        return this;\r\n    } else {\r\n        return new CopyOnWriteHashMap(newRoot, size - 1);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexModule.getEngineFactory",
	"Comment": "the engine factory provided during construction of this index module.",
	"Method": "EngineFactory getEngineFactory(){\r\n    return engineFactory;\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkRequest.add",
	"Comment": "adds a list of requests to be executed. either index or delete requests.",
	"Method": "BulkRequest add(DocWriteRequest<?> requests,BulkRequest add,DocWriteRequest<?> request,BulkRequest add,DocWriteRequest<?> request,Object payload,BulkRequest add,Iterable<DocWriteRequest<?>> requests,BulkRequest add,IndexRequest request,BulkRequest add,IndexRequest request,Object payload,BulkRequest add,UpdateRequest request,BulkRequest add,UpdateRequest request,Object payload,BulkRequest add,DeleteRequest request,BulkRequest add,DeleteRequest request,Object payload,BulkRequest add,byte[] data,int from,int length,XContentType xContentType,BulkRequest add,byte[] data,int from,int length,String defaultIndex,String defaultType,XContentType xContentType,BulkRequest add,BytesReference data,String defaultIndex,String defaultType,XContentType xContentType,BulkRequest add,BytesReference data,String defaultIndex,String defaultType,boolean allowExplicitIndex,XContentType xContentType,BulkRequest add,BytesReference data,String defaultIndex,String defaultType,String defaultRouting,FetchSourceContext defaultFetchSourceContext,String defaultPipeline,Object payload,boolean allowExplicitIndex,XContentType xContentType){\r\n    XContent xContent = xContentType.xContent();\r\n    int line = 0;\r\n    int from = 0;\r\n    int length = data.length();\r\n    byte marker = xContent.streamSeparator();\r\n    while (true) {\r\n        int nextMarker = findNextMarker(marker, from, data, length);\r\n        if (nextMarker == -1) {\r\n            break;\r\n        }\r\n        line++;\r\n        try (InputStream stream = data.slice(from, nextMarker - from).streamInput();\r\n            XContentParser parser = xContent.createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, stream)) {\r\n            from = nextMarker + 1;\r\n            XContentParser.Token token = parser.nextToken();\r\n            if (token == null) {\r\n                continue;\r\n            }\r\n            if (token != XContentParser.Token.START_OBJECT) {\r\n                throw new IllegalArgumentException(\"Malformed action/metadata line [\" + line + \"], expected \" + XContentParser.Token.START_OBJECT + \" but found [\" + token + \"]\");\r\n            }\r\n            token = parser.nextToken();\r\n            if (token != XContentParser.Token.FIELD_NAME) {\r\n                throw new IllegalArgumentException(\"Malformed action/metadata line [\" + line + \"], expected \" + XContentParser.Token.FIELD_NAME + \" but found [\" + token + \"]\");\r\n            }\r\n            String action = parser.currentName();\r\n            String index = defaultIndex;\r\n            String type = defaultType;\r\n            String id = null;\r\n            String routing = valueOrDefault(defaultRouting, globalRouting);\r\n            FetchSourceContext fetchSourceContext = defaultFetchSourceContext;\r\n            String opType = null;\r\n            long version = Versions.MATCH_ANY;\r\n            VersionType versionType = VersionType.INTERNAL;\r\n            int retryOnConflict = 0;\r\n            String pipeline = valueOrDefault(defaultPipeline, globalPipeline);\r\n            token = parser.nextToken();\r\n            if (token == XContentParser.Token.START_OBJECT) {\r\n                String currentFieldName = null;\r\n                while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\r\n                    if (token == XContentParser.Token.FIELD_NAME) {\r\n                        currentFieldName = parser.currentName();\r\n                    } else if (token.isValue()) {\r\n                        if (INDEX.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                            if (!allowExplicitIndex) {\r\n                                throw new IllegalArgumentException(\"explicit index in bulk is not allowed\");\r\n                            }\r\n                            index = parser.text();\r\n                        } else if (TYPE.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                            type = parser.text();\r\n                        } else if (ID.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                            id = parser.text();\r\n                        } else if (ROUTING.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                            routing = parser.text();\r\n                        } else if (OP_TYPE.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                            opType = parser.text();\r\n                        } else if (VERSION.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                            version = parser.longValue();\r\n                        } else if (VERSION_TYPE.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                            versionType = VersionType.fromString(parser.text());\r\n                        } else if (RETRY_ON_CONFLICT.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                            retryOnConflict = parser.intValue();\r\n                        } else if (PIPELINE.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                            pipeline = parser.text();\r\n                        } else if (SOURCE.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                            fetchSourceContext = FetchSourceContext.fromXContent(parser);\r\n                        } else {\r\n                            throw new IllegalArgumentException(\"Action/metadata line [\" + line + \"] contains an unknown parameter [\" + currentFieldName + \"]\");\r\n                        }\r\n                    } else if (token == XContentParser.Token.START_ARRAY) {\r\n                        throw new IllegalArgumentException(\"Malformed action/metadata line [\" + line + \"], expected a simple value for field [\" + currentFieldName + \"] but found [\" + token + \"]\");\r\n                    } else if (token == XContentParser.Token.START_OBJECT && SOURCE.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                        fetchSourceContext = FetchSourceContext.fromXContent(parser);\r\n                    } else if (token != XContentParser.Token.VALUE_NULL) {\r\n                        throw new IllegalArgumentException(\"Malformed action/metadata line [\" + line + \"], expected a simple value for field [\" + currentFieldName + \"] but found [\" + token + \"]\");\r\n                    }\r\n                }\r\n            } else if (token != XContentParser.Token.END_OBJECT) {\r\n                throw new IllegalArgumentException(\"Malformed action/metadata line [\" + line + \"], expected \" + XContentParser.Token.START_OBJECT + \" or \" + XContentParser.Token.END_OBJECT + \" but found [\" + token + \"]\");\r\n            }\r\n            if (\"delete\".equals(action)) {\r\n                add(new DeleteRequest(index, type, id).routing(routing).version(version).versionType(versionType), payload);\r\n            } else {\r\n                nextMarker = findNextMarker(marker, from, data, length);\r\n                if (nextMarker == -1) {\r\n                    break;\r\n                }\r\n                line++;\r\n                if (\"index\".equals(action)) {\r\n                    if (opType == null) {\r\n                        internalAdd(new IndexRequest(index, type, id).routing(routing).version(version).versionType(versionType).setPipeline(pipeline).source(sliceTrimmingCarriageReturn(data, from, nextMarker, xContentType), xContentType), payload);\r\n                    } else {\r\n                        internalAdd(new IndexRequest(index, type, id).routing(routing).version(version).versionType(versionType).create(\"create\".equals(opType)).setPipeline(pipeline).source(sliceTrimmingCarriageReturn(data, from, nextMarker, xContentType), xContentType), payload);\r\n                    }\r\n                } else if (\"create\".equals(action)) {\r\n                    internalAdd(new IndexRequest(index, type, id).routing(routing).version(version).versionType(versionType).create(true).setPipeline(pipeline).source(sliceTrimmingCarriageReturn(data, from, nextMarker, xContentType), xContentType), payload);\r\n                } else if (\"update\".equals(action)) {\r\n                    UpdateRequest updateRequest = new UpdateRequest(index, type, id).routing(routing).retryOnConflict(retryOnConflict).version(version).versionType(versionType).routing(routing);\r\n                    try (InputStream dataStream = sliceTrimmingCarriageReturn(data, from, nextMarker, xContentType).streamInput();\r\n                        XContentParser sliceParser = xContent.createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, dataStream)) {\r\n                        updateRequest.fromXContent(sliceParser);\r\n                    }\r\n                    if (fetchSourceContext != null) {\r\n                        updateRequest.fetchSource(fetchSourceContext);\r\n                    }\r\n                    IndexRequest upsertRequest = updateRequest.upsertRequest();\r\n                    if (upsertRequest != null) {\r\n                        upsertRequest.version(version);\r\n                        upsertRequest.versionType(versionType);\r\n                        upsertRequest.setPipeline(defaultPipeline);\r\n                    }\r\n                    IndexRequest doc = updateRequest.doc();\r\n                    if (doc != null) {\r\n                        doc.version(version);\r\n                        doc.versionType(versionType);\r\n                    }\r\n                    internalAdd(updateRequest, payload);\r\n                }\r\n                from = nextMarker + 1;\r\n            }\r\n        }\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.Cidrs.cidrMaskToMinMax",
	"Comment": "parses an ipv4 address block in cidr notation into a pair oflongs representing the bottom and top of the address block",
	"Method": "long[] cidrMaskToMinMax(String cidr){\r\n    Objects.requireNonNull(cidr, \"cidr\");\r\n    String[] fields = cidr.split(\"/\");\r\n    if (fields.length != 2) {\r\n        throw new IllegalArgumentException(String.format(Locale.ROOT, \"invalid IPv4/CIDR; expected [a.b.c.d, e] but was [%s] after splitting on \\\"/\\\" in [%s]\", Arrays.toString(fields), cidr));\r\n    }\r\n    if (fields[0].contains(\":\")) {\r\n        throw new IllegalArgumentException(String.format(Locale.ROOT, \"invalid IPv4/CIDR; expected [a.b.c.d, e] where a, b, c, d are decimal octets \" + \"but was [%s] after splitting on \\\"/\\\" in [%s]\", Arrays.toString(fields), cidr));\r\n    }\r\n    byte[] addressBytes;\r\n    try {\r\n        addressBytes = InetAddresses.forString(fields[0]).getAddress();\r\n    } catch (Exception e) {\r\n        throw new IllegalArgumentException(String.format(Locale.ROOT, \"invalid IPv4/CIDR; unable to parse [%s] as an IP address literal\", fields[0]), e);\r\n    }\r\n    long accumulator = ((addressBytes[0] & 0xFFL) << 24) + ((addressBytes[1] & 0xFFL) << 16) + ((addressBytes[2] & 0xFFL) << 8) + ((addressBytes[3] & 0xFFL));\r\n    int networkMask;\r\n    try {\r\n        networkMask = Integer.parseInt(fields[1]);\r\n    } catch (NumberFormatException e) {\r\n        throw new IllegalArgumentException(String.format(Locale.ROOT, \"invalid IPv4/CIDR; invalid network mask [%s] in [%s]\", fields[1], cidr), e);\r\n    }\r\n    if (networkMask < 0 || networkMask > 32) {\r\n        throw new IllegalArgumentException(String.format(Locale.ROOT, \"invalid IPv4/CIDR; invalid network mask [%s], out of range in [%s]\", fields[1], cidr));\r\n    }\r\n    long blockSize = 1L << (32 - networkMask);\r\n    if ((accumulator & (blockSize - 1)) != 0) {\r\n        throw new IllegalArgumentException(String.format(Locale.ROOT, \"invalid IPv4/CIDR; invalid address/network mask combination in [%s]; perhaps [%s] was intended?\", cidr, octetsToCIDR(longToOctets(accumulator - (accumulator & (blockSize - 1))), networkMask)));\r\n    }\r\n    return new long[] { accumulator, accumulator + blockSize };\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.resolveSearchRouting",
	"Comment": "resolves the search routing if in the expression aliases are used. if expressions point to concrete indicesor aliases with no routing defined the specified routing is used.",
	"Method": "Map<String, Set<String>> resolveSearchRouting(ClusterState state,String routing,String expressions){\r\n    List<String> resolvedExpressions = expressions != null ? Arrays.asList(expressions) : Collections.<String>emptyList();\r\n    Context context = new Context(state, IndicesOptions.lenientExpandOpen());\r\n    for (ExpressionResolver expressionResolver : expressionResolvers) {\r\n        resolvedExpressions = expressionResolver.resolve(context, resolvedExpressions);\r\n    }\r\n    if (isAllIndices(resolvedExpressions)) {\r\n        return resolveSearchRoutingAllIndices(state.metaData(), routing);\r\n    }\r\n    Map<String, Set<String>> routings = null;\r\n    Set<String> paramRouting = null;\r\n    Set<String> norouting = new HashSet();\r\n    if (routing != null) {\r\n        paramRouting = Sets.newHashSet(Strings.splitStringByCommaToArray(routing));\r\n    }\r\n    for (String expression : resolvedExpressions) {\r\n        AliasOrIndex aliasOrIndex = state.metaData().getAliasAndIndexLookup().get(expression);\r\n        if (aliasOrIndex != null && aliasOrIndex.isAlias()) {\r\n            AliasOrIndex.Alias alias = (AliasOrIndex.Alias) aliasOrIndex;\r\n            for (Tuple<String, AliasMetaData> item : alias.getConcreteIndexAndAliasMetaDatas()) {\r\n                String concreteIndex = item.v1();\r\n                AliasMetaData aliasMetaData = item.v2();\r\n                if (!norouting.contains(concreteIndex)) {\r\n                    if (!aliasMetaData.searchRoutingValues().isEmpty()) {\r\n                        if (routings == null) {\r\n                            routings = new HashMap();\r\n                        }\r\n                        Set<String> r = routings.get(concreteIndex);\r\n                        if (r == null) {\r\n                            r = new HashSet();\r\n                            routings.put(concreteIndex, r);\r\n                        }\r\n                        r.addAll(aliasMetaData.searchRoutingValues());\r\n                        if (paramRouting != null) {\r\n                            r.retainAll(paramRouting);\r\n                        }\r\n                        if (r.isEmpty()) {\r\n                            routings.remove(concreteIndex);\r\n                        }\r\n                    } else {\r\n                        if (!norouting.contains(concreteIndex)) {\r\n                            norouting.add(concreteIndex);\r\n                            if (paramRouting != null) {\r\n                                Set<String> r = new HashSet(paramRouting);\r\n                                if (routings == null) {\r\n                                    routings = new HashMap();\r\n                                }\r\n                                routings.put(concreteIndex, r);\r\n                            } else {\r\n                                if (routings != null) {\r\n                                    routings.remove(concreteIndex);\r\n                                }\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        } else {\r\n            if (!norouting.contains(expression)) {\r\n                norouting.add(expression);\r\n                if (paramRouting != null) {\r\n                    Set<String> r = new HashSet(paramRouting);\r\n                    if (routings == null) {\r\n                        routings = new HashMap();\r\n                    }\r\n                    routings.put(expression, r);\r\n                } else {\r\n                    if (routings != null) {\r\n                        routings.remove(expression);\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if (routings == null || routings.isEmpty()) {\r\n        return null;\r\n    }\r\n    return routings;\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.getLocalCheckpointTracker",
	"Comment": "used only for testing! package private to prevent anyone else from using it",
	"Method": "LocalCheckpointTracker getLocalCheckpointTracker(){\r\n    return localCheckpointTracker;\r\n}"
}, {
	"Path": "org.elasticsearch.common.io.FileSystemUtils.files",
	"Comment": "returns an array of all files in the given directory matching the glob.",
	"Method": "Path[] files(Path from,DirectoryStream.Filter<Path> filter,Path[] files,Path directory,Path[] files,Path directory,String glob){\r\n    try (DirectoryStream<Path> stream = Files.newDirectoryStream(directory, glob)) {\r\n        return toArray(stream);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.patterns.GetPatternsFromDataMultiClass.run",
	"Comment": "execute the system give a properties file or object. returns the model created",
	"Method": "GetPatternsFromDataMultiClass<E> run(Properties props){\r\n    Map<String, Set<CandidatePhrase>> seedWords = readSeedWords(props);\r\n    Map<String, Class> answerClasses = new HashMap();\r\n    String ansClasses = props.getProperty(\"answerClasses\");\r\n    if (ansClasses != null) {\r\n        for (String l : ansClasses.split(\";\")) {\r\n            String[] t = l.split(\",\");\r\n            String label = t[0];\r\n            String cl = t[1];\r\n            Class answerClass = ClassLoader.getSystemClassLoader().loadClass(cl);\r\n            answerClasses.put(label, answerClass);\r\n        }\r\n    }\r\n    Pair<Map<String, DataInstance>, Map<String, DataInstance>> sentsPair = processSents(props, seedWords.keySet());\r\n    boolean labelUsingSeedSets = Boolean.parseBoolean(props.getProperty(\"labelUsingSeedSets\", \"true\"));\r\n    GetPatternsFromDataMultiClass<E> model = new GetPatternsFromDataMultiClass(props, sentsPair.first(), seedWords, labelUsingSeedSets);\r\n    return runNineYards(model, props, sentsPair.second());\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.set.Sets.sortedDifference",
	"Comment": "the relative complement, or difference, of the specified left and right set, returned as a sorted set. namely, the resulting setcontains all the elements that are in the left set but not in the right set, and the set is sorted using the natural ordering ofelement type. neither input is mutated by this operation, an entirely new set is returned.",
	"Method": "SortedSet<T> sortedDifference(Set<T> left,Set<T> right){\r\n    Objects.requireNonNull(left);\r\n    Objects.requireNonNull(right);\r\n    return left.stream().filter(k -> !right.contains(k)).collect(new SortedSetCollector());\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.ConfigurationException.withPartialValue",
	"Comment": "returns a copy of this configuration exception with the specified partial value.",
	"Method": "ConfigurationException withPartialValue(Object partialValue){\r\n    if (this.partialValue != null) {\r\n        String message = String.format(Locale.ROOT, \"Can't clobber existing partial value %s with %s\", this.partialValue, partialValue);\r\n        throw new IllegalStateException(message);\r\n    }\r\n    ConfigurationException result = new ConfigurationException(messages);\r\n    result.partialValue = partialValue;\r\n    return result;\r\n}"
}, {
	"Path": "org.openqa.selenium.remote.server.log.PerSessionLogHandler.getLoggedSessions",
	"Comment": "returns a list of session ids for which there are logs.the type of logs that are available depends on the log types providedby the driver. an included session id will at least have server logs.",
	"Method": "List<SessionId> getLoggedSessions(){\r\n    ImmutableList.Builder<SessionId> builder = new ImmutableList.Builder();\r\n    builder.addAll(perSessionDriverEntries.keySet());\r\n    return builder.build();\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.readerFromStdin",
	"Comment": "open a bufferedreader on stdin. use the specified character encoding.",
	"Method": "BufferedReader readerFromStdin(BufferedReader readerFromStdin,String encoding){\r\n    if (encoding == null) {\r\n        return new BufferedReader(new InputStreamReader(System.in));\r\n    }\r\n    return new BufferedReader(new InputStreamReader(System.in, encoding));\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateTaskExecutor.runOnlyOnMaster",
	"Comment": "indicates whether this executor should only run if the current node is master",
	"Method": "boolean runOnlyOnMaster(){\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoryData.resolveIndexId",
	"Comment": "resolve the index name to the index id specific to the repository,throwing an exception if the index could not be resolved.",
	"Method": "IndexId resolveIndexId(String indexName){\r\n    if (indices.containsKey(indexName)) {\r\n        return indices.get(indexName);\r\n    } else {\r\n        return new IndexId(indexName, indexName);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.settings.put.UpdateSettingsRequest.setPreserveExisting",
	"Comment": "iff set to true this settings update will only add settings not already set on an index. existing settings remainunchanged.",
	"Method": "UpdateSettingsRequest setPreserveExisting(boolean preserveExisting){\r\n    this.preserveExisting = preserveExisting;\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.spanish.pipeline.MultiWordTreeExpander.expandPhrases",
	"Comment": "recognize candidate patterns for expansion in the given tree andperform the expansions. see the class documentation for moreinformation.",
	"Method": "Tree expandPhrases(Tree t,TreeNormalizer tn,TreeFactory tf){\r\n    Tree oldTree;\r\n    do {\r\n        oldTree = t.deepCopy();\r\n        t = Tsurgeon.processPatternsOnTree(firstStepExpansions, t);\r\n    } while (!t.equals(oldTree));\r\n    t = Tsurgeon.processPatternsOnTree(intermediateExpansions, t);\r\n    t = tn.normalizeWholeTree(t, tf);\r\n    t = Tsurgeon.processPatternsOnTree(finalCleanup, t);\r\n    return t;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.hasNodeScope",
	"Comment": "returns true if this setting has a node scope, otherwise false",
	"Method": "boolean hasNodeScope(){\r\n    return properties.contains(Property.NodeScope);\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.IngestDocument.getFieldValue",
	"Comment": "returns the value contained in the document with the provided templated path",
	"Method": "T getFieldValue(String path,Class<T> clazz,T getFieldValue,String path,Class<T> clazz,boolean ignoreMissing,T getFieldValue,TemplateScript.Factory pathTemplate,Class<T> clazz){\r\n    return getFieldValue(renderTemplate(pathTemplate), clazz);\r\n}"
}, {
	"Path": "org.apache.harmony.tests.java.util.LinkedHashMapTest.tearDown",
	"Comment": "tears down the fixture, for example, close a network connection. thismethod is called after a test is executed.",
	"Method": "void tearDown(){\r\n    objArray = null;\r\n    objArray2 = null;\r\n    hm = null;\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.TransportShardBulkAction.executeIndexRequestOnPrimary",
	"Comment": "executes index operation on primary shard after updates mapping if dynamic mappings are found",
	"Method": "void executeIndexRequestOnPrimary(BulkPrimaryExecutionContext context,MappingUpdatePerformer mappingUpdater){\r\n    final IndexRequest request = context.getRequestToExecute();\r\n    final IndexShard primary = context.getPrimary();\r\n    final SourceToParse sourceToParse = SourceToParse.source(request.index(), request.type(), request.id(), request.source(), request.getContentType()).routing(request.routing());\r\n    executeOnPrimaryWhileHandlingMappingUpdates(context, () -> primary.applyIndexOperationOnPrimary(request.version(), request.versionType(), sourceToParse, request.getAutoGeneratedTimestamp(), request.isRetry()), e -> primary.getFailedIndexResult(e, request.version()), context::markOperationAsExecuted, mapping -> mappingUpdater.updateMappings(mapping, primary.shardId(), request.type()));\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.shards.IndicesShardStoresRequest.indicesOptions",
	"Comment": "specifies what type of requested indices to ignore and wildcard indices expressionsby default, expands wildcards to both open and closed indices",
	"Method": "IndicesShardStoresRequest indicesOptions(IndicesOptions indicesOptions,IndicesOptions indicesOptions){\r\n    return indicesOptions;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.SettingsFilter.isValidPattern",
	"Comment": "returns true iff the given string is either a valid settings key pattern or a simple regular expression",
	"Method": "boolean isValidPattern(String pattern){\r\n    return AbstractScopedSettings.isValidKey(pattern) || Regex.isSimpleMatchPattern(pattern);\r\n}"
}, {
	"Path": "org.apache.dubbo.metrics.MetricName.tag",
	"Comment": "add tags to a metric name and return the newly created metricname.",
	"Method": "MetricName tag(Map<String, String> add,MetricName tag,String pairs){\r\n    if (pairs == null) {\r\n        return this;\r\n    }\r\n    if (pairs.length % 2 != 0) {\r\n        throw new IllegalArgumentException(\"Argument count must be even\");\r\n    }\r\n    final Map<String, String> add = new HashMap<String, String>();\r\n    for (int i = 0; i < pairs.length; i += 2) {\r\n        add.put(pairs[i], pairs[i + 1]);\r\n    }\r\n    return tag(add);\r\n}"
}, {
	"Path": "org.elasticsearch.index.snapshots.blobstore.SnapshotFiles.findPhysicalIndexFile",
	"Comment": "returns information about a physical file with the given name",
	"Method": "FileInfo findPhysicalIndexFile(String physicalName){\r\n    if (physicalFiles == null) {\r\n        Map<String, FileInfo> files = new HashMap();\r\n        for (FileInfo fileInfo : indexFiles) {\r\n            files.put(fileInfo.physicalName(), fileInfo);\r\n        }\r\n        this.physicalFiles = files;\r\n    }\r\n    return physicalFiles.get(physicalName);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.FrenchUnknownWordModel.getSignatureIndex",
	"Comment": "returns the index of the signature of the word numbered wordindex, wherethe signature is the string representation of unknown word features.",
	"Method": "int getSignatureIndex(int index,int sentencePosition,String word){\r\n    String uwSig = getSignature(word, sentencePosition);\r\n    int sig = wordIndex.addToIndex(uwSig);\r\n    return sig;\r\n}"
}, {
	"Path": "java.io.EmulatedFieldsForLoading.get",
	"Comment": "find and return the boolean value of a given field namedname in the receiver. if the field has not been assignedany value yet, the default value defaultvalue is returnedinstead.",
	"Method": "byte get(String name,byte defaultValue,char get,String name,char defaultValue,double get,String name,double defaultValue,float get,String name,float defaultValue,int get,String name,int defaultValue,long get,String name,long defaultValue,Object get,String name,Object defaultValue,short get,String name,short defaultValue,boolean get,String name,boolean defaultValue){\r\n    return emulatedFields.get(name, defaultValue);\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksExecutor.createTask",
	"Comment": "creates a allocatedpersistenttask for communicating with task manager",
	"Method": "AllocatedPersistentTask createTask(long id,String type,String action,TaskId parentTaskId,PersistentTask<Params> taskInProgress,Map<String, String> headers){\r\n    return new AllocatedPersistentTask(id, type, action, getDescription(taskInProgress), parentTaskId, headers);\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.spi.Dependency.getKey",
	"Comment": "returns the key to the binding that satisfies this dependency.",
	"Method": "Key<T> getKey(){\r\n    return this.key;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.utils.configuration.json.GridJsonConfiguration.getWithoutServlets",
	"Comment": "default servlets to exclude on the hub or node. default empty.",
	"Method": "List<String> getWithoutServlets(){\r\n    return withoutServlets;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.AllocationId.newRelocation",
	"Comment": "creates a new allocation id for a shard that moves to be relocated, populatingthe transient holder for relocationid.",
	"Method": "AllocationId newRelocation(AllocationId allocationId){\r\n    assert allocationId.getRelocationId() == null;\r\n    return new AllocationId(allocationId.getId(), UUIDs.randomBase64UUID());\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.SloppyMath.pythonMod",
	"Comment": "returns a mod where the sign of the answer is the same as the sign of the second argument.this is how languages like python do it. helpful for array accesses.",
	"Method": "int pythonMod(int num,int modulus){\r\n    return (num % modulus + modulus) % modulus;\r\n}"
}, {
	"Path": "org.elasticsearch.client.Requests.indexAliasesRequest",
	"Comment": "creates an index aliases request allowing to add and remove aliases.",
	"Method": "IndicesAliasesRequest indexAliasesRequest(){\r\n    return new IndicesAliasesRequest();\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.XMoreLikeThis.setMinDocFreq",
	"Comment": "sets the frequency at which words will be ignored which do not occur in at least thismany docs.",
	"Method": "void setMinDocFreq(int minDocFreq){\r\n    this.minDocFreq = minDocFreq;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.getSettingsDependencies",
	"Comment": "returns a set of settings that are required at validation time. unless all of the dependencies are present in the settingsobject validation of setting must fail.",
	"Method": "Set<Setting<?>> getSettingsDependencies(String key,Set<Setting<?>> getSettingsDependencies,String settingsKey){\r\n    return Collections.emptySet();\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.isQueryStringAllowLeadingWildcard",
	"Comment": "returns true if the query string parser should allow leading wildcards. the default is true",
	"Method": "boolean isQueryStringAllowLeadingWildcard(){\r\n    return queryStringAllowLeadingWildcard;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.BulkByScrollTask.isLeader",
	"Comment": "returns true if this task is a leader for other slice subtasks",
	"Method": "boolean isLeader(){\r\n    return leaderState != null;\r\n}"
}, {
	"Path": "org.openqa.grid.web.servlet.console.DefaultProxyHtmlRenderer.getPlatform",
	"Comment": "return the platform for the proxy. it should be the same for all slots of the proxy, so checking that.",
	"Method": "String getPlatform(RemoteProxy proxy,Platform getPlatform,TestSlot slot){\r\n    Object o = slot.getCapabilities().get(CapabilityType.PLATFORM);\r\n    if (o == null) {\r\n        return Platform.ANY;\r\n    }\r\n    if (o instanceof String) {\r\n        return Platform.valueOf((String) o);\r\n    } else if (o instanceof Platform) {\r\n        return (Platform) o;\r\n    } else {\r\n        throw new GridException(\"Cannot cast \" + o + \" to org.openqa.selenium.Platform\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.ClusterNameExpressionResolver.resolveClusterNames",
	"Comment": "resolves the provided cluster expression to matching cluster names. this method onlysupports exact or wildcard matches.",
	"Method": "List<String> resolveClusterNames(Set<String> remoteClusters,String clusterExpression){\r\n    if (remoteClusters.contains(clusterExpression)) {\r\n        return Collections.singletonList(clusterExpression);\r\n    } else if (Regex.isSimpleMatchPattern(clusterExpression)) {\r\n        return wildcardResolver.resolve(remoteClusters, clusterExpression);\r\n    } else {\r\n        return Collections.emptyList();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.KeyStoreWrapper.addBootstrapSeed",
	"Comment": "add the bootstrap seed setting, which may be used as a unique, secure, random value by the node",
	"Method": "void addBootstrapSeed(KeyStoreWrapper wrapper){\r\n    assert wrapper.getSettingNames().contains(SEED_SETTING.getKey()) == false;\r\n    SecureRandom random = Randomness.createSecure();\r\n    int passwordLength = 20;\r\n    char[] characters = new char[passwordLength];\r\n    for (int i = 0; i < passwordLength; ++i) {\r\n        characters[i] = SEED_CHARS[random.nextInt(SEED_CHARS.length)];\r\n    }\r\n    wrapper.setString(SEED_SETTING.getKey(), characters);\r\n    Arrays.fill(characters, (char) 0);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.CommonTermsQueryBuilder.highFreqMinimumShouldMatch",
	"Comment": "sets the minimum number of high frequent query terms that need to match in order toproduce a hit when there are no low frequent terms.",
	"Method": "CommonTermsQueryBuilder highFreqMinimumShouldMatch(String highFreqMinimumShouldMatch,String highFreqMinimumShouldMatch){\r\n    return this.highFreqMinimumShouldMatch;\r\n}"
}, {
	"Path": "android.text.TextUtils.concat",
	"Comment": "returns a charsequence concatenating the specified charsequences,retaining their spans if any.",
	"Method": "CharSequence concat(CharSequence text){\r\n    if (text.length == 0) {\r\n        return \"\";\r\n    }\r\n    if (text.length == 1) {\r\n        return text[0];\r\n    }\r\n    boolean spanned = false;\r\n    for (int i = 0; i < text.length; i++) {\r\n        if (text[i] instanceof Spanned) {\r\n            spanned = true;\r\n            break;\r\n        }\r\n    }\r\n    StringBuilder sb = new StringBuilder();\r\n    for (int i = 0; i < text.length; i++) {\r\n        sb.append(text[i]);\r\n    }\r\n    if (!spanned) {\r\n        return sb.toString();\r\n    }\r\n    SpannableString ss = new SpannableString(sb);\r\n    int off = 0;\r\n    for (int i = 0; i < text.length; i++) {\r\n        int len = text[i].length();\r\n        if (text[i] instanceof Spanned) {\r\n            copySpansFrom((Spanned) text[i], 0, len, Object.class, ss, off);\r\n        }\r\n        off += len;\r\n    }\r\n    return new SpannedString(ss);\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksNodeService.cancelTask",
	"Comment": "unregisters and then cancels the locally running task using the task manager. no notification to master will be send uponcancellation.",
	"Method": "void cancelTask(Long allocationId){\r\n    AllocatedPersistentTask task = runningTasks.remove(allocationId);\r\n    if (task.markAsCancelled()) {\r\n        String reason = \"task has been removed, cancelling locally\";\r\n        persistentTasksService.sendCancelRequest(task.getId(), reason, new ActionListener<CancelTasksResponse>() {\r\n            @Override\r\n            public void onResponse(CancelTasksResponse cancelTasksResponse) {\r\n                logger.trace(\"Persistent task [{}] with id [{}] and allocation id [{}] was cancelled\", task.getAction(), task.getPersistentTaskId(), task.getAllocationId());\r\n            }\r\n            @Override\r\n            public void onFailure(Exception e) {\r\n                logger.warn(() -> new ParameterizedMessage(\"failed to cancel task [{}] with id [{}] and allocation id [{}]\", task.getAction(), task.getPersistentTaskId(), task.getAllocationId()), e);\r\n            }\r\n        });\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksNodeService.cancelTask",
	"Comment": "unregisters and then cancels the locally running task using the task manager. no notification to master will be send uponcancellation.",
	"Method": "void cancelTask(Long allocationId){\r\n    logger.trace(\"Persistent task [{}] with id [{}] and allocation id [{}] was cancelled\", task.getAction(), task.getPersistentTaskId(), task.getAllocationId());\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksNodeService.cancelTask",
	"Comment": "unregisters and then cancels the locally running task using the task manager. no notification to master will be send uponcancellation.",
	"Method": "void cancelTask(Long allocationId){\r\n    logger.warn(() -> new ParameterizedMessage(\"failed to cancel task [{}] with id [{}] and allocation id [{}]\", task.getAction(), task.getPersistentTaskId(), task.getAllocationId()), e);\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.NaturalLogicAnnotator.validateQuantifierByHead",
	"Comment": "try to find which quantifier we matched, given that we matched the head of a quantifier at the given indexedword, and thatthis whole deal is taking place in the given sentence.",
	"Method": "Optional<Triple<Operator, Integer, Integer>> validateQuantifierByHead(CoreMap sentence,IndexedWord quantifier,boolean isUnary){\r\n    List<CoreLabel> tokens = sentence.get(CoreAnnotations.TokensAnnotation.class);\r\n    Function<CoreLabel, String> glossFn = (label) -> \"CD\".equals(label.tag()) ? \"--NUM--\" : label.lemma();\r\n    int quantIndex = quantifier.index();\r\n    int[] positiveOffsetToCheck = \"CD\".equals(tokens.get(quantIndex - 1).tag()) ? new int[] { 2, 1, 0 } : new int[] { 0 };\r\n    for (int offsetEnd : positiveOffsetToCheck) {\r\n        int end = quantIndex + offsetEnd;\r\n        for (int start = Math.max(0, quantIndex - 10); start < quantIndex; ++start) {\r\n            String gloss = StringUtils.join(tokens, \" \", glossFn, start, end).toLowerCase();\r\n            for (Operator q : Operator.valuesByLengthDesc) {\r\n                if (q.surfaceForm.equals(gloss) && (!q.isUnary() || isUnary)) {\r\n                    return Optional.of(Triple.makeTriple(q, start + 1, end + 1));\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return Optional.empty();\r\n}"
}, {
	"Path": "org.openqa.selenium.interactions.BasicMouseInterfaceTest.testDraggingElementWithMouseFiresEvents",
	"Comment": "difference is that this test also verifies the correct events were fired.",
	"Method": "void testDraggingElementWithMouseFiresEvents(){\r\n    performDragAndDropWithMouse();\r\n    WebElement dragReporter = driver.findElement(By.id(\"dragging_reports\"));\r\n    String text = dragReporter.getText();\r\n    assertThat(text).matches(\"Nothing happened. (?:DragOut *)+DropIn RightItem 3\");\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.readMinTranslogGeneration",
	"Comment": "returns the minimum translog generation retained by the translog at the given location.this ensures that the transloguuid from this translog matches with the provided transloguuid.",
	"Method": "long readMinTranslogGeneration(Path location,String expectedTranslogUUID){\r\n    final Checkpoint checkpoint = readCheckpoint(location, expectedTranslogUUID);\r\n    return checkpoint.minTranslogGeneration;\r\n}"
}, {
	"Path": "org.elasticsearch.common.bytes.BytesReference.utf8ToString",
	"Comment": "interprets the referenced bytes as utf8 bytes, returning the resulting string",
	"Method": "String utf8ToString(){\r\n    return toBytesRef().utf8ToString();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequest.partial",
	"Comment": "set to true to allow indices with failed to snapshot shards should be partially restored.",
	"Method": "boolean partial(RestoreSnapshotRequest partial,boolean partial){\r\n    this.partial = partial;\r\n    return this;\r\n}"
}, {
	"Path": "android.util.ArrayMap.append",
	"Comment": "special fast path for appending items to the end of the array without validation.the array must already be large enough to contain the item.",
	"Method": "void append(K key,V value){\r\n    int index = mSize;\r\n    final int hash = key == null ? 0 : key.hashCode();\r\n    if (index >= mHashes.length) {\r\n        throw new IllegalStateException(\"Array is full\");\r\n    }\r\n    if (index > 0 && mHashes[index - 1] > hash) {\r\n        RuntimeException e = new RuntimeException(\"here\");\r\n        e.fillInStackTrace();\r\n        Log.w(TAG, \"New hash \" + hash + \" is before end of array hash \" + mHashes[index - 1] + \" at index \" + index + \" key \" + key, e);\r\n        put(key, value);\r\n        return;\r\n    }\r\n    mSize = index + 1;\r\n    mHashes[index] = hash;\r\n    index <<= 1;\r\n    mArray[index] = key;\r\n    mArray[index + 1] = value;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.AbstractTreebankParserParams.testMemoryTreebank",
	"Comment": "you can often return the same thing for testmemorytreebank asfor memorytreebank",
	"Method": "MemoryTreebank testMemoryTreebank(){\r\n    return memoryTreebank();\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.SimpleQueryStringBuilder.quoteFieldSuffix",
	"Comment": "return the suffix to append to field names for phrase matching.",
	"Method": "SimpleQueryStringBuilder quoteFieldSuffix(String suffix,String quoteFieldSuffix){\r\n    return settings.quoteFieldSuffix();\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.ReaderIteratorFactory.remove",
	"Comment": "removes an object from the underlying collection ofinput sources.",
	"Method": "boolean remove(Object o){\r\n    return this.c.remove(o);\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.ConfigurationException.getErrorMessages",
	"Comment": "returns messages for the errors that caused this exception.",
	"Method": "Collection<Message> getErrorMessages(){\r\n    return messages;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.ParsedDocument.toTombstone",
	"Comment": "makes the processing document as a tombstone document rather than a regular document.tombstone documents are stored in lucene index to represent delete operations or noops.",
	"Method": "ParsedDocument toTombstone(){\r\n    assert docs().size() == 1 : \"Tombstone should have a single doc [\" + docs() + \"]\";\r\n    this.seqID.tombstoneField.setLongValue(1);\r\n    rootDoc().add(this.seqID.tombstoneField);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.RoutingNodes.relocateShard",
	"Comment": "relocate a shard to another node, adding the target initializingshard as well as assigning it.",
	"Method": "Tuple<ShardRouting, ShardRouting> relocateShard(ShardRouting startedShard,String nodeId,long expectedShardSize,RoutingChangesObserver changes){\r\n    ensureMutable();\r\n    relocatingShards++;\r\n    ShardRouting source = startedShard.relocate(nodeId, expectedShardSize);\r\n    ShardRouting target = source.getTargetRelocatingShard();\r\n    updateAssigned(startedShard, source);\r\n    node(target.currentNodeId()).add(target);\r\n    assignedShardsAdd(target);\r\n    addRecovery(target);\r\n    changes.relocationStarted(startedShard, target);\r\n    return Tuple.tuple(source, target);\r\n}"
}, {
	"Path": "org.openqa.grid.internal.NewSessionRequestQueue.getDesiredCapabilities",
	"Comment": "provides the desired capabilities of all the items in this queue.",
	"Method": "Iterable<DesiredCapabilities> getDesiredCapabilities(){\r\n    lock.readLock().lock();\r\n    try {\r\n        return newSessionRequests.stream().map(req -> new DesiredCapabilities(req.getRequest().getDesiredCapabilities())).collect(Collectors.toList());\r\n    } finally {\r\n        lock.readLock().unlock();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.calculateNumRoutingShards",
	"Comment": "returns a default number of routing shards based on the number of shards of the index. the default number of routing shards willallow any index to be split at least once and at most 10 times by a factor of two. the closer the number or shards gets to 1024the less default split operations are supported",
	"Method": "int calculateNumRoutingShards(int numShards,Version indexVersionCreated){\r\n    if (indexVersionCreated.onOrAfter(Version.V_7_0_0)) {\r\n        int log2MaxNumShards = 10;\r\n        int log2NumShards = 32 - Integer.numberOfLeadingZeros(numShards - 1);\r\n        int numSplits = log2MaxNumShards - log2NumShards;\r\n        numSplits = Math.max(1, numSplits);\r\n        return numShards * 1 << numSplits;\r\n    } else {\r\n        return numShards;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TruncateTranslogAction.writeEmptyCheckpoint",
	"Comment": "write a checkpoint file to the given location with the given generation",
	"Method": "void writeEmptyCheckpoint(Path filename,int translogLength,long translogGeneration,long globalCheckpoint){\r\n    Checkpoint emptyCheckpoint = Checkpoint.emptyTranslogCheckpoint(translogLength, translogGeneration, globalCheckpoint, translogGeneration);\r\n    Checkpoint.write(FileChannel::open, filename, emptyCheckpoint, StandardOpenOption.WRITE, StandardOpenOption.READ, StandardOpenOption.CREATE_NEW);\r\n    IOUtils.fsync(filename, false);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.ShardRouting.reinitializeReplicaShard",
	"Comment": "reinitializes a replica shard, giving it a fresh allocation id",
	"Method": "ShardRouting reinitializeReplicaShard(){\r\n    assert state == ShardRoutingState.INITIALIZING : this;\r\n    assert primary == false : this;\r\n    assert isRelocationTarget() == false : this;\r\n    return new ShardRouting(shardId, currentNodeId, null, primary, ShardRoutingState.INITIALIZING, recoverySource, unassignedInfo, AllocationId.newInitializing(), expectedShardSize);\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.waitForOpsToComplete",
	"Comment": "waits for all operations up to the provided sequence number to complete.",
	"Method": "void waitForOpsToComplete(long seqNo){\r\n    getEngine().waitForOpsToComplete(seqNo);\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.AsyncIOProcessor.put",
	"Comment": "adds the given item to the queue. the listener is notified once the item is processed",
	"Method": "void put(Item item,Consumer<Exception> listener){\r\n    Objects.requireNonNull(item, \"item must not be null\");\r\n    Objects.requireNonNull(listener, \"listener must not be null\");\r\n    final boolean promised = promiseSemaphore.tryAcquire();\r\n    final Tuple<Item, Consumer<Exception>> itemTuple = new Tuple(item, listener);\r\n    if (promised == false) {\r\n        try {\r\n            queue.put(new Tuple(item, listener));\r\n        } catch (InterruptedException e) {\r\n            Thread.currentThread().interrupt();\r\n            listener.accept(e);\r\n        }\r\n    }\r\n    if (promised || promiseSemaphore.tryAcquire()) {\r\n        final List<Tuple<Item, Consumer<Exception>>> candidates = new ArrayList();\r\n        try {\r\n            if (promised) {\r\n                candidates.add(itemTuple);\r\n            }\r\n            drainAndProcess(candidates);\r\n        } finally {\r\n            promiseSemaphore.release();\r\n        }\r\n        while (queue.isEmpty() == false && promiseSemaphore.tryAcquire()) {\r\n            try {\r\n                drainAndProcess(candidates);\r\n            } finally {\r\n                promiseSemaphore.release();\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.shiftreduce.ReorderingOracle.reorder",
	"Comment": "given a predicted transition and a state, this method rearrangesthe list of transitions and returns whether or not training cancontinue.",
	"Method": "boolean reorder(State state,Transition chosenTransition,List<Transition> transitions){\r\n    if (transitions.size() == 0) {\r\n        throw new AssertionError();\r\n    }\r\n    Transition goldTransition = transitions.get(0);\r\n    if (chosenTransition.equals(goldTransition)) {\r\n        transitions.remove(0);\r\n        return true;\r\n    }\r\n    if ((goldTransition instanceof UnaryTransition) || (goldTransition instanceof CompoundUnaryTransition)) {\r\n        transitions.remove(0);\r\n        return reorder(state, chosenTransition, transitions);\r\n    }\r\n    if ((chosenTransition instanceof UnaryTransition) || (chosenTransition instanceof CompoundUnaryTransition)) {\r\n        if (state.transitions.size() > 0) {\r\n            Transition previous = state.transitions.peek();\r\n            if ((previous instanceof UnaryTransition) || (previous instanceof CompoundUnaryTransition)) {\r\n                return false;\r\n            }\r\n        }\r\n        if (state.stack.size() == 0) {\r\n            return false;\r\n        }\r\n        return true;\r\n    }\r\n    if (chosenTransition instanceof BinaryTransition) {\r\n        if (state.stack.size() < 2) {\r\n            return false;\r\n        }\r\n        if (goldTransition instanceof ShiftTransition) {\r\n            return op.trainOptions().oracleBinaryToShift && reorderIncorrectBinaryTransition(transitions);\r\n        }\r\n        if (!(goldTransition instanceof BinaryTransition)) {\r\n            return false;\r\n        }\r\n        BinaryTransition chosenBinary = (BinaryTransition) chosenTransition;\r\n        BinaryTransition goldBinary = (BinaryTransition) goldTransition;\r\n        if (chosenBinary.isBinarized()) {\r\n            if (goldBinary.isBinarized() && chosenBinary.label.equals(goldBinary.label)) {\r\n                transitions.remove(0);\r\n                return true;\r\n            } else {\r\n                return false;\r\n            }\r\n        }\r\n        transitions.remove(0);\r\n        return true;\r\n    }\r\n    if ((chosenTransition instanceof ShiftTransition) && (goldTransition instanceof BinaryTransition)) {\r\n        if (state.endOfQueue()) {\r\n            return false;\r\n        }\r\n        BinaryTransition goldBinary = (BinaryTransition) goldTransition;\r\n        if (!goldBinary.isBinarized()) {\r\n            return op.trainOptions().oracleShiftToBinary && reorderIncorrectShiftTransition(transitions);\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.ShardId.fromString",
	"Comment": "parse the string representation of this shardid back to an object.we lose index uuid information here, but since we use tostring inrest responses, this is the best we can do to reconstruct the objecton the client side.",
	"Method": "ShardId fromString(String shardIdString){\r\n    int splitPosition = shardIdString.indexOf(\"][\");\r\n    if (splitPosition <= 0 || shardIdString.charAt(0) != '[' || shardIdString.charAt(shardIdString.length() - 1) != ']') {\r\n        throw new IllegalArgumentException(\"Unexpected shardId string format, expected [indexName][shardId] but got \" + shardIdString);\r\n    }\r\n    String indexName = shardIdString.substring(1, splitPosition);\r\n    int shardId = Integer.parseInt(shardIdString.substring(splitPosition + 2, shardIdString.length() - 1));\r\n    return new ShardId(new Index(indexName, IndexMetaData.INDEX_UUID_NA_VALUE), shardId);\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.TransportShardBulkAction.processUpdateResponse",
	"Comment": "creates a new bulk item result from the given requests and result of performing the update operation on the shard.",
	"Method": "BulkItemResponse processUpdateResponse(UpdateRequest updateRequest,String concreteIndex,BulkItemResponse operationResponse,UpdateHelper.Result translate){\r\n    final BulkItemResponse response;\r\n    DocWriteResponse.Result translatedResult = translate.getResponseResult();\r\n    if (operationResponse.isFailed()) {\r\n        response = new BulkItemResponse(operationResponse.getItemId(), DocWriteRequest.OpType.UPDATE, operationResponse.getFailure());\r\n    } else {\r\n        final UpdateResponse updateResponse;\r\n        if (translatedResult == DocWriteResponse.Result.CREATED || translatedResult == DocWriteResponse.Result.UPDATED) {\r\n            final IndexRequest updateIndexRequest = translate.action();\r\n            final IndexResponse indexResponse = operationResponse.getResponse();\r\n            updateResponse = new UpdateResponse(indexResponse.getShardInfo(), indexResponse.getShardId(), indexResponse.getType(), indexResponse.getId(), indexResponse.getSeqNo(), indexResponse.getPrimaryTerm(), indexResponse.getVersion(), indexResponse.getResult());\r\n            if (updateRequest.fetchSource() != null && updateRequest.fetchSource().fetchSource()) {\r\n                final BytesReference indexSourceAsBytes = updateIndexRequest.source();\r\n                final Tuple<XContentType, Map<String, Object>> sourceAndContent = XContentHelper.convertToMap(indexSourceAsBytes, true, updateIndexRequest.getContentType());\r\n                updateResponse.setGetResult(UpdateHelper.extractGetResult(updateRequest, concreteIndex, indexResponse.getVersion(), sourceAndContent.v2(), sourceAndContent.v1(), indexSourceAsBytes));\r\n            }\r\n        } else if (translatedResult == DocWriteResponse.Result.DELETED) {\r\n            final DeleteResponse deleteResponse = operationResponse.getResponse();\r\n            updateResponse = new UpdateResponse(deleteResponse.getShardInfo(), deleteResponse.getShardId(), deleteResponse.getType(), deleteResponse.getId(), deleteResponse.getSeqNo(), deleteResponse.getPrimaryTerm(), deleteResponse.getVersion(), deleteResponse.getResult());\r\n            final GetResult getResult = UpdateHelper.extractGetResult(updateRequest, concreteIndex, deleteResponse.getVersion(), translate.updatedSourceAsMap(), translate.updateSourceContentType(), null);\r\n            updateResponse.setGetResult(getResult);\r\n        } else {\r\n            throw new IllegalArgumentException(\"unknown operation type: \" + translatedResult);\r\n        }\r\n        response = new BulkItemResponse(operationResponse.getItemId(), DocWriteRequest.OpType.UPDATE, updateResponse);\r\n    }\r\n    return response;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequestBuilder.setRestoreGlobalState",
	"Comment": "if set to true the restore procedure will restore global cluster state.the global cluster state includes persistent settings and index template definitions.",
	"Method": "RestoreSnapshotRequestBuilder setRestoreGlobalState(boolean restoreGlobalState){\r\n    request.includeGlobalState(restoreGlobalState);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.NodeConnectionsService.disconnectFromNodesExcept",
	"Comment": "disconnects from all nodes except the ones provided as parameter",
	"Method": "void disconnectFromNodesExcept(DiscoveryNodes nodesToKeep){\r\n    Set<DiscoveryNode> currentNodes = new HashSet(nodes.keySet());\r\n    for (DiscoveryNode node : nodesToKeep) {\r\n        currentNodes.remove(node);\r\n    }\r\n    for (final DiscoveryNode node : currentNodes) {\r\n        try (Releasable ignored = nodeLocks.acquire(node)) {\r\n            Integer current = nodes.remove(node);\r\n            assert current != null : \"node \" + node + \" was removed in event but not in internal nodes\";\r\n            try {\r\n                transportService.disconnectFromNode(node);\r\n            } catch (Exception e) {\r\n                logger.warn(() -> new ParameterizedMessage(\"failed to disconnect to node [{}]\", node), e);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequestBuilder.setRewrite",
	"Comment": "indicates whether the query should be rewritten into primitive queries",
	"Method": "ValidateQueryRequestBuilder setRewrite(boolean rewrite){\r\n    request.rewrite(rewrite);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.maxent.iis.LambdaSolve.getDerivativesNeg",
	"Comment": "assuming we have the lambdas in the array and we need only thederivatives now.this is for the case where the model is parameterezied such that all weights are negativesee also loglikelihoodneg",
	"Method": "double[] getDerivativesNeg(){\r\n    double[] drvs = new double[lambda.length];\r\n    Experiments exp = p.data;\r\n    for (int fNo = 0; fNo < drvs.length; fNo++) {\r\n        Feature f = p.functions.get(fNo);\r\n        double sum = ftildeArr[fNo] * exp.getNumber();\r\n        double lam = -Math.exp(lambda[fNo]);\r\n        drvs[fNo] = -sum * lam;\r\n        for (int index = 0, length = f.len(); index < length; index++) {\r\n            int x = f.getX(index);\r\n            int y = f.getY(index);\r\n            if (ASSUME_BINARY) {\r\n                drvs[fNo] += probConds[x][y] * exp.ptildeX(x) * exp.getNumber() * lam;\r\n            } else {\r\n                double val = f.getVal(index);\r\n                drvs[fNo] += probConds[x][y] * val * exp.ptildeX(x) * exp.getNumber() * lam;\r\n            }\r\n        }\r\n    }\r\n    return drvs;\r\n}"
}, {
	"Path": "org.elasticsearch.action.delete.DeleteRequest.routing",
	"Comment": "controls the shard routing of the delete request. using this value to hash the shardand not the id.",
	"Method": "DeleteRequest routing(String routing,String routing){\r\n    return this.routing;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractBulkByScrollRequest.setMaxRetries",
	"Comment": "set the total number of retries attempted for rejections. there is no way to ask for unlimited retries.",
	"Method": "Self setMaxRetries(int maxRetries){\r\n    this.maxRetries = maxRetries;\r\n    return self();\r\n}"
}, {
	"Path": "org.elasticsearch.index.analysis.IndexAnalyzers.getNormalizer",
	"Comment": "returns a normalizer mapped to the given name or null if not present",
	"Method": "NamedAnalyzer getNormalizer(String name){\r\n    return normalizers.get(name);\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.internal.SourceProvider.get",
	"Comment": "returns the calling line of code. the selected line is the nearest to the top of the stack thatis not skipped.",
	"Method": "StackTraceElement get(){\r\n    for (final StackTraceElement element : new Throwable().getStackTrace()) {\r\n        String className = element.getClassName();\r\n        if (!classNamesToSkip.contains(className)) {\r\n            return element;\r\n        }\r\n    }\r\n    throw new AssertionError();\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.XMoreLikeThis.getAnalyzer",
	"Comment": "returns an analyzer that will be used to parse source doc with. the default analyzeris not set.",
	"Method": "Analyzer getAnalyzer(){\r\n    return analyzer;\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequestBuilder.setRetryOnConflict",
	"Comment": "sets the number of retries of a version conflict occurs because the document was updated betweengetting it and updating it. defaults to 0.",
	"Method": "UpdateRequestBuilder setRetryOnConflict(int retryOnConflict){\r\n    request.retryOnConflict(retryOnConflict);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequestBuilder.setDetectNoop",
	"Comment": "sets whether to perform extra effort to detect noop updates via docasupsert.defaults to true.",
	"Method": "UpdateRequestBuilder setDetectNoop(boolean detectNoop){\r\n    request.detectNoop(detectNoop);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.WordFactory.newLabelFromString",
	"Comment": "create a new word, where the label is formed fromthe string passed in.",
	"Method": "Label newLabelFromString(String word){\r\n    return new Word(word);\r\n}"
}, {
	"Path": "org.elasticsearch.client.Requests.searchScrollRequest",
	"Comment": "creates a search scroll request allowing to continue searching a previous search request.",
	"Method": "SearchScrollRequest searchScrollRequest(String scrollId){\r\n    return new SearchScrollRequest(scrollId);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequestBuilder.addDocValueField",
	"Comment": "adds a docvalue based field to load and return. the field does not have to be stored,but its recommended to use non analyzed or numeric fields.",
	"Method": "SearchRequestBuilder addDocValueField(String name,String format,SearchRequestBuilder addDocValueField,String name){\r\n    return addDocValueField(name, null);\r\n}"
}, {
	"Path": "android.util.SparseLongArray.delete",
	"Comment": "removes the mapping from the specified key, if there was any.",
	"Method": "void delete(int key){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i >= 0) {\r\n        removeAt(i);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.ZenDiscovery.shouldIgnoreOrRejectNewClusterState",
	"Comment": "in the case we follow an elected master the new cluster state needs to have the same elected master andthe new cluster state version needs to be equal or higher than our cluster state version.if the first condition fails we reject the cluster state and throw an error.if the second condition fails we ignore the cluster state.",
	"Method": "boolean shouldIgnoreOrRejectNewClusterState(Logger logger,ClusterState currentState,ClusterState newClusterState){\r\n    validateStateIsFromCurrentMaster(logger, currentState.nodes(), newClusterState);\r\n    if (currentState.supersedes(newClusterState) || (newClusterState.nodes().getMasterNodeId().equals(currentState.nodes().getMasterNodeId()) && currentState.version() == newClusterState.version())) {\r\n        logger.debug(\"received a cluster state that is not newer than the current one, ignoring (received {}, current {})\", newClusterState.version(), currentState.version());\r\n        return true;\r\n    }\r\n    if (currentState.nodes().getMasterNodeId() != null && newClusterState.version() < currentState.version()) {\r\n        logger.debug(\"received a cluster state that has a lower version than the current one, ignoring (received {}, current {})\", newClusterState.version(), currentState.version());\r\n        return true;\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.NestedQueryBuilder.scoreMode",
	"Comment": "returns how the scores from the matching child documents are mapped into the nested parent document.",
	"Method": "ScoreMode scoreMode(){\r\n    return scoreMode;\r\n}"
}, {
	"Path": "org.elasticsearch.common.Randomness.get",
	"Comment": "provides a reproducible source of randomness seeded by a longseed in the settings with the key setting.",
	"Method": "Random get(Settings settings,Setting<Long> setting,Random get){\r\n    if (currentMethod != null && getRandomMethod != null) {\r\n        try {\r\n            Object randomizedContext = currentMethod.invoke(null);\r\n            return (Random) getRandomMethod.invoke(randomizedContext);\r\n        } catch (ReflectiveOperationException e) {\r\n            throw new IllegalStateException(\"running tests but failed to invoke RandomizedContext#getRandom\", e);\r\n        }\r\n    } else {\r\n        return getWithoutSeed();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequestBuilder.setVersion",
	"Comment": "sets the version, which will cause the get operation to only be performed if a matchingversion exists and no changes happened on the doc since then.",
	"Method": "TermVectorsRequestBuilder setVersion(long version){\r\n    request.version(version);\r\n    return this;\r\n}"
}, {
	"Path": "org.apache.dubbo.remoting.exchange.support.DefaultFuture.newFuture",
	"Comment": "init a defaultfuture1.init a defaultfuture2.timeout check",
	"Method": "DefaultFuture newFuture(Channel channel,Request request,int timeout){\r\n    final DefaultFuture future = new DefaultFuture(channel, request, timeout);\r\n    timeoutCheck(future);\r\n    return future;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.RoutingNodes.allReplicasActive",
	"Comment": "returns true iff all replicas are active for the given shard routing. otherwise false",
	"Method": "boolean allReplicasActive(ShardId shardId,MetaData metaData){\r\n    final List<ShardRouting> shards = assignedShards(shardId);\r\n    if (shards.isEmpty() || shards.size() < metaData.getIndexSafe(shardId.getIndex()).getNumberOfReplicas() + 1) {\r\n        return false;\r\n    }\r\n    for (ShardRouting shard : shards) {\r\n        if (!shard.active()) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDatum.asFeatures",
	"Comment": "returns the collection that this basicdatum was constructed with.",
	"Method": "Collection<FeatureType> asFeatures(){\r\n    return (features);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationAction.acquireReplicaOperationPermit",
	"Comment": "executes the logic for acquiring one or more operation permit on a replica shard. the default is to acquire a single permit but thismethod can be overridden to acquire more.",
	"Method": "void acquireReplicaOperationPermit(IndexShard replica,ReplicaRequest request,ActionListener<Releasable> onAcquired,long primaryTerm,long globalCheckpoint,long maxSeqNoOfUpdatesOrDeletes){\r\n    replica.acquireReplicaOperationPermit(primaryTerm, globalCheckpoint, maxSeqNoOfUpdatesOrDeletes, onAcquired, executor, request);\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.MapperPlugin.getFieldFilter",
	"Comment": "returns a function that given an index name returns a predicate which fields must match in order to be returned by get mappings,get index, get field mappings and field capabilities api. useful to filter the fields that such api return. the predicate receivesthe field name as input argument and should return true to show the field and false to hide it.",
	"Method": "Function<String, Predicate<String>> getFieldFilter(){\r\n    return NOOP_FIELD_FILTER;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateShrinkIndex",
	"Comment": "validates the settings and mappings for shrinking an index.",
	"Method": "List<String> validateShrinkIndex(ClusterState state,String sourceIndex,Set<String> targetIndexMappingsTypes,String targetIndexName,Settings targetIndexSettings){\r\n    IndexMetaData sourceMetaData = validateResize(state, sourceIndex, targetIndexMappingsTypes, targetIndexName, targetIndexSettings);\r\n    assert IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.exists(targetIndexSettings);\r\n    IndexMetaData.selectShrinkShards(0, sourceMetaData, IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.get(targetIndexSettings));\r\n    if (sourceMetaData.getNumberOfShards() == 1) {\r\n        throw new IllegalArgumentException(\"can't shrink an index with only one shard\");\r\n    }\r\n    final IndexRoutingTable table = state.routingTable().index(sourceIndex);\r\n    Map<String, AtomicInteger> nodesToNumRouting = new HashMap();\r\n    int numShards = sourceMetaData.getNumberOfShards();\r\n    for (ShardRouting routing : table.shardsWithState(ShardRoutingState.STARTED)) {\r\n        nodesToNumRouting.computeIfAbsent(routing.currentNodeId(), (s) -> new AtomicInteger(0)).incrementAndGet();\r\n    }\r\n    List<String> nodesToAllocateOn = new ArrayList();\r\n    for (Map.Entry<String, AtomicInteger> entries : nodesToNumRouting.entrySet()) {\r\n        int numAllocations = entries.getValue().get();\r\n        assert numAllocations <= numShards : \"wait what? \" + numAllocations + \" is > than num shards \" + numShards;\r\n        if (numAllocations == numShards) {\r\n            nodesToAllocateOn.add(entries.getKey());\r\n        }\r\n    }\r\n    if (nodesToAllocateOn.isEmpty()) {\r\n        throw new IllegalStateException(\"index \" + sourceIndex + \" must have all shards allocated on the same node to shrink index\");\r\n    }\r\n    return nodesToAllocateOn;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.storage.ModelBatch.writeToFileWithoutFactors",
	"Comment": "convenience function to write the current state of the modelbatch out to a file, without factors.",
	"Method": "void writeToFileWithoutFactors(String filename){\r\n    FileOutputStream fos = new FileOutputStream(filename);\r\n    writeToStreamWithoutFactors(fos);\r\n    fos.close();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.GlobalCheckpointListeners.globalCheckpointUpdated",
	"Comment": "invoke to notify all registered listeners of an updated global checkpoint.",
	"Method": "void globalCheckpointUpdated(long globalCheckpoint){\r\n    assert globalCheckpoint >= NO_OPS_PERFORMED;\r\n    assert globalCheckpoint > lastKnownGlobalCheckpoint : \"updated global checkpoint [\" + globalCheckpoint + \"]\" + \" is not more than the last known global checkpoint [\" + lastKnownGlobalCheckpoint + \"]\";\r\n    lastKnownGlobalCheckpoint = globalCheckpoint;\r\n    notifyListeners(globalCheckpoint, null);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor.createExtractorFromFiles",
	"Comment": "creates an extractor using the specified environment, and reading the rules from the given filenames.",
	"Method": "CoreMapExpressionExtractor<M> createExtractorFromFiles(Env env,String filenames,CoreMapExpressionExtractor<M> createExtractorFromFiles,Env env,List<String> filenames){\r\n    CoreMapExpressionExtractor<M> extractor = new CoreMapExpressionExtractor(env);\r\n    for (String filename : filenames) {\r\n        try (BufferedReader br = IOUtils.readerFromString(filename)) {\r\n            if (verbose)\r\n                log.info(\"Reading TokensRegex rules from \" + filename);\r\n            TokenSequenceParser parser = new TokenSequenceParser();\r\n            parser.updateExpressionExtractor(extractor, br);\r\n        } catch (Exception ex) {\r\n            throw new RuntimeException(\"Error parsing file: \" + filename, ex);\r\n        }\r\n    }\r\n    return extractor;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaData.getTotalNumberOfShards",
	"Comment": "gets the total number of shards from all indices, including replicas andclosed indices.",
	"Method": "int getTotalNumberOfShards(){\r\n    return this.totalNumberOfShards;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.service.ClusterService.addLowPriorityApplier",
	"Comment": "adds an applier which will be called after all high priority and normal appliers have been called.",
	"Method": "void addLowPriorityApplier(ClusterStateApplier applier){\r\n    clusterApplierService.addLowPriorityApplier(applier);\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequest.isRetry",
	"Comment": "returns true if this request has been sent to a shard copy more than once.",
	"Method": "boolean isRetry(){\r\n    return isRetry;\r\n}"
}, {
	"Path": "org.apache.harmony.tests.java.io.RandomAccessFileTest.setUp",
	"Comment": "sets up the fixture, for example, open a network connection. this methodis called before a test is executed.",
	"Method": "void setUp(){\r\n    super.setUp();\r\n    f = File.createTempFile(\"raf\", \"tst\");\r\n    if (!f.delete()) {\r\n        fail(\"Unable to delete test file : \" + f);\r\n    }\r\n    fileName = f.getAbsolutePath();\r\n    filesToDelete = new ArrayList<File>();\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkPrimaryExecutionContext.getExecutionResult",
	"Comment": "returns the result of the request that has been executed on the shard",
	"Method": "BulkItemResponse getExecutionResult(){\r\n    assert assertInvariants(ItemProcessingState.EXECUTED);\r\n    return executionResult;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEvent.changedCustomMetaDataSet",
	"Comment": "returns a set of custom meta data types when any custom metadata for the cluster has changedbetween the previous cluster state and the new cluster state. custom meta data types arereturned iff they have been added, updated or removed between the previous and the current state",
	"Method": "Set<String> changedCustomMetaDataSet(){\r\n    Set<String> result = new HashSet();\r\n    ImmutableOpenMap<String, MetaData.Custom> currentCustoms = state.metaData().customs();\r\n    ImmutableOpenMap<String, MetaData.Custom> previousCustoms = previousState.metaData().customs();\r\n    if (currentCustoms.equals(previousCustoms) == false) {\r\n        for (ObjectObjectCursor<String, MetaData.Custom> currentCustomMetaData : currentCustoms) {\r\n            if (previousCustoms.containsKey(currentCustomMetaData.key) == false || currentCustomMetaData.value.equals(previousCustoms.get(currentCustomMetaData.key)) == false) {\r\n                result.add(currentCustomMetaData.key);\r\n            }\r\n        }\r\n        for (ObjectObjectCursor<String, MetaData.Custom> previousCustomMetaData : previousCustoms) {\r\n            if (currentCustoms.containsKey(previousCustomMetaData.key) == false) {\r\n                result.add(previousCustomMetaData.key);\r\n            }\r\n        }\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.GeoHashUtils.addNeighbors",
	"Comment": "add all geohashes of the cells next to a given geohash to a list.",
	"Method": "E addNeighbors(String geohash,E neighbors,E addNeighbors,String geohash,int length,E neighbors){\r\n    String south = neighbor(geohash, length, 0, -1);\r\n    String north = neighbor(geohash, length, 0, +1);\r\n    if (north != null) {\r\n        neighbors.add(neighbor(north, length, -1, 0));\r\n        neighbors.add(north);\r\n        neighbors.add(neighbor(north, length, +1, 0));\r\n    }\r\n    neighbors.add(neighbor(geohash, length, -1, 0));\r\n    neighbors.add(neighbor(geohash, length, +1, 0));\r\n    if (south != null) {\r\n        neighbors.add(neighbor(south, length, -1, 0));\r\n        neighbors.add(south);\r\n        neighbors.add(neighbor(south, length, +1, 0));\r\n    }\r\n    return neighbors;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Settings.hasValue",
	"Comment": "returns true iff the given key has a value in this settings object",
	"Method": "boolean hasValue(String key){\r\n    return settings.get(key) != null;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.getIndexSort",
	"Comment": "return the sort order of this index, or null if the index has no sort.",
	"Method": "Sort getIndexSort(){\r\n    return indexSortSupplier.get();\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.getMaxTermsCount",
	"Comment": "returns the maximum number of terms that can be used in a terms query request",
	"Method": "int getMaxTermsCount(){\r\n    return this.maxTermsCount;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractBulkByScrollRequest.doForSlice",
	"Comment": "setup a clone of this request with the information needed to process a slice of it.",
	"Method": "Self doForSlice(Self request,TaskId slicingTask,int totalSlices){\r\n    if (totalSlices < 1) {\r\n        throw new IllegalArgumentException(\"Number of total slices must be at least 1 but was [\" + totalSlices + \"]\");\r\n    }\r\n    // Parent task will store result\r\n    request.setAbortOnVersionConflict(abortOnVersionConflict).setRefresh(refresh).setTimeout(timeout).setWaitForActiveShards(activeShardCount).setRetryBackoffInitialTime(retryBackoffInitialTime).setMaxRetries(maxRetries).setShouldStoreResult(// Split requests per second between all slices\r\n    false).setRequestsPerSecond(// Sub requests don't have workers\r\n    requestsPerSecond / totalSlices).setSlices(1);\r\n    if (size != -1) {\r\n        request.setSize(size == SIZE_ALL_MATCHES ? SIZE_ALL_MATCHES : size / totalSlices);\r\n    }\r\n    request.setParentTask(slicingTask);\r\n    return request;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.BigArrays.withCircuitBreaking",
	"Comment": "return an instance of this bigarrays class with circuit breakingexplicitly enabled, instead of only accounting enabled",
	"Method": "BigArrays withCircuitBreaking(){\r\n    return this.circuitBreakingInstance;\r\n}"
}, {
	"Path": "org.elasticsearch.client.transport.TransportClient.removeTransportAddress",
	"Comment": "removes a transport address from the list of transport addresses that are used to connect to.",
	"Method": "TransportClient removeTransportAddress(TransportAddress transportAddress){\r\n    nodesService.removeTransportAddress(transportAddress);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateResponse.parseXContentFields",
	"Comment": "parse the current token and update the parsing context appropriately.",
	"Method": "void parseXContentFields(XContentParser parser,Builder context){\r\n    XContentParser.Token token = parser.currentToken();\r\n    String currentFieldName = parser.currentName();\r\n    if (GET.equals(currentFieldName)) {\r\n        if (token == XContentParser.Token.START_OBJECT) {\r\n            context.setGetResult(GetResult.fromXContentEmbedded(parser));\r\n        }\r\n    } else {\r\n        DocWriteResponse.parseInnerToXContent(parser, context);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.InternalClusterInfoService.updateIndicesStats",
	"Comment": "retrieve the latest indices stats, calling the listener when complete",
	"Method": "CountDownLatch updateIndicesStats(ActionListener<IndicesStatsResponse> listener){\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    final IndicesStatsRequest indicesStatsRequest = new IndicesStatsRequest();\r\n    indicesStatsRequest.clear();\r\n    indicesStatsRequest.store(true);\r\n    client.admin().indices().stats(indicesStatsRequest, new LatchedActionListener(listener, latch));\r\n    return latch;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogWriter.syncNeeded",
	"Comment": "returns true if there are buffered operations that have not been flushed and fsynced to disk or if the latest globalcheckpoint has not yet been fsynced",
	"Method": "boolean syncNeeded(){\r\n    return totalOffset != lastSyncedCheckpoint.offset || globalCheckpointSupplier.getAsLong() != lastSyncedCheckpoint.globalCheckpoint || minTranslogGenerationSupplier.getAsLong() != lastSyncedCheckpoint.minTranslogGeneration;\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.StoreFileMetaData.checksum",
	"Comment": "returns a string representation of the files checksum. since lucene 4.8 this is a crc32 checksum writtenby lucene.",
	"Method": "String checksum(){\r\n    return this.checksum;\r\n}"
}, {
	"Path": "org.json.JSONStringer.open",
	"Comment": "enters a new scope by appending any necessary whitespace and the givenbracket.",
	"Method": "JSONStringer open(Scope empty,String openBracket){\r\n    if (stack.isEmpty() && out.length() > 0) {\r\n        throw new JSONException(\"Nesting problem: multiple top-level roots\");\r\n    }\r\n    beforeValue();\r\n    stack.add(empty);\r\n    out.append(openBracket);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.RecordIterator.firstRecord",
	"Comment": "a static convenience method that returns the first line of thespecified file as list of strings, using the default whitespacedelimiter.",
	"Method": "List<String> firstRecord(String filename,String delim,List<String> firstRecord,String filename){\r\n    return firstRecord(filename, WHITESPACE);\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.IngestDocument.extractMetadata",
	"Comment": "one time operation that extracts the metadata fields from the ingest document and returns them.metadata fields that used to be accessible as ordinary top level fields will be removed as part of this call.",
	"Method": "Map<MetaData, Object> extractMetadata(){\r\n    Map<MetaData, Object> metadataMap = new EnumMap(MetaData.class);\r\n    for (MetaData metaData : MetaData.values()) {\r\n        metadataMap.put(metaData, sourceAndMetadata.remove(metaData.getFieldName()));\r\n    }\r\n    return metadataMap;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequest.explain",
	"Comment": "indicates if detailed information about query is requested",
	"Method": "void explain(boolean explain,boolean explain){\r\n    return explain;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.DocumentParser.createDynamicUpdate",
	"Comment": "creates a mapping containing any dynamically added fields, or returns null if there were no dynamic mappings.",
	"Method": "Mapping createDynamicUpdate(Mapping mapping,DocumentMapper docMapper,List<Mapper> dynamicMappers){\r\n    if (dynamicMappers.isEmpty()) {\r\n        return null;\r\n    }\r\n    Collections.sort(dynamicMappers, (Mapper o1, Mapper o2) -> o1.name().compareTo(o2.name()));\r\n    Iterator<Mapper> dynamicMapperItr = dynamicMappers.iterator();\r\n    List<ObjectMapper> parentMappers = new ArrayList();\r\n    Mapper firstUpdate = dynamicMapperItr.next();\r\n    parentMappers.add(createUpdate(mapping.root(), splitAndValidatePath(firstUpdate.name()), 0, firstUpdate));\r\n    Mapper previousMapper = null;\r\n    while (dynamicMapperItr.hasNext()) {\r\n        Mapper newMapper = dynamicMapperItr.next();\r\n        if (previousMapper != null && newMapper.name().equals(previousMapper.name())) {\r\n            newMapper.merge(previousMapper);\r\n            continue;\r\n        }\r\n        previousMapper = newMapper;\r\n        String[] nameParts = splitAndValidatePath(newMapper.name());\r\n        int i = removeUncommonMappers(parentMappers, nameParts);\r\n        i = expandCommonMappers(parentMappers, nameParts, i);\r\n        if (i < nameParts.length - 1) {\r\n            newMapper = createExistingMapperUpdate(parentMappers, nameParts, i, docMapper, newMapper);\r\n        }\r\n        if (newMapper instanceof ObjectMapper) {\r\n            parentMappers.add((ObjectMapper) newMapper);\r\n        } else {\r\n            addToLastMapper(parentMappers, newMapper, true);\r\n        }\r\n    }\r\n    popMappers(parentMappers, 1, true);\r\n    assert parentMappers.size() == 1;\r\n    return mapping.mappingUpdate(parentMappers.get(0));\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.DiskThresholdSettings.thresholdPercentageFromWatermark",
	"Comment": "attempts to parse the watermark into a percentage, returning 100.0% ifit cannot be parsed.",
	"Method": "double thresholdPercentageFromWatermark(String watermark,double thresholdPercentageFromWatermark,String watermark,boolean lenient){\r\n    try {\r\n        return RatioValue.parseRatioValue(watermark).getAsPercent();\r\n    } catch (ElasticsearchParseException ex) {\r\n        if (lenient) {\r\n            return 100.0;\r\n        }\r\n        throw ex;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.PersistentTaskPlugin.getPersistentTasksExecutor",
	"Comment": "returns additional persistent tasks executors added by this plugin.",
	"Method": "List<PersistentTasksExecutor<?>> getPersistentTasksExecutor(ClusterService clusterService,ThreadPool threadPool,Client client,SettingsModule settingsModule){\r\n    return Collections.emptyList();\r\n}"
}, {
	"Path": "org.elasticsearch.client.transport.TransportClient.filteredNodes",
	"Comment": "the list of filtered nodes that were not connected to, for example, due tomismatch in cluster name.",
	"Method": "List<DiscoveryNode> filteredNodes(){\r\n    return nodesService.filteredNodes();\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.updateIndexMetaData",
	"Comment": "updates the settings and index metadata and notifies all registered settings consumers with the new settings iff at least onesetting has changed.",
	"Method": "boolean updateIndexMetaData(IndexMetaData indexMetaData){\r\n    final Settings newSettings = indexMetaData.getSettings();\r\n    if (version.equals(Version.indexCreated(newSettings)) == false) {\r\n        throw new IllegalArgumentException(\"version mismatch on settings update expected: \" + version + \" but was: \" + Version.indexCreated(newSettings));\r\n    }\r\n    final String newUUID = newSettings.get(IndexMetaData.SETTING_INDEX_UUID, IndexMetaData.INDEX_UUID_NA_VALUE);\r\n    if (newUUID.equals(getUUID()) == false) {\r\n        throw new IllegalArgumentException(\"uuid mismatch on settings update expected: \" + getUUID() + \" but was: \" + newUUID);\r\n    }\r\n    this.indexMetaData = indexMetaData;\r\n    final Settings newIndexSettings = Settings.builder().put(nodeSettings).put(newSettings).build();\r\n    if (same(this.settings, newIndexSettings)) {\r\n        return false;\r\n    }\r\n    scopedSettings.applySettings(newSettings);\r\n    this.settings = newIndexSettings;\r\n    return true;\r\n}"
}, {
	"Path": "org.apache.dubbo.remoting.handler.ConnectChannelHandlerTest.test_Received_Event_invoke_direct",
	"Comment": "events do not pass through the thread pool and execute directly on the io",
	"Method": "void test_Received_Event_invoke_direct(){\r\n    handler = new ConnectionOrderedChannelHandler(new BizChannelHander(false), url);\r\n    ThreadPoolExecutor executor = (ThreadPoolExecutor) getField(handler, \"SHARED_EXECUTOR\", 1);\r\n    executor.shutdown();\r\n    executor = (ThreadPoolExecutor) getField(handler, \"executor\", 1);\r\n    executor.shutdown();\r\n    Request req = new Request();\r\n    req.setHeartbeat(true);\r\n    final AtomicInteger count = new AtomicInteger(0);\r\n    handler.received(new MockedChannel() {\r\n        @Override\r\n        public void send(Object message) throws RemotingException {\r\n            Assert.assertEquals(\"response.heartbeat\", true, ((Response) message).isHeartbeat());\r\n            count.incrementAndGet();\r\n        }\r\n    }, req);\r\n    Assert.assertEquals(\"channel.send must be invoke\", 1, count.get());\r\n}"
}, {
	"Path": "org.apache.dubbo.remoting.handler.ConnectChannelHandlerTest.test_Received_Event_invoke_direct",
	"Comment": "events do not pass through the thread pool and execute directly on the io",
	"Method": "void test_Received_Event_invoke_direct(){\r\n    Assert.assertEquals(\"response.heartbeat\", true, ((Response) message).isHeartbeat());\r\n    count.incrementAndGet();\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.SecureSetting.secureString",
	"Comment": "a setting which contains a sensitive string.this may be any sensitive string, e.g. a username, a password, an auth token, etc.",
	"Method": "Setting<SecureString> secureString(String name,Setting<SecureString> fallback,Property properties){\r\n    return new SecureStringSetting(name, fallback, properties);\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.ForwardEntailerSearchProblem.aggregateDeletedEdges",
	"Comment": "backtrace from a search state, collecting all of the deleted edges used to get there.",
	"Method": "List<String> aggregateDeletedEdges(SearchState state,Iterable<SemanticGraphEdge> justDeleted,Iterable<String> otherEdges){\r\n    List<String> rtn = new ArrayList();\r\n    for (SemanticGraphEdge edge : justDeleted) {\r\n        rtn.add(edge.getRelation().toString());\r\n    }\r\n    for (String edge : otherEdges) {\r\n        rtn.add(edge);\r\n    }\r\n    while (state != null) {\r\n        if (state.lastDeletedEdge != null) {\r\n            rtn.add(state.lastDeletedEdge);\r\n        }\r\n        state = state.source;\r\n    }\r\n    return rtn;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.TokenSequencePattern.compile",
	"Comment": "compiles a sequence of regular expressions into a tokensequencepatternusing the specified environment.",
	"Method": "TokenSequencePattern compile(String string,TokenSequencePattern compile,Env env,String string,TokenSequencePattern compile,String strings,TokenSequencePattern compile,Env env,String strings,TokenSequencePattern compile,SequencePattern.PatternExpr nodeSequencePattern){\r\n    return new TokenSequencePattern(null, nodeSequencePattern);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequestBuilder.setBatchedReduceSize",
	"Comment": "sets the number of shard results that should be reduced at once on the coordinating node. this value should be used as a protectionmechanism to reduce the memory overhead per search request if the potential number of shards in the request can be large.",
	"Method": "SearchRequestBuilder setBatchedReduceSize(int batchedReduceSize){\r\n    this.request.setBatchedReduceSize(batchedReduceSize);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.MoreLikeThisQuery.getMinimumShouldMatch",
	"Comment": "number of terms that must match the generated query expressed in thecommon syntax for minimum should match.",
	"Method": "String getMinimumShouldMatch(){\r\n    return minimumShouldMatch;\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.french.pipeline.FTBDataset.getCanditoTreeID",
	"Comment": "return the id of this tree according to the candito split files.",
	"Method": "String getCanditoTreeID(Tree t){\r\n    String canditoName = null;\r\n    if (t.label() instanceof CoreLabel) {\r\n        String fileName = ((CoreLabel) t.label()).docID();\r\n        fileName = fileName.substring(0, fileName.lastIndexOf('.'));\r\n        String ftbID = ((CoreLabel) t.label()).get(CoreAnnotations.SentenceIDAnnotation.class);\r\n        if (fileName != null && ftbID != null) {\r\n            canditoName = fileName + \"-\" + ftbID;\r\n        } else {\r\n            throw new NullPointerException(\"fileName \" + fileName + \", ftbID \" + ftbID);\r\n        }\r\n    } else {\r\n        throw new IllegalArgumentException(\"Trees constructed without CoreLabels! Can't extract metadata!\");\r\n    }\r\n    return canditoName;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequestBuilder.setScroll",
	"Comment": "if set, will enable scrolling of the search request for the specified timeout.",
	"Method": "SearchRequestBuilder setScroll(Scroll scroll,SearchRequestBuilder setScroll,TimeValue keepAlive,SearchRequestBuilder setScroll,String keepAlive){\r\n    request.scroll(keepAlive);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MatchQueryBuilder.maxExpansions",
	"Comment": "when using fuzzy or prefix type query, the number of term expansions to use.",
	"Method": "MatchQueryBuilder maxExpansions(int maxExpansions,int maxExpansions){\r\n    return this.maxExpansions;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.SessionTimesOutTest.testTimeoutBug",
	"Comment": "a proxy throwing an exception will end up not releasing the resources.",
	"Method": "void testTimeoutBug(){\r\n    final GridRegistry registry = DefaultGridRegistry.newInstance(new Hub(new GridHubConfiguration()));\r\n    RemoteProxy p1 = new MyBuggyRemoteProxyTimeout(req, registry);\r\n    p1.setupTimeoutListener();\r\n    try {\r\n        registry.add(p1);\r\n        RequestHandler newSessionRequest = GridHelper.createNewSessionHandler(registry, app1);\r\n        newSessionRequest.process();\r\n        final RequestHandler newSessionRequest2 = GridHelper.createNewSessionHandler(registry, app1);\r\n        new Thread(new Runnable() {\r\n            public void run() {\r\n                newSessionRequest2.process();\r\n            }\r\n        }).start();\r\n        Thread.sleep(500);\r\n        assertNull(newSessionRequest2.getServerSession());\r\n    } finally {\r\n        registry.stop();\r\n    }\r\n}"
}, {
	"Path": "org.openqa.grid.internal.SessionTimesOutTest.testTimeoutBug",
	"Comment": "a proxy throwing an exception will end up not releasing the resources.",
	"Method": "void testTimeoutBug(){\r\n    newSessionRequest2.process();\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.internal.ProviderMethodsModule.forModule",
	"Comment": "returns a module which creates bindings for provider methods from the given module.",
	"Method": "Module forModule(Module module){\r\n    return forObject(module);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.MultiSearchRequestBuilder.setMaxConcurrentSearchRequests",
	"Comment": "sets how many search requests specified in this multi search requests are allowed to be ran concurrently.",
	"Method": "MultiSearchRequestBuilder setMaxConcurrentSearchRequests(int maxConcurrentSearchRequests){\r\n    request().maxConcurrentSearchRequests(maxConcurrentSearchRequests);\r\n    return this;\r\n}"
}, {
	"Path": "android.text.TextUtils.isDigitsOnly",
	"Comment": "returns whether the given charsequence contains only digits.",
	"Method": "boolean isDigitsOnly(CharSequence str){\r\n    final int len = str.length();\r\n    for (int i = 0; i < len; i++) {\r\n        if (!Character.isDigit(str.charAt(i))) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.getNumVersionLookups",
	"Comment": "returns the number of times a version was looked up either from memory or from the index.note this is only available if assertions are enabled",
	"Method": "long getNumVersionLookups(){\r\n    return numVersionLookups.count();\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.TokenSequencePattern.matcher",
	"Comment": "returns a tokensequencematcher that can be used to match this patternagainst the specified list of tokens.",
	"Method": "TokenSequenceMatcher matcher(List<? extends CoreMap> tokens){\r\n    return getMatcher(tokens);\r\n}"
}, {
	"Path": "org.openqa.grid.internal.TestSession.get",
	"Comment": "allow you to retrieve an object previously stored on the test session.",
	"Method": "Object get(String key){\r\n    return objects.get(key);\r\n}"
}, {
	"Path": "org.elasticsearch.node.Node.newClusterInfoService",
	"Comment": "constructs a clusterinfoservice which may be mocked for tests.",
	"Method": "ClusterInfoService newClusterInfoService(Settings settings,ClusterService clusterService,ThreadPool threadPool,NodeClient client,Consumer<ClusterInfo> listeners){\r\n    return new InternalClusterInfoService(settings, clusterService, threadPool, client, listeners);\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.ReplicaShardAllocator.findStore",
	"Comment": "finds the store for the assigned shard in the fetched data, returns null if none is found.",
	"Method": "TransportNodesListShardStoreMetaData.StoreFilesMetaData findStore(ShardRouting shard,RoutingAllocation allocation,AsyncShardFetch.FetchResult<NodeStoreFilesMetaData> data){\r\n    assert shard.currentNodeId() != null;\r\n    DiscoveryNode primaryNode = allocation.nodes().get(shard.currentNodeId());\r\n    if (primaryNode == null) {\r\n        return null;\r\n    }\r\n    NodeStoreFilesMetaData primaryNodeFilesStore = data.getData().get(primaryNode);\r\n    if (primaryNodeFilesStore == null) {\r\n        return null;\r\n    }\r\n    return primaryNodeFilesStore.storeFilesMetaData();\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.getTranslogRetentionSize",
	"Comment": "returns the transaction log retention size which controls how much of the translog is kept around to allow for ops based recoveries",
	"Method": "ByteSizeValue getTranslogRetentionSize(){\r\n    return translogRetentionSize;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.ReindexRequest.setSourceBatchSize",
	"Comment": "sets the scroll size for setting how many documents are to be processed in one batch during reindex",
	"Method": "ReindexRequest setSourceBatchSize(int size){\r\n    this.getSearchRequest().source().size(size);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.FileSystem.gzipFile",
	"Comment": "similar to the unix gzip command, only it does not delete the file after compressing it.",
	"Method": "void gzipFile(File uncompressedFileName,File compressedFileName){\r\n    try (GZIPOutputStream out = new GZIPOutputStream(new FileOutputStream(compressedFileName));\r\n        FileInputStream in = new FileInputStream(uncompressedFileName)) {\r\n        byte[] buf = new byte[1024];\r\n        for (int len; (len = in.read(buf)) > 0; ) {\r\n            out.write(buf, 0, len);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "android.util.ArraySet.ensureCapacity",
	"Comment": "ensure the array map can hold at least minimumcapacityitems.",
	"Method": "void ensureCapacity(int minimumCapacity){\r\n    if (mHashes.length < minimumCapacity) {\r\n        final int[] ohashes = mHashes;\r\n        final Object[] oarray = mArray;\r\n        allocArrays(minimumCapacity);\r\n        if (mSize > 0) {\r\n            System.arraycopy(ohashes, 0, mHashes, 0, mSize);\r\n            System.arraycopy(oarray, 0, mArray, 0, mSize);\r\n        }\r\n        freeArrays(ohashes, oarray, mSize);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.GeoUtils.parseGeoPoint",
	"Comment": "parse a geopoint represented as an object, string or an array. if the geopoint is represented as a geohash,the left bottom corner of the geohash cell is used as the geopoint coordinates.geoboundingboxquerybuilder.java",
	"Method": "GeoPoint parseGeoPoint(XContentParser parser,GeoPoint parseGeoPoint,XContentParser parser,GeoPoint point,GeoPoint parseGeoPoint,Object value,boolean ignoreZValue,GeoPoint parseGeoPoint,XContentParser parser,GeoPoint point,boolean ignoreZValue,GeoPoint parseGeoPoint,XContentParser parser,GeoPoint point,boolean ignoreZValue,EffectivePoint effectivePoint){\r\n    double lat = Double.NaN;\r\n    double lon = Double.NaN;\r\n    String geohash = null;\r\n    NumberFormatException numberFormatException = null;\r\n    if (parser.currentToken() == Token.START_OBJECT) {\r\n        while (parser.nextToken() != Token.END_OBJECT) {\r\n            if (parser.currentToken() == Token.FIELD_NAME) {\r\n                String field = parser.currentName();\r\n                if (LATITUDE.equals(field)) {\r\n                    parser.nextToken();\r\n                    switch(parser.currentToken()) {\r\n                        case VALUE_NUMBER:\r\n                        case VALUE_STRING:\r\n                            try {\r\n                                lat = parser.doubleValue(true);\r\n                            } catch (NumberFormatException e) {\r\n                                numberFormatException = e;\r\n                            }\r\n                            break;\r\n                        default:\r\n                            throw new ElasticsearchParseException(\"latitude must be a number\");\r\n                    }\r\n                } else if (LONGITUDE.equals(field)) {\r\n                    parser.nextToken();\r\n                    switch(parser.currentToken()) {\r\n                        case VALUE_NUMBER:\r\n                        case VALUE_STRING:\r\n                            try {\r\n                                lon = parser.doubleValue(true);\r\n                            } catch (NumberFormatException e) {\r\n                                numberFormatException = e;\r\n                            }\r\n                            break;\r\n                        default:\r\n                            throw new ElasticsearchParseException(\"longitude must be a number\");\r\n                    }\r\n                } else if (GEOHASH.equals(field)) {\r\n                    if (parser.nextToken() == Token.VALUE_STRING) {\r\n                        geohash = parser.text();\r\n                    } else {\r\n                        throw new ElasticsearchParseException(\"geohash must be a string\");\r\n                    }\r\n                } else {\r\n                    throw new ElasticsearchParseException(\"field must be either [{}], [{}] or [{}]\", LATITUDE, LONGITUDE, GEOHASH);\r\n                }\r\n            } else {\r\n                throw new ElasticsearchParseException(\"token [{}] not allowed\", parser.currentToken());\r\n            }\r\n        }\r\n        if (geohash != null) {\r\n            if (!Double.isNaN(lat) || !Double.isNaN(lon)) {\r\n                throw new ElasticsearchParseException(\"field must be either lat/lon or geohash\");\r\n            } else {\r\n                return parseGeoHash(point, geohash, effectivePoint);\r\n            }\r\n        } else if (numberFormatException != null) {\r\n            throw new ElasticsearchParseException(\"[{}] and [{}] must be valid double values\", numberFormatException, LATITUDE, LONGITUDE);\r\n        } else if (Double.isNaN(lat)) {\r\n            throw new ElasticsearchParseException(\"field [{}] missing\", LATITUDE);\r\n        } else if (Double.isNaN(lon)) {\r\n            throw new ElasticsearchParseException(\"field [{}] missing\", LONGITUDE);\r\n        } else {\r\n            return point.reset(lat, lon);\r\n        }\r\n    } else if (parser.currentToken() == Token.START_ARRAY) {\r\n        int element = 0;\r\n        while (parser.nextToken() != Token.END_ARRAY) {\r\n            if (parser.currentToken() == Token.VALUE_NUMBER) {\r\n                element++;\r\n                if (element == 1) {\r\n                    lon = parser.doubleValue();\r\n                } else if (element == 2) {\r\n                    lat = parser.doubleValue();\r\n                } else {\r\n                    GeoPoint.assertZValue(ignoreZValue, parser.doubleValue());\r\n                }\r\n            } else {\r\n                throw new ElasticsearchParseException(\"numeric value expected\");\r\n            }\r\n        }\r\n        return point.reset(lat, lon);\r\n    } else if (parser.currentToken() == Token.VALUE_STRING) {\r\n        String val = parser.text();\r\n        if (val.contains(\",\")) {\r\n            return point.resetFromString(val, ignoreZValue);\r\n        } else {\r\n            return parseGeoHash(point, val, effectivePoint);\r\n        }\r\n    } else {\r\n        throw new ElasticsearchParseException(\"geo_point expected\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Settings.getAsList",
	"Comment": "the values associated with a setting key as an immutable list.it will also automatically load a comma separated list under the settingprefix and merge withthe numbered format.",
	"Method": "List<String> getAsList(String key,List<String> getAsList,String key,List<String> defaultValue,List<String> getAsList,String key,List<String> defaultValue,Boolean commaDelimited){\r\n    List<String> result = new ArrayList();\r\n    final Object valueFromPrefix = settings.get(key);\r\n    if (valueFromPrefix != null) {\r\n        if (valueFromPrefix instanceof List) {\r\n            return Collections.unmodifiableList((List<String>) valueFromPrefix);\r\n        } else if (commaDelimited) {\r\n            String[] strings = Strings.splitStringByCommaToArray(get(key));\r\n            if (strings.length > 0) {\r\n                for (String string : strings) {\r\n                    result.add(string.trim());\r\n                }\r\n            }\r\n        } else {\r\n            result.add(get(key).trim());\r\n        }\r\n    }\r\n    if (result.isEmpty()) {\r\n        return defaultValue;\r\n    }\r\n    return Collections.unmodifiableList(result);\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.IngestDocument.executePipeline",
	"Comment": "executes the given pipeline with for this document unless the pipeline has already been executedfor this document.",
	"Method": "IngestDocument executePipeline(Pipeline pipeline){\r\n    try {\r\n        if (this.executedPipelines.add(pipeline) == false) {\r\n            throw new IllegalStateException(\"Cycle detected for pipeline: \" + pipeline.getId());\r\n        }\r\n        return pipeline.execute(this);\r\n    } finally {\r\n        executedPipelines.remove(pipeline);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.blobstore.BlobStoreRepository.isCompress",
	"Comment": "returns true if metadata and snapshot files should be compressed",
	"Method": "boolean isCompress(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.openEngineAndRecoverFromTranslog",
	"Comment": "opens the engine on top of the existing lucene engine and translog.operations from the translog will be replayed to bring lucene up to date.",
	"Method": "void openEngineAndRecoverFromTranslog(){\r\n    final RecoveryState.Translog translogRecoveryStats = recoveryState.getTranslog();\r\n    final Engine.TranslogRecoveryRunner translogRecoveryRunner = (engine, snapshot) -> {\r\n        translogRecoveryStats.totalOperations(snapshot.totalOperations());\r\n        translogRecoveryStats.totalOperationsOnStart(snapshot.totalOperations());\r\n        return runTranslogRecovery(engine, snapshot, Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY, translogRecoveryStats::incrementRecoveredOperations);\r\n    };\r\n    innerOpenEngineAndTranslog();\r\n    final Engine engine = getEngine();\r\n    engine.initializeMaxSeqNoOfUpdatesOrDeletes();\r\n    engine.recoverFromTranslog(translogRecoveryRunner, Long.MAX_VALUE);\r\n}"
}, {
	"Path": "org.apache.harmony.tests.java.util.LinkedHashMapTest.setUp",
	"Comment": "sets up the fixture, for example, open a network connection. this methodis called before a test is executed.",
	"Method": "void setUp(){\r\n    objArray = new Object[hmSize];\r\n    objArray2 = new Object[hmSize];\r\n    for (int i = 0; i < objArray.length; i++) {\r\n        objArray[i] = new Integer(i);\r\n        objArray2[i] = objArray[i].toString();\r\n    }\r\n    hm = new LinkedHashMap();\r\n    for (int i = 0; i < objArray.length; i++) hm.put(objArray2[i], objArray[i]);\r\n    hm.put(\"test\", null);\r\n    hm.put(null, \"test\");\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.Key.withoutAttributes",
	"Comment": "returns this key without annotation attributes, i.e. with only theannotation type.",
	"Method": "Key<T> withoutAttributes(AnnotationStrategy withoutAttributes,AnnotationStrategy withoutAttributes,AnnotationStrategy withoutAttributes){\r\n    return new Key(typeLiteral, annotationStrategy.withoutAttributes());\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.dvparser.DVModel.printParameterType",
	"Comment": "prints to stdout the type and key for the given location in the parameter stack",
	"Method": "void printParameterType(int pos,PrintStream out){\r\n    int originalPos = pos;\r\n    Pair<String, String> binary = indexToBinaryTransform(pos);\r\n    if (binary != null) {\r\n        pos = pos % binaryTransformSize;\r\n        out.println(\"Entry \" + originalPos + \" is entry \" + pos + \" of binary transform \" + binary.first() + \":\" + binary.second());\r\n        return;\r\n    }\r\n    String unary = indexToUnaryTransform(pos);\r\n    if (unary != null) {\r\n        pos = (pos - numBinaryMatrices * binaryTransformSize) % unaryTransformSize;\r\n        out.println(\"Entry \" + originalPos + \" is entry \" + pos + \" of unary transform \" + unary);\r\n        return;\r\n    }\r\n    binary = indexToBinaryScore(pos);\r\n    if (binary != null) {\r\n        pos = (pos - numBinaryMatrices * binaryTransformSize - numUnaryMatrices * unaryTransformSize) % binaryScoreSize;\r\n        out.println(\"Entry \" + originalPos + \" is entry \" + pos + \" of binary score \" + binary.first() + \":\" + binary.second());\r\n        return;\r\n    }\r\n    unary = indexToUnaryScore(pos);\r\n    if (unary != null) {\r\n        pos = (pos - (numBinaryMatrices * (binaryTransformSize + binaryScoreSize)) - numUnaryMatrices * unaryTransformSize) % unaryScoreSize;\r\n        out.println(\"Entry \" + originalPos + \" is entry \" + pos + \" of unary score \" + unary);\r\n        return;\r\n    }\r\n    out.println(\"Index \" + originalPos + \" unknown\");\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.get.GetIndexRequest.includeDefaults",
	"Comment": "whether to return all default settings for each of the indices.",
	"Method": "GetIndexRequest includeDefaults(boolean includeDefaults,boolean includeDefaults){\r\n    return includeDefaults;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.AbstractTreebankParserParams.setEvaluateGrammaticalFunctions",
	"Comment": "sets whether to consider grammatical functions in evaluation",
	"Method": "void setEvaluateGrammaticalFunctions(boolean evalGFs){\r\n    this.evalGF = evalGFs;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.TregexPoweredTreebankParserParams.addFeature",
	"Comment": "enable an annotation feature. if the provided feature has alreadybeen enabled, this method does nothing.",
	"Method": "void addFeature(String featureName){\r\n    if (!annotations.containsKey(featureName))\r\n        throw new IllegalArgumentException(\"Invalid feature name '\" + featureName + \"'\");\r\n    if (!annotationPatterns.containsKey(featureName))\r\n        throw new RuntimeException(\"Compiled patterns out of sync with annotations data structure;\" + \"did you call compileAnnotations?\");\r\n    features.add(featureName);\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.inference.TableFactor.marginalize",
	"Comment": "marginalizes out a variable by applying an associative join operation for each possible assignment to themarginalized variable.",
	"Method": "TableFactor marginalize(int variable,double startingValue,BiFunction<Integer, int[], BiFunction<Double, Double, Double>> curriedFoldr){\r\n    assert (getDimensions().length > 1);\r\n    List<Integer> resultDomain = new ArrayList();\r\n    for (int n : neighborIndices) {\r\n        if (n != variable) {\r\n            resultDomain.add(n);\r\n        }\r\n    }\r\n    int[] resultNeighborIndices = new int[resultDomain.size()];\r\n    int[] resultDimensions = new int[resultNeighborIndices.length];\r\n    for (int i = 0; i < resultDomain.size(); i++) {\r\n        int var = resultDomain.get(i);\r\n        resultNeighborIndices[i] = var;\r\n        resultDimensions[i] = getVariableSize(var);\r\n    }\r\n    TableFactor result = new TableFactor(resultNeighborIndices, resultDimensions);\r\n    int[] mapping = new int[neighborIndices.length];\r\n    for (int i = 0; i < neighborIndices.length; i++) {\r\n        mapping[i] = resultDomain.indexOf(neighborIndices[i]);\r\n    }\r\n    for (int[] assignment : result) {\r\n        result.setAssignmentLogValue(assignment, startingValue);\r\n    }\r\n    int[] resultAssignment = new int[result.neighborIndices.length];\r\n    int marginalizedVariableValue = 0;\r\n    Iterator<int[]> fastPassByReferenceIterator = fastPassByReferenceIterator();\r\n    int[] assignment = fastPassByReferenceIterator.next();\r\n    while (true) {\r\n        for (int i = 0; i < assignment.length; i++) {\r\n            if (mapping[i] != -1)\r\n                resultAssignment[mapping[i]] = assignment[i];\r\n            else\r\n                marginalizedVariableValue = assignment[i];\r\n        }\r\n        result.setAssignmentLogValue(resultAssignment, curriedFoldr.apply(marginalizedVariableValue, resultAssignment).apply(result.getAssignmentLogValue(resultAssignment), getAssignmentLogValue(assignment)));\r\n        if (fastPassByReferenceIterator.hasNext())\r\n            fastPassByReferenceIterator.next();\r\n        else\r\n            break;\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequestBuilder.setPatterns",
	"Comment": "sets the match expression that will be used to match on indices created.",
	"Method": "PutIndexTemplateRequestBuilder setPatterns(List<String> indexPatterns){\r\n    request.patterns(indexPatterns);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequestBuilder.setRouting",
	"Comment": "the routing values to control the shards that the search will be executed on.",
	"Method": "SearchRequestBuilder setRouting(String routing,SearchRequestBuilder setRouting,String routing){\r\n    request.routing(routing);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.ReaderIteratorFactory.add",
	"Comment": "adds an object to the underlying collection ofinput sources.",
	"Method": "boolean add(Object o){\r\n    return this.c.add(o);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.UnaryGrammar.readData",
	"Comment": "populates data in this unarygrammar from a character stream.",
	"Method": "void readData(BufferedReader in){\r\n    String line;\r\n    int lineNum = 1;\r\n    line = in.readLine();\r\n    while (line != null && line.length() > 0) {\r\n        try {\r\n            addRule(new UnaryRule(line, index));\r\n        } catch (Exception e) {\r\n            throw new IOException(\"Error on line \" + lineNum);\r\n        }\r\n        lineNum++;\r\n        line = in.readLine();\r\n    }\r\n    purgeRules();\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoriesService.unregisterRepository",
	"Comment": "unregisters repository in the clusterthis method can be only called on the master node. it removes repository information from cluster metadata.",
	"Method": "void unregisterRepository(UnregisterRepositoryRequest request,ActionListener<ClusterStateUpdateResponse> listener){\r\n    clusterService.submitStateUpdateTask(request.cause, new AckedClusterStateUpdateTask<ClusterStateUpdateResponse>(request, listener) {\r\n        @Override\r\n        protected ClusterStateUpdateResponse newResponse(boolean acknowledged) {\r\n            return new ClusterStateUpdateResponse(acknowledged);\r\n        }\r\n        @Override\r\n        public ClusterState execute(ClusterState currentState) {\r\n            ensureRepositoryNotInUse(currentState, request.name);\r\n            MetaData metaData = currentState.metaData();\r\n            MetaData.Builder mdBuilder = MetaData.builder(currentState.metaData());\r\n            RepositoriesMetaData repositories = metaData.custom(RepositoriesMetaData.TYPE);\r\n            if (repositories != null && repositories.repositories().size() > 0) {\r\n                List<RepositoryMetaData> repositoriesMetaData = new ArrayList(repositories.repositories().size());\r\n                boolean changed = false;\r\n                for (RepositoryMetaData repositoryMetaData : repositories.repositories()) {\r\n                    if (Regex.simpleMatch(request.name, repositoryMetaData.name())) {\r\n                        logger.info(\"delete repository [{}]\", repositoryMetaData.name());\r\n                        changed = true;\r\n                    } else {\r\n                        repositoriesMetaData.add(repositoryMetaData);\r\n                    }\r\n                }\r\n                if (changed) {\r\n                    repositories = new RepositoriesMetaData(repositoriesMetaData);\r\n                    mdBuilder.putCustom(RepositoriesMetaData.TYPE, repositories);\r\n                    return ClusterState.builder(currentState).metaData(mdBuilder).build();\r\n                }\r\n            }\r\n            if (Regex.isMatchAllPattern(request.name)) {\r\n                return currentState;\r\n            }\r\n            throw new RepositoryMissingException(request.name);\r\n        }\r\n        @Override\r\n        public boolean mustAck(DiscoveryNode discoveryNode) {\r\n            return discoveryNode.isMasterNode() || discoveryNode.isDataNode();\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoriesService.unregisterRepository",
	"Comment": "unregisters repository in the clusterthis method can be only called on the master node. it removes repository information from cluster metadata.",
	"Method": "void unregisterRepository(UnregisterRepositoryRequest request,ActionListener<ClusterStateUpdateResponse> listener){\r\n    return new ClusterStateUpdateResponse(acknowledged);\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoriesService.unregisterRepository",
	"Comment": "unregisters repository in the clusterthis method can be only called on the master node. it removes repository information from cluster metadata.",
	"Method": "void unregisterRepository(UnregisterRepositoryRequest request,ActionListener<ClusterStateUpdateResponse> listener){\r\n    ensureRepositoryNotInUse(currentState, request.name);\r\n    MetaData metaData = currentState.metaData();\r\n    MetaData.Builder mdBuilder = MetaData.builder(currentState.metaData());\r\n    RepositoriesMetaData repositories = metaData.custom(RepositoriesMetaData.TYPE);\r\n    if (repositories != null && repositories.repositories().size() > 0) {\r\n        List<RepositoryMetaData> repositoriesMetaData = new ArrayList(repositories.repositories().size());\r\n        boolean changed = false;\r\n        for (RepositoryMetaData repositoryMetaData : repositories.repositories()) {\r\n            if (Regex.simpleMatch(request.name, repositoryMetaData.name())) {\r\n                logger.info(\"delete repository [{}]\", repositoryMetaData.name());\r\n                changed = true;\r\n            } else {\r\n                repositoriesMetaData.add(repositoryMetaData);\r\n            }\r\n        }\r\n        if (changed) {\r\n            repositories = new RepositoriesMetaData(repositoriesMetaData);\r\n            mdBuilder.putCustom(RepositoriesMetaData.TYPE, repositories);\r\n            return ClusterState.builder(currentState).metaData(mdBuilder).build();\r\n        }\r\n    }\r\n    if (Regex.isMatchAllPattern(request.name)) {\r\n        return currentState;\r\n    }\r\n    throw new RepositoryMissingException(request.name);\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoriesService.unregisterRepository",
	"Comment": "unregisters repository in the clusterthis method can be only called on the master node. it removes repository information from cluster metadata.",
	"Method": "void unregisterRepository(UnregisterRepositoryRequest request,ActionListener<ClusterStateUpdateResponse> listener){\r\n    return discoveryNode.isMasterNode() || discoveryNode.isDataNode();\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.FactoredLexicon.treebankToLexiconEvents",
	"Comment": "convert a treebank to factored lexicon events for fast iteration in theoptimizer.",
	"Method": "List<FactoredLexiconEvent> treebankToLexiconEvents(List<Tree> treebank,FactoredLexicon lexicon){\r\n    List<FactoredLexiconEvent> events = new ArrayList(70000);\r\n    for (Tree tree : treebank) {\r\n        List<Label> yield = tree.yield();\r\n        List<Label> preterm = tree.preTerminalYield();\r\n        assert yield.size() == preterm.size();\r\n        int yieldLen = yield.size();\r\n        for (int i = 0; i < yieldLen; ++i) {\r\n            String tag = preterm.get(i).value();\r\n            int tagId = lexicon.tagIndex.indexOf(tag);\r\n            String word = yield.get(i).value();\r\n            int wordId = lexicon.wordIndex.indexOf(word);\r\n            if (tagId < 0) {\r\n                log.info(\"Discarding training example: \" + word + \" \" + tag);\r\n                continue;\r\n            }\r\n            String featureStr = ((CoreLabel) yield.get(i)).originalText();\r\n            Pair<String, String> lemmaMorph = MorphoFeatureSpecification.splitMorphString(word, featureStr);\r\n            String lemma = lemmaMorph.first();\r\n            String richTag = lemmaMorph.second();\r\n            String reducedTag = lexicon.morphoSpec.strToFeatures(richTag).toString();\r\n            reducedTag = reducedTag.length() == 0 ? NO_MORPH_ANALYSIS : reducedTag;\r\n            int lemmaId = lexicon.wordIndex.indexOf(lemma);\r\n            int morphId = lexicon.morphIndex.indexOf(reducedTag);\r\n            FactoredLexiconEvent event = new FactoredLexiconEvent(wordId, tagId, lemmaId, morphId, i, word, featureStr);\r\n            events.add(event);\r\n        }\r\n    }\r\n    return events;\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoryData.getSnapshots",
	"Comment": "returns an immutable collection of the snapshot ids for the snapshots that contain the given index.",
	"Method": "Set<SnapshotId> getSnapshots(IndexId indexId){\r\n    Set<SnapshotId> snapshotIds = indexSnapshots.get(indexId);\r\n    if (snapshotIds == null) {\r\n        throw new IllegalArgumentException(\"unknown snapshot index \" + indexId);\r\n    }\r\n    return snapshotIds;\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.NumberRangeFileFilter.accept",
	"Comment": "checks whether a file satisfies the number range selection filter.",
	"Method": "boolean accept(File file){\r\n    if (file.isDirectory()) {\r\n        return recursively;\r\n    } else {\r\n        String filename = file.getName();\r\n        int k = filename.length() - 1;\r\n        char c = filename.charAt(k);\r\n        while (k >= 0 && (c < '0' || c > '9')) {\r\n            k--;\r\n            if (k >= 0) {\r\n                c = filename.charAt(k);\r\n            }\r\n        }\r\n        if (k < 0) {\r\n            return false;\r\n        }\r\n        int j = k;\r\n        c = filename.charAt(j);\r\n        while (j >= 0 && (c >= '0' && c <= '9')) {\r\n            j--;\r\n            if (j >= 0) {\r\n                c = filename.charAt(j);\r\n            }\r\n        }\r\n        j++;\r\n        k++;\r\n        String theNumber = filename.substring(j, k);\r\n        int number = Integer.parseInt(theNumber);\r\n        return (number >= minimum) && (number <= maximum);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.master.MasterNodeOperationRequestBuilder.setMasterNodeTimeout",
	"Comment": "sets the master node timeout in case the master has not yet been discovered.",
	"Method": "RequestBuilder setMasterNodeTimeout(TimeValue timeout,RequestBuilder setMasterNodeTimeout,String timeout){\r\n    request.masterNodeTimeout(timeout);\r\n    return (RequestBuilder) this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.TreeBinarizer.markovInsideBinarizeLocalTreeNew",
	"Comment": "uses tail recursion. the tree t that is passed never changes, only the indices left and right do.",
	"Method": "Tree markovInsideBinarizeLocalTreeNew(Tree t,int headLoc,int left,int right,boolean starting){\r\n    Tree result;\r\n    Tree[] children = t.children();\r\n    if (starting) {\r\n        if (left == headLoc && right == headLoc) {\r\n            return t;\r\n        }\r\n        if (noRebinarization && children.length == 2) {\r\n            return t;\r\n        }\r\n        if (unaryAtTop) {\r\n            result = tf.newTreeNode(t.label(), Collections.singletonList(markovInsideBinarizeLocalTreeNew(t, headLoc, left, right, false)));\r\n            return result;\r\n        }\r\n    }\r\n    List<Tree> newChildren = null;\r\n    if (left == headLoc && right == headLoc) {\r\n        newChildren = Collections.singletonList(children[headLoc]);\r\n    } else if (left < headLoc) {\r\n        newChildren = new ArrayList(2);\r\n        newChildren.add(children[left]);\r\n        newChildren.add(markovInsideBinarizeLocalTreeNew(t, headLoc, left + 1, right, false));\r\n    } else if (right > headLoc) {\r\n        newChildren = new ArrayList(2);\r\n        newChildren.add(markovInsideBinarizeLocalTreeNew(t, headLoc, left, right - 1, false));\r\n        newChildren.add(children[right]);\r\n    } else {\r\n        log.info(\"Uh-oh, bad parameters passed to markovInsideBinarizeLocalTree\");\r\n    }\r\n    Label label;\r\n    if (starting) {\r\n        label = t.label();\r\n    } else {\r\n        label = makeSyntheticLabel(t, left, right, headLoc, markovOrder);\r\n    }\r\n    if (doSelectiveSplit) {\r\n        double stateCount = stateCounter.getCount(label.value());\r\n        if (stateCount < selectiveSplitThreshold) {\r\n            if (starting && !unaryAtTop) {\r\n                label = t.label();\r\n            } else {\r\n                label = makeSyntheticLabel(t, left, right, headLoc, markovOrder - 1);\r\n            }\r\n        }\r\n    } else {\r\n        stateCounter.incrementCount(label.value(), 1.0);\r\n    }\r\n    result = tf.newTreeNode(label, newChildren);\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.shards.ClusterSearchShardsRequestBuilder.setRouting",
	"Comment": "the routing values to control the shards that the search will be executed on.",
	"Method": "ClusterSearchShardsRequestBuilder setRouting(String routing,ClusterSearchShardsRequestBuilder setRouting,String routing){\r\n    request.routing(routing);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexService.clearCaches",
	"Comment": "clears the caches for the given shard id if the shard is still allocated on this node",
	"Method": "boolean clearCaches(boolean queryCache,boolean fieldDataCache,String fields){\r\n    boolean clearedAtLeastOne = false;\r\n    if (queryCache) {\r\n        clearedAtLeastOne = true;\r\n        indexCache.query().clear(\"api\");\r\n    }\r\n    if (fieldDataCache) {\r\n        clearedAtLeastOne = true;\r\n        if (fields.length == 0) {\r\n            indexFieldData.clear();\r\n        } else {\r\n            for (String field : fields) {\r\n                indexFieldData.clearField(field);\r\n            }\r\n        }\r\n    }\r\n    if (clearedAtLeastOne == false) {\r\n        if (fields.length == 0) {\r\n            indexCache.clear(\"api\");\r\n            indexFieldData.clear();\r\n        } else {\r\n            for (String field : fields) {\r\n                indexFieldData.clearField(field);\r\n            }\r\n        }\r\n    }\r\n    return clearedAtLeastOne;\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.PluginsService.findBundles",
	"Comment": "searches subdirectories under the given directory for plugin directories",
	"Method": "Set<Bundle> findBundles(Path directory,String type){\r\n    final Set<Bundle> bundles = new HashSet();\r\n    for (final Path plugin : findPluginDirs(directory)) {\r\n        final Bundle bundle = readPluginBundle(bundles, plugin, type);\r\n        bundles.add(bundle);\r\n    }\r\n    return bundles;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.MappedFieldType.checkCompatibility",
	"Comment": "checks for any conflicts between this field type and other.if strict is true, all properties must be equal.otherwise, only properties which must never change in an index are checked.",
	"Method": "void checkCompatibility(MappedFieldType other,List<String> conflicts){\r\n    checkTypeName(other);\r\n    boolean indexed = indexOptions() != IndexOptions.NONE;\r\n    boolean mergeWithIndexed = other.indexOptions() != IndexOptions.NONE;\r\n    if (indexed != mergeWithIndexed) {\r\n        conflicts.add(\"mapper [\" + name() + \"] has different [index] values\");\r\n    }\r\n    if (stored() != other.stored()) {\r\n        conflicts.add(\"mapper [\" + name() + \"] has different [store] values\");\r\n    }\r\n    if (hasDocValues() != other.hasDocValues()) {\r\n        conflicts.add(\"mapper [\" + name() + \"] has different [doc_values] values\");\r\n    }\r\n    if (omitNorms() && !other.omitNorms()) {\r\n        conflicts.add(\"mapper [\" + name() + \"] has different [norms] values, cannot change from disable to enabled\");\r\n    }\r\n    if (storeTermVectors() != other.storeTermVectors()) {\r\n        conflicts.add(\"mapper [\" + name() + \"] has different [store_term_vector] values\");\r\n    }\r\n    if (storeTermVectorOffsets() != other.storeTermVectorOffsets()) {\r\n        conflicts.add(\"mapper [\" + name() + \"] has different [store_term_vector_offsets] values\");\r\n    }\r\n    if (storeTermVectorPositions() != other.storeTermVectorPositions()) {\r\n        conflicts.add(\"mapper [\" + name() + \"] has different [store_term_vector_positions] values\");\r\n    }\r\n    if (storeTermVectorPayloads() != other.storeTermVectorPayloads()) {\r\n        conflicts.add(\"mapper [\" + name() + \"] has different [store_term_vector_payloads] values\");\r\n    }\r\n    if (indexAnalyzer() == null || \"default\".equals(indexAnalyzer().name())) {\r\n        if (other.indexAnalyzer() != null && \"default\".equals(other.indexAnalyzer().name()) == false) {\r\n            conflicts.add(\"mapper [\" + name() + \"] has different [analyzer]\");\r\n        }\r\n    } else if (other.indexAnalyzer() == null || \"default\".equals(other.indexAnalyzer().name())) {\r\n        conflicts.add(\"mapper [\" + name() + \"] has different [analyzer]\");\r\n    } else if (indexAnalyzer().name().equals(other.indexAnalyzer().name()) == false) {\r\n        conflicts.add(\"mapper [\" + name() + \"] has different [analyzer]\");\r\n    }\r\n    if (Objects.equals(similarity(), other.similarity()) == false) {\r\n        conflicts.add(\"mapper [\" + name() + \"] has different [similarity]\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BackoffPolicy.wrap",
	"Comment": "wraps the backoff policy in one that calls a method every time a new backoff is taken from the policy.",
	"Method": "BackoffPolicy wrap(BackoffPolicy delegate,Runnable onBackoff){\r\n    return new WrappedBackoffPolicy(delegate, onBackoff);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.NamedDiff.getMinimalSupportedVersion",
	"Comment": "the minimal version of the recipient this custom object can be sent to",
	"Method": "Version getMinimalSupportedVersion(){\r\n    return Version.CURRENT.minimumIndexCompatibilityVersion();\r\n}"
}, {
	"Path": "org.elasticsearch.index.recovery.RecoveryStats.currentAsTarget",
	"Comment": "number of ongoing recoveries for which a shard serves as a target",
	"Method": "int currentAsTarget(){\r\n    return currentAsTarget.get();\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.AbstractSearchAsyncAction.start",
	"Comment": "this is the main entry point for a search. this method starts the search execution of the initial phase.",
	"Method": "void start(){\r\n    if (getNumShards() == 0) {\r\n        listener.onResponse(new SearchResponse(InternalSearchResponse.empty(), null, 0, 0, 0, buildTookInMillis(), ShardSearchFailure.EMPTY_ARRAY, clusters));\r\n        return;\r\n    }\r\n    executePhase(this);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.BaseLexicon.addTagging",
	"Comment": "adds the tagging with count to the data structures in this lexicon.",
	"Method": "void addTagging(boolean seen,IntTaggedWord itw,double count){\r\n    if (seen) {\r\n        seenCounter.incrementCount(itw, count);\r\n        if (itw.tag() == nullTag) {\r\n            words.add(itw);\r\n        } else if (itw.word() == nullWord) {\r\n            tags.add(itw);\r\n        } else {\r\n        }\r\n    } else {\r\n        uwModel.addTagging(seen, itw, count);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TruncateTranslogAction.writeEmptyTranslog",
	"Comment": "write a translog containing the given translog uuid to the given location. returns the number of bytes written.",
	"Method": "int writeEmptyTranslog(Path filename,String translogUUID){\r\n    try (FileChannel fc = FileChannel.open(filename, StandardOpenOption.WRITE, StandardOpenOption.CREATE_NEW)) {\r\n        TranslogHeader header = new TranslogHeader(translogUUID, TranslogHeader.UNKNOWN_PRIMARY_TERM);\r\n        header.write(fc);\r\n        return header.sizeInBytes();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.DocumentParser.parseCopyFields",
	"Comment": "creates instances of the fields that the current field should be copied to",
	"Method": "void parseCopyFields(ParseContext context,List<String> copyToFields){\r\n    if (!context.isWithinCopyTo() && copyToFields.isEmpty() == false) {\r\n        context = context.createCopyToContext();\r\n        for (String field : copyToFields) {\r\n            ParseContext.Document targetDoc = null;\r\n            for (ParseContext.Document doc = context.doc(); doc != null; doc = doc.getParent()) {\r\n                if (field.startsWith(doc.getPrefix())) {\r\n                    targetDoc = doc;\r\n                    break;\r\n                }\r\n            }\r\n            assert targetDoc != null;\r\n            final ParseContext copyToContext;\r\n            if (targetDoc == context.doc()) {\r\n                copyToContext = context;\r\n            } else {\r\n                copyToContext = context.switchDoc(targetDoc);\r\n            }\r\n            parseCopy(field, copyToContext);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.ObjectBank.add",
	"Comment": "unsupported operation.if you wish to add a new data source,do so in the underlying readeriteratorfactory",
	"Method": "boolean add(E o){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "android.util.SparseLongArray.get",
	"Comment": "gets the long mapped from the specified key, or the specified valueif no such mapping has been made.",
	"Method": "long get(int key,long get,int key,long valueIfKeyNotFound){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i < 0) {\r\n        return valueIfKeyNotFound;\r\n    } else {\r\n        return mValues[i];\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.TaggedWordFactory.newLabel",
	"Comment": "create a new taggedword label, where the label isformed fromthe label object passed in.depending on what fieldseach label has, other things will be null.",
	"Method": "Label newLabel(String labelStr,Label newLabel,String labelStr,int options,Label newLabel,Label oldLabel){\r\n    return new TaggedWord(oldLabel);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequestBuilder.setMinScore",
	"Comment": "sets the minimum score below which docs will be filtered out.",
	"Method": "SearchRequestBuilder setMinScore(float minScore){\r\n    sourceBuilder().minScore(minScore);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.OpenIE.clausesInSentence",
	"Comment": "find the clauses in a sentence.this runs the clause splitting component of the openie system only.",
	"Method": "List<SentenceFragment> clausesInSentence(SemanticGraph tree,boolean assumedTruth,List<SentenceFragment> clausesInSentence,CoreMap sentence){\r\n    return clausesInSentence(sentence.get(SemanticGraphCoreAnnotations.EnhancedPlusPlusDependenciesAnnotation.class), true);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.SplittingGrammarExtractor.splitStateCounts",
	"Comment": "splits the state counts.root states and the boundary tag do notget their counts increased, and all others are doubled.betasand transition weights are handled later.",
	"Method": "void splitStateCounts(){\r\n    IntCounter<String> newStateSplitCounts = new IntCounter();\r\n    newStateSplitCounts.addAll(stateSplitCounts);\r\n    newStateSplitCounts.addAll(stateSplitCounts);\r\n    for (String root : startSymbols) {\r\n        if (newStateSplitCounts.getCount(root) > 1) {\r\n            newStateSplitCounts.setCount(root, 1);\r\n        }\r\n    }\r\n    if (newStateSplitCounts.getCount(Lexicon.BOUNDARY_TAG) > 1) {\r\n        newStateSplitCounts.setCount(Lexicon.BOUNDARY_TAG, 1);\r\n    }\r\n    stateSplitCounts = newStateSplitCounts;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDatum.addLabel",
	"Comment": "adds the given label to the list of labels for this datum if it is notnull.",
	"Method": "void addLabel(LabelType label){\r\n    if (label != null) {\r\n        labels.add(label);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.DocumentParser.createExistingMapperUpdate",
	"Comment": "creates an update for intermediate object mappers that are not on the stack, but parents of newmapper.",
	"Method": "ObjectMapper createExistingMapperUpdate(List<ObjectMapper> parentMappers,String[] nameParts,int i,DocumentMapper docMapper,Mapper newMapper){\r\n    String updateParentName = nameParts[i];\r\n    final ObjectMapper lastParent = parentMappers.get(parentMappers.size() - 1);\r\n    if (parentMappers.size() > 1) {\r\n        updateParentName = lastParent.name() + '.' + nameParts[i];\r\n    }\r\n    ObjectMapper updateParent = docMapper.objectMappers().get(updateParentName);\r\n    assert updateParent != null : updateParentName + \" doesn't exist\";\r\n    return createUpdate(updateParent, nameParts, i + 1, newMapper);\r\n}"
}, {
	"Path": "org.elasticsearch.indices.recovery.RecoveriesCollection.resetRecovery",
	"Comment": "resets the recovery and performs a recovery restart on the currently recovering index shard",
	"Method": "RecoveryTarget resetRecovery(long recoveryId,TimeValue activityTimeout){\r\n    RecoveryTarget oldRecoveryTarget = null;\r\n    final RecoveryTarget newRecoveryTarget;\r\n    try {\r\n        synchronized (onGoingRecoveries) {\r\n            oldRecoveryTarget = onGoingRecoveries.remove(recoveryId);\r\n            if (oldRecoveryTarget == null) {\r\n                return null;\r\n            }\r\n            newRecoveryTarget = oldRecoveryTarget.retryCopy();\r\n            startRecoveryInternal(newRecoveryTarget, activityTimeout);\r\n        }\r\n        boolean successfulReset = oldRecoveryTarget.resetRecovery(newRecoveryTarget.cancellableThreads());\r\n        if (successfulReset) {\r\n            logger.trace(\"{} restarted recovery from {}, id [{}], previous id [{}]\", newRecoveryTarget.shardId(), newRecoveryTarget.sourceNode(), newRecoveryTarget.recoveryId(), oldRecoveryTarget.recoveryId());\r\n            return newRecoveryTarget;\r\n        } else {\r\n            logger.trace(\"{} recovery could not be reset as it is already cancelled, recovery from {}, id [{}], previous id [{}]\", newRecoveryTarget.shardId(), newRecoveryTarget.sourceNode(), newRecoveryTarget.recoveryId(), oldRecoveryTarget.recoveryId());\r\n            cancelRecovery(newRecoveryTarget.recoveryId(), \"recovery cancelled during reset\");\r\n            return null;\r\n        }\r\n    } catch (Exception e) {\r\n        oldRecoveryTarget.notifyListener(new RecoveryFailedException(oldRecoveryTarget.state(), \"failed to retry recovery\", e), true);\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.ExceptionsHelper.reThrowIfNotNull",
	"Comment": "throws the specified exception. if null if specified then true is returned.",
	"Method": "boolean reThrowIfNotNull(Throwable e){\r\n    if (e != null) {\r\n        if (e instanceof RuntimeException) {\r\n            throw (RuntimeException) e;\r\n        } else {\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.apache.dubbo.registry.dubbo.RegistryDirectoryTest.test_Notified_acceptProtocol0",
	"Comment": "test the matching of protocol and select only the matched protocol for refer",
	"Method": "void test_Notified_acceptProtocol0(){\r\n    URL errorPathUrl = URL.valueOf(\"notsupport:/xxx?refer=\" + URL.encode(\"interface=\" + service));\r\n    RegistryDirectory registryDirectory = getRegistryDirectory(errorPathUrl);\r\n    List<URL> serviceUrls = new ArrayList<URL>();\r\n    URL dubbo1URL = URL.valueOf(\"dubbo://127.0.0.1:9098?lazy=true&methods=getXXX\");\r\n    URL dubbo2URL = URL.valueOf(\"injvm://127.0.0.1:9099?lazy=true&methods=getXXX\");\r\n    serviceUrls.add(dubbo1URL);\r\n    serviceUrls.add(dubbo2URL);\r\n    registryDirectory.notify(serviceUrls);\r\n    invocation = new RpcInvocation();\r\n    List<Invoker<DemoService>> invokers = registryDirectory.list(invocation);\r\n    Assert.assertEquals(2, invokers.size());\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.pascal.ISODateInstance.isYearCompatible",
	"Comment": "looks if the years for the two dates are compatible.this method does not consider ranges and uses only thestart date.",
	"Method": "boolean isYearCompatible(String date1,String date2,boolean isYearCompatible,ISODateInstance other){\r\n    if (this.isUnparseable() || other.isUnparseable()) {\r\n        return this.isoDate.equals(other.isoDate);\r\n    }\r\n    return isYearCompatible(isoDate, other.getDateString());\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.getBestParse",
	"Comment": "return the best parse of the sentence most recently parsed.this will be from the factored parser, if it was used and it succeededelse from the pcfg if it was used and succeed, else from the dependencyparser.",
	"Method": "Tree getBestParse(Tree getBestParse,boolean stripSubcat){\r\n    if (parseSkipped) {\r\n        return null;\r\n    }\r\n    if (bparser != null && parseSucceeded) {\r\n        Tree binaryTree = bparser.getBestParse();\r\n        Tree tree = debinarizer.transformTree(binaryTree);\r\n        if (op.nodePrune) {\r\n            NodePruner np = new NodePruner(pparser, debinarizer);\r\n            tree = np.prune(tree);\r\n        }\r\n        if (stripSubcat) {\r\n            tree = subcategoryStripper.transformTree(tree);\r\n        }\r\n        restoreOriginalWords(tree);\r\n        return tree;\r\n    } else if (pparser != null && pparser.hasParse() && fallbackToPCFG) {\r\n        return getBestPCFGParse();\r\n    } else if (dparser != null && dparser.hasParse()) {\r\n        return getBestDependencyParse(true);\r\n    } else {\r\n        throw new NoSuchParseException();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest.getConcreteIndex",
	"Comment": "returns a concrete index for this mapping or null if no concrete index is defined",
	"Method": "Index getConcreteIndex(){\r\n    return concreteIndex;\r\n}"
}, {
	"Path": "org.apache.harmony.tests.java.io.RandomAccessFileTest.tearDown",
	"Comment": "tears down the fixture, for example, close a network connection. thismethod is called after a test is executed.",
	"Method": "void tearDown(){\r\n    if (f.exists()) {\r\n        f.delete();\r\n    }\r\n    for (File f : filesToDelete) {\r\n        f.delete();\r\n    }\r\n    super.tearDown();\r\n}"
}, {
	"Path": "org.elasticsearch.indices.IndexingMemoryController.getIndexBufferRAMBytesUsed",
	"Comment": "returns how much heap this shard is using for its indexing buffer",
	"Method": "long getIndexBufferRAMBytesUsed(IndexShard shard){\r\n    return shard.getIndexBufferRAMBytesUsed();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.shards.IndicesShardStoreRequestBuilder.setIndicesOptions",
	"Comment": "specifies what type of requested indices to ignore and wildcard indices expressionsby default, expands wildcards to both open and closed indices",
	"Method": "IndicesShardStoreRequestBuilder setIndicesOptions(IndicesOptions indicesOptions){\r\n    request.indicesOptions(indicesOptions);\r\n    return this;\r\n}"
}, {
	"Path": "android.util.ArrayMap.removeAll",
	"Comment": "remove all keys in the array map that exist in the given collection.",
	"Method": "boolean removeAll(Collection<?> collection){\r\n    return MapCollections.removeAllHelper(this, collection);\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.ObjectMapper.mappingUpdate",
	"Comment": "build a mapping update with the provided sub mapping update.",
	"Method": "ObjectMapper mappingUpdate(Mapper mapper){\r\n    ObjectMapper mappingUpdate = clone();\r\n    mappingUpdate.mappers = new CopyOnWriteHashMap();\r\n    mappingUpdate.putMapper(mapper);\r\n    return mappingUpdate;\r\n}"
}, {
	"Path": "org.apache.dubbo.registry.dubbo.RegistryDirectoryTest.test_Notified_acceptProtocol2",
	"Comment": "test the matching of protocol and select only the matched protocol for refer",
	"Method": "void test_Notified_acceptProtocol2(){\r\n    URL errorPathUrl = URL.valueOf(\"notsupport:/xxx\");\r\n    errorPathUrl = errorPathUrl.addParameterAndEncoded(Constants.REFER_KEY, \"interface=\" + service + \"&protocol=dubbo,injvm\");\r\n    RegistryDirectory registryDirectory = getRegistryDirectory(errorPathUrl);\r\n    List<URL> serviceUrls = new ArrayList<URL>();\r\n    URL dubbo1URL = URL.valueOf(\"dubbo://127.0.0.1:9098?lazy=true&methods=getXXX\");\r\n    URL dubbo2URL = URL.valueOf(\"injvm://127.0.0.1:9099?lazy=true&methods=getXXX\");\r\n    serviceUrls.add(dubbo1URL);\r\n    serviceUrls.add(dubbo2URL);\r\n    registryDirectory.notify(serviceUrls);\r\n    invocation = new RpcInvocation();\r\n    List<Invoker<DemoService>> invokers = registryDirectory.list(invocation);\r\n    Assert.assertEquals(2, invokers.size());\r\n}"
}, {
	"Path": "org.apache.dubbo.registry.dubbo.RegistryDirectoryTest.test_Notified_acceptProtocol1",
	"Comment": "test the matching of protocol and select only the matched protocol for refer",
	"Method": "void test_Notified_acceptProtocol1(){\r\n    URL errorPathUrl = URL.valueOf(\"notsupport:/xxx\");\r\n    errorPathUrl = errorPathUrl.addParameterAndEncoded(Constants.REFER_KEY, \"interface=\" + service + \"&protocol=dubbo\");\r\n    RegistryDirectory registryDirectory = getRegistryDirectory(errorPathUrl);\r\n    List<URL> serviceUrls = new ArrayList<URL>();\r\n    URL dubbo1URL = URL.valueOf(\"dubbo://127.0.0.1:9098?lazy=true&methods=getXXX\");\r\n    URL dubbo2URL = URL.valueOf(\"injvm://127.0.0.1:9098?lazy=true&methods=getXXX\");\r\n    serviceUrls.add(dubbo1URL);\r\n    serviceUrls.add(dubbo2URL);\r\n    registryDirectory.notify(serviceUrls);\r\n    invocation = new RpcInvocation();\r\n    List<Invoker<DemoService>> invokers = registryDirectory.list(invocation);\r\n    Assert.assertEquals(1, invokers.size());\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.flush.SyncedFlushResponse.totalShards",
	"Comment": "total number shards, including replicas, both assigned and unassigned",
	"Method": "int totalShards(){\r\n    return shardCounts.total;\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.ActionPlugin.getRestHeaders",
	"Comment": "returns headers which should be copied through rest requests on to internal requests.",
	"Method": "Collection<String> getRestHeaders(){\r\n    return Collections.emptyList();\r\n}"
}, {
	"Path": "org.elasticsearch.index.get.GetResult.sourceRef",
	"Comment": "returns bytes reference, also un compress the source if needed.",
	"Method": "BytesReference sourceRef(){\r\n    if (source == null) {\r\n        return null;\r\n    }\r\n    try {\r\n        this.source = CompressorFactory.uncompressIfNeeded(this.source);\r\n        return this.source;\r\n    } catch (IOException e) {\r\n        throw new ElasticsearchParseException(\"failed to decompress source\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.Pipeline.getDescription",
	"Comment": "an optional description of what this pipeline is doing to the data gets processed by this pipeline.",
	"Method": "String getDescription(){\r\n    return description;\r\n}"
}, {
	"Path": "org.elasticsearch.monitor.os.OsProbe.getCgroupCpuAcctCpuStat",
	"Comment": "the cpu time statistics for all tasks in the elasticsearch control group.",
	"Method": "OsStats.Cgroup.CpuStat getCgroupCpuAcctCpuStat(String controlGroup){\r\n    final List<String> lines = readSysFsCgroupCpuAcctCpuStat(controlGroup);\r\n    long numberOfPeriods = -1;\r\n    long numberOfTimesThrottled = -1;\r\n    long timeThrottledNanos = -1;\r\n    for (final String line : lines) {\r\n        final String[] fields = line.split(\"\\\\s+\");\r\n        switch(fields[0]) {\r\n            case \"nr_periods\":\r\n                numberOfPeriods = Long.parseLong(fields[1]);\r\n                break;\r\n            case \"nr_throttled\":\r\n                numberOfTimesThrottled = Long.parseLong(fields[1]);\r\n                break;\r\n            case \"throttled_time\":\r\n                timeThrottledNanos = Long.parseLong(fields[1]);\r\n                break;\r\n        }\r\n    }\r\n    assert numberOfPeriods != -1;\r\n    assert numberOfTimesThrottled != -1;\r\n    assert timeThrottledNanos != -1;\r\n    return new OsStats.Cgroup.CpuStat(numberOfPeriods, numberOfTimesThrottled, timeThrottledNanos);\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoryData.removeSnapshot",
	"Comment": "remove a snapshot and remove any indices that no longer exist in the repository due to the deletion of the snapshot.",
	"Method": "RepositoryData removeSnapshot(SnapshotId snapshotId){\r\n    Map<String, SnapshotId> newSnapshotIds = snapshotIds.values().stream().filter(id -> !snapshotId.equals(id)).collect(Collectors.toMap(SnapshotId::getUUID, Function.identity()));\r\n    if (newSnapshotIds.size() == snapshotIds.size()) {\r\n        throw new ResourceNotFoundException(\"Attempting to remove non-existent snapshot [{}] from repository data\", snapshotId);\r\n    }\r\n    Map<String, SnapshotState> newSnapshotStates = new HashMap(snapshotStates);\r\n    newSnapshotStates.remove(snapshotId.getUUID());\r\n    Map<IndexId, Set<SnapshotId>> indexSnapshots = new HashMap();\r\n    for (final IndexId indexId : indices.values()) {\r\n        Set<SnapshotId> set;\r\n        Set<SnapshotId> snapshotIds = this.indexSnapshots.get(indexId);\r\n        assert snapshotIds != null;\r\n        if (snapshotIds.contains(snapshotId)) {\r\n            if (snapshotIds.size() == 1) {\r\n                continue;\r\n            }\r\n            set = new LinkedHashSet(snapshotIds);\r\n            set.remove(snapshotId);\r\n        } else {\r\n            set = snapshotIds;\r\n        }\r\n        indexSnapshots.put(indexId, set);\r\n    }\r\n    return new RepositoryData(genId, newSnapshotIds, newSnapshotStates, indexSnapshots, incompatibleSnapshotIds);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.IndexedWord.pseudoPosition",
	"Comment": "in most cases, this is just the index of the word.however, this should be the value used to sort nodes ina tree.",
	"Method": "double pseudoPosition(){\r\n    if (!Double.isNaN(pseudoPosition)) {\r\n        return pseudoPosition;\r\n    } else {\r\n        return (double) index();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.pendingInSync",
	"Comment": "whether the are shards blocking global checkpoint advancement. used by tests.",
	"Method": "boolean pendingInSync(){\r\n    assert primaryMode;\r\n    return pendingInSync.isEmpty() == false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.EnglishUnknownWordModel.getSignatureIndex",
	"Comment": "returns the index of the signature of the word numbered wordindex, wherethe signature is the string representation of unknown word features.",
	"Method": "int getSignatureIndex(int index,int sentencePosition,String word){\r\n    String uwSig = getSignature(word, sentencePosition);\r\n    int sig = wordIndex.addToIndex(uwSig);\r\n    if (DEBUG_UWM) {\r\n        log.info(\"Signature (\" + unknownLevel + \"): mapped \" + word + \" (\" + index + \") to \" + uwSig + \" (\" + sig + ')');\r\n    }\r\n    return sig;\r\n}"
}, {
	"Path": "android.os.SystemClock.uptimeMillis",
	"Comment": "returns milliseconds since boot, not counting time spent in deep sleep.",
	"Method": "long uptimeMillis(){\r\n    return elapsedRealtime();\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.LiveVersionMap.getAllCurrent",
	"Comment": "returns the current internal versions as a point in time snapshot",
	"Method": "Map<BytesRef, VersionValue> getAllCurrent(){\r\n    return maps.current.map;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.SpanishUnknownWordModel.getSignature",
	"Comment": "todo can add various signatures, setting the signature via options.",
	"Method": "String getSignature(String word,int loc){\r\n    final String BASE_LABEL = \"UNK\";\r\n    StringBuilder sb = new StringBuilder(BASE_LABEL);\r\n    switch(unknownLevel) {\r\n        case 1:\r\n            if (StringUtils.isNumeric(word)) {\r\n                sb.append('#');\r\n                break;\r\n            } else if (StringUtils.isPunct(word)) {\r\n                sb.append('!');\r\n                break;\r\n            }\r\n            sb.append(SpanishUnknownWordSignatures.conditionalSuffix(word));\r\n            sb.append(SpanishUnknownWordSignatures.imperfectSuffix(word));\r\n            sb.append(SpanishUnknownWordSignatures.infinitiveSuffix(word));\r\n            sb.append(SpanishUnknownWordSignatures.adverbSuffix(word));\r\n            if (sb.toString().equals(BASE_LABEL)) {\r\n                if (SpanishUnknownWordSignatures.hasVerbFirstPersonPluralSuffix(word)) {\r\n                    sb.append(\"-vb1p\");\r\n                } else if (SpanishUnknownWordSignatures.hasGerundSuffix(word)) {\r\n                    sb.append(\"-ger\");\r\n                } else if (word.endsWith(\"s\")) {\r\n                    sb.append(\"-s\");\r\n                }\r\n            }\r\n            if (unknownSuffixSize > 0 && sb.toString().equals(BASE_LABEL)) {\r\n                int min = word.length() < unknownSuffixSize ? word.length() : unknownSuffixSize;\r\n                sb.append('-').append(word.substring(word.length() - min));\r\n            }\r\n            char first = word.charAt(0);\r\n            if ((Character.isUpperCase(first) || Character.isTitleCase(first)) && !isUpperCase(word)) {\r\n                sb.append(\"-C\");\r\n            } else {\r\n                sb.append(\"-c\");\r\n            }\r\n            break;\r\n        default:\r\n            log.error(String.format(\"%s: Invalid unknown word signature! (%d)%n\", this.getClass().getName(), unknownLevel));\r\n    }\r\n    return sb.toString();\r\n}"
}, {
	"Path": "org.elasticsearch.ElasticsearchException.status",
	"Comment": "returns the rest status code associated with this exception.",
	"Method": "RestStatus status(){\r\n    Throwable cause = unwrapCause();\r\n    if (cause == this) {\r\n        return RestStatus.INTERNAL_SERVER_ERROR;\r\n    } else {\r\n        return ExceptionsHelper.status(cause);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices",
	"Comment": "translates the provided index expression into actual concrete indices, properly deduplicated.",
	"Method": "Index[] concreteIndices(ClusterState state,IndicesRequest request,Index[] concreteIndices,ClusterState state,IndicesOptions options,String indexExpressions,Index[] concreteIndices,ClusterState state,IndicesOptions options,long startTime,String indexExpressions,Index[] concreteIndices,Context context,String indexExpressions){\r\n    if (indexExpressions == null || indexExpressions.length == 0) {\r\n        indexExpressions = new String[] { MetaData.ALL };\r\n    }\r\n    MetaData metaData = context.getState().metaData();\r\n    IndicesOptions options = context.getOptions();\r\n    final boolean failClosed = options.forbidClosedIndices() && options.ignoreUnavailable() == false;\r\n    final boolean failNoIndices = indexExpressions.length == 1 ? !options.allowNoIndices() : !options.ignoreUnavailable();\r\n    List<String> expressions = Arrays.asList(indexExpressions);\r\n    for (ExpressionResolver expressionResolver : expressionResolvers) {\r\n        expressions = expressionResolver.resolve(context, expressions);\r\n    }\r\n    if (expressions.isEmpty()) {\r\n        if (!options.allowNoIndices()) {\r\n            IndexNotFoundException infe = new IndexNotFoundException((String) null);\r\n            infe.setResources(\"index_expression\", indexExpressions);\r\n            throw infe;\r\n        } else {\r\n            return Index.EMPTY_ARRAY;\r\n        }\r\n    }\r\n    final Set<Index> concreteIndices = new HashSet(expressions.size());\r\n    for (String expression : expressions) {\r\n        AliasOrIndex aliasOrIndex = metaData.getAliasAndIndexLookup().get(expression);\r\n        if (aliasOrIndex == null) {\r\n            if (failNoIndices) {\r\n                IndexNotFoundException infe = new IndexNotFoundException(expression);\r\n                infe.setResources(\"index_expression\", expression);\r\n                throw infe;\r\n            } else {\r\n                continue;\r\n            }\r\n        } else if (aliasOrIndex.isAlias() && context.getOptions().ignoreAliases()) {\r\n            if (failNoIndices) {\r\n                throw aliasesNotSupportedException(expression);\r\n            } else {\r\n                continue;\r\n            }\r\n        }\r\n        if (aliasOrIndex.isAlias() && context.isResolveToWriteIndex()) {\r\n            AliasOrIndex.Alias alias = (AliasOrIndex.Alias) aliasOrIndex;\r\n            IndexMetaData writeIndex = alias.getWriteIndex();\r\n            if (writeIndex == null) {\r\n                throw new IllegalArgumentException(\"no write index is defined for alias [\" + alias.getAliasName() + \"].\" + \" The write index may be explicitly disabled using is_write_index=false or the alias points to multiple\" + \" indices without one being designated as a write index\");\r\n            }\r\n            if (addIndex(writeIndex, context)) {\r\n                concreteIndices.add(writeIndex.getIndex());\r\n            }\r\n        } else {\r\n            if (aliasOrIndex.getIndices().size() > 1 && !options.allowAliasesToMultipleIndices()) {\r\n                String[] indexNames = new String[aliasOrIndex.getIndices().size()];\r\n                int i = 0;\r\n                for (IndexMetaData indexMetaData : aliasOrIndex.getIndices()) {\r\n                    indexNames[i++] = indexMetaData.getIndex().getName();\r\n                }\r\n                throw new IllegalArgumentException(\"Alias [\" + expression + \"] has more than one indices associated with it [\" + Arrays.toString(indexNames) + \"], can't execute a single index op\");\r\n            }\r\n            for (IndexMetaData index : aliasOrIndex.getIndices()) {\r\n                if (index.getState() == IndexMetaData.State.CLOSE) {\r\n                    if (failClosed) {\r\n                        throw new IndexClosedException(index.getIndex());\r\n                    } else {\r\n                        if (options.forbidClosedIndices() == false && addIndex(index, context)) {\r\n                            concreteIndices.add(index.getIndex());\r\n                        }\r\n                    }\r\n                } else if (index.getState() == IndexMetaData.State.OPEN) {\r\n                    if (addIndex(index, context)) {\r\n                        concreteIndices.add(index.getIndex());\r\n                    }\r\n                } else {\r\n                    throw new IllegalStateException(\"index state [\" + index.getState() + \"] not supported\");\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if (options.allowNoIndices() == false && concreteIndices.isEmpty()) {\r\n        IndexNotFoundException infe = new IndexNotFoundException((String) null);\r\n        infe.setResources(\"index_expression\", indexExpressions);\r\n        throw infe;\r\n    }\r\n    return concreteIndices.toArray(new Index[concreteIndices.size()]);\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.failShard",
	"Comment": "fails the shard and marks the shard store as corrupted ife is caused by index corruption",
	"Method": "void failShard(String reason,Exception e){\r\n    getEngine().failEngine(reason, e);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaData.isAllTypes",
	"Comment": "identifies whether the array containing type names given as argument refers to all typesthe empty or null array identifies all types",
	"Method": "boolean isAllTypes(String[] types){\r\n    return types == null || types.length == 0 || isExplicitAllType(types);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.AllocationId.cancelRelocation",
	"Comment": "creates a new allocation id representing a cancelled relocation.note that this is expected to be called on the allocation idshard",
	"Method": "AllocationId cancelRelocation(AllocationId allocationId){\r\n    assert allocationId.getRelocationId() != null;\r\n    return new AllocationId(allocationId.getId(), null);\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.set.Sets.difference",
	"Comment": "the relative complement, or difference, of the specified left and right set. namely, the resulting set contains all the elements thatare in the left set but not in the right set. neither input is mutated by this operation, an entirely new set is returned.",
	"Method": "Set<T> difference(Set<T> left,Set<T> right){\r\n    Objects.requireNonNull(left);\r\n    Objects.requireNonNull(right);\r\n    return left.stream().filter(k -> !right.contains(k)).collect(Collectors.toSet());\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoShapeQueryBuilder.indexedShapeRouting",
	"Comment": "sets the optional routing to the indexed shape that will be used in the query",
	"Method": "GeoShapeQueryBuilder indexedShapeRouting(String indexedShapeRouting,String indexedShapeRouting){\r\n    return indexedShapeRouting;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.ShardRouting.equalsIgnoringMetaData",
	"Comment": "returns true if the current routing is identical to the other routing in all but meta fields, i.e., unassigned info",
	"Method": "boolean equalsIgnoringMetaData(ShardRouting other){\r\n    if (primary != other.primary) {\r\n        return false;\r\n    }\r\n    if (shardId != null ? !shardId.equals(other.shardId) : other.shardId != null) {\r\n        return false;\r\n    }\r\n    if (currentNodeId != null ? !currentNodeId.equals(other.currentNodeId) : other.currentNodeId != null) {\r\n        return false;\r\n    }\r\n    if (relocatingNodeId != null ? !relocatingNodeId.equals(other.relocatingNodeId) : other.relocatingNodeId != null) {\r\n        return false;\r\n    }\r\n    if (allocationId != null ? !allocationId.equals(other.allocationId) : other.allocationId != null) {\r\n        return false;\r\n    }\r\n    if (state != other.state) {\r\n        return false;\r\n    }\r\n    if (recoverySource != null ? !recoverySource.equals(other.recoverySource) : other.recoverySource != null) {\r\n        return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.EngineConfig.getExternalRefreshListener",
	"Comment": "the refresh listeners to add to lucene for externally visible refreshes",
	"Method": "List<ReferenceManager.RefreshListener> getExternalRefreshListener(){\r\n    return externalRefreshListener;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.analyze.AnalyzeRequestBuilder.addCharFilter",
	"Comment": "add a name of char filter that will be used before the tokenizer.",
	"Method": "AnalyzeRequestBuilder addCharFilter(Map<String, ?> charFilter,AnalyzeRequestBuilder addCharFilter,String tokenFilter){\r\n    request.addCharFilter(tokenFilter);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.FailedShard.getMessage",
	"Comment": "the failure message, if available, explaining why the shard failed.",
	"Method": "String getMessage(){\r\n    return message;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.BaseTranslogReader.checksummedStream",
	"Comment": "reads an operation at the given position and returns it. the buffer length is equal to the numberof bytes reads.",
	"Method": "BufferedChecksumStreamInput checksummedStream(ByteBuffer reusableBuffer,long position,int opSize,BufferedChecksumStreamInput reuse){\r\n    final ByteBuffer buffer;\r\n    if (reusableBuffer.capacity() >= opSize) {\r\n        buffer = reusableBuffer;\r\n    } else {\r\n        buffer = ByteBuffer.allocate(opSize);\r\n    }\r\n    buffer.clear();\r\n    buffer.limit(opSize);\r\n    readBytes(buffer, position);\r\n    buffer.flip();\r\n    return new BufferedChecksumStreamInput(new ByteBufferStreamInput(buffer), path.toString(), reuse);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchResponse.getProfileResults",
	"Comment": "if profiling was enabled, this returns an object containing the profile results fromeach shard.if profiling was not enabled, this will return null",
	"Method": "Map<String, ProfileShardResult> getProfileResults(){\r\n    return internalResponse.profile();\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.nndep.Classifier.computeScores",
	"Comment": "feed a feature vector forward through the network. returns thevalues of the output layer.",
	"Method": "double[] computeScores(int[] feature,double[] computeScores,int[] feature,Map<Integer, Integer> preMap){\r\n    final double[] hidden = new double[config.hiddenSize];\r\n    final int numTokens = config.numTokens;\r\n    final int embeddingSize = config.embeddingSize;\r\n    int offset = 0;\r\n    for (int j = 0; j < feature.length; j++) {\r\n        int tok = feature[j];\r\n        int index = tok * numTokens + j;\r\n        Integer idInteger = preMap.get(index);\r\n        if (idInteger != null) {\r\n            ArrayMath.pairwiseAddInPlace(hidden, saved[idInteger]);\r\n        } else {\r\n            matrixMultiplySliceSum(hidden, W1, E[tok], offset);\r\n        }\r\n        offset += embeddingSize;\r\n    }\r\n    addCubeInPlace(hidden, b1);\r\n    return matrixMultiply(W2, hidden);\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.OpenIE.main",
	"Comment": "an entry method for annotating standard in with openie extractions.",
	"Method": "void main(String[] args){\r\n    Properties props = StringUtils.argsToProperties(args, new HashMap<String, Integer>() {\r\n        {\r\n            put(\"openie.resolve_coref\", 0);\r\n            put(\"resolve_coref\", 0);\r\n            put(\"openie.splitter.nomodel\", 0);\r\n            put(\"splitter.nomodel\", 0);\r\n            put(\"openie.splitter.disable\", 0);\r\n            put(\"splitter.disable\", 0);\r\n            put(\"openie.ignore_affinity\", 0);\r\n            put(\"splitter.ignore_affinity\", 0);\r\n            put(\"openie.triple.strict\", 0);\r\n            put(\"splitter.triple.strict\", 0);\r\n            put(\"openie.triple.all_nominals\", 0);\r\n            put(\"splitter.triple.all_nominals\", 0);\r\n        }\r\n    });\r\n    ArgumentParser.fillOptions(new Class[] { OpenIE.class, ArgumentParser.class }, props);\r\n    AtomicInteger exceptionCount = new AtomicInteger(0);\r\n    ExecutorService exec = Executors.newFixedThreadPool(ArgumentParser.threads);\r\n    String[] filesToProcess;\r\n    if (FILELIST != null) {\r\n        filesToProcess = IOUtils.linesFromFile(FILELIST.getPath()).stream().map(String::trim).map(path -> path.replaceAll(\"^~\", \"$HOME\")).map(path -> new File(path).exists() ? path : StringUtils.expandEnvironmentVariables(path)).toArray(String[]::new);\r\n    } else if (!\"\".equals(props.getProperty(\"\", \"\"))) {\r\n        filesToProcess = props.getProperty(\"\", \"\").split(\"\\\\s+\");\r\n    } else {\r\n        filesToProcess = new String[0];\r\n    }\r\n    if (\"\".equals(props.getProperty(\"annotators\", \"\"))) {\r\n        if (!\"false\".equalsIgnoreCase(props.getProperty(\"resolve_coref\", props.getProperty(\"openie.resolve_coref\", \"false\")))) {\r\n            props.setProperty(\"coref.md.type\", \"dep\");\r\n            props.setProperty(\"coref.mode\", \"statistical\");\r\n            props.setProperty(\"annotators\", \"tokenize,ssplit,pos,lemma,depparse,ner,coref,natlog,openie\");\r\n        } else {\r\n            props.setProperty(\"annotators\", \"tokenize,ssplit,pos,lemma,depparse,natlog,openie\");\r\n        }\r\n    }\r\n    if (\"\".equals(props.getProperty(\"depparse.extradependencies\", \"\"))) {\r\n        props.setProperty(\"depparse.extradependencies\", \"ref_only_uncollapsed\");\r\n    }\r\n    if (\"\".equals(props.getProperty(\"parse.extradependencies\", \"\"))) {\r\n        props.setProperty(\"parse.extradependencies\", \"ref_only_uncollapsed\");\r\n    }\r\n    if (\"\".equals(props.getProperty(\"tokenize.class\", \"\"))) {\r\n        props.setProperty(\"tokenize.class\", \"PTBTokenizer\");\r\n    }\r\n    if (\"\".equals(props.getProperty(\"tokenize.language\", \"\"))) {\r\n        props.setProperty(\"tokenize.language\", \"en\");\r\n    }\r\n    if (filesToProcess.length == 0 && \"\".equals(props.getProperty(\"ssplit.isOneSentence\", \"\"))) {\r\n        props.setProperty(\"ssplit.isOneSentence\", \"true\");\r\n    }\r\n    if (!props.getProperty(\"annotators\").toLowerCase().contains(\"openie\")) {\r\n        log.error(\"If you specify custom annotators, you must at least include 'openie'\");\r\n        System.exit(1);\r\n    }\r\n    new HashSet(props.keySet()).stream().filter(key -> !key.toString().startsWith(\"openie.\")).forEach(key -> props.setProperty(\"openie.\" + key.toString(), props.getProperty(key.toString())));\r\n    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\r\n    if (filesToProcess.length == 0) {\r\n        log.info(\"Processing from stdin. Enter one sentence per line.\");\r\n        Scanner scanner = new Scanner(System.in);\r\n        String line;\r\n        try {\r\n            line = scanner.nextLine();\r\n        } catch (NoSuchElementException e) {\r\n            log.info(\"No lines found on standard in\");\r\n            return;\r\n        }\r\n        while (line != null) {\r\n            processDocument(pipeline, \"stdin\", line);\r\n            try {\r\n                line = scanner.nextLine();\r\n            } catch (NoSuchElementException e) {\r\n                return;\r\n            }\r\n        }\r\n    } else {\r\n        for (String file : filesToProcess) {\r\n            if (!new File(file).exists() || !new File(file).canRead()) {\r\n                log.error(\"Cannot read file (or file does not exist: '\" + file + '\\'');\r\n            }\r\n        }\r\n        for (String file : filesToProcess) {\r\n            log.info(\"Processing file: \" + file);\r\n            if (ArgumentParser.threads > 1) {\r\n                final String fileToSubmit = file;\r\n                exec.submit(() -> {\r\n                    try {\r\n                        processDocument(pipeline, file, IOUtils.slurpFile(new File(fileToSubmit)));\r\n                    } catch (Throwable t) {\r\n                        t.printStackTrace();\r\n                        exceptionCount.incrementAndGet();\r\n                    }\r\n                });\r\n            } else {\r\n                processDocument(pipeline, file, IOUtils.slurpFile(new File(file)));\r\n            }\r\n        }\r\n    }\r\n    exec.shutdown();\r\n    log.info(\"All files have been queued; awaiting termination...\");\r\n    exec.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);\r\n    log.info(\"DONE processing files. \" + exceptionCount.get() + \" exceptions encountered.\");\r\n    System.exit(exceptionCount.get());\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.Util.stripPrepCases",
	"Comment": "strip away case edges, if the incoming edge is a preposition.this replicates the behavior of the old stanford dependencies on universal dependencies.",
	"Method": "void stripPrepCases(SemanticGraph tree){\r\n    List<SemanticGraphEdge> toClean = new ArrayList();\r\n    for (SemanticGraphEdge edge : tree.edgeIterable()) {\r\n        if (\"case\".equals(edge.getRelation().toString())) {\r\n            boolean isPrepTarget = false;\r\n            for (SemanticGraphEdge incoming : tree.incomingEdgeIterable(edge.getGovernor())) {\r\n                if (\"nmod\".equals(incoming.getRelation().getShortName())) {\r\n                    isPrepTarget = true;\r\n                    break;\r\n                }\r\n            }\r\n            if (isPrepTarget && !tree.outgoingEdgeIterator(edge.getDependent()).hasNext()) {\r\n                toClean.add(edge);\r\n            }\r\n        }\r\n    }\r\n    for (SemanticGraphEdge edge : toClean) {\r\n        tree.removeEdge(edge);\r\n        tree.removeVertex(edge.getDependent());\r\n        assert isTree(tree);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.rollover.RolloverResponse.isRolledOver",
	"Comment": "returns true if the rollover was not simulated and the conditions were met",
	"Method": "boolean isRolledOver(){\r\n    return rolledOver;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.GlobalCheckpointListeners.pendingListeners",
	"Comment": "the number of listeners currently pending for notification.",
	"Method": "int pendingListeners(){\r\n    return listeners.size();\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.Store.ensureIndexHasHistoryUUID",
	"Comment": "checks that the lucene index contains a history uuid marker. if not, a new one is generated and committed.",
	"Method": "void ensureIndexHasHistoryUUID(){\r\n    metadataLock.writeLock().lock();\r\n    try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {\r\n        final Map<String, String> userData = getUserData(writer);\r\n        if (userData.containsKey(Engine.HISTORY_UUID_KEY) == false) {\r\n            updateCommitData(writer, Collections.singletonMap(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID()));\r\n        }\r\n    } finally {\r\n        metadataLock.writeLock().unlock();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.IndexGraveyard.containsIndex",
	"Comment": "returns true if the graveyard contains a tombstone for the given index.",
	"Method": "boolean containsIndex(Index index){\r\n    for (Tombstone tombstone : tombstones) {\r\n        if (tombstone.getIndex().equals(index)) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.apache.dubbo.metrics.MetricName.getMetricLevel",
	"Comment": "return the level of this metricthe level indicates the importance of the metric",
	"Method": "MetricLevel getMetricLevel(){\r\n    return level;\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationAction.acquirePrimaryOperationPermit",
	"Comment": "executes the logic for acquiring one or more operation permit on a primary shard. the default is to acquire a single permit but thismethod can be overridden to acquire more.",
	"Method": "void acquirePrimaryOperationPermit(IndexShard primary,Request request,ActionListener<Releasable> onAcquired){\r\n    primary.acquirePrimaryOperationPermit(onAcquired, executor, request);\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.LeaderBulkByScrollTaskState.getSlices",
	"Comment": "returns the number of slices this bulkbyscrollrequest will use",
	"Method": "int getSlices(){\r\n    return slices;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.DiffableUtils.readImmutableOpenMapDiff",
	"Comment": "loads an object that represents difference between two immutableopenmaps of diffable objects using diffable proto object",
	"Method": "MapDiff<K, T, ImmutableOpenMap<K, T>> readImmutableOpenMapDiff(StreamInput in,KeySerializer<K> keySerializer,ValueSerializer<K, T> valueSerializer,MapDiff<K, T, ImmutableOpenMap<K, T>> readImmutableOpenMapDiff,StreamInput in,KeySerializer<K> keySerializer,Reader<T> reader,Reader<Diff<T>> diffReader){\r\n    return new ImmutableOpenMapDiff(in, keySerializer, new DiffableValueReader(reader, diffReader));\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.SimpleQueryStringBuilder.fields",
	"Comment": "returns the fields including their respective boosts to run the query against.",
	"Method": "SimpleQueryStringBuilder fields(Map<String, Float> fields,Map<String, Float> fields){\r\n    return this.fieldsAndWeights;\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.SimpleTensor.bilinearProducts",
	"Comment": "returns a column vector where each entry is the nth bilinearproduct of the nth slices of the two tensors.",
	"Method": "SimpleMatrix bilinearProducts(SimpleMatrix in){\r\n    if (in.numCols() != 1) {\r\n        throw new AssertionError(\"Expected a column vector\");\r\n    }\r\n    if (in.numRows() != numCols) {\r\n        throw new AssertionError(\"Number of rows in the input does not match number of columns in tensor\");\r\n    }\r\n    if (numRows != numCols) {\r\n        throw new AssertionError(\"Can only perform this operation on a SimpleTensor with square slices\");\r\n    }\r\n    SimpleMatrix inT = in.transpose();\r\n    SimpleMatrix out = new SimpleMatrix(numSlices, 1);\r\n    for (int slice = 0; slice < numSlices; ++slice) {\r\n        double result = inT.mult(slices[slice]).mult(in).get(0);\r\n        out.set(slice, result);\r\n    }\r\n    return out;\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BackoffPolicy.constantBackoff",
	"Comment": "creates an new constant backoff policy with the provided configuration.",
	"Method": "BackoffPolicy constantBackoff(TimeValue delay,int maxNumberOfRetries){\r\n    return new ConstantBackoff(checkDelay(delay), maxNumberOfRetries);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEvent.nodesAdded",
	"Comment": "returns true iff nodes have been added from the cluster since the last cluster state.",
	"Method": "boolean nodesAdded(){\r\n    return nodesDelta.added();\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.MetaStateService.loadFullState",
	"Comment": "loads the full state, which includes both the global state and all the indicesmeta state.",
	"Method": "MetaData loadFullState(){\r\n    MetaData globalMetaData = loadGlobalState();\r\n    MetaData.Builder metaDataBuilder;\r\n    if (globalMetaData != null) {\r\n        metaDataBuilder = MetaData.builder(globalMetaData);\r\n    } else {\r\n        metaDataBuilder = MetaData.builder();\r\n    }\r\n    for (String indexFolderName : nodeEnv.availableIndexFolders()) {\r\n        IndexMetaData indexMetaData = IndexMetaData.FORMAT.loadLatestState(logger, namedXContentRegistry, nodeEnv.resolveIndexFolder(indexFolderName));\r\n        if (indexMetaData != null) {\r\n            metaDataBuilder.put(indexMetaData, false);\r\n        } else {\r\n            logger.debug(\"[{}] failed to find metadata for existing index location\", indexFolderName);\r\n        }\r\n    }\r\n    return metaDataBuilder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryStringQueryBuilder.phraseSlop",
	"Comment": "sets the default slop for phrases.if zero, then exact phrase matchesare required. default value is zero.",
	"Method": "QueryStringQueryBuilder phraseSlop(int phraseSlop,int phraseSlop){\r\n    return phraseSlop;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.BoolQueryBuilder.hasClauses",
	"Comment": "returns true iff this query builder has at least one should, must, must not or filter clause.otherwise false.",
	"Method": "boolean hasClauses(){\r\n    return !(mustClauses.isEmpty() && shouldClauses.isEmpty() && mustNotClauses.isEmpty() && filterClauses.isEmpty());\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequestBuilder.setVersion",
	"Comment": "sets the version, which will cause the index operation to only be performed if a matchingversion exists and no changes happened on the doc since then.",
	"Method": "IndexRequestBuilder setVersion(long version){\r\n    request.version(version);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.BigByteArray.estimateRamBytes",
	"Comment": "estimates the number of bytes that would be consumed by an array of the given size.",
	"Method": "long estimateRamBytes(long size){\r\n    return ESTIMATOR.ramBytesEstimated(size);\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.startRelocationHandoff",
	"Comment": "initiates a relocation handoff and returns the corresponding primary context.",
	"Method": "PrimaryContext startRelocationHandoff(){\r\n    assert invariant();\r\n    assert primaryMode;\r\n    assert handoffInProgress == false;\r\n    assert pendingInSync.isEmpty() : \"relocation handoff started while there are still shard copies pending in-sync: \" + pendingInSync;\r\n    handoffInProgress = true;\r\n    Map<String, CheckpointState> localCheckpointsCopy = new HashMap();\r\n    for (Map.Entry<String, CheckpointState> entry : checkpoints.entrySet()) {\r\n        localCheckpointsCopy.put(entry.getKey(), entry.getValue().copy());\r\n    }\r\n    assert invariant();\r\n    return new PrimaryContext(appliedClusterStateVersion, localCheckpointsCopy, routingTable);\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.storage.ModelBatch.writeToStreamWithoutFactors",
	"Comment": "this writes the whole batch, without factors, which means that anyone loading this batch will need to includetheir own featurizer. make sure that you have sufficient metadata to be able to do full featurizations.",
	"Method": "void writeToStreamWithoutFactors(OutputStream outputStream){\r\n    Set<GraphicalModel.Factor> emptySet = new HashSet();\r\n    for (GraphicalModel model : this) {\r\n        Set<GraphicalModel.Factor> cachedFactors = model.factors;\r\n        model.factors = emptySet;\r\n        model.writeToStream(outputStream);\r\n        model.factors = cachedFactors;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.isExplicitRefresh",
	"Comment": "returns true iff the refresh setting exists or in other words is explicitly set.",
	"Method": "boolean isExplicitRefresh(){\r\n    return INDEX_REFRESH_INTERVAL_SETTING.exists(settings);\r\n}"
}, {
	"Path": "org.elasticsearch.index.analysis.IndexAnalyzers.get",
	"Comment": "returns an analyzer mapped to the given name or null if not present",
	"Method": "NamedAnalyzer get(String name){\r\n    return analyzers.get(name);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.metrics.TaggingEval.measureOOV",
	"Comment": "measures the percentage of incorrect taggings that can be attributed to oov words.",
	"Method": "void measureOOV(Tree guess,Tree gold){\r\n    List<CoreLabel> goldTagging = gold.taggedLabeledYield();\r\n    List<CoreLabel> guessTagging = guess.taggedLabeledYield();\r\n    assert goldTagging.size() == guessTagging.size();\r\n    for (int i = 0; i < goldTagging.size(); i++) {\r\n        if (!(goldTagging.get(i) == guessTagging.get(i))) {\r\n            percentOOV2.incrementCount(goldTagging.get(i).tag());\r\n            if (!lex.isKnown(goldTagging.get(i).word()))\r\n                percentOOV.incrementCount(goldTagging.get(i).tag());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexModule.newIndexMapperService",
	"Comment": "creates a new mapper service to do administrative work like mapping updates. this should not be used for document parsing.doing so will result in an exception.",
	"Method": "MapperService newIndexMapperService(NamedXContentRegistry xContentRegistry,MapperRegistry mapperRegistry,ScriptService scriptService){\r\n    return new MapperService(indexSettings, analysisRegistry.build(indexSettings), xContentRegistry, new SimilarityService(indexSettings, scriptService, similarities), mapperRegistry, () -> {\r\n        throw new UnsupportedOperationException(\"no index query shard context available\");\r\n    });\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.ArrayMath.sampleFromDistribution",
	"Comment": "samples from the distribution over values 0 through d.length given by d.assumes that the distribution sums to 1.0.",
	"Method": "int sampleFromDistribution(double[] d,int sampleFromDistribution,double[] d,Random random,int sampleFromDistribution,float[] d,Random random){\r\n    double r = random.nextDouble();\r\n    double total = 0;\r\n    for (int i = 0; i < d.length - 1; i++) {\r\n        if (Float.isNaN(d[i])) {\r\n            throw new RuntimeException(\"Can't sample from NaN\");\r\n        }\r\n        total += d[i];\r\n        if (r < total) {\r\n            return i;\r\n        }\r\n    }\r\n    return d.length - 1;\r\n}"
}, {
	"Path": "org.elasticsearch.common.logging.LogConfigurator.configureLoggerLevels",
	"Comment": "configures the logging levels for loggers configured in the specified settings.",
	"Method": "void configureLoggerLevels(Settings settings){\r\n    if (Loggers.LOG_DEFAULT_LEVEL_SETTING.exists(settings)) {\r\n        final Level level = Loggers.LOG_DEFAULT_LEVEL_SETTING.get(settings);\r\n        Loggers.setLevel(LogManager.getRootLogger(), level);\r\n    }\r\n    // do not set a log level for a logger named level (from the default log setting)\r\n    Loggers.LOG_LEVEL_SETTING.getAllConcreteSettings(settings).filter(s -> s.getKey().equals(Loggers.LOG_DEFAULT_LEVEL_SETTING.getKey()) == false).forEach(s -> {\r\n        final Level level = s.get(settings);\r\n        Loggers.setLevel(LogManager.getLogger(s.getKey().substring(\"logger.\".length())), level);\r\n    });\r\n}"
}, {
	"Path": "android.text.util.Rfc822Token.quoteName",
	"Comment": "returns the name, with internal backslashes and quotation markspreceded by backslashes.the outer quote marks themselves are notadded by this method.",
	"Method": "String quoteName(String name){\r\n    StringBuilder sb = new StringBuilder();\r\n    int len = name.length();\r\n    for (int i = 0; i < len; i++) {\r\n        char c = name.charAt(i);\r\n        if (c == '\\\\' || c == '\"') {\r\n            sb.append('\\\\');\r\n        }\r\n        sb.append(c);\r\n    }\r\n    return sb.toString();\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogDeletionPolicy.pendingTranslogRefCount",
	"Comment": "returns the number of generations that were acquired for snapshots",
	"Method": "int pendingTranslogRefCount(){\r\n    return translogRefCounts.size();\r\n}"
}, {
	"Path": "org.elasticsearch.indices.cluster.IndicesClusterStateService.removeShards",
	"Comment": "removes shards that are currently loaded by indicesservice but have disappeared from the routing table of the current node.this method does not delete the shard data.",
	"Method": "void removeShards(ClusterState state){\r\n    final String localNodeId = state.nodes().getLocalNodeId();\r\n    assert localNodeId != null;\r\n    RoutingNode localRoutingNode = state.getRoutingNodes().node(localNodeId);\r\n    for (AllocatedIndex<? extends Shard> indexService : indicesService) {\r\n        for (Shard shard : indexService) {\r\n            ShardRouting currentRoutingEntry = shard.routingEntry();\r\n            ShardId shardId = currentRoutingEntry.shardId();\r\n            ShardRouting newShardRouting = localRoutingNode == null ? null : localRoutingNode.getByShardId(shardId);\r\n            if (newShardRouting == null) {\r\n                logger.debug(\"{} removing shard (not allocated)\", shardId);\r\n                indexService.removeShard(shardId.id(), \"removing shard (not allocated)\");\r\n            } else if (newShardRouting.isSameAllocation(currentRoutingEntry) == false) {\r\n                logger.debug(\"{} removing shard (stale allocation id, stale {}, new {})\", shardId, currentRoutingEntry, newShardRouting);\r\n                indexService.removeShard(shardId.id(), \"removing shard (stale copy)\");\r\n            } else if (newShardRouting.initializing() && currentRoutingEntry.active()) {\r\n                logger.debug(\"{} removing shard (not active, current {}, new {})\", shardId, currentRoutingEntry, newShardRouting);\r\n                indexService.removeShard(shardId.id(), \"removing shard (stale copy)\");\r\n            } else if (newShardRouting.primary() && currentRoutingEntry.primary() == false && newShardRouting.initializing()) {\r\n                assert currentRoutingEntry.initializing() : currentRoutingEntry;\r\n                logger.debug(\"{} removing shard (not active, current {}, new {})\", shardId, currentRoutingEntry, newShardRouting);\r\n                indexService.removeShard(shardId.id(), \"removing shard (stale copy)\");\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.getSettings",
	"Comment": "returns the settings for this index. these settings contain the node and index level settings wheresettings that are specified on both index and node level are overwritten by the index settings.",
	"Method": "Settings getSettings(){\r\n    return settings;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.storedscripts.PutStoredScriptRequestBuilder.setContent",
	"Comment": "set the source of the script along with the content type of the source",
	"Method": "PutStoredScriptRequestBuilder setContent(BytesReference source,XContentType xContentType){\r\n    request.content(source, xContentType);\r\n    return this;\r\n}"
}, {
	"Path": "org.apache.dubbo.metrics.MetricName.resolve",
	"Comment": "build the metricname that is this with another path appended to it.the new metricname inherits the tags of this one.",
	"Method": "MetricName resolve(String p,MetricName resolve,String p,boolean inheritTags){\r\n    final String next;\r\n    if (p != null && !p.isEmpty()) {\r\n        if (key != null && !key.isEmpty()) {\r\n            next = key + SEPARATOR + p;\r\n        } else {\r\n            next = p;\r\n        }\r\n    } else {\r\n        next = this.key;\r\n    }\r\n    return inheritTags ? new MetricName(next, tags, level) : new MetricName(next, level);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoShapeQueryBuilder.strategy",
	"Comment": "defines which spatial strategy will be used for building the geo shapequery. when not set, the strategy that will be used will be the one thatis associated with the geo shape field in the mappings.",
	"Method": "GeoShapeQueryBuilder strategy(SpatialStrategy strategy,SpatialStrategy strategy){\r\n    return strategy;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusRequestBuilder.setIgnoreUnavailable",
	"Comment": "set to true to ignore unavailable snapshots, instead of throwing an exception.defaults to false, which means unavailable snapshots cause an exception to be thrown.",
	"Method": "SnapshotsStatusRequestBuilder setIgnoreUnavailable(boolean ignoreUnavailable){\r\n    request.ignoreUnavailable(ignoreUnavailable);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.rollover.RolloverResponse.isDryRun",
	"Comment": "returns if the rollover execution was skipped even when conditions were met",
	"Method": "boolean isDryRun(){\r\n    return dryRun;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MultiMatchQueryBuilder.field",
	"Comment": "adds a field to run the multi match against with a specific boost.",
	"Method": "MultiMatchQueryBuilder field(String field,MultiMatchQueryBuilder field,String field,float boost){\r\n    if (Strings.isEmpty(field)) {\r\n        throw new IllegalArgumentException(\"supplied field is null or empty.\");\r\n    }\r\n    this.fieldsBoosts.put(field, boost);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsRequestBuilder.setExpandWildcardsOpen",
	"Comment": "controls whether wildcard expressions will be expanded to existing open indices",
	"Method": "IndicesExistsRequestBuilder setExpandWildcardsOpen(boolean expandWildcardsOpen){\r\n    request.expandWilcardsOpen(expandWildcardsOpen);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BackoffPolicy.exponentialBackoff",
	"Comment": "creates an new exponential backoff policy with the provided configuration.",
	"Method": "BackoffPolicy exponentialBackoff(BackoffPolicy exponentialBackoff,TimeValue initialDelay,int maxNumberOfRetries){\r\n    return new ExponentialBackoff((int) checkDelay(initialDelay).millis(), maxNumberOfRetries);\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.OpenIE.grokCorefMention",
	"Comment": "a utility to get useful information out of a corefmention. in particular, it returns the corelabels which areassociated with this mention, and it returns a score for how much we think this mention should be the canonicalmention.",
	"Method": "Pair<List<CoreLabel>, Double> grokCorefMention(Annotation doc,CorefChain.CorefMention mention){\r\n    List<CoreLabel> tokens = doc.get(CoreAnnotations.SentencesAnnotation.class).get(mention.sentNum - 1).get(CoreAnnotations.TokensAnnotation.class);\r\n    List<CoreLabel> mentionAsTokens = tokens.subList(mention.startIndex - 1, mention.endIndex - 1);\r\n    Counter<String> nerVotes = new ClassicCounter();\r\n    mentionAsTokens.stream().filter(token -> token.ner() != null && !\"O\".equals(token.ner())).forEach(token -> nerVotes.incrementCount(token.ner()));\r\n    String ner = Counters.argmax(nerVotes, (o1, o2) -> o1 == null ? 0 : o1.compareTo(o2));\r\n    double nerCount = nerVotes.getCount(ner);\r\n    double nerScore = nerCount * nerCount / ((double) mentionAsTokens.size());\r\n    return Pair.makePair(mentionAsTokens, nerScore);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequest.setBatchedReduceSize",
	"Comment": "sets the number of shard results that should be reduced at once on the coordinating node. this value should be used as a protectionmechanism to reduce the memory overhead per search request if the potential number of shards in the request can be large.",
	"Method": "void setBatchedReduceSize(int batchedReduceSize){\r\n    if (batchedReduceSize <= 1) {\r\n        throw new IllegalArgumentException(\"batchedReduceSize must be >= 2\");\r\n    }\r\n    this.batchedReduceSize = batchedReduceSize;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.AnnotationLookup.getValueType",
	"Comment": "returns the runtime value type associated with the given key.cachesresults in a private map.",
	"Method": "Class<?> getValueType(Class<? extends CoreAnnotation<?>> key){\r\n    Class type = valueCache.get(key);\r\n    if (type == null) {\r\n        try {\r\n            type = key.newInstance().getType();\r\n        } catch (Exception e) {\r\n            throw new RuntimeException(\"Unexpected failure to instantiate - is your key class fancy?\", e);\r\n        }\r\n        valueCache.put((Class) key, type);\r\n    }\r\n    return type;\r\n}"
}, {
	"Path": "org.elasticsearch.env.NodeEnvironment.deleteShardDirectorySafe",
	"Comment": "deletes a shard data directory iff the shards locks were successfully acquired.",
	"Method": "void deleteShardDirectorySafe(ShardId shardId,IndexSettings indexSettings){\r\n    final Path[] paths = availableShardPaths(shardId);\r\n    logger.trace(\"deleting shard {} directory, paths: [{}]\", shardId, paths);\r\n    try (ShardLock lock = shardLock(shardId)) {\r\n        deleteShardDirectoryUnderLock(lock, indexSettings);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName",
	"Comment": "validate the name for an index against some static rules and a cluster state.",
	"Method": "void validateIndexName(String index,ClusterState state){\r\n    validateIndexOrAliasName(index, InvalidIndexNameException::new);\r\n    if (!index.toLowerCase(Locale.ROOT).equals(index)) {\r\n        throw new InvalidIndexNameException(index, \"must be lowercase\");\r\n    }\r\n    if (state.routingTable().hasIndex(index)) {\r\n        throw new ResourceAlreadyExistsException(state.routingTable().index(index).getIndex());\r\n    }\r\n    if (state.metaData().hasIndex(index)) {\r\n        throw new ResourceAlreadyExistsException(state.metaData().index(index).getIndex());\r\n    }\r\n    if (state.metaData().hasAlias(index)) {\r\n        throw new InvalidIndexNameException(index, \"already exists as alias\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.circuitBreak",
	"Comment": "method used to trip the breaker, delegates to the parent to determinewhether to trip the breaker or not",
	"Method": "void circuitBreak(String fieldName,long bytesNeeded){\r\n    this.trippedCount.incrementAndGet();\r\n    final String message = \"[\" + this.name + \"] Data too large, data for [\" + fieldName + \"]\" + \" would be [\" + bytesNeeded + \"/\" + new ByteSizeValue(bytesNeeded) + \"]\" + \", which is larger than the limit of [\" + memoryBytesLimit + \"/\" + new ByteSizeValue(memoryBytesLimit) + \"]\";\r\n    logger.debug(\"{}\", message);\r\n    throw new CircuitBreakingException(message, bytesNeeded, memoryBytesLimit, durability);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotShardsStats.getInitializingShards",
	"Comment": "number of shards with the snapshot in the initializing stage",
	"Method": "int getInitializingShards(){\r\n    return initializingShards;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateObserver.setAndGetObservedState",
	"Comment": "sets the last observed state to the currently applied cluster state and returns it",
	"Method": "ClusterState setAndGetObservedState(){\r\n    if (observingContext.get() != null) {\r\n        throw new ElasticsearchException(\"cannot set current cluster state while waiting for a cluster state change\");\r\n    }\r\n    ClusterState clusterState = clusterApplierService.state();\r\n    lastObservedState.set(new StoredState(clusterState));\r\n    return clusterState;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.PriorityTestLoad.validate",
	"Comment": "validate that the one with priority max has been assigned a proxy",
	"Method": "void validate(){\r\n    while (!reqDone) {\r\n        Thread.sleep(20);\r\n    }\r\n    assertNotNull(requests.get(requests.size() - 1).getSession());\r\n    assertEquals(requests.get(requests.size() - 1).getRequest().getDesiredCapabilities().get(\"_priority\"), MAX);\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.slurpInputStream",
	"Comment": "read the contents of an input stream, decoding it according to the given character encoding.",
	"Method": "String slurpInputStream(InputStream input,String encoding){\r\n    return slurpReader(encodedInputStreamReader(input, encoding));\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.RoutingAllocation.decision",
	"Comment": "create a routing decision, including the reason if the debug flag isturned on",
	"Method": "Decision decision(Decision decision,String deciderLabel,String reason,Object params){\r\n    if (debugDecision()) {\r\n        return Decision.single(decision.type(), deciderLabel, reason, params);\r\n    } else {\r\n        return decision;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.arabic.process.IOBUtils.StringToIOB",
	"Comment": "this version is for turning an unsegmented string to an iob input, i.e.,for processing raw text.",
	"Method": "List<CoreLabel> StringToIOB(List<CoreLabel> tokenList,Character segMarker,boolean applyRewriteRules,List<CoreLabel> StringToIOB,List<CoreLabel> tokenList,Character segMarker,boolean applyRewriteRules,TokenizerFactory<CoreLabel> tf,String origText,List<CoreLabel> StringToIOB,List<CoreLabel> tokenList,Character segMarker,boolean applyRewriteRules,boolean stripRewrites,List<CoreLabel> StringToIOB,List<CoreLabel> tokenList,Character segMarker,boolean applyRewriteRules,boolean stripRewrites,TokenizerFactory<CoreLabel> tf,String origText,List<CoreLabel> StringToIOB,String string,List<CoreLabel> StringToIOB,String str,Character segMarker){\r\n    List<CoreLabel> toks = SentenceUtils.toCoreLabelList(str.trim().split(\"\\\\s+\"));\r\n    return StringToIOB(toks, segMarker, false);\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return new LeafReader() {\r\n        final Bits liveDocs = new Bits.MatchNoBits(maxDoc);\r\n        public Terms terms(String field) {\r\n            return null;\r\n        }\r\n        public NumericDocValues getNumericDocValues(String field) {\r\n            return null;\r\n        }\r\n        public BinaryDocValues getBinaryDocValues(String field) {\r\n            return null;\r\n        }\r\n        public SortedDocValues getSortedDocValues(String field) {\r\n            return null;\r\n        }\r\n        public SortedNumericDocValues getSortedNumericDocValues(String field) {\r\n            return null;\r\n        }\r\n        public SortedSetDocValues getSortedSetDocValues(String field) {\r\n            return null;\r\n        }\r\n        public NumericDocValues getNormValues(String field) {\r\n            return null;\r\n        }\r\n        public FieldInfos getFieldInfos() {\r\n            return new FieldInfos(new FieldInfo[0]);\r\n        }\r\n        public Bits getLiveDocs() {\r\n            return this.liveDocs;\r\n        }\r\n        public PointValues getPointValues(String fieldName) {\r\n            return null;\r\n        }\r\n        public void checkIntegrity() {\r\n        }\r\n        public Fields getTermVectors(int docID) {\r\n            return null;\r\n        }\r\n        public int numDocs() {\r\n            return 0;\r\n        }\r\n        public int maxDoc() {\r\n            return maxDoc;\r\n        }\r\n        public void document(int docID, StoredFieldVisitor visitor) {\r\n        }\r\n        protected void doClose() {\r\n        }\r\n        public LeafMetaData getMetaData() {\r\n            return new LeafMetaData(Version.LATEST.major, Version.LATEST, (Sort) null);\r\n        }\r\n        public CacheHelper getCoreCacheHelper() {\r\n            return null;\r\n        }\r\n        public CacheHelper getReaderCacheHelper() {\r\n            return null;\r\n        }\r\n    };\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return new FieldInfos(new FieldInfo[0]);\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return this.liveDocs;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return 0;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return maxDoc;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return new LeafMetaData(Version.LATEST.major, Version.LATEST, (Sort) null);\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.emptyReader",
	"Comment": "returns an empty leaf reader with the given max docs. the reader will be fully deleted.",
	"Method": "LeafReader emptyReader(int maxDoc){\r\n    return null;\r\n}"
}, {
	"Path": "org.apache.dubbo.registry.integration.RegistryDirectory.toMethodInvokers",
	"Comment": "transform the invokers list into a mapping relationship with a method",
	"Method": "Map<String, List<Invoker<T>>> toMethodInvokers(Map<String, Invoker<T>> invokersMap){\r\n    Map<String, List<Invoker<T>>> newMethodInvokerMap = new HashMap<String, List<Invoker<T>>>();\r\n    List<Invoker<T>> invokersList = new ArrayList<Invoker<T>>();\r\n    if (invokersMap != null && invokersMap.size() > 0) {\r\n        for (Invoker<T> invoker : invokersMap.values()) {\r\n            String parameter = invoker.getUrl().getParameter(Constants.METHODS_KEY);\r\n            if (parameter != null && parameter.length() > 0) {\r\n                String[] methods = Constants.COMMA_SPLIT_PATTERN.split(parameter);\r\n                if (methods != null && methods.length > 0) {\r\n                    for (String method : methods) {\r\n                        if (method != null && method.length() > 0 && !Constants.ANY_VALUE.equals(method)) {\r\n                            List<Invoker<T>> methodInvokers = newMethodInvokerMap.get(method);\r\n                            if (methodInvokers == null) {\r\n                                methodInvokers = new ArrayList<Invoker<T>>();\r\n                                newMethodInvokerMap.put(method, methodInvokers);\r\n                            }\r\n                            methodInvokers.add(invoker);\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n            invokersList.add(invoker);\r\n        }\r\n    }\r\n    List<Invoker<T>> newInvokersList = route(invokersList, null);\r\n    newMethodInvokerMap.put(Constants.ANY_VALUE, newInvokersList);\r\n    if (serviceMethods != null && serviceMethods.length > 0) {\r\n        for (String method : serviceMethods) {\r\n            List<Invoker<T>> methodInvokers = newMethodInvokerMap.get(method);\r\n            if (methodInvokers == null || methodInvokers.isEmpty()) {\r\n                methodInvokers = newInvokersList;\r\n            }\r\n            newMethodInvokerMap.put(method, route(methodInvokers, method));\r\n        }\r\n    }\r\n    for (String method : new HashSet<String>(newMethodInvokerMap.keySet())) {\r\n        List<Invoker<T>> methodInvokers = newMethodInvokerMap.get(method);\r\n        Collections.sort(methodInvokers, InvokerComparator.getComparator());\r\n        newMethodInvokerMap.put(method, Collections.unmodifiableList(methodInvokers));\r\n    }\r\n    return Collections.unmodifiableMap(newMethodInvokerMap);\r\n}"
}, {
	"Path": "org.elasticsearch.index.EsTieredMergePolicy.setMaxMergedSegmentMB",
	"Comment": "only setter that must not delegate to the forced merge policy",
	"Method": "void setMaxMergedSegmentMB(double mbFrac){\r\n    regularMergePolicy.setMaxMergedSegmentMB(mbFrac);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotRequest.includeGlobalState",
	"Comment": "returns true if global state should be stored as part of the snapshot",
	"Method": "CreateSnapshotRequest includeGlobalState(boolean includeGlobalState,boolean includeGlobalState){\r\n    return includeGlobalState;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.BlockingClusterStatePublishResponseHandler.awaitAllNodes",
	"Comment": "allows to wait for all non master nodes to reply to the publish event up to a timeout",
	"Method": "boolean awaitAllNodes(TimeValue timeout){\r\n    boolean success = latch.await(timeout.millis(), TimeUnit.MILLISECONDS);\r\n    assert !success || pendingNodes.isEmpty() : \"response count reached 0 but still waiting for some nodes\";\r\n    return success;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.WorkerBulkByScrollTaskState.perfectlyThrottledBatchTime",
	"Comment": "how many nanoseconds should a batch of lastbatchsize have taken if it were perfectly throttled? package private for testing.",
	"Method": "float perfectlyThrottledBatchTime(int lastBatchSize){\r\n    if (requestsPerSecond == Float.POSITIVE_INFINITY) {\r\n        return 0;\r\n    }\r\n    float targetBatchTimeInSeconds = lastBatchSize / requestsPerSecond;\r\n    return TimeUnit.SECONDS.toNanos(1) * targetBatchTimeInSeconds;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchTransportService.getConnection",
	"Comment": "returns a connection to the given node on the provided cluster. if the cluster alias is null the node will be resolvedagainst the local cluster.",
	"Method": "Transport.Connection getConnection(String clusterAlias,DiscoveryNode node){\r\n    if (clusterAlias == null) {\r\n        return transportService.getConnection(node);\r\n    } else {\r\n        return transportService.getRemoteClusterService().getConnection(node, clusterAlias);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.FastFactoredParser.parse",
	"Comment": "parse a sentence.it is assumed that when this is called, the pparserhas already been called to parse the sentence.",
	"Method": "boolean parse(List<? extends HasWord> words){\r\n    nGoodTrees.clear();\r\n    int numParsesToConsider = numToFind * op.testOptions.fastFactoredCandidateMultiplier + op.testOptions.fastFactoredCandidateAddend;\r\n    if (pparser.hasParse()) {\r\n        List<ScoredObject<Tree>> pcfgBest = pparser.getKBestParses(numParsesToConsider);\r\n        Beam<ScoredObject<Tree>> goodParses = new Beam(numToFind);\r\n        for (ScoredObject<Tree> candidate : pcfgBest) {\r\n            if (Thread.interrupted()) {\r\n                throw new RuntimeInterruptedException();\r\n            }\r\n            double depScore = depScoreTree(candidate.object());\r\n            ScoredObject<Tree> x = new ScoredObject(candidate.object(), candidate.score() + depScore);\r\n            goodParses.add(x);\r\n        }\r\n        nGoodTrees = goodParses.asSortedList();\r\n    }\r\n    return !nGoodTrees.isEmpty();\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MultiMatchQueryBuilder.maxExpansions",
	"Comment": "when using fuzzy or prefix type query, the number of term expansions to use. defaults to unboundedso its recommended to set it to a reasonable value for faster execution.",
	"Method": "MultiMatchQueryBuilder maxExpansions(int maxExpansions,int maxExpansions){\r\n    return maxExpansions;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMapLongWithAggressiveConcurrency",
	"Comment": "creates a new chm with an aggressive concurrency level, aimed at highly updateable long living maps.",
	"Method": "ConcurrentMapLong<V> newConcurrentMapLongWithAggressiveConcurrency(){\r\n    return new ConcurrentHashMapLong(ConcurrentCollections.<Long, V>newConcurrentMapWithAggressiveConcurrency());\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.ClauseSplitterSearchProblem.simpleClause",
	"Comment": "the basic method for splitting off a clause of a tree.this modifies the tree in place.this method addtionally follows ref edges.",
	"Method": "void simpleClause(SemanticGraph tree,SemanticGraphEdge toKeep){\r\n    splitToChildOfEdge(tree, toKeep);\r\n    Map<IndexedWord, IndexedWord> refReplaceMap = new HashMap();\r\n    for (IndexedWord vertex : tree.vertexSet()) {\r\n        for (SemanticGraphEdge edge : extraEdgesByDependent.get(vertex)) {\r\n            if (\"ref\".equals(edge.getRelation().toString()) && !tree.containsVertex(edge.getGovernor())) {\r\n                refReplaceMap.put(vertex, edge.getGovernor());\r\n            }\r\n        }\r\n    }\r\n    for (Map.Entry<IndexedWord, IndexedWord> entry : refReplaceMap.entrySet()) {\r\n        Iterator<SemanticGraphEdge> iter = tree.incomingEdgeIterator(entry.getKey());\r\n        if (!iter.hasNext()) {\r\n            continue;\r\n        }\r\n        SemanticGraphEdge incomingEdge = iter.next();\r\n        IndexedWord governor = incomingEdge.getGovernor();\r\n        tree.removeVertex(entry.getKey());\r\n        addSubtree(tree, governor, incomingEdge.getRelation().toString(), this.tree, entry.getValue(), this.tree.incomingEdgeList(tree.getFirstRoot()));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.EsExecutors.newAutoQueueFixed",
	"Comment": "return a new executor that will automatically adjust the queue size based on queue throughput.",
	"Method": "EsThreadPoolExecutor newAutoQueueFixed(String name,int size,int initialQueueCapacity,int minQueueSize,int maxQueueSize,int frameSize,TimeValue targetedResponseTime,ThreadFactory threadFactory,ThreadContext contextHolder){\r\n    if (initialQueueCapacity <= 0) {\r\n        throw new IllegalArgumentException(\"initial queue capacity for [\" + name + \"] executor must be positive, got: \" + initialQueueCapacity);\r\n    }\r\n    ResizableBlockingQueue<Runnable> queue = new ResizableBlockingQueue(ConcurrentCollections.<Runnable>newBlockingQueue(), initialQueueCapacity);\r\n    return new QueueResizingEsThreadPoolExecutor(name, size, size, 0, TimeUnit.MILLISECONDS, queue, minQueueSize, maxQueueSize, TimedRunnable::new, frameSize, targetedResponseTime, threadFactory, new EsAbortPolicy(), contextHolder);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataIndexStateService.validateShardLimit",
	"Comment": "validates whether a list of indices can be opened without going over the cluster shard limit.only counts indices which arecurrently closed and will be opened, ignores indices which are already open.",
	"Method": "void validateShardLimit(ClusterState currentState,Index[] indices){\r\n    int shardsToOpen = Arrays.stream(indices).filter(index -> currentState.metaData().index(index).getState().equals(IndexMetaData.State.CLOSE)).mapToInt(index -> getTotalShardCount(currentState, index)).sum();\r\n    Optional<String> error = IndicesService.checkShardLimit(shardsToOpen, currentState);\r\n    if (error.isPresent()) {\r\n        ValidationException ex = new ValidationException();\r\n        ex.addValidationError(error.get());\r\n        throw ex;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.getNumDocDeletes",
	"Comment": "returns the number of documents have been deleted since this engine was opened.this count does not include the deletions from the existing segments before opening engine.",
	"Method": "long getNumDocDeletes(){\r\n    return numDocDeletes.count();\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.DocumentReader.setKeepOriginalText",
	"Comment": "sets whether created documents should store their source text along with tokenized words.",
	"Method": "void setKeepOriginalText(boolean keepOriginalText){\r\n    this.keepOriginalText = keepOriginalText;\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequestBuilder.setOffsets",
	"Comment": "sets whether to return the start and stop offsets for each term if they were stored orskip offsets.",
	"Method": "TermVectorsRequestBuilder setOffsets(boolean offsets){\r\n    request.offsets(offsets);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.InjectorShell.bindInjector",
	"Comment": "the injector is a special case because we allow both parent and child injectors to both havea binding for that key.",
	"Method": "void bindInjector(InjectorImpl injector){\r\n    Key<Injector> key = Key.get(Injector.class);\r\n    InjectorFactory injectorFactory = new InjectorFactory(injector);\r\n    injector.state.putBinding(key, new ProviderInstanceBindingImpl(injector, key, SourceProvider.UNKNOWN_SOURCE, injectorFactory, Scoping.UNSCOPED, injectorFactory, emptySet()));\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.AbstractTreebankParserParams.supportsBasicDependencies",
	"Comment": "by default, parsers are assumed to not support dependencies.only english and chinese do at present.",
	"Method": "boolean supportsBasicDependencies(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.node.InternalSettingsPreparer.prepareEnvironment",
	"Comment": "prepares the settings by gathering all elasticsearch system properties, optionally loading the configuration settings.",
	"Method": "Environment prepareEnvironment(Settings input,Map<String, String> properties,Path configPath,Supplier<String> defaultNodeName){\r\n    Settings.Builder output = Settings.builder();\r\n    initializeSettings(output, input, properties);\r\n    Environment environment = new Environment(output.build(), configPath);\r\n    if (Files.exists(environment.configFile().resolve(\"elasticsearch.yaml\"))) {\r\n        throw new SettingsException(\"elasticsearch.yaml was deprecated in 5.5.0 and must be renamed to elasticsearch.yml\");\r\n    }\r\n    if (Files.exists(environment.configFile().resolve(\"elasticsearch.json\"))) {\r\n        throw new SettingsException(\"elasticsearch.json was deprecated in 5.5.0 and must be converted to elasticsearch.yml\");\r\n    }\r\n    output = Settings.builder();\r\n    Path path = environment.configFile().resolve(\"elasticsearch.yml\");\r\n    if (Files.exists(path)) {\r\n        try {\r\n            output.loadFromPath(path);\r\n        } catch (IOException e) {\r\n            throw new SettingsException(\"Failed to load settings from \" + path.toString(), e);\r\n        }\r\n    }\r\n    initializeSettings(output, input, properties);\r\n    checkSettingsForTerminalDeprecation(output);\r\n    finalizeSettings(output, defaultNodeName);\r\n    environment = new Environment(output.build(), configPath);\r\n    output.put(Environment.PATH_LOGS_SETTING.getKey(), environment.logsFile().toAbsolutePath().normalize().toString());\r\n    return new Environment(output.build(), configPath);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequest.scroll",
	"Comment": "if set, will enable scrolling of the search request for the specified timeout.",
	"Method": "Scroll scroll(SearchRequest scroll,Scroll scroll,SearchRequest scroll,TimeValue keepAlive,SearchRequest scroll,String keepAlive){\r\n    return scroll(new Scroll(TimeValue.parseTimeValue(keepAlive, null, getClass().getSimpleName() + \".Scroll.keepAlive\")));\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.Guice.createInjector",
	"Comment": "creates an injector for the given set of modules, in a given developmentstage.",
	"Method": "Injector createInjector(Module modules,Injector createInjector,Iterable<? extends Module> modules,Injector createInjector,Stage stage,Module modules,Injector createInjector,Stage stage,Iterable<? extends Module> modules){\r\n    return new InjectorBuilder().stage(stage).addModules(modules).build();\r\n}"
}, {
	"Path": "org.elasticsearch.ElasticsearchException.getDetailedMessage",
	"Comment": "return the detail message, including the message from the nested exceptionif there is one.",
	"Method": "String getDetailedMessage(){\r\n    if (getCause() != null) {\r\n        StringBuilder sb = new StringBuilder();\r\n        sb.append(toString()).append(\"; \");\r\n        if (getCause() instanceof ElasticsearchException) {\r\n            sb.append(((ElasticsearchException) getCause()).getDetailedMessage());\r\n        } else {\r\n            sb.append(getCause());\r\n        }\r\n        return sb.toString();\r\n    } else {\r\n        return super.toString();\r\n    }\r\n}"
}, {
	"Path": "android.util.Base64OutputStream.embiggen",
	"Comment": "if b.length is at least len, return b.otherwise return a newbyte array of length len.",
	"Method": "byte[] embiggen(byte[] b,int len){\r\n    if (b == null || b.length < len) {\r\n        return new byte[len];\r\n    } else {\r\n        return b;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.SequenceMatcher.findNextNonOverlapping",
	"Comment": "searches for pattern in the region starting at the next index",
	"Method": "boolean findNextNonOverlapping(){\r\n    if (nextMatchStart < 0) {\r\n        return false;\r\n    }\r\n    return find(nextMatchStart, false);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.functionscore.RandomScoreFunctionBuilder.seed",
	"Comment": "sets the seed based on which the random number will be generated. using the same seed is guaranteed to generate the samerandom number for a specific doc.",
	"Method": "RandomScoreFunctionBuilder seed(int seed,RandomScoreFunctionBuilder seed,long seed,RandomScoreFunctionBuilder seed,String seed){\r\n    if (seed == null) {\r\n        throw new IllegalArgumentException(\"random_score function: seed must not be null\");\r\n    }\r\n    this.seed = seed.hashCode();\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.DelayedAllocationService.scheduleIfNeeded",
	"Comment": "figure out if an existing scheduled reroute is good enough or whether we need to cancel and reschedule.",
	"Method": "void scheduleIfNeeded(long currentNanoTime,ClusterState state){\r\n    assertClusterOrMasterStateThread();\r\n    long nextDelayNanos = UnassignedInfo.findNextDelayedAllocation(currentNanoTime, state);\r\n    if (nextDelayNanos < 0) {\r\n        logger.trace(\"no need to schedule reroute - no delayed unassigned shards\");\r\n        removeTaskAndCancel();\r\n    } else {\r\n        TimeValue nextDelay = TimeValue.timeValueNanos(nextDelayNanos);\r\n        final boolean earlierRerouteNeeded;\r\n        DelayedRerouteTask existingTask = delayedRerouteTask.get();\r\n        DelayedRerouteTask newTask = new DelayedRerouteTask(nextDelay, currentNanoTime);\r\n        if (existingTask == null) {\r\n            earlierRerouteNeeded = true;\r\n        } else if (newTask.scheduledTimeToRunInNanos() < existingTask.scheduledTimeToRunInNanos()) {\r\n            logger.trace(\"cancelling existing delayed reroute task as delayed reroute has to happen [{}] earlier\", TimeValue.timeValueNanos(existingTask.scheduledTimeToRunInNanos() - newTask.scheduledTimeToRunInNanos()));\r\n            existingTask.cancelScheduling();\r\n            earlierRerouteNeeded = true;\r\n        } else {\r\n            earlierRerouteNeeded = false;\r\n        }\r\n        if (earlierRerouteNeeded) {\r\n            logger.info(\"scheduling reroute for delayed shards in [{}] ({} delayed shards)\", nextDelay, UnassignedInfo.getNumberOfDelayedUnassigned(state));\r\n            DelayedRerouteTask currentTask = delayedRerouteTask.getAndSet(newTask);\r\n            assert existingTask == currentTask || currentTask == null;\r\n            newTask.schedule();\r\n        } else {\r\n            logger.trace(\"no need to reschedule delayed reroute - currently scheduled delayed reroute in [{}] is enough\", nextDelay);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.builders.PolygonBuilder.multipolygonS4J",
	"Comment": "create a multipolygon from a set of coordinates. each primary array contains a polygon whichin turn contains an array of linestrings. these line strings are represented as an array ofcoordinates. the first linestring will be the shell of the polygon the others define holeswithin the polygon.",
	"Method": "MultiPolygon multipolygonS4J(GeometryFactory factory,Coordinate[][][] polygons){\r\n    Polygon[] polygonSet = new Polygon[polygons.length];\r\n    for (int i = 0; i < polygonSet.length; i++) {\r\n        polygonSet[i] = polygonS4J(factory, polygons[i]);\r\n    }\r\n    return factory.createMultiPolygon(polygonSet);\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.acquireRetentionLock",
	"Comment": "acquires a lock on the translog files, preventing them from being trimmed",
	"Method": "Closeable acquireRetentionLock(){\r\n    try (ReleasableLock lock = readLock.acquire()) {\r\n        ensureOpen();\r\n        final long viewGen = getMinFileGeneration();\r\n        return acquireTranslogGenFromDeletionPolicy(viewGen);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.upgrade.post.UpgradeSettingsClusterStateUpdateRequest.versions",
	"Comment": "sets the index to version map for indices that should be updated",
	"Method": "Map<String, Tuple<Version, String>> versions(UpgradeSettingsClusterStateUpdateRequest versions,Map<String, Tuple<Version, String>> versions){\r\n    this.versions = versions;\r\n    return this;\r\n}"
}, {
	"Path": "org.openqa.grid.web.servlet.console.MiniCapability.getConsoleIconPath",
	"Comment": "get the icon representing the browser for the grid. if the icon cannot be located, returnsnull.",
	"Method": "String getConsoleIconPath(DesiredCapabilities cap){\r\n    String name = consoleIconName(cap);\r\n    String path = \"org/openqa/grid/images/\";\r\n    InputStream in = Thread.currentThread().getContextClassLoader().getResourceAsStream(path + name + \".png\");\r\n    if (in == null) {\r\n        return null;\r\n    }\r\n    return \"/grid/resources/\" + path + name + \".png\";\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.OpenIE.relationInFragment",
	"Comment": "returns the possible relation triple in this sentence fragment.",
	"Method": "Optional<RelationTriple> relationInFragment(SentenceFragment fragment,Optional<RelationTriple> relationInFragment,SentenceFragment fragment,CoreMap sentence){\r\n    return segmenter.segment(fragment.parseTree, Optional.of(fragment.score), consumeAll);\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.AbstractScopedSettings.isValidKey",
	"Comment": "returns true iff the given key is a valid settings key otherwise false",
	"Method": "boolean isValidKey(String key){\r\n    return KEY_PATTERN.matcher(key).matches();\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.NeuralUtils.concatenate",
	"Comment": "concatenates several column vectors into one large column vector",
	"Method": "SimpleMatrix concatenate(SimpleMatrix vectors){\r\n    int size = 0;\r\n    for (SimpleMatrix vector : vectors) {\r\n        size += vector.numRows();\r\n    }\r\n    SimpleMatrix result = new SimpleMatrix(size, 1);\r\n    int index = 0;\r\n    for (SimpleMatrix vector : vectors) {\r\n        result.insertIntoThis(index, 0, vector);\r\n        index += vector.numRows();\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.matcher.MultiMatch.getMultioffsets",
	"Comment": "offsets in the original string to which each multimatch is aligned to",
	"Method": "List<HasInterval<Integer>> getMultioffsets(){\r\n    if (multimatches == null)\r\n        return null;\r\n    List<HasInterval<Integer>> multioffsets = new ArrayList(multimatches.size());\r\n    for (Match<K, V> m : multimatches) {\r\n        multioffsets.add(m.getInterval());\r\n    }\r\n    return multioffsets;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.NDArrayDoubles.getTableAccessOffset",
	"Comment": "compute the distance into the one dimensional factortable array that corresponds to a setting of all theneighbors of the factor.",
	"Method": "int getTableAccessOffset(int[] assignment){\r\n    assert (assignment.length == dimensions.length);\r\n    int offset = 0;\r\n    for (int i = 0; i < assignment.length; i++) {\r\n        assert (assignment[i] < dimensions[i]);\r\n        offset = (offset * dimensions[i]) + assignment[i];\r\n    }\r\n    return offset;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.FieldTypeLookup.simpleMatchToFullName",
	"Comment": "returns a list of the full names of a simple match regex like pattern against full name and index name.",
	"Method": "Collection<String> simpleMatchToFullName(String pattern){\r\n    Set<String> fields = new HashSet();\r\n    for (MappedFieldType fieldType : this) {\r\n        if (Regex.simpleMatch(pattern, fieldType.name())) {\r\n            fields.add(fieldType.name());\r\n        }\r\n    }\r\n    for (String aliasName : aliasToConcreteName.keySet()) {\r\n        if (Regex.simpleMatch(pattern, aliasName)) {\r\n            fields.add(aliasName);\r\n        }\r\n    }\r\n    return fields;\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.CreateClauseDataset.processDirectory",
	"Comment": "process all the trees in the given directory. for example, the wsj section of the penn treebank.",
	"Method": "List<Pair<CoreMap, Collection<Pair<Span, Span>>>> processDirectory(String name,File directory){\r\n    forceTrack(\"Processing \" + name);\r\n    Iterable<File> files = IOUtils.iterFilesRecursive(directory, \"mrg\");\r\n    int numTreesProcessed = 0;\r\n    List<Pair<CoreMap, Collection<Pair<Span, Span>>>> trainingData = new ArrayList(1024);\r\n    for (File file : files) {\r\n        TreeReader reader = new PennTreeReader(IOUtils.readerFromFile(file));\r\n        Tree tree;\r\n        while ((tree = reader.readTree()) != null) {\r\n            try {\r\n                tree.indexSpans();\r\n                tree.setSpans();\r\n                List<CoreLabel> tokens = // .filter(leaf -> !TRACE_SOURCE_PATTERN.matcher(leaf.word()).matches() && !leaf.tag().equals(\"-NONE-\"))\r\n                tree.getLeaves().stream().map(leaf -> (CoreLabel) leaf.label()).collect(Collectors.toList());\r\n                SemanticGraph graph = parse(tree);\r\n                Map<Integer, Span> targets = findTraceTargets(tree);\r\n                Map<Integer, Integer> sources = findTraceSources(tree);\r\n                CoreMap sentence = new ArrayCoreMap(4) {\r\n                    {\r\n                        set(CoreAnnotations.TokensAnnotation.class, tokens);\r\n                        set(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class, graph);\r\n                        set(SemanticGraphCoreAnnotations.EnhancedDependenciesAnnotation.class, graph);\r\n                        set(SemanticGraphCoreAnnotations.EnhancedPlusPlusDependenciesAnnotation.class, graph);\r\n                    }\r\n                };\r\n                natlog.doOneSentence(null, sentence);\r\n                Collection<Pair<Span, Span>> trainingDataFromSentence = subjectObjectPairs(graph, tokens, targets, sources);\r\n                trainingData.add(Pair.makePair(sentence, trainingDataFromSentence));\r\n                numTreesProcessed += 1;\r\n                if (numTreesProcessed % 100 == 0) {\r\n                    log(\"[\" + new DecimalFormat(\"00000\").format(numTreesProcessed) + \"] \" + countDatums(trainingData) + \" known extractions\");\r\n                }\r\n            } catch (Throwable t) {\r\n                t.printStackTrace();\r\n            }\r\n        }\r\n    }\r\n    log(\"\" + numTreesProcessed + \" trees processed yielding \" + countDatums(trainingData) + \" known extractions\");\r\n    endTrack(\"Processing \" + name);\r\n    return trainingData;\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksService.waitForPersistentTasksCondition",
	"Comment": "waits for persistent tasks to comply with a given predicate, then call back the listener accordingly.",
	"Method": "void waitForPersistentTasksCondition(Predicate<PersistentTasksCustomMetaData> predicate,TimeValue timeout,ActionListener<Boolean> listener){\r\n    final Predicate<ClusterState> clusterStatePredicate = clusterState -> predicate.test(clusterState.metaData().custom(PersistentTasksCustomMetaData.TYPE));\r\n    final ClusterStateObserver observer = new ClusterStateObserver(clusterService, timeout, logger, threadPool.getThreadContext());\r\n    if (clusterStatePredicate.test(observer.setAndGetObservedState())) {\r\n        listener.onResponse(true);\r\n    } else {\r\n        observer.waitForNextChange(new ClusterStateObserver.Listener() {\r\n            @Override\r\n            public void onNewClusterState(ClusterState state) {\r\n                listener.onResponse(true);\r\n            }\r\n            @Override\r\n            public void onClusterServiceClose() {\r\n                listener.onFailure(new NodeClosedException(clusterService.localNode()));\r\n            }\r\n            @Override\r\n            public void onTimeout(TimeValue timeout) {\r\n                listener.onFailure(new IllegalStateException(\"Timed out when waiting for persistent tasks after \" + timeout));\r\n            }\r\n        }, clusterStatePredicate, timeout);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksService.waitForPersistentTasksCondition",
	"Comment": "waits for persistent tasks to comply with a given predicate, then call back the listener accordingly.",
	"Method": "void waitForPersistentTasksCondition(Predicate<PersistentTasksCustomMetaData> predicate,TimeValue timeout,ActionListener<Boolean> listener){\r\n    listener.onResponse(true);\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksService.waitForPersistentTasksCondition",
	"Comment": "waits for persistent tasks to comply with a given predicate, then call back the listener accordingly.",
	"Method": "void waitForPersistentTasksCondition(Predicate<PersistentTasksCustomMetaData> predicate,TimeValue timeout,ActionListener<Boolean> listener){\r\n    listener.onFailure(new NodeClosedException(clusterService.localNode()));\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksService.waitForPersistentTasksCondition",
	"Comment": "waits for persistent tasks to comply with a given predicate, then call back the listener accordingly.",
	"Method": "void waitForPersistentTasksCondition(Predicate<PersistentTasksCustomMetaData> predicate,TimeValue timeout,ActionListener<Boolean> listener){\r\n    listener.onFailure(new IllegalStateException(\"Timed out when waiting for persistent tasks after \" + timeout));\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.GeoPointFieldMapper.parseGeoPointIgnoringMalformed",
	"Comment": "parses geopoint represented as an object or an array, ignores malformed geopoints if needed",
	"Method": "void parseGeoPointIgnoringMalformed(ParseContext context,GeoPoint sparse){\r\n    try {\r\n        parse(context, GeoUtils.parseGeoPoint(context.parser(), sparse));\r\n    } catch (ElasticsearchParseException e) {\r\n        if (ignoreMalformed.value() == false) {\r\n            throw e;\r\n        }\r\n        context.addIgnoredField(fieldType.name());\r\n    }\r\n}"
}, {
	"Path": "org.apache.dubbo.remoting.transport.netty.ClientReconnectTest.testReconnectWarnLog",
	"Comment": "reconnect log check, when the time is not enough for shutdown time, there is no error log, but there must be a warn log",
	"Method": "void testReconnectWarnLog(){\r\n    int port = NetUtils.getAvailablePort();\r\n    DubboAppender.doStart();\r\n    String url = \"exchange://127.0.0.1:\" + port + \"/client.reconnect.test?check=false&client=netty3&\" + Constants.RECONNECT_KEY + \"=\" + 1;\r\n    try {\r\n        Exchangers.connect(url);\r\n    } catch (Exception e) {\r\n    }\r\n    Thread.sleep(1500);\r\n    Assert.assertEquals(\"no error message \", 0, LogUtil.findMessage(Level.ERROR, \"client reconnect to \"));\r\n    Assert.assertEquals(\"must have one warn message \", 1, LogUtil.findMessage(Level.WARN, \"client reconnect to \"));\r\n    DubboAppender.doStop();\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.getMaxRegexLength",
	"Comment": "the maximum length of regex string allowed in a regexp query.",
	"Method": "int getMaxRegexLength(){\r\n    return maxRegexLength;\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.tasks.BaseTasksRequest.getActions",
	"Comment": "return the list of action masks for the actions that should be returned",
	"Method": "String[] getActions(){\r\n    return actions;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDocument.init",
	"Comment": "initializes a new basicdocument with the given list of words and title.",
	"Method": "BasicDocument<L> init(String text,String title,boolean keepOriginalText,BasicDocument<L> init,String text,String title,BasicDocument<L> init,String text,boolean keepOriginalText,BasicDocument<L> init,String text,BasicDocument<L> init,BasicDocument<L> init,Reader textReader,String title,boolean keepOriginalText,BasicDocument<L> init,Reader textReader,String title,BasicDocument<L> init,Reader textReader,boolean keepOriginalText,BasicDocument<L> init,Reader textReader,BasicDocument<L> init,File textFile,String title,boolean keepOriginalText,BasicDocument<L> init,File textFile,String title,BasicDocument<L> init,File textFile,boolean keepOriginalText,BasicDocument<L> init,File textFile,BasicDocument<L> init,URL textURL,String title,boolean keepOriginalText,BasicDocument<L> init,URL textURL,String title,BasicDocument<L> init,URL textURL,boolean keepOriginalText,BasicDocument<L> init,URL textURL,BasicDocument<L> init,List<? extends Word> words,String title,BasicDocument<L> init,List<? extends Word> words){\r\n    return init(words, null);\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.readObjectFromFile",
	"Comment": "read an object from a stored file. it is silently ungzipped, regardless of name.",
	"Method": "T readObjectFromFile(File file,T readObjectFromFile,String filename){\r\n    return ErasureUtils.uncheckedCast(readObjectFromFile(new File(filename)));\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryStringQueryBuilder.analyzer",
	"Comment": "the optional analyzer used to analyze the query string. note, if a field has search analyzerdefined for it, then it will be used automatically. defaults to the smart search analyzer.",
	"Method": "QueryStringQueryBuilder analyzer(String analyzer,String analyzer){\r\n    return analyzer;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.MappedFieldType.nullValueAsString",
	"Comment": "returns the null value stringified or null if there is no null value",
	"Method": "String nullValueAsString(){\r\n    return nullValueAsString;\r\n}"
}, {
	"Path": "org.elasticsearch.monitor.os.OsProbe.getMethod",
	"Comment": "returns a given method of the operatingsystemmxbean, or null if the method is not found or unavailable.",
	"Method": "Method getMethod(String methodName){\r\n    try {\r\n        return Class.forName(\"com.sun.management.OperatingSystemMXBean\").getMethod(methodName);\r\n    } catch (Exception e) {\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "org.openqa.grid.internal.StatusServletTests.testHubGetSpecifiedConfigWithQueryString",
	"Comment": "if a certain set of parameters are requested to the hub, only those params are returned.",
	"Method": "void testHubGetSpecifiedConfigWithQueryString(){\r\n    List<String> keys = new ArrayList();\r\n    keys.add(URLEncoder.encode(\"timeout\", \"UTF-8\"));\r\n    keys.add(URLEncoder.encode(\"I'm not a valid key\", \"UTF-8\"));\r\n    keys.add(URLEncoder.encode(\"servlets\", \"UTF-8\"));\r\n    String query = \"?configuration=\" + String.join(\",\", keys);\r\n    String url = hubApi.toExternalForm() + query;\r\n    HttpRequest r = new HttpRequest(GET, url);\r\n    HttpResponse response = client.execute(r);\r\n    assertEquals(200, response.getStatus());\r\n    Map<String, Object> o = extractObject(response);\r\n    assertTrue((Boolean) o.get(\"success\"));\r\n    assertEquals(12345L, o.get(\"timeout\"));\r\n    assertNull(o.get(\"I'm not a valid key\"));\r\n    assertTrue(((Collection<?>) o.get(\"servlets\")).size() == 0);\r\n    assertFalse(o.containsKey(\"capabilityMatcher\"));\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.EngineConfig.getInternalRefreshListener",
	"Comment": "the refresh listeners to add to lucene for internally visible refreshes. these listeners will also be invoked on external refreshes",
	"Method": "List<ReferenceManager.RefreshListener> getInternalRefreshListener(){\r\n    return internalRefreshListener;\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.PluginsService.checkBundleJarHell",
	"Comment": "the plugin cli does it, but we do it again, in case lusers mess with jar files manually",
	"Method": "void checkBundleJarHell(Set<URL> classpath,Bundle bundle,Map<String, Set<URL>> transitiveUrls){\r\n    List<String> exts = bundle.plugin.getExtendedPlugins();\r\n    try {\r\n        final Logger logger = LogManager.getLogger(JarHell.class);\r\n        Set<URL> urls = new HashSet();\r\n        for (String extendedPlugin : exts) {\r\n            Set<URL> pluginUrls = transitiveUrls.get(extendedPlugin);\r\n            assert pluginUrls != null : \"transitive urls should have already been set for \" + extendedPlugin;\r\n            Set<URL> intersection = new HashSet(urls);\r\n            intersection.retainAll(pluginUrls);\r\n            if (intersection.isEmpty() == false) {\r\n                throw new IllegalStateException(\"jar hell! extended plugins \" + exts + \" have duplicate codebases with each other: \" + intersection);\r\n            }\r\n            intersection = new HashSet(bundle.urls);\r\n            intersection.retainAll(pluginUrls);\r\n            if (intersection.isEmpty() == false) {\r\n                throw new IllegalStateException(\"jar hell! duplicate codebases with extended plugin [\" + extendedPlugin + \"]: \" + intersection);\r\n            }\r\n            urls.addAll(pluginUrls);\r\n            JarHell.checkJarHell(urls, logger::debug);\r\n        }\r\n        urls.addAll(bundle.urls);\r\n        JarHell.checkJarHell(urls, logger::debug);\r\n        transitiveUrls.put(bundle.plugin.getName(), urls);\r\n        Set<URL> intersection = new HashSet(classpath);\r\n        intersection.retainAll(bundle.urls);\r\n        if (intersection.isEmpty() == false) {\r\n            throw new IllegalStateException(\"jar hell! duplicate codebases between plugin and core: \" + intersection);\r\n        }\r\n        Set<URL> union = new HashSet(classpath);\r\n        union.addAll(bundle.urls);\r\n        JarHell.checkJarHell(union, logger::debug);\r\n    } catch (Exception e) {\r\n        throw new IllegalStateException(\"failed to load plugin \" + bundle.plugin.getName() + \" due to jar hell\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.DeferredLookups.initialize",
	"Comment": "initialize the specified lookups, either immediately or when the injector is created.",
	"Method": "void initialize(Errors errors){\r\n    injector.lookups = injector;\r\n    new LookupProcessor(errors).process(injector, lookups);\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.DocsStats.getAverageSizeInBytes",
	"Comment": "returns the average size in bytes of all documents in this stats.",
	"Method": "long getAverageSizeInBytes(){\r\n    long totalDocs = count + deleted;\r\n    return totalDocs == 0 ? 0 : totalSizeInBytes / totalDocs;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.node.DiscoveryNodes.delta",
	"Comment": "returns the changes comparing this nodes to the provided nodes.",
	"Method": "Delta delta(DiscoveryNodes other){\r\n    final List<DiscoveryNode> removed = new ArrayList();\r\n    final List<DiscoveryNode> added = new ArrayList();\r\n    for (DiscoveryNode node : other) {\r\n        if (this.nodeExists(node) == false) {\r\n            removed.add(node);\r\n        }\r\n    }\r\n    for (DiscoveryNode node : this) {\r\n        if (other.nodeExists(node) == false) {\r\n            added.add(node);\r\n        }\r\n    }\r\n    return new Delta(other.getMasterNode(), getMasterNode(), localNodeId, Collections.unmodifiableList(removed), Collections.unmodifiableList(added));\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.NeuralUtils.elementwiseApplyTanh",
	"Comment": "applies tanh to each of the entries in the matrix.returns a new matrix.",
	"Method": "SimpleMatrix elementwiseApplyTanh(SimpleMatrix input){\r\n    SimpleMatrix output = new SimpleMatrix(input);\r\n    for (int i = 0; i < output.numRows(); ++i) {\r\n        for (int j = 0; j < output.numCols(); ++j) {\r\n            output.set(i, j, Math.tanh(output.get(i, j)));\r\n        }\r\n    }\r\n    return output;\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequestBuilder.setVersion",
	"Comment": "sets the version, which will cause the index operation to only be performed if a matchingversion exists and no changes happened on the doc since then.",
	"Method": "UpdateRequestBuilder setVersion(long version){\r\n    request.version(version);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.parsers.GeoWKTParser.parseExpectedType",
	"Comment": "throws an exception if the parsed geometry type does not match the expected shape type",
	"Method": "ShapeBuilder parseExpectedType(XContentParser parser,GeoShapeType shapeType,ShapeBuilder parseExpectedType,XContentParser parser,GeoShapeType shapeType,GeoShapeFieldMapper shapeMapper){\r\n    try (StringReader reader = new StringReader(parser.text())) {\r\n        Explicit<Boolean> ignoreZValue = (shapeMapper == null) ? GeoShapeFieldMapper.Defaults.IGNORE_Z_VALUE : shapeMapper.ignoreZValue();\r\n        Explicit<Boolean> coerce = (shapeMapper == null) ? GeoShapeFieldMapper.Defaults.COERCE : shapeMapper.coerce();\r\n        StreamTokenizer tokenizer = new StreamTokenizer(reader);\r\n        tokenizer.resetSyntax();\r\n        tokenizer.wordChars('a', 'z');\r\n        tokenizer.wordChars('A', 'Z');\r\n        tokenizer.wordChars(128 + 32, 255);\r\n        tokenizer.wordChars('0', '9');\r\n        tokenizer.wordChars('-', '-');\r\n        tokenizer.wordChars('+', '+');\r\n        tokenizer.wordChars('.', '.');\r\n        tokenizer.whitespaceChars(0, ' ');\r\n        tokenizer.commentChar('#');\r\n        ShapeBuilder builder = parseGeometry(tokenizer, shapeType, ignoreZValue.value(), coerce.value());\r\n        checkEOF(tokenizer);\r\n        return builder;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.SloppyMath.exactBinomial",
	"Comment": "find a one tailed exact binomial test probability.finds the chanceof this or a higher result",
	"Method": "double exactBinomial(int k,int n,double p){\r\n    double total = 0.0;\r\n    for (int m = k; m <= n; m++) {\r\n        double nChooseM = 1.0;\r\n        for (int r = 1; r <= m; r++) {\r\n            nChooseM *= (n - r) + 1;\r\n            nChooseM /= r;\r\n        }\r\n        total += nChooseM * Math.pow(p, m) * Math.pow(1.0 - p, n - m);\r\n    }\r\n    return total;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.TestSession.setExternalKey",
	"Comment": "associate this session to the session provided by the remote.",
	"Method": "void setExternalKey(ExternalSessionKey externalKey){\r\n    this.externalKey = externalKey;\r\n    sessionCreatedAt = lastActivity;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsRequestBuilder.setExpandWildcardsClosed",
	"Comment": "controls whether wildcard expressions will be expanded to existing closed indices",
	"Method": "IndicesExistsRequestBuilder setExpandWildcardsClosed(boolean expandWildcardsClosed){\r\n    request.expandWilcardsClosed(expandWildcardsClosed);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotIndexShardStatus.getNodeId",
	"Comment": "returns node id of the node where snapshot is currently running",
	"Method": "String getNodeId(){\r\n    return nodeId;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.demo.ParserDemo.demoDP",
	"Comment": "demodp demonstrates turning a file into tokens and then parsetrees.note that the trees are printed by calling pennprint onthe tree object.it is also possible to pass a printwriter topennprint if you want to capture the output.this code will work with any supported language.",
	"Method": "void demoDP(LexicalizedParser lp,String filename){\r\n    TreebankLanguagePack tlp = lp.treebankLanguagePack();\r\n    GrammaticalStructureFactory gsf = null;\r\n    if (tlp.supportsGrammaticalStructures()) {\r\n        gsf = tlp.grammaticalStructureFactory();\r\n    }\r\n    for (List<HasWord> sentence : new DocumentPreprocessor(filename)) {\r\n        Tree parse = lp.apply(sentence);\r\n        parse.pennPrint();\r\n        System.out.println();\r\n        if (gsf != null) {\r\n            GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);\r\n            Collection tdl = gs.typedDependenciesCCprocessed();\r\n            System.out.println(tdl);\r\n            System.out.println();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVector.increaseSizeTo",
	"Comment": "this increases the length of the vector, while preserving its contents",
	"Method": "void increaseSizeTo(int newSize){\r\n    assert (newSize > pointers.length);\r\n    double[][] pointersBuf = new double[newSize][];\r\n    boolean[] sparseBuf = new boolean[newSize];\r\n    boolean[] copyOnWriteBuf = new boolean[newSize];\r\n    System.arraycopy(pointers, 0, pointersBuf, 0, pointers.length);\r\n    System.arraycopy(sparse, 0, sparseBuf, 0, pointers.length);\r\n    System.arraycopy(copyOnWrite, 0, copyOnWriteBuf, 0, pointers.length);\r\n    pointers = pointersBuf;\r\n    sparse = sparseBuf;\r\n    copyOnWrite = copyOnWriteBuf;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.IndexShardRoutingTable.activeInitializingShardsIt",
	"Comment": "returns an iterator over active and initializing shards. making sure though thatits random within the active shards, and initializing shards are the last to iterate through.",
	"Method": "ShardIterator activeInitializingShardsIt(int seed){\r\n    if (allInitializingShards.isEmpty()) {\r\n        return new PlainShardIterator(shardId, shuffler.shuffle(activeShards, seed));\r\n    }\r\n    ArrayList<ShardRouting> ordered = new ArrayList(activeShards.size() + allInitializingShards.size());\r\n    ordered.addAll(shuffler.shuffle(activeShards, seed));\r\n    ordered.addAll(allInitializingShards);\r\n    return new PlainShardIterator(shardId, ordered);\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.getMaxInnerResultWindow",
	"Comment": "returns the max result window for an individual inner hit definition or top hits aggregation.",
	"Method": "int getMaxInnerResultWindow(){\r\n    return maxInnerResultWindow;\r\n}"
}, {
	"Path": "org.elasticsearch.http.DefaultRestChannel.isCloseConnection",
	"Comment": "determine if the request connection should be closed on completion.",
	"Method": "boolean isCloseConnection(){\r\n    final boolean http10 = isHttp10();\r\n    return CLOSE.equalsIgnoreCase(request.header(CONNECTION)) || (http10 && !KEEP_ALIVE.equalsIgnoreCase(request.header(CONNECTION)));\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.MLEDependencyGrammar.addRule",
	"Comment": "add this dependency with the given count to the grammar. this is the main entry point of mledependencygrammarextractor. this is a dependency represented in the full tag space.",
	"Method": "void addRule(IntDependency dependency,double count){\r\n    if (!directional) {\r\n        dependency = new IntDependency(dependency.head, dependency.arg, false, dependency.distance);\r\n    }\r\n    if (verbose)\r\n        log.info(\"Adding dep \" + dependency);\r\n    expandDependency(dependency, count);\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.OpenIE.relationsInFragments",
	"Comment": "returns a list of openie relations from the given set of sentence fragments.",
	"Method": "List<RelationTriple> relationsInFragments(Collection<SentenceFragment> fragments,List<RelationTriple> relationsInFragments,Collection<SentenceFragment> fragments,CoreMap sentence){\r\n    return fragments.stream().map(x -> relationInFragment(x, sentence)).filter(Optional::isPresent).map(Optional::get).collect(Collectors.toList());\r\n}"
}, {
	"Path": "org.openqa.grid.common.RegistrationRequest.validate",
	"Comment": "validate the current setting and throw a config exception is an invalid setup is detected.",
	"Method": "void validate(){\r\n    try {\r\n        configuration.getHubHost();\r\n        configuration.getHubPort();\r\n    } catch (RuntimeException e) {\r\n        throw new GridConfigurationException(e.getMessage());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksService.sendRemoveRequest",
	"Comment": "notifies the master node to remove a persistent task from the cluster state",
	"Method": "void sendRemoveRequest(String taskId,ActionListener<PersistentTask<?>> listener){\r\n    RemovePersistentTaskAction.Request request = new RemovePersistentTaskAction.Request(taskId);\r\n    execute(request, RemovePersistentTaskAction.INSTANCE, listener);\r\n}"
}, {
	"Path": "org.json.JSONTokener.back",
	"Comment": "unreads the most recent character of input. if no input characters havebeen read, the input is unchanged.",
	"Method": "void back(){\r\n    if (--pos == -1) {\r\n        pos = 0;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.AckedClusterStateUpdateTask.mustAck",
	"Comment": "called to determine which nodes the acknowledgement is expected from",
	"Method": "boolean mustAck(DiscoveryNode discoveryNode){\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.PluginsService.getPluginBundles",
	"Comment": "get bundles for plugins installed in the given plugins directory.",
	"Method": "Set<Bundle> getPluginBundles(Path pluginsDirectory){\r\n    return findBundles(pluginsDirectory, \"plugin\");\r\n}"
}, {
	"Path": "org.elasticsearch.common.io.FileSystemUtils.isDesktopServicesStore",
	"Comment": "check whether the file denoted by the given path is a desktop services store created by finder on macos.",
	"Method": "boolean isDesktopServicesStore(Path path){\r\n    return Constants.MAC_OS_X && Files.isRegularFile(path) && \".DS_Store\".equals(path.getFileName().toString());\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryShardContext.simpleMatchToIndexNames",
	"Comment": "returns all the fields that match a given pattern. if prefixed with atype then the fields will be returned with a type prefix.",
	"Method": "Collection<String> simpleMatchToIndexNames(String pattern){\r\n    return mapperService.simpleMatchToFullName(pattern);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStats.getProcessedFileCount",
	"Comment": "returns number of files in the snapshot that were processed so far",
	"Method": "int getProcessedFileCount(){\r\n    return processedFileCount;\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.AsyncShardFetch.getNumberOfInFlightFetches",
	"Comment": "returns the number of async fetches that are currently ongoing.",
	"Method": "int getNumberOfInFlightFetches(){\r\n    int count = 0;\r\n    for (NodeEntry<T> nodeEntry : cache.values()) {\r\n        if (nodeEntry.isFetching()) {\r\n            count++;\r\n        }\r\n    }\r\n    return count;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.pascal.ISODateInstance.contains",
	"Comment": "returns true iff this datecontains the date represented by other.a range contains a date if itis equal to or after the start date and equal to orbefore the end date.for open ranges, containsis also inclusive of the one end point.",
	"Method": "boolean contains(ISODateInstance other){\r\n    if (this.isUnparseable() || other.isUnparseable()) {\r\n        return this.isoDate.equals(other.isoDate);\r\n    }\r\n    String start = this.getStartDate();\r\n    if (!start.equals(\"\")) {\r\n        String startOther = other.getStartDate();\r\n        if (startOther.equals(\"\")) {\r\n            return false;\r\n        } else {\r\n            if (!isAfter(startOther, start)) {\r\n                return false;\r\n            }\r\n        }\r\n    }\r\n    String end = this.getEndDate();\r\n    if (!end.isEmpty()) {\r\n        String endOther = other.getEndDate();\r\n        if (endOther.isEmpty()) {\r\n            return false;\r\n        } else {\r\n            if (!isAfter(end, endOther)) {\r\n                return false;\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.analysis.HunspellService.scanAndLoadDictionaries",
	"Comment": "scans the hunspell directory and loads all found dictionaries",
	"Method": "void scanAndLoadDictionaries(){\r\n    if (Files.isDirectory(hunspellDir)) {\r\n        try (DirectoryStream<Path> stream = Files.newDirectoryStream(hunspellDir)) {\r\n            for (Path file : stream) {\r\n                if (Files.isDirectory(file)) {\r\n                    try (DirectoryStream<Path> inner = Files.newDirectoryStream(hunspellDir.resolve(file), \"*.dic\")) {\r\n                        if (inner.iterator().hasNext()) {\r\n                            try {\r\n                                getDictionary(file.getFileName().toString());\r\n                            } catch (Exception e) {\r\n                                logger.error(() -> new ParameterizedMessage(\"exception while loading dictionary {}\", file.getFileName()), e);\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksExecutor.selectLeastLoadedNode",
	"Comment": "finds the least loaded node that satisfies the selector criteria",
	"Method": "DiscoveryNode selectLeastLoadedNode(ClusterState clusterState,Predicate<DiscoveryNode> selector){\r\n    long minLoad = Long.MAX_VALUE;\r\n    DiscoveryNode minLoadedNode = null;\r\n    PersistentTasksCustomMetaData persistentTasks = clusterState.getMetaData().custom(PersistentTasksCustomMetaData.TYPE);\r\n    for (DiscoveryNode node : clusterState.getNodes()) {\r\n        if (selector.test(node)) {\r\n            if (persistentTasks == null) {\r\n                return node;\r\n            }\r\n            long numberOfTasks = persistentTasks.getNumberOfTasksOnNode(node.getId(), taskName);\r\n            if (minLoad > numberOfTasks) {\r\n                minLoad = numberOfTasks;\r\n                minLoadedNode = node;\r\n            }\r\n        }\r\n    }\r\n    return minLoadedNode;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryStringQueryBuilder.fuzzyPrefixLength",
	"Comment": "set the minimum prefix length for fuzzy queries. default is 1.",
	"Method": "QueryStringQueryBuilder fuzzyPrefixLength(int fuzzyPrefixLength,int fuzzyPrefixLength){\r\n    return fuzzyPrefixLength;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.AckedClusterStateUpdateTask.ackTimeout",
	"Comment": "acknowledgement timeout, maximum time interval to wait for acknowledgements",
	"Method": "TimeValue ackTimeout(){\r\n    return request.ackTimeout();\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.UpdateByQueryRequest.setRouting",
	"Comment": "set routing limiting the process to the shards that match that routing value",
	"Method": "UpdateByQueryRequest setRouting(String routing){\r\n    if (routing != null) {\r\n        getSearchRequest().routing(routing);\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.ElasticsearchException.readStackTrace",
	"Comment": "deserializes stacktrace elements as well as suppressed exceptions from the given output stream andadds it to the given exception.",
	"Method": "T readStackTrace(T throwable,StreamInput in){\r\n    final int stackTraceElements = in.readVInt();\r\n    StackTraceElement[] stackTrace = new StackTraceElement[stackTraceElements];\r\n    for (int i = 0; i < stackTraceElements; i++) {\r\n        final String declaringClasss = in.readString();\r\n        final String fileName = in.readOptionalString();\r\n        final String methodName = in.readString();\r\n        final int lineNumber = in.readVInt();\r\n        stackTrace[i] = new StackTraceElement(declaringClasss, methodName, fileName, lineNumber);\r\n    }\r\n    throwable.setStackTrace(stackTrace);\r\n    int numSuppressed = in.readVInt();\r\n    for (int i = 0; i < numSuppressed; i++) {\r\n        throwable.addSuppressed(in.readException());\r\n    }\r\n    return throwable;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.ValueLabel.hashCode",
	"Comment": "return the hashcode of the string value providing there is one.otherwise, returns an arbitrary constant for the case ofnull.",
	"Method": "int hashCode(){\r\n    String val = value();\r\n    return val == null ? 3 : val.hashCode();\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.rnn.TopNGramRecord.simplifyTree",
	"Comment": "remove everything but the skeleton, the predictions, and the labels",
	"Method": "Tree simplifyTree(Tree tree){\r\n    CoreLabel newLabel = new CoreLabel();\r\n    newLabel.set(RNNCoreAnnotations.Predictions.class, RNNCoreAnnotations.getPredictions(tree));\r\n    newLabel.setValue(tree.label().value());\r\n    if (tree.isLeaf()) {\r\n        return tree.treeFactory().newLeaf(newLabel);\r\n    }\r\n    List<Tree> children = Generics.newArrayList(tree.children().length);\r\n    for (int i = 0; i < tree.children().length; ++i) {\r\n        children.add(simplifyTree(tree.children()[i]));\r\n    }\r\n    return tree.treeFactory().newTreeNode(newLabel, children);\r\n}"
}, {
	"Path": "org.openqa.grid.common.RegistrationRequestTest.testConstructorWithConfigurationAndNullCapabilities",
	"Comment": "should not result in any npe during the internal call to fixupcapabilities",
	"Method": "void testConstructorWithConfigurationAndNullCapabilities(){\r\n    GridNodeConfiguration config = new GridNodeConfiguration();\r\n    config.capabilities = null;\r\n    RegistrationRequest req = new RegistrationRequest(config);\r\n    assertNull(req.getConfiguration().capabilities);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.DiffableUtils.readJdkMapDiff",
	"Comment": "loads an object that represents difference between two maps of diffable objects using diffable proto object",
	"Method": "MapDiff<K, T, Map<K, T>> readJdkMapDiff(StreamInput in,KeySerializer<K> keySerializer,ValueSerializer<K, T> valueSerializer,MapDiff<K, T, Map<K, T>> readJdkMapDiff,StreamInput in,KeySerializer<K> keySerializer,Reader<T> reader,Reader<Diff<T>> diffReader){\r\n    return new JdkMapDiff(in, keySerializer, new DiffableValueReader(reader, diffReader));\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogDeletionPolicy.minTranslogGenRequired",
	"Comment": "returns the minimum translog generation that is still required by the system. any generation belowthe returned value may be safely deleted",
	"Method": "long minTranslogGenRequired(List<TranslogReader> readers,TranslogWriter writer){\r\n    long minByLocks = getMinTranslogGenRequiredByLocks();\r\n    long minByAge = getMinTranslogGenByAge(readers, writer, retentionAgeInMillis, currentTime());\r\n    long minBySize = getMinTranslogGenBySize(readers, writer, retentionSizeInBytes);\r\n    final long minByAgeAndSize;\r\n    if (minBySize == Long.MIN_VALUE && minByAge == Long.MIN_VALUE) {\r\n        minByAgeAndSize = Long.MAX_VALUE;\r\n    } else {\r\n        minByAgeAndSize = Math.max(minByAge, minBySize);\r\n    }\r\n    return Math.min(minByAgeAndSize, Math.min(minByLocks, minTranslogGenerationForRecovery));\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.nndep.DependencyTree.visitTree",
	"Comment": "inner recursive function for checking projectivity of tree",
	"Method": "boolean visitTree(int w){\r\n    for (int i = 1; i < w; ++i) if (getHead(i) == w && visitTree(i) == false)\r\n        return false;\r\n    counter = counter + 1;\r\n    if (w != counter)\r\n        return false;\r\n    for (int i = w + 1; i <= n; ++i) if (getHead(i) == w && visitTree(i) == false)\r\n        return false;\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.arabic.process.ArabicLexer.yypushback",
	"Comment": "pushes the specified amount of characters back into the input stream.they will be read again by then next call of the scanning method",
	"Method": "void yypushback(int number){\r\n    if (number > yylength())\r\n        zzScanError(ZZ_PUSHBACK_2BIG);\r\n    zzMarkedPos -= number;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.LinearGrammarSmoother.apply",
	"Comment": "destructively modifies the input and returns it as a convenience.",
	"Method": "Pair<UnaryGrammar, BinaryGrammar> apply(Pair<UnaryGrammar, BinaryGrammar> bgug){\r\n    ALPHA = trainOptions.ruleSmoothingAlpha;\r\n    Counter<String> symWeights = new ClassicCounter();\r\n    Counter<String> symCounts = new ClassicCounter();\r\n    for (UnaryRule rule : bgug.first()) {\r\n        if (!tagIndex.contains(rule.parent)) {\r\n            updateCounters(rule, symWeights, symCounts);\r\n        }\r\n    }\r\n    for (BinaryRule rule : bgug.second()) {\r\n        updateCounters(rule, symWeights, symCounts);\r\n    }\r\n    for (UnaryRule rule : bgug.first()) {\r\n        if (!tagIndex.contains(rule.parent)) {\r\n            rule.score = smoothRuleWeight(rule, symWeights, symCounts);\r\n        }\r\n    }\r\n    for (BinaryRule rule : bgug.second()) {\r\n        rule.score = smoothRuleWeight(rule, symWeights, symCounts);\r\n    }\r\n    if (DEBUG) {\r\n        System.err.printf(\"%s: %d basic symbols in the grammar%n\", this.getClass().getName(), symWeights.keySet().size());\r\n        for (String s : symWeights.keySet()) log.info(s + \",\");\r\n        log.info();\r\n    }\r\n    return bgug;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.inference.TableFactor.getMaxedMarginals",
	"Comment": "convenience function to max out all but one variable, and return the marginal array.",
	"Method": "double[][] getMaxedMarginals(){\r\n    double[][] maxValues = new double[neighborIndices.length][];\r\n    for (int i = 0; i < neighborIndices.length; i++) {\r\n        maxValues[i] = new double[getDimensions()[i]];\r\n        for (int j = 0; j < maxValues[i].length; j++) maxValues[i][j] = Double.NEGATIVE_INFINITY;\r\n    }\r\n    Iterator<int[]> fastPassByReferenceIterator = fastPassByReferenceIterator();\r\n    int[] assignment = fastPassByReferenceIterator.next();\r\n    while (true) {\r\n        double v = getAssignmentLogValue(assignment);\r\n        for (int i = 0; i < neighborIndices.length; i++) {\r\n            if (maxValues[i][assignment[i]] < v)\r\n                maxValues[i][assignment[i]] = v;\r\n        }\r\n        if (fastPassByReferenceIterator.hasNext()) {\r\n            fastPassByReferenceIterator.next();\r\n        } else\r\n            break;\r\n    }\r\n    for (int i = 0; i < neighborIndices.length; i++) {\r\n        normalizeLogArr(maxValues[i]);\r\n    }\r\n    return maxValues;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.TestSlot.getProtocol",
	"Comment": "the type of protocol for the testslot.ideally should always be webdriver, but can also beselenium1 protocol for backward compatibility purposes.",
	"Method": "SeleniumProtocol getProtocol(){\r\n    return protocol;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDocument.main",
	"Comment": "for internal debugging purposes only. creates and tests various instancesof basicdocument.",
	"Method": "void main(String[] args){\r\n    try {\r\n        printState(BasicDocument.init(\"this is the text\", \"this is the title [String]\", true));\r\n        printState(BasicDocument.init(new StringReader(\"this is the text\"), \"this is the title [Reader]\", true));\r\n        File f = File.createTempFile(\"BasicDocumentTestFile\", null);\r\n        f.deleteOnExit();\r\n        PrintWriter out = new PrintWriter(new FileWriter(f));\r\n        out.print(\"this is the text\");\r\n        out.flush();\r\n        out.close();\r\n        printState(new BasicDocument<String>().init(f, \"this is the title [File]\", true));\r\n        printState(new BasicDocument<String>().init(new URL(\"http://www.stanford.edu/~jsmarr/BasicDocumentTestFile.txt\"), \"this is the title [URL]\", true));\r\n    } catch (Exception e) {\r\n        e.printStackTrace();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.ElasticsearchException.getRootCause",
	"Comment": "retrieve the innermost cause of this exception, if none, returns the current exception.",
	"Method": "Throwable getRootCause(){\r\n    Throwable rootCause = this;\r\n    Throwable cause = getCause();\r\n    while (cause != null && cause != rootCause) {\r\n        rootCause = cause;\r\n        cause = cause.getCause();\r\n    }\r\n    return rootCause;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.recovery.RecoveryTarget.renameAllTempFiles",
	"Comment": "renames all temporary files to their true name, potentially overriding existing files",
	"Method": "void renameAllTempFiles(){\r\n    ensureRefCount();\r\n    store.renameTempFilesSafe(tempFileNames);\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogSnapshot.readBytes",
	"Comment": "reads an operation at the given position into the given buffer.",
	"Method": "void readBytes(ByteBuffer buffer,long position){\r\n    if (position >= length) {\r\n        throw new EOFException(\"read requested past EOF. pos [\" + position + \"] end: [\" + length + \"], generation: [\" + getGeneration() + \"], path: [\" + path + \"]\");\r\n    }\r\n    if (position < getFirstOperationOffset()) {\r\n        throw new IOException(\"read requested before position of first ops. pos [\" + position + \"] first op on: [\" + getFirstOperationOffset() + \"], generation: [\" + getGeneration() + \"], path: [\" + path + \"]\");\r\n    }\r\n    Channels.readFromFileChannelWithEofException(channel, position, buffer);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.loadTranslogUUIDFromLastCommit",
	"Comment": "reads the current stored translog id from the last commit data.",
	"Method": "String loadTranslogUUIDFromLastCommit(){\r\n    final Map<String, String> commitUserData = store.readLastCommittedSegmentsInfo().getUserData();\r\n    if (commitUserData.containsKey(Translog.TRANSLOG_GENERATION_KEY) == false) {\r\n        throw new IllegalStateException(\"commit doesn't contain translog generation id\");\r\n    }\r\n    return commitUserData.get(Translog.TRANSLOG_UUID_KEY);\r\n}"
}, {
	"Path": "android.text.TextUtils.join",
	"Comment": "returns a string containing the tokens joined by delimiters.",
	"Method": "CharSequence join(Iterable<CharSequence> list,String join,CharSequence delimiter,Object[] tokens,String join,CharSequence delimiter,Iterable tokens){\r\n    StringBuilder sb = new StringBuilder();\r\n    boolean firstTime = true;\r\n    for (Object token : tokens) {\r\n        if (firstTime) {\r\n            firstTime = false;\r\n        } else {\r\n            sb.append(delimiter);\r\n        }\r\n        sb.append(token);\r\n    }\r\n    return sb.toString();\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchPhaseController.merge",
	"Comment": "enriches search hits and completion suggestion hits from sorteddocs using fetchresultsarr,merges suggestions, aggregations and profile resultsexpects sorteddocs to have top search docs across all shards, optionally followed by top suggest docs for each namedcompletion suggestion ordered by suggestion name",
	"Method": "InternalSearchResponse merge(boolean ignoreFrom,ReducedQueryPhase reducedQueryPhase,Collection<? extends SearchPhaseResult> fetchResults,IntFunction<SearchPhaseResult> resultsLookup){\r\n    if (reducedQueryPhase.isEmptyResult) {\r\n        return InternalSearchResponse.empty();\r\n    }\r\n    ScoreDoc[] sortedDocs = reducedQueryPhase.scoreDocs;\r\n    SearchHits hits = getHits(reducedQueryPhase, ignoreFrom, fetchResults, resultsLookup);\r\n    if (reducedQueryPhase.suggest != null) {\r\n        if (!fetchResults.isEmpty()) {\r\n            int currentOffset = hits.getHits().length;\r\n            for (CompletionSuggestion suggestion : reducedQueryPhase.suggest.filter(CompletionSuggestion.class)) {\r\n                final List<CompletionSuggestion.Entry.Option> suggestionOptions = suggestion.getOptions();\r\n                for (int scoreDocIndex = currentOffset; scoreDocIndex < currentOffset + suggestionOptions.size(); scoreDocIndex++) {\r\n                    ScoreDoc shardDoc = sortedDocs[scoreDocIndex];\r\n                    SearchPhaseResult searchResultProvider = resultsLookup.apply(shardDoc.shardIndex);\r\n                    if (searchResultProvider == null) {\r\n                        continue;\r\n                    }\r\n                    FetchSearchResult fetchResult = searchResultProvider.fetchResult();\r\n                    final int index = fetchResult.counterGetAndIncrement();\r\n                    assert index < fetchResult.hits().getHits().length : \"not enough hits fetched. index [\" + index + \"] length: \" + fetchResult.hits().getHits().length;\r\n                    SearchHit hit = fetchResult.hits().getHits()[index];\r\n                    CompletionSuggestion.Entry.Option suggestOption = suggestionOptions.get(scoreDocIndex - currentOffset);\r\n                    hit.score(shardDoc.score);\r\n                    hit.shard(fetchResult.getSearchShardTarget());\r\n                    suggestOption.setHit(hit);\r\n                }\r\n                currentOffset += suggestionOptions.size();\r\n            }\r\n            assert currentOffset == sortedDocs.length : \"expected no more score doc slices\";\r\n        }\r\n    }\r\n    return reducedQueryPhase.buildResponse(hits);\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.GeoHashUtils.fromMorton",
	"Comment": "convert from a morton encoded long from a geohash encoded long",
	"Method": "long fromMorton(long morton,int level){\r\n    long mFlipped = BitUtil.flipFlop(morton);\r\n    mFlipped >>>= (((GeoHashUtils.PRECISION - level) * 5) + MORTON_OFFSET);\r\n    return (mFlipped << 4) | level;\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.builders.CoordinatesBuilder.close",
	"Comment": "makes a closed ring out of the current coordinates by adding the starting point as the end point.will have no effect of starting and end point are already the same coordinate.",
	"Method": "CoordinatesBuilder close(){\r\n    Coordinate start = points.get(0);\r\n    Coordinate end = points.get(points.size() - 1);\r\n    if (start.x != end.x || start.y != end.y) {\r\n        points.add(start);\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.XMoreLikeThis.addToQuery",
	"Comment": "add to an existing boolean query the more like this query from this priorityqueue",
	"Method": "void addToQuery(PriorityQueue<ScoreTerm> q,BooleanQuery.Builder query){\r\n    ScoreTerm scoreTerm;\r\n    float bestScore = -1;\r\n    while ((scoreTerm = q.pop()) != null) {\r\n        Query tq = new TermQuery(new Term(scoreTerm.topField, scoreTerm.word));\r\n        if (boost) {\r\n            if (bestScore == -1) {\r\n                bestScore = (scoreTerm.score);\r\n            }\r\n            float myScore = (scoreTerm.score);\r\n            tq = new BoostQuery(tq, boostFactor * myScore / bestScore);\r\n        }\r\n        try {\r\n            query.add(tq, BooleanClause.Occur.SHOULD);\r\n        } catch (BooleanQuery.TooManyClauses ignore) {\r\n            break;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.AbstractScopedSettings.isPrivateSetting",
	"Comment": "todo this should be replaced by setting.property.hidden or something like this.",
	"Method": "boolean isPrivateSetting(String key){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.monitor.process.ProcessProbe.getMethod",
	"Comment": "returns a given method of the operatingsystemmxbean,or null if the method is not found or unavailable.",
	"Method": "Method getMethod(String methodName){\r\n    try {\r\n        return Class.forName(\"com.sun.management.OperatingSystemMXBean\").getMethod(methodName);\r\n    } catch (Exception t) {\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryStringQueryBuilder.tieBreaker",
	"Comment": "when more than one field is used with the query string, and combined queries are usingdis max, control the tie breaker for it.",
	"Method": "QueryStringQueryBuilder tieBreaker(float tieBreaker,Float tieBreaker){\r\n    return this.tieBreaker;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.CategoryWordTag.toString",
	"Comment": "returns a string representation of the label.this attempts to be somewhat clever in choosing to print orsuppress null components and the details of words or categoriesdepending on the setting of printwordtag andsuppressterminaldetails.",
	"Method": "String toString(String toString,String mode){\r\n    if (\"full\".equals(mode)) {\r\n        return category() + \"[\" + word() + \"/\" + tag() + \"]\";\r\n    }\r\n    return toString();\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.master.MasterNodeRequest.masterNodeTimeout",
	"Comment": "a timeout value in case the master has not been discovered yet or disconnected.",
	"Method": "Request masterNodeTimeout(TimeValue timeout,Request masterNodeTimeout,String timeout,TimeValue masterNodeTimeout){\r\n    return this.masterNodeTimeout;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.MappedFieldType.checkTypeName",
	"Comment": "checks this type is the same type as other. adds a conflict if they are different.",
	"Method": "void checkTypeName(MappedFieldType other){\r\n    if (typeName().equals(other.typeName()) == false) {\r\n        throw new IllegalArgumentException(\"mapper [\" + name + \"] cannot be changed from type [\" + typeName() + \"] to [\" + other.typeName() + \"]\");\r\n    } else if (getClass() != other.getClass()) {\r\n        throw new IllegalStateException(\"Type names equal for class \" + getClass().getSimpleName() + \" and \" + other.getClass().getSimpleName());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.ReplicaShardAllocator.processExistingRecoveries",
	"Comment": "process existing recoveries of replicas and see if we need to cancel them if we find a bettermatch. today, a better match is one that has full sync id match compared to not having one inthe previous recovery.",
	"Method": "void processExistingRecoveries(RoutingAllocation allocation){\r\n    MetaData metaData = allocation.metaData();\r\n    RoutingNodes routingNodes = allocation.routingNodes();\r\n    List<Runnable> shardCancellationActions = new ArrayList();\r\n    for (RoutingNode routingNode : routingNodes) {\r\n        for (ShardRouting shard : routingNode) {\r\n            if (shard.primary()) {\r\n                continue;\r\n            }\r\n            if (shard.initializing() == false) {\r\n                continue;\r\n            }\r\n            if (shard.relocatingNodeId() != null) {\r\n                continue;\r\n            }\r\n            if (shard.unassignedInfo() != null && shard.unassignedInfo().getReason() == UnassignedInfo.Reason.INDEX_CREATED) {\r\n                continue;\r\n            }\r\n            AsyncShardFetch.FetchResult<NodeStoreFilesMetaData> shardStores = fetchData(shard, allocation);\r\n            if (shardStores.hasData() == false) {\r\n                logger.trace(\"{}: fetching new stores for initializing shard\", shard);\r\n                continue;\r\n            }\r\n            ShardRouting primaryShard = allocation.routingNodes().activePrimary(shard.shardId());\r\n            assert primaryShard != null : \"the replica shard can be allocated on at least one node, so there must be an active primary\";\r\n            TransportNodesListShardStoreMetaData.StoreFilesMetaData primaryStore = findStore(primaryShard, allocation, shardStores);\r\n            if (primaryStore == null) {\r\n                logger.trace(\"{}: no primary shard store found or allocated, letting actual allocation figure it out\", shard);\r\n                continue;\r\n            }\r\n            MatchingNodes matchingNodes = findMatchingNodes(shard, allocation, primaryStore, shardStores, false);\r\n            if (matchingNodes.getNodeWithHighestMatch() != null) {\r\n                DiscoveryNode currentNode = allocation.nodes().get(shard.currentNodeId());\r\n                DiscoveryNode nodeWithHighestMatch = matchingNodes.getNodeWithHighestMatch();\r\n                final String currentSyncId;\r\n                if (shardStores.getData().containsKey(currentNode)) {\r\n                    currentSyncId = shardStores.getData().get(currentNode).storeFilesMetaData().syncId();\r\n                } else {\r\n                    currentSyncId = null;\r\n                }\r\n                if (currentNode.equals(nodeWithHighestMatch) == false && Objects.equals(currentSyncId, primaryStore.syncId()) == false && matchingNodes.isNodeMatchBySyncID(nodeWithHighestMatch)) {\r\n                    logger.debug(\"cancelling allocation of replica on [{}], sync id match found on node [{}]\", currentNode, nodeWithHighestMatch);\r\n                    UnassignedInfo unassignedInfo = new UnassignedInfo(UnassignedInfo.Reason.REALLOCATED_REPLICA, \"existing allocation of replica to [\" + currentNode + \"] cancelled, sync id match found on node [\" + nodeWithHighestMatch + \"]\", null, 0, allocation.getCurrentNanoTime(), System.currentTimeMillis(), false, UnassignedInfo.AllocationStatus.NO_ATTEMPT);\r\n                    shardCancellationActions.add(() -> routingNodes.failShard(logger, shard, unassignedInfo, metaData.getIndexSafe(shard.index()), allocation.changes()));\r\n                }\r\n            }\r\n        }\r\n    }\r\n    for (Runnable action : shardCancellationActions) {\r\n        action.run();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.support.NestedScope.nextLevel",
	"Comment": "sets the new current nested level and pushes old current nested level down the stack returns that level.",
	"Method": "ObjectMapper nextLevel(ObjectMapper level){\r\n    ObjectMapper previous = levelStack.peek();\r\n    levelStack.push(level);\r\n    return previous;\r\n}"
}, {
	"Path": "org.elasticsearch.common.bytes.BytesReference.fromByteBuffers",
	"Comment": "returns bytesreference composed of the provided bytebuffers.",
	"Method": "BytesReference fromByteBuffers(ByteBuffer[] buffers){\r\n    ByteBufferReference[] references = new ByteBufferReference[buffers.length];\r\n    for (int i = 0; i < references.length; ++i) {\r\n        references[i] = new ByteBufferReference(buffers[i]);\r\n    }\r\n    return new CompositeBytesReference(references);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.LexicalizedParser.parseStrings",
	"Comment": "will process a list of strings into a list of hasword and returnthe parse tree associated with that list.",
	"Method": "Tree parseStrings(List<String> lst){\r\n    List<Word> words = new ArrayList();\r\n    for (String word : lst) {\r\n        words.add(new Word(word));\r\n    }\r\n    return parse(words);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDocument.addLabel",
	"Comment": "adds the given label to the list of labels for this document if it is not null.",
	"Method": "void addLabel(L label){\r\n    if (label != null) {\r\n        labels.add(label);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.IndicesOptions.isIndicesOptions",
	"Comment": "returns true if the name represents a valid name for one of the indices optionfalse otherwise",
	"Method": "boolean isIndicesOptions(String name){\r\n    return \"expand_wildcards\".equals(name) || \"expandWildcards\".equals(name) || \"ignore_unavailable\".equals(name) || \"ignoreUnavailable\".equals(name) || \"ignore_throttled\".equals(name) || \"ignoreThrottled\".equals(name) || \"allow_no_indices\".equals(name) || \"allowNoIndices\".equals(name);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStatus.includeGlobalState",
	"Comment": "returns true if global state is included in the snapshot, false otherwise.can be null if this information is unknown.",
	"Method": "Boolean includeGlobalState(){\r\n    return includeGlobalState;\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.tasks.BaseTasksRequest.setActions",
	"Comment": "sets the list of action masks for the actions that should be returned",
	"Method": "Request setActions(String actions){\r\n    this.actions = actions;\r\n    return (Request) this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.XMoreLikeThis.like",
	"Comment": "return a query that will return docs like the passed lucene document id.",
	"Method": "Query like(int docNum,Query like,String fieldName,Reader readers,Query like,Terms likeTerms,Query like,Fields likeFields){\r\n    Set<String> fieldNames = new HashSet();\r\n    for (Fields fields : likeFields) {\r\n        for (String fieldName : fields) {\r\n            fieldNames.add(fieldName);\r\n        }\r\n    }\r\n    BooleanQuery.Builder bq = new BooleanQuery.Builder();\r\n    for (String fieldName : fieldNames) {\r\n        Map<String, Int> termFreqMap = new HashMap();\r\n        for (Fields fields : likeFields) {\r\n            Terms vector = fields.terms(fieldName);\r\n            if (vector != null) {\r\n                addTermFrequencies(termFreqMap, vector, fieldName);\r\n            }\r\n        }\r\n        addToQuery(createQueue(termFreqMap, fieldName), bq);\r\n    }\r\n    return bq.build();\r\n}"
}, {
	"Path": "org.elasticsearch.indices.mapper.MapperRegistry.getMetadataMapperParsers",
	"Comment": "return a map of the meta mappers that have been registered. thereturned map uses the name of the field as a key.",
	"Method": "Map<String, MetadataFieldMapper.TypeParser> getMetadataMapperParsers(){\r\n    return metadataMapperParsers;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.closeOnTragicEvent",
	"Comment": "closes the translog if the current translog writer experienced a tragic exception.note that in case this thread closes the translog it must not already be holding a read lock on the translog as it will acquire awrite lock in the course of closing the translog",
	"Method": "void closeOnTragicEvent(Exception ex){\r\n    assert readLock.isHeldByCurrentThread() == false : Thread.currentThread().getName();\r\n    if (tragedy.get() != null) {\r\n        try {\r\n            close();\r\n        } catch (final AlreadyClosedException inner) {\r\n        } catch (final Exception inner) {\r\n            assert ex != inner.getCause();\r\n            ex.addSuppressed(inner);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotResponse.getRestoreInfo",
	"Comment": "returns restore information if snapshot was completed before this method returned, null otherwise",
	"Method": "RestoreInfo getRestoreInfo(){\r\n    return restoreInfo;\r\n}"
}, {
	"Path": "edu.stanford.nlp.optimization.StochasticMinimizer.gainSchedule",
	"Comment": "this is the scaling factor for the gains to ensure convergence",
	"Method": "double gainSchedule(int it,double tau){\r\n    return (tau / (tau + it));\r\n}"
}, {
	"Path": "org.elasticsearch.common.logging.DeprecationLogger.deprecatedAndMaybeLog",
	"Comment": "adds a formatted warning message as a response header on the thread context, and logs a deprecation message if the associated key hasnot recently been seen.",
	"Method": "void deprecatedAndMaybeLog(String key,String msg,Object params){\r\n    deprecated(THREAD_CONTEXT, msg, keys.add(key), params);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.copyCoreLabel",
	"Comment": "copies only the fields required for numeric entity extraction intothe new corelabel.",
	"Method": "CoreLabel copyCoreLabel(CoreLabel src,Integer startOffset,Integer endOffset){\r\n    CoreLabel dst = new CoreLabel();\r\n    dst.setWord(src.word());\r\n    dst.setTag(src.tag());\r\n    if (src.containsKey(CoreAnnotations.OriginalTextAnnotation.class)) {\r\n        dst.set(CoreAnnotations.OriginalTextAnnotation.class, src.get(CoreAnnotations.OriginalTextAnnotation.class));\r\n    }\r\n    if (startOffset == null) {\r\n        dst.set(CoreAnnotations.CharacterOffsetBeginAnnotation.class, src.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class));\r\n    } else {\r\n        dst.set(CoreAnnotations.CharacterOffsetBeginAnnotation.class, startOffset);\r\n    }\r\n    if (endOffset == null) {\r\n        dst.set(CoreAnnotations.CharacterOffsetEndAnnotation.class, src.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));\r\n    } else {\r\n        dst.set(CoreAnnotations.CharacterOffsetEndAnnotation.class, endOffset);\r\n    }\r\n    transferAnnotations(src, dst);\r\n    return dst;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.InternalClusterInfoService.updateNodeStats",
	"Comment": "retrieve the latest nodes stats, calling the listener when complete",
	"Method": "CountDownLatch updateNodeStats(ActionListener<NodesStatsResponse> listener){\r\n    final CountDownLatch latch = new CountDownLatch(1);\r\n    final NodesStatsRequest nodesStatsRequest = new NodesStatsRequest(\"data:true\");\r\n    nodesStatsRequest.clear();\r\n    nodesStatsRequest.fs(true);\r\n    nodesStatsRequest.timeout(fetchTimeout);\r\n    client.admin().cluster().nodesStats(nodesStatsRequest, new LatchedActionListener(listener, latch));\r\n    return latch;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEvent.blocksChanged",
	"Comment": "returns true iff the cluster level blocks have changed between cluster states.note that this is an object reference equality test, not an equals test.",
	"Method": "boolean blocksChanged(){\r\n    return state.blocks() != previousState.blocks();\r\n}"
}, {
	"Path": "org.elasticsearch.index.recovery.RecoveryStats.currentAsSource",
	"Comment": "number of ongoing recoveries for which a shard serves as a source",
	"Method": "int currentAsSource(){\r\n    return currentAsSource.get();\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.exists",
	"Comment": "returns true if and only if this setting is present in the given settings instance. note that fallback settings are excluded.",
	"Method": "boolean exists(Settings settings,boolean exists,Settings settings){\r\n    return settings.keySet().contains(getKey());\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.Spawner.getProcesses",
	"Comment": "the collection of processes representing spawned native controllers.",
	"Method": "List<Process> getProcesses(){\r\n    return Collections.unmodifiableList(processes);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.InnerHitBuilder.setFieldNames",
	"Comment": "sets the stored fields to load and return.if none are specified, the source of the document will be returned.",
	"Method": "InnerHitBuilder setFieldNames(List<String> fieldNames){\r\n    return setStoredFieldNames(fieldNames);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndexNames",
	"Comment": "translates the provided index expression into actual concrete indices, properly deduplicated.",
	"Method": "String[] concreteIndexNames(ClusterState state,IndicesRequest request,String[] concreteIndexNames,ClusterState state,IndicesOptions options,String indexExpressions,String[] concreteIndexNames,Context context,String indexExpressions){\r\n    Index[] indexes = concreteIndices(context, indexExpressions);\r\n    String[] names = new String[indexes.length];\r\n    for (int i = 0; i < indexes.length; i++) {\r\n        names[i] = indexes[i].getName();\r\n    }\r\n    return names;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractBulkByScrollRequestBuilder.timeout",
	"Comment": "timeout to wait for the shards on to be available for each bulk request.",
	"Method": "Self timeout(TimeValue timeout){\r\n    request.setTimeout(timeout);\r\n    return self();\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.DeleteByQueryRequest.setIndicesOptions",
	"Comment": "set the indicesoptions for controlling unavailable indices",
	"Method": "DeleteByQueryRequest setIndicesOptions(IndicesOptions indicesOptions){\r\n    getSearchRequest().indicesOptions(indicesOptions);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.getFlushThresholdSize",
	"Comment": "returns the transaction log threshold size when to forcefully flush the index and clear the transaction log.",
	"Method": "ByteSizeValue getFlushThresholdSize(){\r\n    return flushThresholdSize;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.ShardPath.isCustomDataPath",
	"Comment": "returns true iff the data location is a custom data location and therefore outside of the nodes configured data paths.",
	"Method": "boolean isCustomDataPath(){\r\n    return isCustomDataPath;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.BinaryHeadFinder.determineHead",
	"Comment": "determine which daughter of the current parse tree is the head.it assumes that the daughters already have had their headsdetermined. another method has to do the tree walking.",
	"Method": "Tree determineHead(Tree t,Tree determineHead,Tree t,Tree parent){\r\n    Tree result = determineBinaryHead(t);\r\n    if (result == null && fallbackHF != null) {\r\n        result = fallbackHF.determineHead(t, parent);\r\n    }\r\n    if (result != null) {\r\n        return result;\r\n    }\r\n    throw new IllegalStateException(\"BinaryHeadFinder: unexpected tree: \" + t);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotStats.getProcessedSize",
	"Comment": "returns total size of files in the snapshot that were processed so far",
	"Method": "long getProcessedSize(){\r\n    return processedSize;\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.StoreFileMetaData.isSame",
	"Comment": "returns true iff the length and the checksums are the same. otherwise false",
	"Method": "boolean isSame(StoreFileMetaData other){\r\n    if (checksum == null || other.checksum == null) {\r\n        return false;\r\n    }\r\n    return length == other.length && checksum.equals(other.checksum) && hash.equals(other.hash);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MoreLikeThisQueryBuilder.analyzer",
	"Comment": "the analyzer that will be used to analyze the text. defaults to the analyzer associated with the field.",
	"Method": "MoreLikeThisQueryBuilder analyzer(String analyzer,String analyzer){\r\n    return analyzer;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.getLastSearcherAccess",
	"Comment": "returns the last timestamp the searcher was accessed. this is a relative timestamp in milliseconds.",
	"Method": "long getLastSearcherAccess(){\r\n    return lastSearcherAccess.get();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.stats.CommonStatsFlags.fieldDataFields",
	"Comment": "sets specific search group stats to retrieve the stats for. mainly affects searchwhen enabled.",
	"Method": "CommonStatsFlags fieldDataFields(String fieldDataFields,String[] fieldDataFields){\r\n    return this.fieldDataFields;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.AbstractBigArray.ramBytesEstimated",
	"Comment": "given the size of the array, estimate the number of bytes it will use.",
	"Method": "long ramBytesEstimated(long size){\r\n    return ((long) pageIndex(size - 1) + 1) * pageSize() * numBytesPerElement();\r\n}"
}, {
	"Path": "org.openqa.grid.internal.listener.SessionListenerTest.doubleRelease",
	"Comment": "using a proxy that times out instantly and spends a long time in the after method. checkaftermethod cannot be executed twice for a session.",
	"Method": "void doubleRelease(){\r\n    RegistrationRequest req = new RegistrationRequest();\r\n    Map<String, Object> cap = new HashMap();\r\n    cap.put(CapabilityType.APPLICATION_NAME, \"app1\");\r\n    req.getConfiguration().timeout = 1;\r\n    req.getConfiguration().cleanUpCycle = 1;\r\n    req.getConfiguration().maxSession = 2;\r\n    req.getConfiguration().capabilities.add(new DesiredCapabilities(cap));\r\n    GridRegistry registry = DefaultGridRegistry.newInstance(new Hub(new GridHubConfiguration()));\r\n    try {\r\n        final SlowAfterSession proxy = new SlowAfterSession(req, registry);\r\n        proxy.setupTimeoutListener();\r\n        registry.add(proxy);\r\n        RequestHandler r = GridHelper.createNewSessionHandler(registry, app1);\r\n        r.process();\r\n        TestSession session = r.getSession();\r\n        Thread.sleep(1100);\r\n        assertEquals(session.get(\"after\"), true);\r\n        registry.terminate(session, SessionTerminationReason.CLIENT_STOPPED_SESSION);\r\n        assertNull(session.get(\"ERROR\"));\r\n    } finally {\r\n        registry.stop();\r\n    }\r\n}"
}, {
	"Path": "org.openqa.selenium.remote.server.log.SessionLogsToFileRepository.getLogRecords",
	"Comment": "this returns the log records storied in the corresponding log file. this doesclear thelog records in the file.",
	"Method": "List<LogRecord> getLogRecords(SessionId sessionId){\r\n    LogFile logFile = sessionToLogFileMap.get(sessionId);\r\n    if (logFile == null) {\r\n        return new ArrayList();\r\n    }\r\n    List<LogRecord> logRecords = new ArrayList();\r\n    try {\r\n        logFile.openLogReader();\r\n        ObjectInputStream logObjInStream = logFile.getLogReader();\r\n        LogRecord tmpLogRecord;\r\n        while (null != (tmpLogRecord = (LogRecord) logObjInStream.readObject())) {\r\n            logRecords.add(tmpLogRecord);\r\n        }\r\n    } catch (IOException ex) {\r\n        logFile.closeLogReader();\r\n        return logRecords;\r\n    } catch (ClassNotFoundException e) {\r\n        logFile.closeLogReader();\r\n        return logRecords;\r\n    }\r\n    logFile.closeLogReader();\r\n    return logRecords;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.ShardRouting.isRelocationSourceOf",
	"Comment": "returns true if the routing is the relocation source for the given routing",
	"Method": "boolean isRelocationSourceOf(ShardRouting other){\r\n    boolean b = this.allocationId != null && other.allocationId != null && other.state == ShardRoutingState.INITIALIZING && other.allocationId.getId().equals(this.allocationId.getRelocationId());\r\n    assert b == false || this.state == ShardRoutingState.RELOCATING : \"ShardRouting is a relocation source but shard state isn't relocating. This [\" + this + \"], other [\" + other + \"]\";\r\n    assert b == false || this.allocationId.getId().equals(other.allocationId.getRelocationId()) : \"ShardRouting is a relocation source but the allocation id isn't equal to other.allocationId.getRelocationId.\" + \" This [\" + this + \"], other [\" + other + \"]\";\r\n    assert b == false || this.currentNodeId().equals(other.relocatingNodeId) : \"ShardRouting is a relocation source but current node isn't equal to other's relocating node.\" + \" This [\" + this + \"], other [\" + other + \"]\";\r\n    assert b == false || other.currentNodeId().equals(this.relocatingNodeId) : \"ShardRouting is a relocation source but relocating node isn't equal to other's current node.\" + \" This [\" + this + \"], other [\" + other + \"]\";\r\n    assert b == false || this.shardId.equals(other.shardId) : \"ShardRouting is a relocation source but both indexRoutings are not of the same shard.\" + \" This [\" + this + \"], target [\" + other + \"]\";\r\n    assert b == false || this.primary == other.primary : \"ShardRouting is a relocation source but primary flag is different. This [\" + this + \"], target [\" + other + \"]\";\r\n    return b;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.TestSession.getInactivityTime",
	"Comment": "give the time in milliseconds since the last access to this test session, or 0 is ignore timeout has been set to true.",
	"Method": "long getInactivityTime(){\r\n    if (ignoreTimeout) {\r\n        return 0;\r\n    }\r\n    return clock.millis() - lastActivity;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.ExhaustivePCFGParser.doInsideScores",
	"Comment": "fills in the iscore array of each category over each span of length 2 or more.",
	"Method": "void doInsideScores(){\r\n    for (int diff = 2; diff <= length; diff++) {\r\n        if (Thread.interrupted()) {\r\n            throw new RuntimeInterruptedException();\r\n        }\r\n        for (int start = 0; start < ((diff == length) ? 1 : length - diff); start++) {\r\n            doInsideChartCell(diff, start);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.apache.dubbo.rpc.protocol.dubbo.ReferenceCountExchangeClientTest.test_counter_error",
	"Comment": "test against invocation still succeed even if counter has error",
	"Method": "void test_counter_error(){\r\n    init(0);\r\n    DubboAppender.doStart();\r\n    DubboAppender.clear();\r\n    ReferenceCountExchangeClient client = getReferenceClient(helloServiceInvoker);\r\n    client.close();\r\n    Assert.assertEquals(\"hello\", helloService.hello());\r\n    Assert.assertEquals(\"should not warning message\", 0, LogUtil.findMessage(errorMsg));\r\n    client.close();\r\n    try {\r\n        Thread.sleep(1000);\r\n    } catch (InterruptedException e) {\r\n        Assert.fail();\r\n    }\r\n    Assert.assertEquals(\"hello\", helloService.hello());\r\n    Assert.assertEquals(\"should warning message\", 1, LogUtil.findMessage(errorMsg));\r\n    Assert.assertEquals(\"hello\", helloService.hello());\r\n    Assert.assertEquals(\"should warning message\", 1, LogUtil.findMessage(errorMsg));\r\n    DubboAppender.doStop();\r\n    Assert.assertEquals(\"client status available\", true, helloServiceInvoker.isAvailable());\r\n    client.close();\r\n    Assert.assertEquals(\"client status close\", false, client.isClosed());\r\n    Assert.assertEquals(\"client status close\", false, helloServiceInvoker.isAvailable());\r\n    destoy();\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.internal.AbstractBindingBuilder.annotatedWithInternal",
	"Comment": "sets the binding to a copy with the specified annotation on the bound key",
	"Method": "BindingImpl<T> annotatedWithInternal(Class<? extends Annotation> annotationType,BindingImpl<T> annotatedWithInternal,Annotation annotation){\r\n    Objects.requireNonNull(annotation, \"annotation\");\r\n    checkNotAnnotated();\r\n    return setBinding(binding.withKey(Key.get(this.binding.getKey().getTypeLiteral(), annotation)));\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.ParseFieldRegistry.lookupReturningNullIfNotFound",
	"Comment": "lookup a value from the registry by name while checking that the name matches the parsefield.",
	"Method": "T lookupReturningNullIfNotFound(String name,DeprecationHandler deprecationHandler){\r\n    Tuple<ParseField, T> parseFieldAndValue = registry.get(name);\r\n    if (parseFieldAndValue == null) {\r\n        return null;\r\n    }\r\n    ParseField parseField = parseFieldAndValue.v1();\r\n    T value = parseFieldAndValue.v2();\r\n    boolean match = parseField.match(name, deprecationHandler);\r\n    assert match : \"ParseField did not match registered name [\" + name + \"][\" + registryName + \"]\";\r\n    return value;\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequestBuilder.setDocAsUpsert",
	"Comment": "sets whether the specified doc parameter should be used as upsert document.",
	"Method": "UpdateRequestBuilder setDocAsUpsert(boolean shouldUpsertDoc){\r\n    request.docAsUpsert(shouldUpsertDoc);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.bytes.ByteBufferReference.toBytesRef",
	"Comment": "this will return a bytes ref composed of the bytes. if this is a direct byte buffer, the bytes willhave to be copied.",
	"Method": "BytesRef toBytesRef(){\r\n    if (buffer.hasArray()) {\r\n        return new BytesRef(buffer.array(), buffer.arrayOffset() + offset, length);\r\n    }\r\n    final byte[] copy = new byte[length];\r\n    buffer.get(copy, offset, length);\r\n    return new BytesRef(copy);\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.InetAddresses.parseCidr",
	"Comment": "parse an ip address and its prefix length using the cidr notation.",
	"Method": "Tuple<InetAddress, Integer> parseCidr(String maskedAddress){\r\n    String[] fields = maskedAddress.split(\"/\");\r\n    if (fields.length == 2) {\r\n        final String addressString = fields[0];\r\n        final InetAddress address = forString(addressString);\r\n        if (addressString.contains(\":\") && address.getAddress().length == 4) {\r\n            throw new IllegalArgumentException(\"CIDR notation is not allowed with IPv6-mapped IPv4 address [\" + addressString + \" as it introduces ambiguity as to whether the prefix length should be interpreted as a v4 prefix length or a\" + \" v6 prefix length\");\r\n        }\r\n        final int prefixLength = Integer.parseInt(fields[1]);\r\n        if (prefixLength < 0 || prefixLength > 8 * address.getAddress().length) {\r\n            throw new IllegalArgumentException(\"Illegal prefix length [\" + prefixLength + \"] in [\" + maskedAddress + \"]. Must be 0-32 for IPv4 ranges, 0-128 for IPv6 ranges\");\r\n        }\r\n        return new Tuple(address, prefixLength);\r\n    } else {\r\n        throw new IllegalArgumentException(\"Expected [ip/prefix] but was [\" + maskedAddress + \"]\");\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDatum.label",
	"Comment": "returns the first label for this datum, or null if none have been set.",
	"Method": "LabelType label(){\r\n    return ((labels.size() > 0) ? labels.get(0) : null);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MatchQueryBuilder.minimumShouldMatch",
	"Comment": "sets optional minimumshouldmatch value to apply to the query",
	"Method": "MatchQueryBuilder minimumShouldMatch(String minimumShouldMatch,String minimumShouldMatch){\r\n    return this.minimumShouldMatch;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.AllocationId.getRelocationId",
	"Comment": "the transient relocation id holding the unique id that is used for relocation.",
	"Method": "String getRelocationId(){\r\n    return relocationId;\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.isQueryStringLenient",
	"Comment": "returns true if query string parsing should be lenient. the default is false",
	"Method": "boolean isQueryStringLenient(){\r\n    return queryStringLenient;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.hasIndexScope",
	"Comment": "returns true if this setting has an index scope, otherwise false",
	"Method": "boolean hasIndexScope(){\r\n    return properties.contains(Property.IndexScope);\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.LeaderBulkByScrollTaskState.getStatus",
	"Comment": "get the combined statuses of slice subtasks, merged with the given list of statuses",
	"Method": "BulkByScrollTask.Status getStatus(List<BulkByScrollTask.StatusOrException> statuses,BulkByScrollTask.Status getStatus){\r\n    return getStatus(Arrays.asList(new BulkByScrollTask.StatusOrException[results.length()]));\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.GeoHashUtils.stringEncodeFromMortonLong",
	"Comment": "encode to a full precision geohash string from a given morton encoded long value",
	"Method": "String stringEncodeFromMortonLong(long hashedVal,String stringEncodeFromMortonLong,long hashedVal,int level){\r\n    hashedVal = BitUtil.flipFlop(hashedVal);\r\n    StringBuilder geoHash = new StringBuilder();\r\n    short precision = 0;\r\n    final short msf = (BITS << 1) - 5;\r\n    long mask = 31L << msf;\r\n    do {\r\n        geoHash.append(BASE_32[(int) ((mask & hashedVal) >>> (msf - (precision * 5)))]);\r\n        mask >>>= 5;\r\n    } while (++precision < level);\r\n    return geoHash.toString();\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.initiateTracking",
	"Comment": "called when the recovery process for a shard has opened the engine on the target shard. ensures that the right data structureshave been set up locally to track local checkpoint information for the shard and that the shard is added to the replication group.",
	"Method": "void initiateTracking(String allocationId){\r\n    assert invariant();\r\n    assert primaryMode;\r\n    assert handoffInProgress == false;\r\n    CheckpointState cps = checkpoints.get(allocationId);\r\n    if (cps == null) {\r\n        throw new IllegalStateException(\"no local checkpoint tracking information available\");\r\n    }\r\n    cps.tracked = true;\r\n    replicationGroup = calculateReplicationGroup();\r\n    assert invariant();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.UnassignedInfo.getNumberOfDelayedUnassigned",
	"Comment": "returns the number of shards that are unassigned and currently being delayed.",
	"Method": "int getNumberOfDelayedUnassigned(ClusterState state){\r\n    int count = 0;\r\n    for (ShardRouting shard : state.routingTable().shardsWithState(ShardRoutingState.UNASSIGNED)) {\r\n        if (shard.unassignedInfo().isDelayed()) {\r\n            count++;\r\n        }\r\n    }\r\n    return count;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.ShardUtils.extractShardId",
	"Comment": "tries to extract the shard id from a reader if possible, when its not possible,will return null.",
	"Method": "ShardId extractShardId(LeafReader reader,ShardId extractShardId,DirectoryReader reader){\r\n    final ElasticsearchDirectoryReader esReader = ElasticsearchDirectoryReader.getElasticsearchDirectoryReader(reader);\r\n    if (esReader != null) {\r\n        return esReader.shardId();\r\n    }\r\n    throw new IllegalArgumentException(\"can't extract shard ID, can't unwrap ElasticsearchDirectoryReader\");\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.node.DiscoveryNodes.getMaxNodeVersion",
	"Comment": "returns the version of the node with the youngest version in the cluster",
	"Method": "Version getMaxNodeVersion(){\r\n    return maxNodeVersion;\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.PluginSecurity.parsePermissions",
	"Comment": "parses plugin policy into a set of permissions. each permission is formatted for output to users.",
	"Method": "Set<String> parsePermissions(Path file,Path tmpDir){\r\n    Path emptyPolicyFile = Files.createTempFile(tmpDir, \"empty\", \"tmp\");\r\n    final Policy emptyPolicy;\r\n    try {\r\n        emptyPolicy = Policy.getInstance(\"JavaPolicy\", new URIParameter(emptyPolicyFile.toUri()));\r\n    } catch (NoSuchAlgorithmException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    IOUtils.rm(emptyPolicyFile);\r\n    final Policy policy;\r\n    try {\r\n        policy = Policy.getInstance(\"JavaPolicy\", new URIParameter(file.toUri()));\r\n    } catch (NoSuchAlgorithmException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n    PermissionCollection permissions = policy.getPermissions(PluginSecurity.class.getProtectionDomain());\r\n    if (permissions == Policy.UNSUPPORTED_EMPTY_COLLECTION) {\r\n        throw new UnsupportedOperationException(\"JavaPolicy implementation does not support retrieving permissions\");\r\n    }\r\n    PermissionCollection actualPermissions = new Permissions();\r\n    for (Permission permission : Collections.list(permissions.elements())) {\r\n        if (!emptyPolicy.implies(PluginSecurity.class.getProtectionDomain(), permission)) {\r\n            actualPermissions.add(permission);\r\n        }\r\n    }\r\n    return Collections.list(actualPermissions.elements()).stream().map(PluginSecurity::formatPermission).collect(Collectors.toSet());\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.ReplicationGroup.getReplicationTargets",
	"Comment": "returns the subset of shards in the routing table that should be replicated to. includes relocation targets.",
	"Method": "List<ShardRouting> getReplicationTargets(){\r\n    return replicationTargets;\r\n}"
}, {
	"Path": "org.elasticsearch.common.logging.LogConfigurator.registerErrorListener",
	"Comment": "registers a listener for status logger errors. this listener should be registered as early as possible to ensure that no errors arelogged by the status logger before logging is configured.",
	"Method": "void registerErrorListener(){\r\n    error.set(false);\r\n    StatusLogger.getLogger().registerListener(ERROR_LISTENER);\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkPrimaryExecutionContext.requiresImmediateRetry",
	"Comment": "returns true if the current request should be retried without waiting for an external event",
	"Method": "boolean requiresImmediateRetry(){\r\n    return currentItemState == ItemProcessingState.IMMEDIATE_RETRY;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.ClientScrollableHitSource.searchWithRetry",
	"Comment": "run a search action and call onresponse when a the response comes in, retrying if the action fails with an exception caused byrejected execution.",
	"Method": "void searchWithRetry(Consumer<ActionListener<SearchResponse>> action,Consumer<SearchResponse> onResponse){\r\n    class RetryHelper extends AbstractRunnable implements ActionListener<SearchResponse> {\r\n        private final Iterator<TimeValue> retries = backoffPolicy.iterator();\r\n        private Runnable retryWithContext;\r\n        private volatile int retryCount = 0;\r\n        @Override\r\n        protected void doRun() throws Exception {\r\n            action.accept(this);\r\n        }\r\n        @Override\r\n        public void onResponse(SearchResponse response) {\r\n            onResponse.accept(response);\r\n        }\r\n        @Override\r\n        public void onFailure(Exception e) {\r\n            if (ExceptionsHelper.unwrap(e, EsRejectedExecutionException.class) != null) {\r\n                if (retries.hasNext()) {\r\n                    retryCount += 1;\r\n                    TimeValue delay = retries.next();\r\n                    logger.trace(() -> new ParameterizedMessage(\"retrying rejected search after [{}]\", delay), e);\r\n                    countSearchRetry.run();\r\n                    threadPool.schedule(delay, ThreadPool.Names.SAME, retryWithContext);\r\n                } else {\r\n                    logger.warn(() -> new ParameterizedMessage(\"giving up on search because we retried [{}] times without success\", retryCount), e);\r\n                    fail.accept(e);\r\n                }\r\n            } else {\r\n                logger.warn(\"giving up on search because it failed with a non-retryable exception\", e);\r\n                fail.accept(e);\r\n            }\r\n        }\r\n    }\r\n    RetryHelper helper = new RetryHelper();\r\n    helper.retryWithContext = threadPool.getThreadContext().preserveContext(helper);\r\n    helper.run();\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.ClientScrollableHitSource.searchWithRetry",
	"Comment": "run a search action and call onresponse when a the response comes in, retrying if the action fails with an exception caused byrejected execution.",
	"Method": "void searchWithRetry(Consumer<ActionListener<SearchResponse>> action,Consumer<SearchResponse> onResponse){\r\n    action.accept(this);\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.ClientScrollableHitSource.searchWithRetry",
	"Comment": "run a search action and call onresponse when a the response comes in, retrying if the action fails with an exception caused byrejected execution.",
	"Method": "void searchWithRetry(Consumer<ActionListener<SearchResponse>> action,Consumer<SearchResponse> onResponse){\r\n    onResponse.accept(response);\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.ClientScrollableHitSource.searchWithRetry",
	"Comment": "run a search action and call onresponse when a the response comes in, retrying if the action fails with an exception caused byrejected execution.",
	"Method": "void searchWithRetry(Consumer<ActionListener<SearchResponse>> action,Consumer<SearchResponse> onResponse){\r\n    if (ExceptionsHelper.unwrap(e, EsRejectedExecutionException.class) != null) {\r\n        if (retries.hasNext()) {\r\n            retryCount += 1;\r\n            TimeValue delay = retries.next();\r\n            logger.trace(() -> new ParameterizedMessage(\"retrying rejected search after [{}]\", delay), e);\r\n            countSearchRetry.run();\r\n            threadPool.schedule(delay, ThreadPool.Names.SAME, retryWithContext);\r\n        } else {\r\n            logger.warn(() -> new ParameterizedMessage(\"giving up on search because we retried [{}] times without success\", retryCount), e);\r\n            fail.accept(e);\r\n        }\r\n    } else {\r\n        logger.warn(\"giving up on search because it failed with a non-retryable exception\", e);\r\n        fail.accept(e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.arabic.IBMArabicEscaper.apply",
	"Comment": "applies escaping to a single word. interns the escaped string.",
	"Method": "List<HasWord> apply(List<HasWord> sentence,String apply,String w){\r\n    String escapedWord = (annotationsAndClassingOnly) ? stripAnnotationsAndClassing(w) : escapeString(w);\r\n    if (escapedWord.isEmpty()) {\r\n        throw new RuntimeException(String.format(\"Word (%s) mapped to null\", w));\r\n    }\r\n    return escapedWord.intern();\r\n}"
}, {
	"Path": "android.util.Pair.create",
	"Comment": "convenience method for creating an appropriately typed pair.",
	"Method": "Pair<A, B> create(A a,B b){\r\n    return new Pair<A, B>(a, b);\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.isCurrent",
	"Comment": "returns true iff the given generation is the current generation of this translog",
	"Method": "boolean isCurrent(TranslogGeneration generation){\r\n    try (ReleasableLock lock = writeLock.acquire()) {\r\n        if (generation != null) {\r\n            if (generation.translogUUID.equals(translogUUID) == false) {\r\n                throw new IllegalArgumentException(\"commit belongs to a different translog: \" + generation.translogUUID + \" vs. \" + translogUUID);\r\n            }\r\n            return generation.translogFileGeneration == currentFileGeneration();\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.openqa.grid.common.RegistrationRequestTest.testBuildWithConfigurationAndNullCapabilities",
	"Comment": "should not result in any npe during the internal call to fixupcapabilities",
	"Method": "void testBuildWithConfigurationAndNullCapabilities(){\r\n    GridNodeConfiguration config = new GridNodeConfiguration();\r\n    config.capabilities = null;\r\n    RegistrationRequest req = RegistrationRequest.build(config);\r\n    assertNull(req.getConfiguration().capabilities);\r\n}"
}, {
	"Path": "org.elasticsearch.indices.IndicesService.canDeleteShardContent",
	"Comment": "returns sharddeletioncheckresult signaling whether the shards content for the given shard can be deleted.",
	"Method": "ShardDeletionCheckResult canDeleteShardContent(ShardId shardId,IndexSettings indexSettings){\r\n    assert shardId.getIndex().equals(indexSettings.getIndex());\r\n    final IndexService indexService = indexService(shardId.getIndex());\r\n    if (nodeEnv.hasNodeFile()) {\r\n        final boolean isAllocated = indexService != null && indexService.hasShard(shardId.id());\r\n        if (isAllocated) {\r\n            return ShardDeletionCheckResult.STILL_ALLOCATED;\r\n        } else if (indexSettings.hasCustomDataPath()) {\r\n            return Files.exists(nodeEnv.resolveCustomLocation(indexSettings, shardId)) ? ShardDeletionCheckResult.FOLDER_FOUND_CAN_DELETE : ShardDeletionCheckResult.NO_FOLDER_FOUND;\r\n        } else {\r\n            return FileSystemUtils.exists(nodeEnv.availableShardPaths(shardId)) ? ShardDeletionCheckResult.FOLDER_FOUND_CAN_DELETE : ShardDeletionCheckResult.NO_FOLDER_FOUND;\r\n        }\r\n    } else {\r\n        return ShardDeletionCheckResult.NO_LOCAL_STORAGE;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.ChineseCharacterBasedLexicon.sampleFrom",
	"Comment": "samples from the distribution over words with this pos according to the lexicon.",
	"Method": "String sampleFrom(String tag,String sampleFrom){\r\n    String POS = POSDistribution.sampleFrom();\r\n    return sampleFrom(POS);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.ShardRouting.relocating",
	"Comment": "returns true iff the this shard is currently relocating toanother node. otherwise false",
	"Method": "boolean relocating(){\r\n    return state == ShardRoutingState.RELOCATING;\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.spanish.pipeline.AnCoraProcessor.split",
	"Comment": "split the given tree based on a split point such that theterminals leading up to the split point are in the left returnedtree and those following the split point are in the left returnedtree.ancora contains a nontrivial amount of trees with multiplesentences in them. this method is used to break apart thesesentences into separate trees.",
	"Method": "Pair<Tree, Tree> split(Tree t,Tree splitPoint){\r\n    if (splitPoint == null)\r\n        return new Pair(t, null);\r\n    Tree left = t.prune(new LeftOfFilter(splitPoint, t));\r\n    Tree right = t.prune(new RightOfExclusiveFilter(splitPoint, t));\r\n    left = splittingNormalizer.normalizeWholeTree(left, splittingTreeFactory);\r\n    right = splittingNormalizer.normalizeWholeTree(right, splittingTreeFactory);\r\n    return new Pair(left, right);\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkPrimaryExecutionContext.markOperationAsExecuted",
	"Comment": "the current operation has been executed on the primary with the specified result",
	"Method": "void markOperationAsExecuted(Engine.Result result){\r\n    assertInvariants(ItemProcessingState.TRANSLATED);\r\n    final BulkItemRequest current = getCurrentItem();\r\n    DocWriteRequest docWriteRequest = getRequestToExecute();\r\n    switch(result.getResultType()) {\r\n        case SUCCESS:\r\n            final DocWriteResponse response;\r\n            if (result.getOperationType() == Engine.Operation.TYPE.INDEX) {\r\n                Engine.IndexResult indexResult = (Engine.IndexResult) result;\r\n                response = new IndexResponse(primary.shardId(), requestToExecute.type(), requestToExecute.id(), result.getSeqNo(), result.getTerm(), indexResult.getVersion(), indexResult.isCreated());\r\n            } else if (result.getOperationType() == Engine.Operation.TYPE.DELETE) {\r\n                Engine.DeleteResult deleteResult = (Engine.DeleteResult) result;\r\n                response = new DeleteResponse(primary.shardId(), requestToExecute.type(), requestToExecute.id(), deleteResult.getSeqNo(), result.getTerm(), deleteResult.getVersion(), deleteResult.isFound());\r\n            } else {\r\n                throw new AssertionError(\"unknown result type :\" + result.getResultType());\r\n            }\r\n            executionResult = new BulkItemResponse(current.id(), current.request().opType(), response);\r\n            executionResult.getResponse().setShardInfo(new ReplicationResponse.ShardInfo());\r\n            locationToSync = TransportWriteAction.locationToSync(locationToSync, result.getTranslogLocation());\r\n            break;\r\n        case FAILURE:\r\n            executionResult = new // concrete index instead of an alias if used!\r\n            BulkItemResponse(// concrete index instead of an alias if used!\r\n            current.id(), docWriteRequest.opType(), new BulkItemResponse.Failure(request.index(), docWriteRequest.type(), docWriteRequest.id(), result.getFailure(), result.getSeqNo()));\r\n            break;\r\n        default:\r\n            throw new AssertionError(\"unknown result type for \" + getCurrentItem() + \": \" + result.getResultType());\r\n    }\r\n    currentItemState = ItemProcessingState.EXECUTED;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.LongObjectPagedHashMap.get",
	"Comment": "get the value that is associated with key or null if keywas not present in the hash table.",
	"Method": "T get(long key){\r\n    for (long i = slot(hash(key), mask); ; i = nextSlot(i, mask)) {\r\n        final T value = values.get(i);\r\n        if (value == null) {\r\n            return null;\r\n        } else if (keys.get(i) == key) {\r\n            return value;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.MoveDecision.cannotRebalance",
	"Comment": "creates a move decision for when rebalancing the shard is not allowed.",
	"Method": "MoveDecision cannotRebalance(Decision canRebalanceDecision,AllocationDecision allocationDecision,int currentNodeRanking,List<NodeAllocationResult> nodeDecisions){\r\n    return new MoveDecision(null, canRebalanceDecision, allocationDecision, null, nodeDecisions, currentNodeRanking);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.AllocationService.currentNanoTime",
	"Comment": "override this to control time based decisions during allocation",
	"Method": "long currentNanoTime(){\r\n    return System.nanoTime();\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.Polarity.isUpwards",
	"Comment": "ignoring exclusion, determine if this word has upward polarity.",
	"Method": "boolean isUpwards(){\r\n    return projectLexicalRelation(NaturalLogicRelation.FORWARD_ENTAILMENT) == NaturalLogicRelation.FORWARD_ENTAILMENT && projectLexicalRelation(NaturalLogicRelation.REVERSE_ENTAILMENT) == NaturalLogicRelation.REVERSE_ENTAILMENT;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.rollGeneration",
	"Comment": "roll the current translog generation into a new generation. this does not commit thetranslog.",
	"Method": "void rollGeneration(){\r\n    try (Releasable ignored = writeLock.acquire()) {\r\n        try {\r\n            final TranslogReader reader = current.closeIntoReader();\r\n            readers.add(reader);\r\n            assert Checkpoint.read(location.resolve(CHECKPOINT_FILE_NAME)).generation == current.getGeneration();\r\n            copyCheckpointTo(location.resolve(getCommitCheckpointFileName(current.getGeneration())));\r\n            current = createWriter(current.getGeneration() + 1);\r\n            logger.trace(\"current translog set to [{}]\", current.getGeneration());\r\n        } catch (final Exception e) {\r\n            tragedy.setTragicException(e);\r\n            closeOnTragicEvent(e);\r\n            throw e;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.PluginsService.getModuleBundles",
	"Comment": "get bundles for plugins installed in the given modules directory.",
	"Method": "Set<Bundle> getModuleBundles(Path modulesDirectory){\r\n    return findBundles(modulesDirectory, \"module\");\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaData.getTotalOpenIndexShards",
	"Comment": "gets the total number of open shards from all indices. includesreplicas, but does not include shards that are part of closed indices.",
	"Method": "int getTotalOpenIndexShards(){\r\n    return this.totalOpenIndexShards;\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkItemResponse.getItemId",
	"Comment": "the numeric order of the item matching the same request order in the bulk request.",
	"Method": "int getItemId(){\r\n    return id;\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.CreationException.getErrorMessages",
	"Comment": "returns messages for the errors that caused this exception.",
	"Method": "Collection<Message> getErrorMessages(){\r\n    return messages;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.memorySizeSetting",
	"Comment": "creates a setting which specifies a memory size. this can either bespecified as an absolute bytes value or as a percentage of the heapmemory.",
	"Method": "Setting<ByteSizeValue> memorySizeSetting(String key,ByteSizeValue defaultValue,Property properties,Setting<ByteSizeValue> memorySizeSetting,String key,Function<Settings, String> defaultValue,Property properties,Setting<ByteSizeValue> memorySizeSetting,String key,String defaultPercentage,Property properties){\r\n    return new Setting(key, (s) -> defaultPercentage, (s) -> MemorySizeValue.parseBytesSizeValueOrHeapRatio(s, key), properties);\r\n}"
}, {
	"Path": "org.apache.dubbo.common.serialize.kryo.utils.AbstractKryoFactory.registerClass",
	"Comment": "only supposed to be called at startup time later may consider adding support for custom serializer, custom id, etc",
	"Method": "void registerClass(Class clazz){\r\n    if (kryoCreated) {\r\n        throw new IllegalStateException(\"Can't register class after creating kryo instance\");\r\n    }\r\n    registrations.add(clazz);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.TreeBinarizer.simpleTreeBinarizer",
	"Comment": "builds a treebinarizer with all of the options set to simple values",
	"Method": "TreeBinarizer simpleTreeBinarizer(HeadFinder hf,TreebankLanguagePack tlp){\r\n    return new TreeBinarizer(hf, tlp, false, false, 0, false, false, 0.0, false, true, true);\r\n}"
}, {
	"Path": "org.elasticsearch.action.get.GetResponse.getSourceAsBytesRef",
	"Comment": "returns bytes reference, also un compress the source if needed.",
	"Method": "BytesReference getSourceAsBytesRef(){\r\n    return getResult.sourceRef();\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.builders.LineStringBuilder.decompose",
	"Comment": "decompose a linestring given as array of coordinates at a vertical line.",
	"Method": "Coordinate[][] decompose(double dateline,Coordinate[] coordinates){\r\n    int offset = 0;\r\n    ArrayList<Coordinate[]> parts = new ArrayList();\r\n    double shift = coordinates[0].x > DATELINE ? DATELINE : (coordinates[0].x < -DATELINE ? -DATELINE : 0);\r\n    for (int i = 1; i < coordinates.length; i++) {\r\n        double t = intersection(coordinates[i - 1], coordinates[i], dateline);\r\n        if (!Double.isNaN(t)) {\r\n            Coordinate[] part;\r\n            if (t < 1) {\r\n                part = Arrays.copyOfRange(coordinates, offset, i + 1);\r\n                part[part.length - 1] = Edge.position(coordinates[i - 1], coordinates[i], t);\r\n                coordinates[offset + i - 1] = Edge.position(coordinates[i - 1], coordinates[i], t);\r\n                shift(shift, part);\r\n                offset = i - 1;\r\n                shift = coordinates[i].x > DATELINE ? DATELINE : (coordinates[i].x < -DATELINE ? -DATELINE : 0);\r\n            } else {\r\n                part = shift(shift, Arrays.copyOfRange(coordinates, offset, i + 1));\r\n                offset = i;\r\n            }\r\n            parts.add(part);\r\n        }\r\n    }\r\n    if (offset == 0) {\r\n        parts.add(shift(shift, coordinates));\r\n    } else if (offset < coordinates.length - 1) {\r\n        Coordinate[] part = Arrays.copyOfRange(coordinates, offset, coordinates.length);\r\n        parts.add(shift(shift, part));\r\n    }\r\n    return parts.toArray(new Coordinate[parts.size()][]);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryStringQueryBuilder.quoteFieldSuffix",
	"Comment": "an optional field name suffix to automatically try and add to the field searched when using quoted text.",
	"Method": "QueryStringQueryBuilder quoteFieldSuffix(String quoteFieldSuffix,String quoteFieldSuffix){\r\n    return this.quoteFieldSuffix;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.MappedFieldType.nullValue",
	"Comment": "returns the value that should be added when json null is found, or null if no value should be added",
	"Method": "Object nullValue(){\r\n    return nullValue;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.command.CancelAllocationCommand.index",
	"Comment": "get the index of the shard which allocation should be canceled",
	"Method": "String index(){\r\n    return this.index;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.SimpleQueryStringBuilder.analyzeWildcard",
	"Comment": "specifies whether wildcards should be analyzed. defaults to false.",
	"Method": "SimpleQueryStringBuilder analyzeWildcard(boolean analyzeWildcard,boolean analyzeWildcard){\r\n    return this.settings.analyzeWildcard();\r\n}"
}, {
	"Path": "android.util.Pair.hashCode",
	"Comment": "compute a hash code using the hash codes of the underlying objects",
	"Method": "int hashCode(){\r\n    return (first == null ? 0 : first.hashCode()) ^ (second == null ? 0 : second.hashCode());\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.server.LexicalizedParserServer.listen",
	"Comment": "runs in a loop, getting requests from new clients until a clienttells us to exit.",
	"Method": "void listen(){\r\n    while (stillRunning) {\r\n        Socket clientSocket = null;\r\n        try {\r\n            clientSocket = serverSocket.accept();\r\n            log.info(\"Got a connection\");\r\n            processRequest(clientSocket);\r\n            log.info(\"Goodbye!\");\r\n            log.info();\r\n        } catch (IOException e) {\r\n            clientSocket.close();\r\n            log.info(e);\r\n            continue;\r\n        }\r\n    }\r\n    serverSocket.close();\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.arabic.ArabicMorphoFeatureSpecification.processInflectionalFeatures",
	"Comment": "extract features from a standard phi feature specification.",
	"Method": "void processInflectionalFeatures(MorphoFeatures feats,String spec){\r\n    Matcher m = pFeatureTuple.matcher(spec);\r\n    if (m.find()) {\r\n        spec = m.group(1);\r\n        processInflectionalFeaturesHelper(feats, spec);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.Key.ofType",
	"Comment": "returns a new key of the specified type with the same annotation as thiskey.",
	"Method": "Key<T> ofType(Class<T> type,Key<?> ofType,Type type,Key<T> ofType,TypeLiteral<T> type){\r\n    return new Key(type, annotationStrategy);\r\n}"
}, {
	"Path": "org.elasticsearch.indices.flush.SyncedFlushService.sendPreSyncRequests",
	"Comment": "send presync requests to all started copies of the given shard",
	"Method": "void sendPreSyncRequests(List<ShardRouting> shards,ClusterState state,ShardId shardId,ActionListener<Map<String, PreSyncedFlushResponse>> listener){\r\n    final CountDown countDown = new CountDown(shards.size());\r\n    final ConcurrentMap<String, PreSyncedFlushResponse> presyncResponses = ConcurrentCollections.newConcurrentMap();\r\n    for (final ShardRouting shard : shards) {\r\n        logger.trace(\"{} sending pre-synced flush request to {}\", shardId, shard);\r\n        final DiscoveryNode node = state.nodes().get(shard.currentNodeId());\r\n        if (node == null) {\r\n            logger.trace(\"{} shard routing {} refers to an unknown node. skipping.\", shardId, shard);\r\n            if (countDown.countDown()) {\r\n                listener.onResponse(presyncResponses);\r\n            }\r\n            continue;\r\n        }\r\n        transportService.sendRequest(node, PRE_SYNCED_FLUSH_ACTION_NAME, new PreShardSyncedFlushRequest(shard.shardId()), new TransportResponseHandler<PreSyncedFlushResponse>() {\r\n            @Override\r\n            public PreSyncedFlushResponse read(StreamInput in) throws IOException {\r\n                PreSyncedFlushResponse response = new PreSyncedFlushResponse();\r\n                response.readFrom(in);\r\n                return response;\r\n            }\r\n            @Override\r\n            public void handleResponse(PreSyncedFlushResponse response) {\r\n                PreSyncedFlushResponse existing = presyncResponses.putIfAbsent(node.getId(), response);\r\n                assert existing == null : \"got two answers for node [\" + node + \"]\";\r\n                if (countDown.countDown()) {\r\n                    listener.onResponse(presyncResponses);\r\n                }\r\n            }\r\n            @Override\r\n            public void handleException(TransportException exp) {\r\n                logger.trace(() -> new ParameterizedMessage(\"{} error while performing pre synced flush on [{}], skipping\", shardId, shard), exp);\r\n                if (countDown.countDown()) {\r\n                    listener.onResponse(presyncResponses);\r\n                }\r\n            }\r\n            @Override\r\n            public String executor() {\r\n                return ThreadPool.Names.SAME;\r\n            }\r\n        });\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.indices.flush.SyncedFlushService.sendPreSyncRequests",
	"Comment": "send presync requests to all started copies of the given shard",
	"Method": "void sendPreSyncRequests(List<ShardRouting> shards,ClusterState state,ShardId shardId,ActionListener<Map<String, PreSyncedFlushResponse>> listener){\r\n    PreSyncedFlushResponse response = new PreSyncedFlushResponse();\r\n    response.readFrom(in);\r\n    return response;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.flush.SyncedFlushService.sendPreSyncRequests",
	"Comment": "send presync requests to all started copies of the given shard",
	"Method": "void sendPreSyncRequests(List<ShardRouting> shards,ClusterState state,ShardId shardId,ActionListener<Map<String, PreSyncedFlushResponse>> listener){\r\n    PreSyncedFlushResponse existing = presyncResponses.putIfAbsent(node.getId(), response);\r\n    assert existing == null : \"got two answers for node [\" + node + \"]\";\r\n    if (countDown.countDown()) {\r\n        listener.onResponse(presyncResponses);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.indices.flush.SyncedFlushService.sendPreSyncRequests",
	"Comment": "send presync requests to all started copies of the given shard",
	"Method": "void sendPreSyncRequests(List<ShardRouting> shards,ClusterState state,ShardId shardId,ActionListener<Map<String, PreSyncedFlushResponse>> listener){\r\n    logger.trace(() -> new ParameterizedMessage(\"{} error while performing pre synced flush on [{}], skipping\", shardId, shard), exp);\r\n    if (countDown.countDown()) {\r\n        listener.onResponse(presyncResponses);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.indices.flush.SyncedFlushService.sendPreSyncRequests",
	"Comment": "send presync requests to all started copies of the given shard",
	"Method": "void sendPreSyncRequests(List<ShardRouting> shards,ClusterState state,ShardId shardId,ActionListener<Map<String, PreSyncedFlushResponse>> listener){\r\n    return ThreadPool.Names.SAME;\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.ReplicaShardAllocator.augmentExplanationsWithStoreInfo",
	"Comment": "takes the store info for nodes that have a shard store and adds them to the node decisions,leaving the node explanations untouched for those nodes that do not have any store information.",
	"Method": "List<NodeAllocationResult> augmentExplanationsWithStoreInfo(Map<String, NodeAllocationResult> nodeDecisions,Map<String, NodeAllocationResult> withShardStores){\r\n    if (nodeDecisions == null || withShardStores == null) {\r\n        return null;\r\n    }\r\n    List<NodeAllocationResult> augmented = new ArrayList();\r\n    for (Map.Entry<String, NodeAllocationResult> entry : nodeDecisions.entrySet()) {\r\n        if (withShardStores.containsKey(entry.getKey())) {\r\n            augmented.add(withShardStores.get(entry.getKey()));\r\n        } else {\r\n            augmented.add(entry.getValue());\r\n        }\r\n    }\r\n    return augmented;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.MembershipAction.ensureIndexCompatibility",
	"Comment": "ensures that all indices are compatible with the given node version. this will ensure that all indices in the given metadatawill not be created with a newer version of elasticsearch as well as that all indices are newer or equal to the minimum indexcompatibility version.",
	"Method": "void ensureIndexCompatibility(Version nodeVersion,MetaData metaData){\r\n    Version supportedIndexVersion = nodeVersion.minimumIndexCompatibilityVersion();\r\n    for (IndexMetaData idxMetaData : metaData) {\r\n        if (idxMetaData.getCreationVersion().after(nodeVersion)) {\r\n            throw new IllegalStateException(\"index \" + idxMetaData.getIndex() + \" version not supported: \" + idxMetaData.getCreationVersion() + \" the node version is: \" + nodeVersion);\r\n        }\r\n        if (idxMetaData.getCreationVersion().before(supportedIndexVersion)) {\r\n            throw new IllegalStateException(\"index \" + idxMetaData.getIndex() + \" version not supported: \" + idxMetaData.getCreationVersion() + \" minimum compatible index version is: \" + supportedIndexVersion);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.GraphicalModel.readFromStream",
	"Comment": "static function to deserialize a graphical model from an input stream.",
	"Method": "GraphicalModel readFromStream(InputStream stream){\r\n    return readFromProto(GraphicalModelProto.GraphicalModel.parseDelimitedFrom(stream));\r\n}"
}, {
	"Path": "org.openqa.selenium.remote.server.log.SessionLogsToFileRepository.flushRecordsToLogFile",
	"Comment": "this creates a mapping between session and file representation of logs if doesnt exist already.writes the log records to the log file. this doesflush the logs to file. this doesclear the records after writing to file.",
	"Method": "void flushRecordsToLogFile(SessionId sessionId,List<LogRecord> records){\r\n    LogFile logFile = sessionToLogFileMap.get(sessionId);\r\n    if (logFile == null) {\r\n        createLogFileAndAddToMap(sessionId);\r\n        logFile = sessionToLogFileMap.get(sessionId);\r\n    }\r\n    logFile.openLogWriter();\r\n    for (LogRecord record : records) {\r\n        logFile.getLogWriter().writeObject(record);\r\n    }\r\n    logFile.closeLogWriter();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.checkShardLimit",
	"Comment": "checks whether an index can be created without going over the cluster shard limit.",
	"Method": "Optional<String> checkShardLimit(Settings settings,ClusterState clusterState){\r\n    int shardsToCreate = IndexMetaData.INDEX_NUMBER_OF_SHARDS_SETTING.get(settings) * (1 + IndexMetaData.INDEX_NUMBER_OF_REPLICAS_SETTING.get(settings));\r\n    return IndicesService.checkShardLimit(shardsToCreate, clusterState);\r\n}"
}, {
	"Path": "org.openqa.selenium.ReferrerTest.basicHistoryNavigationWithADirectProxy",
	"Comment": "tests navigation when all of the files are hosted on the same domain and the browser isconfigured to use a proxy that permits direct access to that domain.",
	"Method": "void basicHistoryNavigationWithADirectProxy(){\r\n    testServer1.start();\r\n    pacFileServer.setPacFileContents(\"function FindProxyForURL(url, host) { return 'DIRECT'; }\");\r\n    pacFileServer.start();\r\n    WebDriver driver = customDriverFactory.createDriver(pacFileServer.getBaseUrl());\r\n    String page1Url = buildPage1Url(testServer1, buildPage2Url(testServer1));\r\n    String page2Url = buildPage2Url(testServer1, buildPage3Url(testServer1));\r\n    String page3Url = buildPage3Url(testServer1);\r\n    performNavigation(driver, page1Url);\r\n    assertThat(testServer1.getRequests()).isEqualTo(ImmutableList.of(new HttpRequest(page1Url, null), new HttpRequest(page2Url, page1Url), new HttpRequest(page3Url, page2Url)));\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequestBuilder.setRouting",
	"Comment": "controls the shard routing of the request. using this value to hash the shardand not the id.",
	"Method": "UpdateRequestBuilder setRouting(String routing){\r\n    request.routing(routing);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.readObjectFromFileNoExceptions",
	"Comment": "read an object from a stored file without throwing exceptions.",
	"Method": "T readObjectFromFileNoExceptions(File file){\r\n    Object o = null;\r\n    try {\r\n        ObjectInputStream ois = new ObjectInputStream(new BufferedInputStream(new GZIPInputStream(new FileInputStream(file))));\r\n        o = ois.readObject();\r\n        ois.close();\r\n    } catch (IOException | ClassNotFoundException e) {\r\n        logger.err(throwableToStackTrace(e));\r\n    }\r\n    return ErasureUtils.uncheckedCast(o);\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.ShardCoreKeyMap.getCoreKeysForIndex",
	"Comment": "get the set of core cache keys associated with the given index.",
	"Method": "Set<Object> getCoreKeysForIndex(String index){\r\n    final Set<IndexReader.CacheKey> objects = indexToCoreKey.get(index);\r\n    if (objects == null) {\r\n        return Collections.emptySet();\r\n    }\r\n    return Collections.unmodifiableSet(new HashSet(objects));\r\n}"
}, {
	"Path": "org.elasticsearch.http.AbstractHttpServerTransport.onNonChannelException",
	"Comment": "exception handler for exceptions that are not associated with a specific channel.",
	"Method": "void onNonChannelException(Exception exception){\r\n    String threadName = Thread.currentThread().getName();\r\n    logger.warn(new ParameterizedMessage(\"exception caught on transport layer [thread={}]\", threadName), exception);\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.getTrackedLocalCheckpointForShard",
	"Comment": "returns the local checkpoint information tracked for a specific shard. used by tests.",
	"Method": "CheckpointState getTrackedLocalCheckpointForShard(String allocationId){\r\n    assert primaryMode;\r\n    return checkpoints.get(allocationId);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.RoutingExplanations.getYesDecisionMessages",
	"Comment": "provides feedback from commands with a yes decision that should be displayed to the user after the command has been applied",
	"Method": "List<String> getYesDecisionMessages(){\r\n    return explanations().stream().filter(explanation -> explanation.decisions().type().equals(Decision.Type.YES)).map(explanation -> explanation.command().getMessage()).filter(Optional::isPresent).map(Optional::get).collect(Collectors.toList());\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.NetworkUtils.sortKey",
	"Comment": "sorts an address by preference. this way code like publishing can just pick the first one",
	"Method": "int sortKey(InetAddress address,boolean prefer_v6){\r\n    int key = address.getAddress().length;\r\n    if (prefer_v6) {\r\n        key = -key;\r\n    }\r\n    if (address.isAnyLocalAddress()) {\r\n        key += 5;\r\n    }\r\n    if (address.isMulticastAddress()) {\r\n        key += 4;\r\n    }\r\n    if (address.isLoopbackAddress()) {\r\n        key += 3;\r\n    }\r\n    if (address.isLinkLocalAddress()) {\r\n        key += 2;\r\n    }\r\n    if (address.isSiteLocalAddress()) {\r\n        key += 1;\r\n    }\r\n    return key;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractBulkByScrollRequestBuilder.setRetryBackoffInitialTime",
	"Comment": "initial delay after a rejection before retrying a bulk request. with the default maxretries the total backoff for retrying rejectionsis about one minute per bulk request. once the entire bulk request is successful the retry counter resets.",
	"Method": "Self setRetryBackoffInitialTime(TimeValue retryBackoffInitialTime){\r\n    request.setRetryBackoffInitialTime(retryBackoffInitialTime);\r\n    return self();\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.isFiltered",
	"Comment": "returns true if this setting must be filtered, otherwise false",
	"Method": "boolean isFiltered(){\r\n    return properties.contains(Property.Filtered);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.dvparser.DVModel.randomContextMatrix",
	"Comment": "creates a random context matrix.this will be numrows x numcols big.these can be appended to the end of either aunary or binary transform matrix to get the transform matrixwhich uses context words.",
	"Method": "SimpleMatrix randomContextMatrix(){\r\n    SimpleMatrix matrix = new SimpleMatrix(numRows, numCols * 2);\r\n    matrix.insertIntoThis(0, 0, identity.scale(op.trainOptions.scalingForInit * 0.1));\r\n    matrix.insertIntoThis(0, numCols, identity.scale(op.trainOptions.scalingForInit * 0.1));\r\n    matrix = matrix.plus(SimpleMatrix.random(numRows, numCols * 2, -1.0 / Math.sqrt((double) numCols * 100.0), 1.0 / Math.sqrt((double) numCols * 100.0), rand));\r\n    return matrix;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.InnerHitBuilder.isIgnoreUnmapped",
	"Comment": "whether to include inner hits in the search response hits if required mappings is missing",
	"Method": "boolean isIgnoreUnmapped(){\r\n    return ignoreUnmapped;\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.NeuralUtils.concatenateWithBias",
	"Comment": "concatenates several column vectors into one large columnvector, adds a 1.0 at the end as a bias term",
	"Method": "SimpleMatrix concatenateWithBias(SimpleMatrix vectors){\r\n    int size = 0;\r\n    for (SimpleMatrix vector : vectors) {\r\n        size += vector.numRows();\r\n    }\r\n    size++;\r\n    SimpleMatrix result = new SimpleMatrix(size, 1);\r\n    int index = 0;\r\n    for (SimpleMatrix vector : vectors) {\r\n        result.insertIntoThis(index, 0, vector);\r\n        index += vector.numRows();\r\n    }\r\n    result.set(index, 0, 1.0);\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterInfo.getShardSize",
	"Comment": "returns the shard size for the given shard routing or defaultvalue it that metric is not available.",
	"Method": "Long getShardSize(ShardRouting shardRouting,long getShardSize,ShardRouting shardRouting,long defaultValue){\r\n    Long shardSize = getShardSize(shardRouting);\r\n    return shardSize == null ? defaultValue : shardSize;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.IndexRoutingTable.primaryShardsActive",
	"Comment": "calculates the number of primary shards in active state in routing table",
	"Method": "int primaryShardsActive(){\r\n    int counter = 0;\r\n    for (IndexShardRoutingTable shardRoutingTable : this) {\r\n        if (shardRoutingTable.primaryShard().active()) {\r\n            counter++;\r\n        }\r\n    }\r\n    return counter;\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.Store.loadSeqNoInfo",
	"Comment": "loads the maximum sequence number and local checkpoint from the given lucene commit point or the latest if not provided.",
	"Method": "SequenceNumbers.CommitInfo loadSeqNoInfo(IndexCommit commit){\r\n    final Map<String, String> userData = commit.getUserData();\r\n    return SequenceNumbers.loadSeqNoInfoFromLuceneCommit(userData.entrySet());\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkPrimaryExecutionContext.isOperationExecuted",
	"Comment": "returns true if the current request has been executed on the primary",
	"Method": "boolean isOperationExecuted(){\r\n    return currentItemState == ItemProcessingState.EXECUTED;\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksService.execute",
	"Comment": "executes an asynchronous persistent task action using the client.the origin is set in the context and the listener is wrapped to ensure the proper context is restored",
	"Method": "void execute(Req request,Action<Resp> action,ActionListener<PersistentTask<?>> listener){\r\n    try {\r\n        client.execute(action, request, ActionListener.wrap(r -> listener.onResponse(r.getTask()), listener::onFailure));\r\n    } catch (Exception e) {\r\n        listener.onFailure(e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.NeuralUtils.vectorToParams",
	"Comment": "given a sequence of iterators over simplematrix, fill in all ofthe matrices with the entries in the theta vector.errors arethrown if the theta vector does not exactly fill the matrices.",
	"Method": "void vectorToParams(double[] theta,Iterator<SimpleMatrix> matrices){\r\n    int index = 0;\r\n    for (Iterator<SimpleMatrix> matrixIterator : matrices) {\r\n        while (matrixIterator.hasNext()) {\r\n            SimpleMatrix matrix = matrixIterator.next();\r\n            int numElements = matrix.getNumElements();\r\n            for (int i = 0; i < numElements; ++i) {\r\n                matrix.set(i, theta[index]);\r\n                ++index;\r\n            }\r\n        }\r\n    }\r\n    if (index != theta.length) {\r\n        throw new AssertionError(\"Did not entirely use the theta vector\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.BigArrays.grow",
	"Comment": "grow an array to a size that is larger than minsize,preserving content, and potentially reusing part of the provided array.",
	"Method": "ByteArray grow(ByteArray array,long minSize,IntArray grow,IntArray array,long minSize,LongArray grow,LongArray array,long minSize,DoubleArray grow,DoubleArray array,long minSize,FloatArray grow,FloatArray array,long minSize,ObjectArray<T> grow,ObjectArray<T> array,long minSize){\r\n    if (minSize <= array.size()) {\r\n        return array;\r\n    }\r\n    final long newSize = overSize(minSize, OBJECT_PAGE_SIZE, RamUsageEstimator.NUM_BYTES_OBJECT_REF);\r\n    return resize(array, newSize);\r\n}"
}, {
	"Path": "android.text.TextUtils.dumpSpans",
	"Comment": "debugging tool to print the spans in a charsequence.the output willbe printed one span per line.if the charsequence is not a spanned,then the entire string will be printed on a single line.",
	"Method": "void dumpSpans(CharSequence cs,Printer printer,String prefix){\r\n    if (cs instanceof Spanned) {\r\n        Spanned sp = (Spanned) cs;\r\n        Object[] os = sp.getSpans(0, cs.length(), Object.class);\r\n        for (int i = 0; i < os.length; i++) {\r\n            Object o = os[i];\r\n            printer.println(prefix + cs.subSequence(sp.getSpanStart(o), sp.getSpanEnd(o)) + \": \" + Integer.toHexString(System.identityHashCode(o)) + \" \" + o.getClass().getCanonicalName() + \" (\" + sp.getSpanStart(o) + \"-\" + sp.getSpanEnd(o) + \") fl=#\" + sp.getSpanFlags(o));\r\n        }\r\n    } else {\r\n        printer.println(prefix + cs + \": (no spans)\");\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVector.setSparseComponent",
	"Comment": "sets a single component of the concat vector value as a sparse, one hot value.",
	"Method": "void setSparseComponent(int component,int index,double value){\r\n    if (component >= pointers.length) {\r\n        increaseSizeTo(component + 1);\r\n    }\r\n    double[] sparseInfo = new double[2];\r\n    sparseInfo[0] = index;\r\n    sparseInfo[1] = value;\r\n    pointers[component] = sparseInfo;\r\n    sparse[component] = true;\r\n    copyOnWrite[component] = false;\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequestBuilder.setRouting",
	"Comment": "controls the shard routing of the request. using this value to hash the shardand not the id.",
	"Method": "IndexRequestBuilder setRouting(String routing){\r\n    request.routing(routing);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.EnglishTreebankParserParams.treeReaderFactory",
	"Comment": "makes appropriate treereaderfactory with all options specified",
	"Method": "TreeReaderFactory treeReaderFactory(){\r\n    return in -> new PennTreeReader(in, new LabeledScoredTreeFactory(), new NPTmpRetainingTreeNormalizer(englishTrain.splitTMP, englishTrain.splitSGapped == 5, englishTrain.leaveItAll, englishTrain.splitNPADV >= 1, headFinder()));\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.Uid.encodeNumericId",
	"Comment": "with numeric ids, we just fold two consecutive chars in a single byte and use 0x0f as an end marker.",
	"Method": "BytesRef encodeNumericId(String id){\r\n    byte[] b = new byte[1 + (id.length() + 1) / 2];\r\n    b[0] = (byte) NUMERIC;\r\n    for (int i = 0; i < id.length(); i += 2) {\r\n        int b1 = id.charAt(i) - '0';\r\n        int b2;\r\n        if (i + 1 == id.length()) {\r\n            b2 = 0x0f;\r\n        } else {\r\n            b2 = id.charAt(i + 1) - '0';\r\n        }\r\n        b[1 + i / 2] = (byte) ((b1 << 4) | b2);\r\n    }\r\n    return new BytesRef(b);\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequest.termStatistics",
	"Comment": "return the term statistics for each term in the shard or skip.",
	"Method": "boolean termStatistics(TermVectorsRequest termStatistics,boolean termStatistics){\r\n    setFlag(Flag.TermStatistics, termStatistics);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.node.InternalSettingsPreparer.prepareSettings",
	"Comment": "prepares settings for the transport client by gathering allelasticsearch system properties and setting defaults.",
	"Method": "Settings prepareSettings(Settings input){\r\n    Settings.Builder output = Settings.builder();\r\n    initializeSettings(output, input, Collections.emptyMap());\r\n    finalizeSettings(output, () -> null);\r\n    return output.build();\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogWriter.syncUpTo",
	"Comment": "syncs the translog up to at least the given offset unless already synced",
	"Method": "boolean syncUpTo(long offset){\r\n    if (lastSyncedCheckpoint.offset < offset && syncNeeded()) {\r\n        synchronized (syncLock) {\r\n            if (lastSyncedCheckpoint.offset < offset && syncNeeded()) {\r\n                final Checkpoint checkpointToSync;\r\n                synchronized (this) {\r\n                    ensureOpen();\r\n                    try {\r\n                        outputStream.flush();\r\n                        checkpointToSync = getCheckpoint();\r\n                    } catch (final Exception ex) {\r\n                        closeWithTragicEvent(ex);\r\n                        throw ex;\r\n                    }\r\n                }\r\n                try {\r\n                    channel.force(false);\r\n                    writeCheckpoint(channelFactory, path.getParent(), checkpointToSync);\r\n                } catch (final Exception ex) {\r\n                    closeWithTragicEvent(ex);\r\n                    throw ex;\r\n                }\r\n                assert lastSyncedCheckpoint.offset <= checkpointToSync.offset : \"illegal state: \" + lastSyncedCheckpoint.offset + \" <= \" + checkpointToSync.offset;\r\n                lastSyncedCheckpoint = checkpointToSync;\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "android.text.SpannableStringBuilder.subSequence",
	"Comment": "return a new charsequence containing a copy of the specifiedrange of this buffer, including the overlapping spans.",
	"Method": "CharSequence subSequence(int start,int end){\r\n    return new SpannableStringBuilder(this, start, end);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.analyze.AnalyzeRequestBuilder.setNormalizer",
	"Comment": "instead of setting the analyzer and tokenizer, sets the normalizer as name",
	"Method": "AnalyzeRequestBuilder setNormalizer(String normalizer){\r\n    request.normalizer(normalizer);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.SequenceMatcher.toBasicSequenceMatchResult",
	"Comment": "returns a copy of the current match results.use this methodto save away match results for later use, since future operationsusing the sequencematcher changes the match results.",
	"Method": "BasicSequenceMatchResult<T> toBasicSequenceMatchResult(){\r\n    if (matchingCompleted && matched) {\r\n        return super.toBasicSequenceMatchResult();\r\n    } else {\r\n        String message = getStateMessage();\r\n        throw new IllegalStateException(message);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MoreLikeThisQueryBuilder.unlike",
	"Comment": "sets the documents from which the terms should not be selected from.",
	"Method": "MoreLikeThisQueryBuilder unlike(String[] unlikeTexts,MoreLikeThisQueryBuilder unlike,Item[] unlikeItems){\r\n    this.unlikeItems = Optional.ofNullable(unlikeItems).orElse(new Item[0]);\r\n    return this;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.listener.RegistrationListenerTest.testBugRegistration",
	"Comment": "proxy not registered when throw an exception during registration",
	"Method": "void testBugRegistration(){\r\n    GridRegistry registry = DefaultGridRegistry.newInstance(new Hub(new GridHubConfiguration()));\r\n    registry.add(new MyBuggyRemoteProxy(req, registry));\r\n    registry.add(new MyBuggyRemoteProxy(req, registry));\r\n    assertEquals(registry.getAllProxies().size(), 1);\r\n}"
}, {
	"Path": "org.openqa.grid.e2e.utils.RegistryTestHelper.waitForNode",
	"Comment": "wait for the registry to have exactly nodenumber nodes registered.",
	"Method": "void waitForNode(GridRegistry r,int nodeNumber){\r\n    newWait().until(input -> {\r\n        Integer i = r.getAllProxies().size();\r\n        if (i != nodeNumber) {\r\n            return null;\r\n        }\r\n        return i;\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.notifyAllWaiters",
	"Comment": "notify all threads waiting on the monitor on this tracker. these threads should be waiting for the local checkpoint on a specificallocation id to catch up to the global checkpoint.",
	"Method": "void notifyAllWaiters(){\r\n    this.notifyAll();\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.assistedinject.AssistedConstructor.newInstance",
	"Comment": "returns an instance of t, constructed using this constructor, with thesupplied arguments.",
	"Method": "T newInstance(Object[] args){\r\n    try {\r\n        return constructor.newInstance(args);\r\n    } catch (InvocationTargetException e) {\r\n        throw e.getCause();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVectorNamespace.newWeightsVector",
	"Comment": "this constructs a fresh vector that is sized correctly to accommodate all the known sparse values for vectorsthat are possibly sparse.",
	"Method": "ConcatVector newWeightsVector(){\r\n    ConcatVector vector = new ConcatVector(featureToIndex.size());\r\n    for (String s : sparseFeatureIndex.keySet()) {\r\n        int size = sparseFeatureIndex.get(s).size();\r\n        vector.setDenseComponent(ensureFeature(s), new double[size]);\r\n    }\r\n    return vector;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.LexicalizedParser.saveParserToSerialized",
	"Comment": "saves the parser defined by pd to the given filename.if there is an error, a runtimeioexception is thrown.",
	"Method": "void saveParserToSerialized(String filename){\r\n    try {\r\n        log.info(\"Writing parser in serialized format to file \" + filename + ' ');\r\n        ObjectOutputStream out = IOUtils.writeStreamFromString(filename);\r\n        out.writeObject(this);\r\n        out.close();\r\n        log.info(\"done.\");\r\n    } catch (IOException ioe) {\r\n        throw new RuntimeIOException(ioe);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequestBuilder.setTypes",
	"Comment": "the types of documents the query will run against. defaults to all types.",
	"Method": "ValidateQueryRequestBuilder setTypes(String types){\r\n    request.types(types);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchResponse.getClusters",
	"Comment": "returns info about what clusters the search was executed against. available only in responses obtainedfrom a cross cluster search request, otherwise null",
	"Method": "Clusters getClusters(){\r\n    return clusters;\r\n}"
}, {
	"Path": "org.openqa.grid.web.servlet.handler.RequestHandler.beforeSessionEvent",
	"Comment": "calls the testsessionlistener is the proxy for that node has one specified.",
	"Method": "void beforeSessionEvent(){\r\n    RemoteProxy p = session.getSlot().getProxy();\r\n    if (p instanceof TestSessionListener) {\r\n        try {\r\n            ((TestSessionListener) p).beforeSession(session);\r\n        } catch (Exception e) {\r\n            log.severe(\"Error running the beforeSessionListener : \" + e.getMessage());\r\n            e.printStackTrace();\r\n            throw new NewSessionException(\"The listener threw an exception ( listener bug )\", e);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.monitor.os.OsProbe.getControlGroups",
	"Comment": "a map of the control groups to which the elasticsearch process belongs. note that this is a map because the control groups can varyfrom subsystem to subsystem. additionally, this map can not be cached because a running process can be reclassified.",
	"Method": "Map<String, String> getControlGroups(){\r\n    final List<String> lines = readProcSelfCgroup();\r\n    final Map<String, String> controllerMap = new HashMap();\r\n    for (final String line : lines) {\r\n        final String[] fields = line.split(\":\");\r\n        assert fields.length == 3;\r\n        final String[] controllers = fields[1].split(\",\");\r\n        for (final String controller : controllers) {\r\n            final String controlGroupPath;\r\n            if (CONTROL_GROUPS_HIERARCHY_OVERRIDE != null) {\r\n                controlGroupPath = CONTROL_GROUPS_HIERARCHY_OVERRIDE;\r\n            } else {\r\n                controlGroupPath = fields[2];\r\n            }\r\n            final String previous = controllerMap.put(controller, controlGroupPath);\r\n            assert previous == null;\r\n        }\r\n    }\r\n    return controllerMap;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterModule.addAllocationDecider",
	"Comment": "add the given allocation decider to the given deciders collection, erroring if the class name is already used.",
	"Method": "void addAllocationDecider(Map<Class, AllocationDecider> deciders,AllocationDecider decider){\r\n    if (deciders.put(decider.getClass(), decider) != null) {\r\n        throw new IllegalArgumentException(\"Cannot specify allocation decider [\" + decider.getClass().getName() + \"] twice\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.StoreFileMetaData.writtenBy",
	"Comment": "returns the lucene version this file has been written by or null if unknown",
	"Method": "Version writtenBy(){\r\n    return writtenBy;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequestBuilder.addIndexBoost",
	"Comment": "sets the boost a specific index will receive when the query is executed against it.",
	"Method": "SearchRequestBuilder addIndexBoost(String index,float indexBoost){\r\n    sourceBuilder().indexBoost(index, indexBoost);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterModule.filterCustomsForPre63Clients",
	"Comment": "for interoperability with transport clients older than 6.3, we need to strip customsfrom the cluster state that the client might not be able to deserialize",
	"Method": "ClusterState filterCustomsForPre63Clients(ClusterState clusterState){\r\n    final ClusterState.Builder builder = ClusterState.builder(clusterState);\r\n    clusterState.customs().keysIt().forEachRemaining(name -> {\r\n        if (PRE_6_3_CLUSTER_CUSTOMS_WHITE_LIST.contains(name) == false) {\r\n            builder.removeCustom(name);\r\n        }\r\n    });\r\n    final MetaData.Builder metaBuilder = MetaData.builder(clusterState.metaData());\r\n    clusterState.metaData().customs().keysIt().forEachRemaining(name -> {\r\n        if (PRE_6_3_METADATA_CUSTOMS_WHITE_LIST.contains(name) == false) {\r\n            metaBuilder.removeCustom(name);\r\n        }\r\n    });\r\n    return builder.metaData(metaBuilder).build();\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.exists",
	"Comment": "check whether there is one or more documents matching the provided query.",
	"Method": "boolean exists(IndexSearcher searcher,Query query){\r\n    final Weight weight = searcher.createWeight(searcher.rewrite(query), ScoreMode.COMPLETE_NO_SCORES, 1f);\r\n    for (LeafReaderContext context : searcher.getIndexReader().leaves()) {\r\n        final Scorer scorer = weight.scorer(context);\r\n        if (scorer == null) {\r\n            continue;\r\n        }\r\n        final Bits liveDocs = context.reader().getLiveDocs();\r\n        final DocIdSetIterator iterator = scorer.iterator();\r\n        for (int doc = iterator.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = iterator.nextDoc()) {\r\n            if (liveDocs == null || liveDocs.get(doc)) {\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.NeuralUtils.elementwiseApplyReLU",
	"Comment": "applies relu to each of the entries in the matrix.returns a new matrix.",
	"Method": "SimpleMatrix elementwiseApplyReLU(SimpleMatrix input){\r\n    SimpleMatrix output = new SimpleMatrix(input);\r\n    for (int i = 0; i < output.numRows(); ++i) {\r\n        for (int j = 0; j < output.numCols(); ++j) {\r\n            output.set(i, j, Math.max(0, output.get(i, j)));\r\n        }\r\n    }\r\n    return output;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.getMinFileGeneration",
	"Comment": "returns the minimum file generation referenced by the translog",
	"Method": "long getMinFileGeneration(){\r\n    try (ReleasableLock ignored = readLock.acquire()) {\r\n        if (readers.isEmpty()) {\r\n            return current.getGeneration();\r\n        } else {\r\n            assert readers.stream().map(TranslogReader::getGeneration).min(Long::compareTo).get().equals(readers.get(0).getGeneration()) : \"the first translog isn't the one with the minimum generation:\" + readers;\r\n            return readers.get(0).getGeneration();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "android.text.SpannableStringBuilder.nextSpanTransition",
	"Comment": "return the next offset after start but less than orequal to limit where a span of the specified typebegins or ends.",
	"Method": "int nextSpanTransition(int start,int limit,Class kind){\r\n    int count = mSpanCount;\r\n    Object[] spans = mSpans;\r\n    int[] starts = mSpanStarts;\r\n    int[] ends = mSpanEnds;\r\n    int gapstart = mGapStart;\r\n    int gaplen = mGapLength;\r\n    if (kind == null) {\r\n        kind = Object.class;\r\n    }\r\n    for (int i = 0; i < count; i++) {\r\n        int st = starts[i];\r\n        int en = ends[i];\r\n        if (st > gapstart)\r\n            st -= gaplen;\r\n        if (en > gapstart)\r\n            en -= gaplen;\r\n        if (st > start && st < limit && kind.isInstance(spans[i]))\r\n            limit = st;\r\n        if (en > start && en < limit && kind.isInstance(spans[i]))\r\n            limit = en;\r\n    }\r\n    return limit;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.dvparser.DVModel.randomTransformMatrix",
	"Comment": "create a random transform matrix based on the initializationparameters.this will be numrows x numcols big.these can beplugged into either unary or binary transform matrices.",
	"Method": "SimpleMatrix randomTransformMatrix(){\r\n    SimpleMatrix matrix;\r\n    switch(op.trainOptions.transformMatrixType) {\r\n        case DIAGONAL:\r\n            matrix = SimpleMatrix.random(numRows, numCols, -1.0 / Math.sqrt((double) numCols * 100.0), 1.0 / Math.sqrt((double) numCols * 100.0), rand).plus(identity);\r\n            break;\r\n        case RANDOM:\r\n            matrix = SimpleMatrix.random(numRows, numCols, -1.0 / Math.sqrt((double) numCols), 1.0 / Math.sqrt((double) numCols), rand);\r\n            break;\r\n        case OFF_DIAGONAL:\r\n            matrix = SimpleMatrix.random(numRows, numCols, -1.0 / Math.sqrt((double) numCols * 100.0), 1.0 / Math.sqrt((double) numCols * 100.0), rand).plus(identity);\r\n            for (int i = 0; i < numCols; ++i) {\r\n                int x = rand.nextInt(numCols);\r\n                int y = rand.nextInt(numCols);\r\n                int scale = rand.nextInt(3) - 1;\r\n                matrix.set(x, y, matrix.get(x, y) + scale);\r\n            }\r\n            break;\r\n        case RANDOM_ZEROS:\r\n            matrix = SimpleMatrix.random(numRows, numCols, -1.0 / Math.sqrt((double) numCols * 100.0), 1.0 / Math.sqrt((double) numCols * 100.0), rand).plus(identity);\r\n            for (int i = 0; i < numCols; ++i) {\r\n                int x = rand.nextInt(numCols);\r\n                int y = rand.nextInt(numCols);\r\n                matrix.set(x, y, 0.0);\r\n            }\r\n            break;\r\n        default:\r\n            throw new IllegalArgumentException(\"Unexpected matrix initialization type \" + op.trainOptions.transformMatrixType);\r\n    }\r\n    return matrix;\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.ClauseSplitterSearchProblem.stripAuxMark",
	"Comment": "strips aux and mark edges when we are splitting into a clause.",
	"Method": "void stripAuxMark(SemanticGraph toModify){\r\n    List<SemanticGraphEdge> toClean = new ArrayList();\r\n    for (SemanticGraphEdge edge : toModify.outgoingEdgeIterable(toModify.getFirstRoot())) {\r\n        String rel = edge.getRelation().toString();\r\n        if ((\"aux\".equals(rel) || \"mark\".equals(rel)) && !toModify.outgoingEdgeIterator(edge.getDependent()).hasNext()) {\r\n            toClean.add(edge);\r\n        }\r\n    }\r\n    for (SemanticGraphEdge edge : toClean) {\r\n        toModify.removeEdge(edge);\r\n        toModify.removeVertex(edge.getDependent());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.command.AbstractAllocateAllocationCommand.explainOrThrowRejectedCommand",
	"Comment": "utility method for rejecting the current allocation command based on provided exception",
	"Method": "RerouteExplanation explainOrThrowRejectedCommand(boolean explain,RoutingAllocation allocation,String reason,RerouteExplanation explainOrThrowRejectedCommand,boolean explain,RoutingAllocation allocation,RuntimeException rte){\r\n    if (explain) {\r\n        return new RerouteExplanation(this, allocation.decision(Decision.NO, name() + \" (allocation command)\", rte.getMessage()));\r\n    }\r\n    throw rte;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MatchQueryBuilder.fuzziness",
	"Comment": "gets the fuzziness used when evaluated to a fuzzy query type.",
	"Method": "MatchQueryBuilder fuzziness(Object fuzziness,Fuzziness fuzziness){\r\n    return this.fuzziness;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.ConcurrentCollections.newConcurrentMapWithAggressiveConcurrency",
	"Comment": "creates a new chm with an aggressive concurrency level, aimed at high concurrent update rate long living maps.",
	"Method": "ConcurrentMap<K, V> newConcurrentMapWithAggressiveConcurrency(ConcurrentMap<K, V> newConcurrentMapWithAggressiveConcurrency,int initalCapacity){\r\n    return new ConcurrentHashMap(initalCapacity, 0.75f, aggressiveConcurrencyLevel);\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.recoveryStats",
	"Comment": "returns stats about ongoing recoveries, both source and target",
	"Method": "RecoveryStats recoveryStats(){\r\n    return recoveryStats;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.upgrade",
	"Comment": "upgrades the shard to the current version of lucene and returns the minimum segment version",
	"Method": "org.apache.lucene.util.Version upgrade(UpgradeRequest upgrade){\r\n    verifyActive();\r\n    if (logger.isTraceEnabled()) {\r\n        logger.trace(\"upgrade with {}\", upgrade);\r\n    }\r\n    org.apache.lucene.util.Version previousVersion = minimumCompatibleVersion();\r\n    final Engine engine = getEngine();\r\n    // we need to flush at the end to make sure the upgrade is durable\r\n    engine.forceMerge(true, Integer.MAX_VALUE, false, true, upgrade.upgradeOnlyAncientSegments());\r\n    org.apache.lucene.util.Version version = minimumCompatibleVersion();\r\n    if (logger.isTraceEnabled()) {\r\n        logger.trace(\"upgraded segments for {} from version {} to version {}\", shardId, previousVersion, version);\r\n    }\r\n    return version;\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.VectorMap.deserialize",
	"Comment": "read word vectors from an input stream. the stream is not closed on finishing the function.",
	"Method": "VectorMap deserialize(String file,VectorMap deserialize,InputStream in){\r\n    DataInputStream dataIn = new DataInputStream(in);\r\n    itype keyIntType = itype.getType(dataIn.readInt());\r\n    int dim = dataIn.readInt();\r\n    int size = dataIn.readInt();\r\n    VectorMap vectors = new VectorMap();\r\n    for (int i = 0; i < size; ++i) {\r\n        int strlen = keyIntType.read(dataIn);\r\n        byte[] buffer = new byte[strlen];\r\n        if (dataIn.read(buffer, 0, strlen) != strlen) {\r\n            throw new IOException(\"Could not read string buffer fully!\");\r\n        }\r\n        String key = new String(buffer);\r\n        float[] vector = new float[dim];\r\n        for (int k = 0; k < vector.length; ++k) {\r\n            vector[k] = toFloat(dataIn.readShort());\r\n        }\r\n        vectors.put(key, vector);\r\n    }\r\n    return vectors;\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.ArrayMath.standardize",
	"Comment": "standardize values in this array, i.e., subtract the mean and divide by the standard deviation.if standard deviation is 0.0, throws a runtimeexception.",
	"Method": "void standardize(double[] a){\r\n    double m = mean(a);\r\n    if (Double.isNaN(m)) {\r\n        throw new RuntimeException(\"Can't standardize array whose mean is NaN\");\r\n    }\r\n    double s = stdev(a);\r\n    if (s == 0.0 || Double.isNaN(s)) {\r\n        throw new RuntimeException(\"Can't standardize array whose standard deviation is 0.0 or NaN\");\r\n    }\r\n    addInPlace(a, -m);\r\n    multiplyInPlace(a, 1.0 / s);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.RoutingAllocation.changes",
	"Comment": "returns observer to use for changes made to the routing nodes",
	"Method": "RoutingChangesObserver changes(){\r\n    return routingChangesObserver;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogDeletionPolicy.getTranslogGenerationOfLastCommit",
	"Comment": "returns a translog generation that will be used to calculate the number of uncommitted operations since the last index commit.",
	"Method": "long getTranslogGenerationOfLastCommit(){\r\n    return translogGenerationOfLastCommit;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.util.RelationTriple.asDependencyTree",
	"Comment": "an optional method, returning the dependency tree this triple was extracted from",
	"Method": "Optional<SemanticGraph> asDependencyTree(Optional<SemanticGraph> asDependencyTree){\r\n    return Optional.empty();\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.slurpFile",
	"Comment": "returns all the text in the given file with the given encoding.the string may be empty, if the file is empty.",
	"Method": "String slurpFile(File file,String slurpFile,File file,String encoding,String slurpFile,String filename,String encoding,String slurpFile,String filename){\r\n    return slurpFile(filename, defaultEncoding);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.MLEDependencyGrammar.readData",
	"Comment": "populates data in this dependencygrammar from the character streamgiven by the reader r.",
	"Method": "void readData(BufferedReader in){\r\n    final String LEFT = \"left\";\r\n    int lineNum = 1;\r\n    boolean doingStop = false;\r\n    for (String line = in.readLine(); line != null && line.length() > 0; line = in.readLine()) {\r\n        try {\r\n            if (line.equals(\"BEGIN_STOP\")) {\r\n                doingStop = true;\r\n                continue;\r\n            }\r\n            String[] fields = StringUtils.splitOnCharWithQuoting(line, ' ', '\\\"', '\\\\');\r\n            short distance = (short) Integer.parseInt(fields[4]);\r\n            IntTaggedWord tempHead = new IntTaggedWord(fields[0], '/', wordIndex, tagIndex);\r\n            IntTaggedWord tempArg = new IntTaggedWord(fields[2], '/', wordIndex, tagIndex);\r\n            IntDependency tempDependency = new IntDependency(tempHead, tempArg, fields[3].equals(LEFT), distance);\r\n            double count = Double.parseDouble(fields[5]);\r\n            if (doingStop) {\r\n                expandStop(tempDependency, distance, count, false);\r\n            } else {\r\n                expandArg(tempDependency, distance, count);\r\n            }\r\n        } catch (Exception e) {\r\n            IOException ioe = new IOException(\"Error on line \" + lineNum + \": \" + line);\r\n            ioe.initCause(e);\r\n            throw ioe;\r\n        }\r\n        lineNum++;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.SecureSetting.insecureString",
	"Comment": "a setting which contains a sensitive string, but which for legacy reasons must be found outside secure settings.",
	"Method": "Setting<SecureString> insecureString(String name){\r\n    return new InsecureStringSetting(name);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.Engine.flush",
	"Comment": "flushes the state of the engine including the transaction log, clearing memory.",
	"Method": "CommitId flush(boolean force,boolean waitIfOngoing,CommitId flush){\r\n    return flush(false, false);\r\n}"
}, {
	"Path": "org.openqa.selenium.ReferrerTest.crossDomainHistoryNavigationWithAProxiedHost",
	"Comment": "tests navigation across multiple domains when the browser is configured to use a proxy thatredirects the second domain to another host.",
	"Method": "void crossDomainHistoryNavigationWithAProxiedHost(){\r\n    testServer1.start();\r\n    testServer2.start();\r\n    pacFileServer.setPacFileContents(Joiner.on('\\n').join(\"function FindProxyForURL(url, host) {\", \"  if (host.indexOf('example') != -1) {\", \"    return 'PROXY \" + testServer2.getHostAndPort() + \"';\", \"  }\", \"  return 'DIRECT';\", \" }\"));\r\n    pacFileServer.start();\r\n    WebDriver driver = customDriverFactory.createDriver(pacFileServer.getBaseUrl());\r\n    String page1Url = buildPage1Url(testServer1, \"http://www.example.com\" + buildPage2Url());\r\n    String page2Url = buildPage2Url(\"http://www.example.com\", buildPage3Url(testServer1));\r\n    String page3Url = buildPage3Url(testServer1);\r\n    performNavigation(driver, page1Url);\r\n    assertThat(testServer1.getRequests()).isEqualTo(ImmutableList.of(new HttpRequest(page1Url, null), new HttpRequest(page3Url, page2Url)));\r\n    assertThat(testServer2.getRequests()).isEqualTo(ImmutableList.of(new HttpRequest(page2Url, page1Url)));\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.ArrayMath.sigLevelByApproxRand",
	"Comment": "computes the significance level by approximate randomization, using adefault value of 1000 iterations.see documentation for other versionof method.",
	"Method": "double sigLevelByApproxRand(double[] A,double[] B,double sigLevelByApproxRand,double[] A,double[] B,int iterations,double sigLevelByApproxRand,int[] A,int[] B,double sigLevelByApproxRand,int[] A,int[] B,int iterations,double sigLevelByApproxRand,boolean[] A,boolean[] B,double sigLevelByApproxRand,boolean[] A,boolean[] B,int iterations){\r\n    if (A.length == 0)\r\n        throw new IllegalArgumentException(\"Input arrays must not be empty!\");\r\n    if (A.length != B.length)\r\n        throw new IllegalArgumentException(\"Input arrays must have equal length!\");\r\n    if (iterations <= 0)\r\n        throw new IllegalArgumentException(\"Number of iterations must be positive!\");\r\n    double[] X = new double[A.length];\r\n    double[] Y = new double[B.length];\r\n    for (int i = 0; i < A.length; i++) {\r\n        X[i] = (A[i] ? 1.0 : 0.0);\r\n        Y[i] = (B[i] ? 1.0 : 0.0);\r\n    }\r\n    return sigLevelByApproxRand(X, Y, iterations);\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.MapperService.documentMapperWithAutoCreate",
	"Comment": "returns the document mapper created, including a mapping update if thetype has been dynamically created.",
	"Method": "DocumentMapperForType documentMapperWithAutoCreate(String type){\r\n    DocumentMapper mapper = documentMapper(type);\r\n    if (mapper != null) {\r\n        return new DocumentMapperForType(mapper, null);\r\n    }\r\n    mapper = parse(type, null, true);\r\n    return new DocumentMapperForType(mapper, mapper.mapping());\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.ProvisionException.getErrorMessages",
	"Comment": "returns messages for the errors that caused this exception.",
	"Method": "Collection<Message> getErrorMessages(){\r\n    return messages;\r\n}"
}, {
	"Path": "org.elasticsearch.ElasticsearchException.isRegistered",
	"Comment": "returns true iff the given class is a registered for an exception to be read.",
	"Method": "boolean isRegistered(Class<? extends Throwable> exception,Version version){\r\n    ElasticsearchExceptionHandle elasticsearchExceptionHandle = CLASS_TO_ELASTICSEARCH_EXCEPTION_HANDLE.get(exception);\r\n    if (elasticsearchExceptionHandle != null) {\r\n        return version.onOrAfter(elasticsearchExceptionHandle.versionAdded);\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.getLineIterable",
	"Comment": "given a reader, returns the lines from the reader as an iterable.",
	"Method": "Iterable<String> getLineIterable(Reader r,boolean includeEol,Iterable<String> getLineIterable,Reader r,int bufferSize,boolean includeEol){\r\n    if (includeEol) {\r\n        return new EolPreservingLineReaderIterable(r, bufferSize);\r\n    } else {\r\n        return new LineReaderIterable((r instanceof BufferedReader) ? (BufferedReader) r : new BufferedReader(r, bufferSize));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.isSupportedVersion",
	"Comment": "returns true if this index can be supported by the current version of elasticsearch",
	"Method": "boolean isSupportedVersion(IndexMetaData indexMetaData,Version minimumIndexCompatibilityVersion){\r\n    return indexMetaData.getCreationVersion().onOrAfter(minimumIndexCompatibilityVersion);\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoriesService.repository",
	"Comment": "returns registered repositorythis method is called only on the master node",
	"Method": "Repository repository(String repositoryName){\r\n    Repository repository = repositories.get(repositoryName);\r\n    if (repository != null) {\r\n        return repository;\r\n    }\r\n    throw new RepositoryMissingException(repositoryName);\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequest.perFieldAnalyzer",
	"Comment": "override the analyzer used at each field when generating term vectors.",
	"Method": "Map<String, String> perFieldAnalyzer(TermVectorsRequest perFieldAnalyzer,Map<String, String> perFieldAnalyzer){\r\n    this.perFieldAnalyzer = perFieldAnalyzer != null && perFieldAnalyzer.size() != 0 ? new HashMap(perFieldAnalyzer) : null;\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.slurpReader",
	"Comment": "returns all the text from the given reader.closes the reader when done.",
	"Method": "String slurpReader(Reader reader){\r\n    StringBuilder buff = new StringBuilder();\r\n    try (BufferedReader r = new BufferedReader(reader)) {\r\n        char[] chars = new char[SLURP_BUFFER_SIZE];\r\n        while (true) {\r\n            int amountRead = r.read(chars, 0, SLURP_BUFFER_SIZE);\r\n            if (amountRead < 0) {\r\n                break;\r\n            }\r\n            buff.append(chars, 0, amountRead);\r\n        }\r\n    } catch (Exception e) {\r\n        throw new RuntimeIOException(\"slurpReader IO problem\", e);\r\n    }\r\n    return buff.toString();\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.UpdateByQueryRequest.setBatchSize",
	"Comment": "the scroll size to control number of documents processed per batch",
	"Method": "UpdateByQueryRequest setBatchSize(int size){\r\n    getSearchRequest().source().size(size);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.misc.DependencyAnalyzer.transitiveClosure",
	"Comment": "constructs the transitive closure of outgoing dependencies startingfrom the given classes. that is, the returned collection is all theclasses that might be needed in order to use the given classes.if none of the given classes are found, an empty collection is returned.",
	"Method": "Collection<Identifier> transitiveClosure(List<String> startingClassNames){\r\n    Set<Identifier> closure = Generics.newHashSet();\r\n    LinkedList<Identifier> depQueue = new LinkedList();\r\n    addStartingClasses(depQueue, closure, startingClassNames);\r\n    while (!depQueue.isEmpty()) {\r\n        Identifier id = depQueue.removeFirst();\r\n        for (Identifier outgoingDependency : id.outgoingDependencies) {\r\n            if (outgoingDependency.isClass && !closure.contains(outgoingDependency)) {\r\n                if (VERBOSE)\r\n                    log.info(\"Added \" + outgoingDependency + \" due to \" + id);\r\n                depQueue.addLast(outgoingDependency);\r\n                closure.add(outgoingDependency);\r\n            }\r\n        }\r\n    }\r\n    return closure;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.ShardRouting.isRelocationTargetOf",
	"Comment": "returns true if the routing is the relocation target of the given routing",
	"Method": "boolean isRelocationTargetOf(ShardRouting other){\r\n    boolean b = this.allocationId != null && other.allocationId != null && this.state == ShardRoutingState.INITIALIZING && this.allocationId.getId().equals(other.allocationId.getRelocationId());\r\n    assert b == false || other.state == ShardRoutingState.RELOCATING : \"ShardRouting is a relocation target but the source shard state isn't relocating. This [\" + this + \"], other [\" + other + \"]\";\r\n    assert b == false || other.allocationId.getId().equals(this.allocationId.getRelocationId()) : \"ShardRouting is a relocation target but the source id isn't equal to source's allocationId.getRelocationId.\" + \" This [\" + this + \"], other [\" + other + \"]\";\r\n    assert b == false || other.currentNodeId().equals(this.relocatingNodeId) : \"ShardRouting is a relocation target but source current node id isn't equal to target relocating node.\" + \" This [\" + this + \"], other [\" + other + \"]\";\r\n    assert b == false || this.currentNodeId().equals(other.relocatingNodeId) : \"ShardRouting is a relocation target but current node id isn't equal to source relocating node.\" + \" This [\" + this + \"], other [\" + other + \"]\";\r\n    assert b == false || this.shardId.equals(other.shardId) : \"ShardRouting is a relocation target but both indexRoutings are not of the same shard id.\" + \" This [\" + this + \"], other [\" + other + \"]\";\r\n    assert b == false || this.primary == other.primary : \"ShardRouting is a relocation target but primary flag is different.\" + \" This [\" + this + \"], target [\" + other + \"]\";\r\n    return b;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.SplittingGrammarExtractor.mergeTransitions",
	"Comment": "given a tree and the original set of transition probabilitiesfrom one state to the next in the tree, along with a list of theweights in the tree and a count of the mass in each substate atthe current node, this method merges the probabilities asnecessary.the results go into newunarytransitions andnewbinarytransitions.",
	"Method": "void mergeTransitions(Tree parent,IdentityHashMap<Tree, double[][]> oldUnaryTransitions,IdentityHashMap<Tree, double[][][]> oldBinaryTransitions,IdentityHashMap<Tree, double[][]> newUnaryTransitions,IdentityHashMap<Tree, double[][][]> newBinaryTransitions,double[] stateWeights,Map<String, int[]> mergeCorrespondence){\r\n    if (parent.isPreTerminal() || parent.isLeaf()) {\r\n        return;\r\n    }\r\n    if (parent.children().length == 1) {\r\n        double[][] oldTransitions = oldUnaryTransitions.get(parent);\r\n        String parentLabel = parent.label().value();\r\n        int[] parentCorrespondence = mergeCorrespondence.get(parentLabel);\r\n        int parentStates = parentCorrespondence[parentCorrespondence.length - 1] + 1;\r\n        String childLabel = parent.children()[0].label().value();\r\n        int[] childCorrespondence = mergeCorrespondence.get(childLabel);\r\n        int childStates = childCorrespondence[childCorrespondence.length - 1] + 1;\r\n        double[][] newTransitions = new double[parentStates][childStates];\r\n        for (int i = 0; i < parentStates; ++i) {\r\n            for (int j = 0; j < childStates; ++j) {\r\n                newTransitions[i][j] = Double.NEGATIVE_INFINITY;\r\n            }\r\n        }\r\n        newUnaryTransitions.put(parent, newTransitions);\r\n        for (int i = 0; i < oldTransitions.length; ++i) {\r\n            int ti = parentCorrespondence[i];\r\n            for (int j = 0; j < oldTransitions[0].length; ++j) {\r\n                int tj = childCorrespondence[j];\r\n                newTransitions[ti][tj] = SloppyMath.logAdd(newTransitions[ti][tj], oldTransitions[i][j] + stateWeights[i]);\r\n            }\r\n        }\r\n        for (int i = 0; i < parentStates; ++i) {\r\n            double total = Double.NEGATIVE_INFINITY;\r\n            for (int j = 0; j < childStates; ++j) {\r\n                total = SloppyMath.logAdd(total, newTransitions[i][j]);\r\n            }\r\n            if (Double.isInfinite(total)) {\r\n                for (int j = 0; j < childStates; ++j) {\r\n                    newTransitions[i][j] = -Math.log(childStates);\r\n                }\r\n            } else {\r\n                for (int j = 0; j < childStates; ++j) {\r\n                    newTransitions[i][j] -= total;\r\n                }\r\n            }\r\n        }\r\n        double[] childWeights = neginfDoubles(oldTransitions[0].length);\r\n        for (int i = 0; i < oldTransitions.length; ++i) {\r\n            for (int j = 0; j < oldTransitions[0].length; ++j) {\r\n                double weight = oldTransitions[i][j];\r\n                childWeights[j] = SloppyMath.logAdd(childWeights[j], weight + stateWeights[i]);\r\n            }\r\n        }\r\n        mergeTransitions(parent.children()[0], oldUnaryTransitions, oldBinaryTransitions, newUnaryTransitions, newBinaryTransitions, childWeights, mergeCorrespondence);\r\n    } else {\r\n        double[][][] oldTransitions = oldBinaryTransitions.get(parent);\r\n        String parentLabel = parent.label().value();\r\n        int[] parentCorrespondence = mergeCorrespondence.get(parentLabel);\r\n        int parentStates = parentCorrespondence[parentCorrespondence.length - 1] + 1;\r\n        String leftLabel = parent.children()[0].label().value();\r\n        int[] leftCorrespondence = mergeCorrespondence.get(leftLabel);\r\n        int leftStates = leftCorrespondence[leftCorrespondence.length - 1] + 1;\r\n        String rightLabel = parent.children()[1].label().value();\r\n        int[] rightCorrespondence = mergeCorrespondence.get(rightLabel);\r\n        int rightStates = rightCorrespondence[rightCorrespondence.length - 1] + 1;\r\n        double[][][] newTransitions = new double[parentStates][leftStates][rightStates];\r\n        for (int i = 0; i < parentStates; ++i) {\r\n            for (int j = 0; j < leftStates; ++j) {\r\n                for (int k = 0; k < rightStates; ++k) {\r\n                    newTransitions[i][j][k] = Double.NEGATIVE_INFINITY;\r\n                }\r\n            }\r\n        }\r\n        newBinaryTransitions.put(parent, newTransitions);\r\n        for (int i = 0; i < oldTransitions.length; ++i) {\r\n            int ti = parentCorrespondence[i];\r\n            for (int j = 0; j < oldTransitions[0].length; ++j) {\r\n                int tj = leftCorrespondence[j];\r\n                for (int k = 0; k < oldTransitions[0][0].length; ++k) {\r\n                    int tk = rightCorrespondence[k];\r\n                    newTransitions[ti][tj][tk] = SloppyMath.logAdd(newTransitions[ti][tj][tk], oldTransitions[i][j][k] + stateWeights[i]);\r\n                }\r\n            }\r\n        }\r\n        for (int i = 0; i < parentStates; ++i) {\r\n            double total = Double.NEGATIVE_INFINITY;\r\n            for (int j = 0; j < leftStates; ++j) {\r\n                for (int k = 0; k < rightStates; ++k) {\r\n                    total = SloppyMath.logAdd(total, newTransitions[i][j][k]);\r\n                }\r\n            }\r\n            if (Double.isInfinite(total)) {\r\n                for (int j = 0; j < leftStates; ++j) {\r\n                    for (int k = 0; k < rightStates; ++k) {\r\n                        newTransitions[i][j][k] = -Math.log(leftStates * rightStates);\r\n                    }\r\n                }\r\n            } else {\r\n                for (int j = 0; j < leftStates; ++j) {\r\n                    for (int k = 0; k < rightStates; ++k) {\r\n                        newTransitions[i][j][k] -= total;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        double[] leftWeights = neginfDoubles(oldTransitions[0].length);\r\n        double[] rightWeights = neginfDoubles(oldTransitions[0][0].length);\r\n        for (int i = 0; i < oldTransitions.length; ++i) {\r\n            for (int j = 0; j < oldTransitions[0].length; ++j) {\r\n                for (int k = 0; k < oldTransitions[0][0].length; ++k) {\r\n                    double weight = oldTransitions[i][j][k];\r\n                    leftWeights[j] = SloppyMath.logAdd(leftWeights[j], weight + stateWeights[i]);\r\n                    rightWeights[k] = SloppyMath.logAdd(rightWeights[k], weight + stateWeights[i]);\r\n                }\r\n            }\r\n        }\r\n        mergeTransitions(parent.children()[0], oldUnaryTransitions, oldBinaryTransitions, newUnaryTransitions, newBinaryTransitions, leftWeights, mergeCorrespondence);\r\n        mergeTransitions(parent.children()[1], oldUnaryTransitions, oldBinaryTransitions, newUnaryTransitions, newBinaryTransitions, rightWeights, mergeCorrespondence);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.ingest.GetPipelineResponse.pipelines",
	"Comment": "get the list of pipelines that were a part of this response.the pipeline id can be obtained using getid on the pipelineconfiguration object.",
	"Method": "List<PipelineConfiguration> pipelines(){\r\n    return Collections.unmodifiableList(pipelines);\r\n}"
}, {
	"Path": "org.elasticsearch.client.transport.TransportClient.connectedNodes",
	"Comment": "returns the current connected transport nodes that this client will use.the nodes include all the nodes that are currently alive based on the transportaddresses provided.",
	"Method": "List<DiscoveryNode> connectedNodes(){\r\n    return nodesService.connectedNodes();\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.AbstractScopedSettings.isDynamicSetting",
	"Comment": "returns true if the setting for the given key is dynamically updateable. otherwise false.",
	"Method": "boolean isDynamicSetting(String key){\r\n    final Setting<?> setting = get(key);\r\n    return setting != null && setting.isDynamic();\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.parser.SimpleCharStream.adjustBeginLineColumn",
	"Comment": "method to adjust line and column numbers for the start of a token.",
	"Method": "void adjustBeginLineColumn(int newLine,int newCol){\r\n    int start = tokenBegin;\r\n    int len;\r\n    if (bufpos >= tokenBegin) {\r\n        len = bufpos - tokenBegin + inBuf + 1;\r\n    } else {\r\n        len = bufsize - tokenBegin + bufpos + 1 + inBuf;\r\n    }\r\n    int i = 0, j = 0, k = 0;\r\n    int nextColDiff = 0, columnDiff = 0;\r\n    while (i < len && bufline[j = start % bufsize] == bufline[k = ++start % bufsize]) {\r\n        bufline[j] = newLine;\r\n        nextColDiff = columnDiff + bufcolumn[k] - bufcolumn[j];\r\n        bufcolumn[j] = newCol + columnDiff;\r\n        columnDiff = nextColDiff;\r\n        i++;\r\n    }\r\n    if (i < len) {\r\n        bufline[j] = newLine++;\r\n        bufcolumn[j] = newCol + columnDiff;\r\n        while (i++ < len) {\r\n            if (bufline[j = start % bufsize] != bufline[++start % bufsize])\r\n                bufline[j] = newLine++;\r\n            else\r\n                bufline[j] = newLine;\r\n        }\r\n    }\r\n    line = bufline[j];\r\n    column = bufcolumn[j];\r\n}"
}, {
	"Path": "org.elasticsearch.common.bytes.BytesReference.toByteBuffers",
	"Comment": "returns an array of byte buffers from the given bytesreference.",
	"Method": "ByteBuffer[] toByteBuffers(BytesReference reference){\r\n    BytesRefIterator byteRefIterator = reference.iterator();\r\n    BytesRef r;\r\n    try {\r\n        ArrayList<ByteBuffer> buffers = new ArrayList();\r\n        while ((r = byteRefIterator.next()) != null) {\r\n            buffers.add(ByteBuffer.wrap(r.bytes, r.offset, r.length));\r\n        }\r\n        return buffers.toArray(new ByteBuffer[buffers.size()]);\r\n    } catch (IOException e) {\r\n        throw new AssertionError(\"won't happen\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.BigIntArray.estimateRamBytes",
	"Comment": "estimates the number of bytes that would be consumed by an array of the given size.",
	"Method": "long estimateRamBytes(long size){\r\n    return ESTIMATOR.ramBytesEstimated(size);\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.Store.readSegmentsInfo",
	"Comment": "returns the segments info for the given commit or for the latest commit if the given commit is null",
	"Method": "SegmentInfos readSegmentsInfo(IndexCommit commit,Directory directory){\r\n    assert commit == null || commit.getDirectory() == directory;\r\n    try {\r\n        return commit == null ? Lucene.readSegmentInfos(directory) : Lucene.readSegmentInfos(commit);\r\n    } catch (EOFException eof) {\r\n        throw new CorruptIndexException(\"Read past EOF while reading segment infos\", \"commit(\" + commit + \")\", eof);\r\n    } catch (IOException exception) {\r\n        throw exception;\r\n    } catch (Exception ex) {\r\n        throw new CorruptIndexException(\"Hit unexpected exception while reading segment infos\", \"commit(\" + commit + \")\", ex);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.create.CreateIndexRequest.alias",
	"Comment": "adds an alias that will be associated with the index when it gets created",
	"Method": "CreateIndexRequest alias(Alias alias){\r\n    this.aliases.add(alias);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.wrapperQuery",
	"Comment": "a query builder which allows building a query thanks to a json string or binary data.",
	"Method": "WrapperQueryBuilder wrapperQuery(String source,WrapperQueryBuilder wrapperQuery,BytesReference source,WrapperQueryBuilder wrapperQuery,byte[] source){\r\n    return new WrapperQueryBuilder(source);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationAction.indexBlockLevel",
	"Comment": "index level block to check before request execution. returning null means that no blocks need to be checked.",
	"Method": "ClusterBlockLevel indexBlockLevel(){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MatchQueryBuilder.analyzer",
	"Comment": "explicitly set the analyzer to use. defaults to use explicit mapping config for the field, or, if notset, the default search analyzer.",
	"Method": "MatchQueryBuilder analyzer(String analyzer,String analyzer){\r\n    return this.analyzer;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.AllocationService.firstListElementsToCommaDelimitedString",
	"Comment": "internal helper to cap the number of elements in a potentially long list for logging.",
	"Method": "String firstListElementsToCommaDelimitedString(List<T> elements,Function<T, String> formatter){\r\n    final int maxNumberOfElements = 10;\r\n    return elements.stream().limit(maxNumberOfElements).map(formatter).collect(Collectors.joining(\", \"));\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.pascal.ISODateInstance.getRangeDates",
	"Comment": "attempts to find the two sides of a range in the given string.uses rangeindicators to find possible matches.",
	"Method": "Pair<String, String> getRangeDates(String inputDate){\r\n    for (String curIndicator : rangeIndicators) {\r\n        String[] dates = inputDate.split(curIndicator);\r\n        if (dates.length == 2) {\r\n            return new Pair(dates[0], dates[1]);\r\n        }\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.IndexedWord.hashCode",
	"Comment": "this hashcode uses only the docid, sentenceindex, and index.see compareto for more info.",
	"Method": "int hashCode(){\r\n    if (cachedHashCode != 0) {\r\n        return cachedHashCode;\r\n    }\r\n    boolean sensible = false;\r\n    int result = 0;\r\n    if (get(CoreAnnotations.DocIDAnnotation.class) != null) {\r\n        result = get(CoreAnnotations.DocIDAnnotation.class).hashCode();\r\n        sensible = true;\r\n    }\r\n    if (containsKey(CoreAnnotations.SentenceIndexAnnotation.class)) {\r\n        result = 29 * result + get(CoreAnnotations.SentenceIndexAnnotation.class).hashCode();\r\n        sensible = true;\r\n    }\r\n    if (containsKey(CoreAnnotations.IndexAnnotation.class)) {\r\n        result = 29 * result + get(CoreAnnotations.IndexAnnotation.class).hashCode();\r\n        sensible = true;\r\n    }\r\n    if (!sensible) {\r\n        log.info(\"WARNING!!!  You have hashed an IndexedWord with no docID, sentIndex or wordIndex. You will almost certainly lose\");\r\n    }\r\n    cachedHashCode = result;\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.DeleteByQueryRequest.getDocTypes",
	"Comment": "gets the document types on which this request would be executed. returns an empty array if alltypes are to be processed.",
	"Method": "String[] getDocTypes(){\r\n    if (getSearchRequest().types() != null) {\r\n        return getSearchRequest().types();\r\n    } else {\r\n        return new String[0];\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.DestructiveOperations.failDestructive",
	"Comment": "fail if there is wildcard usage in indices and the named is required for destructive operations.",
	"Method": "void failDestructive(String[] aliasesOrIndices){\r\n    if (!destructiveRequiresName) {\r\n        return;\r\n    }\r\n    if (aliasesOrIndices == null || aliasesOrIndices.length == 0) {\r\n        throw new IllegalArgumentException(\"Wildcard expressions or all indices are not allowed\");\r\n    } else if (aliasesOrIndices.length == 1) {\r\n        if (hasWildcardUsage(aliasesOrIndices[0])) {\r\n            throw new IllegalArgumentException(\"Wildcard expressions or all indices are not allowed\");\r\n        }\r\n    } else {\r\n        for (String aliasesOrIndex : aliasesOrIndices) {\r\n            if (hasWildcardUsage(aliasesOrIndex)) {\r\n                throw new IllegalArgumentException(\"Wildcard expressions or all indices are not allowed\");\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEvent.nodesRemoved",
	"Comment": "returns true iff nodes have been removed from the cluster since the last cluster state.",
	"Method": "boolean nodesRemoved(){\r\n    return nodesDelta.removed();\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.UpdateByQueryRequest.getDocTypes",
	"Comment": "gets the document types on which this request would be executed. returns an empty array if alltypes are to be processed.",
	"Method": "String[] getDocTypes(){\r\n    if (getSearchRequest().types() != null) {\r\n        return getSearchRequest().types();\r\n    } else {\r\n        return new String[0];\r\n    }\r\n}"
}, {
	"Path": "org.openqa.selenium.remote.server.log.PerSessionLogHandler.removeSessionLogs",
	"Comment": "removes session logs for the given session id.nb! if the handler has been configured to capture logs on quit no logs will be removed.",
	"Method": "void removeSessionLogs(SessionId sessionId){\r\n    if (storeLogsOnSessionQuit) {\r\n        return;\r\n    }\r\n    ThreadKey threadId = sessionToThreadMap.get(sessionId);\r\n    SessionId sessionIdForThread = threadToSessionMap.get(threadId);\r\n    if (threadId != null && sessionIdForThread != null && sessionIdForThread.equals(sessionId)) {\r\n        threadToSessionMap.remove(threadId);\r\n        sessionToThreadMap.remove(sessionId);\r\n    }\r\n    perSessionRecords.remove(sessionId);\r\n    logFileRepository.removeLogFile(sessionId);\r\n}"
}, {
	"Path": "edu.stanford.nlp.maxent.iis.LambdaSolve.divide",
	"Comment": "given a numerator and denominator in log form, this calculatesthe conditional model probabilities.",
	"Method": "double divide(double first,double second){\r\n    return Math.exp(first - second);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder.setAliases",
	"Comment": "sets the aliases that will be associated with the index when it gets created",
	"Method": "CreateIndexRequestBuilder setAliases(Map<String, ?> source,CreateIndexRequestBuilder setAliases,String source,CreateIndexRequestBuilder setAliases,XContentBuilder source,CreateIndexRequestBuilder setAliases,BytesReference source){\r\n    request.aliases(source);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.GatewayMetaState.resolveStatesToBeWritten",
	"Comment": "loads the current meta state for each index in the new cluster state and checks if it has to be persisted.each index state that should be written to disk will be returned. this is only run for data only nodes.it will return only the states for indices that actually have a shard allocated on the current node.",
	"Method": "Iterable<GatewayMetaState.IndexMetaWriteInfo> resolveStatesToBeWritten(Set<Index> previouslyWrittenIndices,Set<Index> potentiallyUnwrittenIndices,MetaData previousMetaData,MetaData newMetaData){\r\n    List<GatewayMetaState.IndexMetaWriteInfo> indicesToWrite = new ArrayList();\r\n    for (Index index : potentiallyUnwrittenIndices) {\r\n        IndexMetaData newIndexMetaData = newMetaData.getIndexSafe(index);\r\n        IndexMetaData previousIndexMetaData = previousMetaData == null ? null : previousMetaData.index(index);\r\n        String writeReason = null;\r\n        if (previouslyWrittenIndices.contains(index) == false || previousIndexMetaData == null) {\r\n            writeReason = \"freshly created\";\r\n        } else if (previousIndexMetaData.getVersion() != newIndexMetaData.getVersion()) {\r\n            writeReason = \"version changed from [\" + previousIndexMetaData.getVersion() + \"] to [\" + newIndexMetaData.getVersion() + \"]\";\r\n        }\r\n        if (writeReason != null) {\r\n            indicesToWrite.add(new GatewayMetaState.IndexMetaWriteInfo(newIndexMetaData, previousIndexMetaData, writeReason));\r\n        }\r\n    }\r\n    return indicesToWrite;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder.boostMode",
	"Comment": "returns the boost mode, meaning how the combined result of score functions will influence the final score together with the sub queryscore.",
	"Method": "FunctionScoreQueryBuilder boostMode(CombineFunction combineFunction,CombineFunction boostMode){\r\n    return this.boostMode;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.IndexingMemoryController.indexingBufferSize",
	"Comment": "returns the current budget for the total amount of indexing buffers ofactive shards on this node",
	"Method": "ByteSizeValue indexingBufferSize(){\r\n    return indexingBuffer;\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.french.process.FrenchTokenizer.factory",
	"Comment": "returns a factory for frenchtokenizer. this is needed for creation by reflection.",
	"Method": "TokenizerFactory<CoreLabel> factory(TokenizerFactory<T> factory,LexedTokenFactory<T> factory,String options){\r\n    return new FrenchTokenizerFactory(factory, options);\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVectorNamespace.setDenseFeature",
	"Comment": "this adds a dense feature to a vector, setting the appropriate component of the given vector to the passed invalue.",
	"Method": "void setDenseFeature(ConcatVector vector,String featureName,double[] value){\r\n    vector.setDenseComponent(ensureFeature(featureName), value);\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.SimpleTensor.elementMult",
	"Comment": "performs elementwise multiplication on the tensors.the originalobjects are unaffected.",
	"Method": "SimpleTensor elementMult(SimpleTensor other){\r\n    if (other.numRows != numRows || other.numCols != numCols || other.numSlices != numSlices) {\r\n        throw new IllegalArgumentException(\"Sizes of tensors do not match.  Our size: \" + numRows + \",\" + numCols + \",\" + numSlices + \"; other size \" + other.numRows + \",\" + other.numCols + \",\" + other.numSlices);\r\n    }\r\n    SimpleTensor result = new SimpleTensor(numRows, numCols, numSlices);\r\n    for (int i = 0; i < numSlices; ++i) {\r\n        result.slices[i] = slices[i].elementMult(other.slices[i]);\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.geoShapeQuery",
	"Comment": "a filter based on the relationship of a shape and indexed shapes",
	"Method": "GeoShapeQueryBuilder geoShapeQuery(String name,ShapeBuilder shape,GeoShapeQueryBuilder geoShapeQuery,String name,String indexedShapeId,String indexedShapeType){\r\n    return new GeoShapeQueryBuilder(name, indexedShapeId, indexedShapeType);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.readHistoryOperations",
	"Comment": "creates a new history snapshot for reading operations since the provided seqno.the returned snapshot can be retrieved from either lucene index or translog files.",
	"Method": "Translog.Snapshot readHistoryOperations(String source,MapperService mapperService,long startingSeqNo){\r\n    if (engineConfig.getIndexSettings().isSoftDeleteEnabled()) {\r\n        return newChangesSnapshot(source, mapperService, Math.max(0, startingSeqNo), Long.MAX_VALUE, false);\r\n    } else {\r\n        return getTranslog().newSnapshotFromMinSeqNo(startingSeqNo);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.LocalCheckpointTracker.updateCheckpoint",
	"Comment": "moves the checkpoint to the last consecutively processed sequence number. this method assumes that the sequence number following thecurrent checkpoint is processed.",
	"Method": "void updateCheckpoint(){\r\n    assert Thread.holdsLock(this);\r\n    assert getBitSetForSeqNo(checkpoint + 1).get(seqNoToBitSetOffset(checkpoint + 1)) : \"updateCheckpoint is called but the bit following the checkpoint is not set\";\r\n    try {\r\n        long bitSetKey = getBitSetKey(checkpoint);\r\n        CountedBitSet current = processedSeqNo.get(bitSetKey);\r\n        if (current == null) {\r\n            assert checkpoint % BIT_SET_SIZE == BIT_SET_SIZE - 1;\r\n            current = processedSeqNo.get(++bitSetKey);\r\n        }\r\n        do {\r\n            checkpoint++;\r\n            if (checkpoint == lastSeqNoInBitSet(bitSetKey)) {\r\n                assert current != null;\r\n                final CountedBitSet removed = processedSeqNo.remove(bitSetKey);\r\n                assert removed == current;\r\n                current = processedSeqNo.get(++bitSetKey);\r\n            }\r\n        } while (current != null && current.get(seqNoToBitSetOffset(checkpoint + 1)));\r\n    } finally {\r\n        this.notifyAll();\r\n    }\r\n}"
}, {
	"Path": "org.apache.harmony.tests.java.util.HashMapTest.setUp",
	"Comment": "sets up the fixture, for example, open a network connection. this methodis called before a test is executed.",
	"Method": "void setUp(){\r\n    objArray = new Object[hmSize];\r\n    objArray2 = new Object[hmSize];\r\n    for (int i = 0; i < objArray.length; i++) {\r\n        objArray[i] = new Integer(i);\r\n        objArray2[i] = objArray[i].toString();\r\n    }\r\n    hm = new HashMap();\r\n    for (int i = 0; i < objArray.length; i++) hm.put(objArray2[i], objArray[i]);\r\n    hm.put(\"test\", null);\r\n    hm.put(null, \"test\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.FactoredLexicon.train",
	"Comment": "this method should populate wordindex, tagindex, and morphindex.",
	"Method": "void train(Collection<Tree> trees,Collection<Tree> rawTrees){\r\n    double weight = 1.0;\r\n    uwModelTrainer.train(trees, weight);\r\n    final double numTrees = trees.size();\r\n    Iterator<Tree> rawTreesItr = rawTrees == null ? null : rawTrees.iterator();\r\n    Iterator<Tree> treeItr = trees.iterator();\r\n    int treeId = 0;\r\n    while (treeItr.hasNext()) {\r\n        Tree tree = treeItr.next();\r\n        List<Label> yield = rawTrees == null ? tree.yield() : rawTreesItr.next().yield();\r\n        List<Label> pretermYield = tree.preTerminalYield();\r\n        int yieldLen = yield.size();\r\n        for (int i = 0; i < yieldLen; ++i) {\r\n            String word = yield.get(i).value();\r\n            int wordId = wordIndex.addToIndex(word);\r\n            String tag = pretermYield.get(i).value();\r\n            int tagId = tagIndex.addToIndex(tag);\r\n            String featureStr = ((CoreLabel) yield.get(i)).originalText();\r\n            Pair<String, String> lemmaMorph = MorphoFeatureSpecification.splitMorphString(word, featureStr);\r\n            String lemma = lemmaMorph.first();\r\n            int lemmaId = wordIndex.addToIndex(lemma);\r\n            String richMorphTag = lemmaMorph.second();\r\n            String reducedMorphTag = morphoSpec.strToFeatures(richMorphTag).toString().trim();\r\n            reducedMorphTag = reducedMorphTag.isEmpty() ? NO_MORPH_ANALYSIS : reducedMorphTag;\r\n            int morphId = morphIndex.addToIndex(reducedMorphTag);\r\n            wordTag.incrementCount(wordId, tagId);\r\n            lemmaTag.incrementCount(lemmaId, tagId);\r\n            morphTag.incrementCount(morphId, tagId);\r\n            tagCounter.incrementCount(tagId);\r\n            if (treeId > op.trainOptions.fractionBeforeUnseenCounting * numTrees) {\r\n                if (!wordTag.firstKeySet().contains(wordId) || wordTag.getCounter(wordId).totalCount() < 2) {\r\n                    wordTagUnseen.incrementCount(tagId);\r\n                }\r\n                if (!lemmaTag.firstKeySet().contains(lemmaId) || lemmaTag.getCounter(lemmaId).totalCount() < 2) {\r\n                    lemmaTagUnseen.incrementCount(tagId);\r\n                }\r\n                if (!morphTag.firstKeySet().contains(morphId) || morphTag.getCounter(morphId).totalCount() < 2) {\r\n                    morphTagUnseen.incrementCount(tagId);\r\n                }\r\n            }\r\n        }\r\n        ++treeId;\r\n        if (DEBUG && (treeId % 100) == 0) {\r\n            System.err.printf(\"[%d]\", treeId);\r\n        }\r\n        if (DEBUG && (treeId % 10000) == 0) {\r\n            log.info();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequestBuilder.setIndexSettings",
	"Comment": "sets index settings that should be added or replaced during restore",
	"Method": "RestoreSnapshotRequestBuilder setIndexSettings(Settings settings,RestoreSnapshotRequestBuilder setIndexSettings,Settings.Builder settings,RestoreSnapshotRequestBuilder setIndexSettings,String source,XContentType xContentType,RestoreSnapshotRequestBuilder setIndexSettings,Map<String, Object> source){\r\n    request.indexSettings(source);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequestBuilder.addScriptField",
	"Comment": "adds a script based field to load and return. the field does not have to be stored,but its recommended to use non analyzed or numeric fields.",
	"Method": "SearchRequestBuilder addScriptField(String name,Script script){\r\n    sourceBuilder().scriptField(name, script);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.stats.CommonStatsFlags.groups",
	"Comment": "sets specific search group stats to retrieve the stats for. mainly affects searchwhen enabled.",
	"Method": "CommonStatsFlags groups(String groups,String[] groups){\r\n    return this.groups;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoDistanceQueryBuilder.distance",
	"Comment": "sets the distance from the center using the default distance unit.",
	"Method": "GeoDistanceQueryBuilder distance(String distance,GeoDistanceQueryBuilder distance,String distance,DistanceUnit unit,GeoDistanceQueryBuilder distance,double distance,DistanceUnit unit,double distance){\r\n    return distance;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.BinaryGrammar.readData",
	"Comment": "populates data in this binarygrammar from the character streamgiven by the reader r.",
	"Method": "void readData(BufferedReader in){\r\n    String line;\r\n    int lineNum = 1;\r\n    line = in.readLine();\r\n    while (line != null && line.length() > 0) {\r\n        try {\r\n            addRule(new BinaryRule(line, index));\r\n        } catch (Exception e) {\r\n            throw new IOException(\"Error on line \" + lineNum);\r\n        }\r\n        lineNum++;\r\n        line = in.readLine();\r\n    }\r\n    splitRules();\r\n}"
}, {
	"Path": "android.os.AsyncTask.get",
	"Comment": "waits if necessary for at most the given time for the computationto complete, and then retrieves its result.",
	"Method": "Result get(Result get,long timeout,TimeUnit unit){\r\n    return mFuture.get(timeout, unit);\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.spi.Dependency.get",
	"Comment": "returns a new dependency that is not attached to an injection point. the returned dependency isnullable.",
	"Method": "Dependency<T> get(Key<T> key){\r\n    return new Dependency(null, key, true, -1);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterStateUpdateTask.execute",
	"Comment": "update the cluster state based on the current state. return the same instance if no stateshould be changed.",
	"Method": "ClusterTasksResult<ClusterStateUpdateTask> execute(ClusterState currentState,List<ClusterStateUpdateTask> tasks,ClusterState execute,ClusterState currentState){\r\n    ClusterState result = execute(currentState);\r\n    return ClusterTasksResult.<ClusterStateUpdateTask>builder().successes(tasks).build(result);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.settings.get.GetSettingsResponse.getSetting",
	"Comment": "returns the string value for the specified index and setting.if the includedefaultsflag was not set or set to false on the getsettingsrequest, this method will onlyreturn a value where the setting was explicitly set on the index.if the includedefaultsflag was set to true on the getsettingsrequest, this method will fall back to return the defaultvalue if the setting was not explicitly set.",
	"Method": "String getSetting(String index,String setting){\r\n    Settings settings = indexToSettings.get(index);\r\n    if (setting != null) {\r\n        if (settings != null && settings.hasValue(setting)) {\r\n            return settings.get(setting);\r\n        } else {\r\n            Settings defaultSettings = indexToDefaultSettings.get(index);\r\n            if (defaultSettings != null) {\r\n                return defaultSettings.get(setting);\r\n            } else {\r\n                return null;\r\n            }\r\n        }\r\n    } else {\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.ShardRouting.getTargetRelocatingShard",
	"Comment": "returns a shard routing representing the target shard.the target shard routing will be the initializing state and have relocatingnodeid set to thesource node.",
	"Method": "ShardRouting getTargetRelocatingShard(){\r\n    assert relocating();\r\n    return targetRelocatingShard;\r\n}"
}, {
	"Path": "org.elasticsearch.common.collect.ImmutableOpenIntMap.containsKey",
	"Comment": "returns true if this container has an association to a value forthe given key.",
	"Method": "boolean containsKey(int key,boolean containsKey,int key){\r\n    return map.containsKey(key);\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.completeRelocationHandoff",
	"Comment": "marks a relocation handoff attempt as successful. moves the tracker into replica mode.",
	"Method": "void completeRelocationHandoff(){\r\n    assert invariant();\r\n    assert primaryMode;\r\n    assert handoffInProgress;\r\n    assert relocated == false;\r\n    primaryMode = false;\r\n    handoffInProgress = false;\r\n    relocated = true;\r\n    checkpoints.entrySet().stream().forEach(e -> {\r\n        final CheckpointState cps = e.getValue();\r\n        if (cps.localCheckpoint != SequenceNumbers.UNASSIGNED_SEQ_NO && cps.localCheckpoint != SequenceNumbers.PRE_60_NODE_CHECKPOINT) {\r\n            cps.localCheckpoint = SequenceNumbers.UNASSIGNED_SEQ_NO;\r\n        }\r\n        if (e.getKey().equals(shardAllocationId) == false) {\r\n            if (cps.globalCheckpoint != SequenceNumbers.UNASSIGNED_SEQ_NO && cps.globalCheckpoint != SequenceNumbers.PRE_60_NODE_CHECKPOINT) {\r\n                cps.globalCheckpoint = SequenceNumbers.UNASSIGNED_SEQ_NO;\r\n            }\r\n        }\r\n    });\r\n    assert invariant();\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.WriteRequestBuilder.setRefreshPolicy",
	"Comment": "parse the refresh policy from a string, only modifying it if the string is non null. convenient to use with request parsing.",
	"Method": "B setRefreshPolicy(RefreshPolicy refreshPolicy,B setRefreshPolicy,String refreshPolicy){\r\n    request().setRefreshPolicy(refreshPolicy);\r\n    return (B) this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.XMoreLikeThis.setMaxWordLen",
	"Comment": "sets the maximum word length above which words will be ignored.",
	"Method": "void setMaxWordLen(int maxWordLen){\r\n    this.maxWordLen = maxWordLen;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest.groups",
	"Comment": "sets specific search group stats to retrieve the stats for. mainly affects searchwhen enabled.",
	"Method": "IndicesStatsRequest groups(String groups,String[] groups){\r\n    return this.flags.groups();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.MoveDecision.cannotRemain",
	"Comment": "creates a move decision for the shard not being allowed to remain on its current node.",
	"Method": "MoveDecision cannotRemain(Decision canRemainDecision,AllocationDecision allocationDecision,DiscoveryNode assignedNode,List<NodeAllocationResult> nodeDecisions){\r\n    assert canRemainDecision != null;\r\n    assert canRemainDecision.type() != Type.YES : \"create decision with MoveDecision#stay instead\";\r\n    if (nodeDecisions == null && allocationDecision == AllocationDecision.NO) {\r\n        return CACHED_CANNOT_MOVE_DECISION;\r\n    } else {\r\n        assert ((assignedNode == null) == (allocationDecision != AllocationDecision.YES));\r\n        return new MoveDecision(canRemainDecision, null, allocationDecision, assignedNode, nodeDecisions, 0);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.getMaxRefreshListeners",
	"Comment": "the maximum number of refresh listeners allows on this shard.",
	"Method": "int getMaxRefreshListeners(){\r\n    return maxRefreshListeners;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.rollover.TransportRolloverAction.checkNoDuplicatedAliasInIndexTemplate",
	"Comment": "if the newly created index matches with an index template whose aliases contains the rollover alias,the rollover alias will point to multiple indices. this causes indexing requests to be rejected.to avoid this, we make sure that there is no duplicated alias in index templates before creating a new index.",
	"Method": "void checkNoDuplicatedAliasInIndexTemplate(MetaData metaData,String rolloverIndexName,String rolloverRequestAlias){\r\n    final List<IndexTemplateMetaData> matchedTemplates = MetaDataIndexTemplateService.findTemplates(metaData, rolloverIndexName);\r\n    for (IndexTemplateMetaData template : matchedTemplates) {\r\n        if (template.aliases().containsKey(rolloverRequestAlias)) {\r\n            throw new IllegalArgumentException(String.format(Locale.ROOT, \"Rollover alias [%s] can point to multiple indices, found duplicated alias [%s] in index template [%s]\", rolloverRequestAlias, template.aliases().keys(), template.name()));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.IngestDocument.getFieldValueAsBytes",
	"Comment": "returns the value contained in the document for the provided path as a byte array.if the path value is a string, a base64 decode operation will happen.if the path value is a byte array, it is just returned",
	"Method": "byte[] getFieldValueAsBytes(String path,byte[] getFieldValueAsBytes,String path,boolean ignoreMissing){\r\n    Object object = getFieldValue(path, Object.class, ignoreMissing);\r\n    if (object == null) {\r\n        return null;\r\n    } else if (object instanceof byte[]) {\r\n        return (byte[]) object;\r\n    } else if (object instanceof String) {\r\n        return Base64.getDecoder().decode(object.toString());\r\n    } else {\r\n        throw new IllegalArgumentException(\"Content field [\" + path + \"] of unknown type [\" + object.getClass().getName() + \"], must be string or byte array\");\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.demo.ParserDemo.demoAPI",
	"Comment": "demoapi demonstrates other ways of calling the parser withalready tokenized text, or in some cases, raw text that needs tobe tokenized as a single sentence.output is handled with atreeprint object.note that the options used when creating thetreeprint can determine what results to print out.once again,one can capture the output by passing a printwriter totreeprint.printtree. this code is for english.",
	"Method": "void demoAPI(LexicalizedParser lp){\r\n    String[] sent = { \"This\", \"is\", \"an\", \"easy\", \"sentence\", \".\" };\r\n    List<CoreLabel> rawWords = SentenceUtils.toCoreLabelList(sent);\r\n    Tree parse = lp.apply(rawWords);\r\n    parse.pennPrint();\r\n    System.out.println();\r\n    String sent2 = \"This is another sentence.\";\r\n    TokenizerFactory<CoreLabel> tokenizerFactory = PTBTokenizer.factory(new CoreLabelTokenFactory(), \"\");\r\n    Tokenizer<CoreLabel> tok = tokenizerFactory.getTokenizer(new StringReader(sent2));\r\n    List<CoreLabel> rawWords2 = tok.tokenize();\r\n    parse = lp.apply(rawWords2);\r\n    TreebankLanguagePack tlp = lp.treebankLanguagePack();\r\n    GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();\r\n    GrammaticalStructure gs = gsf.newGrammaticalStructure(parse);\r\n    List<TypedDependency> tdl = gs.typedDependenciesCCprocessed();\r\n    System.out.println(tdl);\r\n    System.out.println();\r\n    TreePrint tp = new TreePrint(\"penn,typedDependenciesCollapsed\");\r\n    tp.printTree(parse);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.block.ClusterBlock.retryable",
	"Comment": "should operations get into retry state if this block is present.",
	"Method": "boolean retryable(){\r\n    return this.retryable;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.TregexPoweredTreebankParserParams.removeFeature",
	"Comment": "disable a feature. if the feature was never enabled, this methodreturns without error.",
	"Method": "void removeFeature(String featureName){\r\n    features.remove(featureName);\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.getTranslogUUID",
	"Comment": "returns the translog uuid used to associate a lucene index with a translog.",
	"Method": "String getTranslogUUID(){\r\n    return translogUUID;\r\n}"
}, {
	"Path": "org.apache.harmony.tests.javax.xml.parsers.SAXParserTestSupport.readFile",
	"Comment": "initialize the saxparsertest reference by filling in the data from thefile passed to the method. this will be the reference to compareagainst with the output of the parser.",
	"Method": "HashMap<String, String> readFile(String fileName){\r\n    HashMap<String, String> storage = new HashMap<String, String>();\r\n    try {\r\n        InputStream is = new FileInputStream(fileName);\r\n        int c = is.read();\r\n        StringBuffer str = new StringBuffer();\r\n        int i = 0;\r\n        while (c != -1) {\r\n            if ((char) c == SEPARATOR_DATA) {\r\n                if (i < KEYS.length) {\r\n                    storage.put(KEYS[i], str.toString());\r\n                    str.setLength(0);\r\n                    i++;\r\n                }\r\n            } else {\r\n                str.append((char) c);\r\n            }\r\n            try {\r\n                c = is.read();\r\n            } catch (Exception e) {\r\n                c = -1;\r\n            }\r\n        }\r\n        try {\r\n            is.close();\r\n        } catch (IOException e) {\r\n        }\r\n    } catch (IOException ioe) {\r\n        System.out.println(\"IOException during processing the file: \" + fileName);\r\n    }\r\n    return storage;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.MultiSearchRequestBuilder.add",
	"Comment": "add a search request to execute. note, the order is important, the search response will be returned in thesame order as the search requests.",
	"Method": "MultiSearchRequestBuilder add(SearchRequest request,MultiSearchRequestBuilder add,SearchRequestBuilder request){\r\n    if (request.request().indicesOptions() == SearchRequest.DEFAULT_INDICES_OPTIONS && request().indicesOptions() != SearchRequest.DEFAULT_INDICES_OPTIONS) {\r\n        request.request().indicesOptions(request().indicesOptions());\r\n    }\r\n    super.request.add(request);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDocument.setLabels",
	"Comment": "removes all currently assigned labels for this document then adds allof the given labels.",
	"Method": "void setLabels(Collection<L> labels){\r\n    this.labels.clear();\r\n    if (labels != null) {\r\n        this.labels.addAll(labels);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.ParseContext.createNestedContext",
	"Comment": "return a new context that will be used within a nested document.",
	"Method": "ParseContext createNestedContext(String fullPath){\r\n    final Document doc = new Document(fullPath, doc());\r\n    addDoc(doc);\r\n    return switchDoc(doc);\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.FieldTypeLookup.validateAlias",
	"Comment": "checks that the new field alias is valid.note that this method assumes that new concrete fields have already been processed, so that itcan verify that an alias refers to an existing concrete field.",
	"Method": "void validateAlias(String aliasName,String path,CopyOnWriteHashMap<String, String> aliasToConcreteName,CopyOnWriteHashMap<String, MappedFieldType> fullNameToFieldType){\r\n    if (fullNameToFieldType.containsKey(aliasName)) {\r\n        throw new IllegalArgumentException(\"The name for field alias [\" + aliasName + \"] has already\" + \" been used to define a concrete field.\");\r\n    }\r\n    if (path.equals(aliasName)) {\r\n        throw new IllegalArgumentException(\"Invalid [path] value [\" + path + \"] for field alias [\" + aliasName + \"]: an alias cannot refer to itself.\");\r\n    }\r\n    if (aliasToConcreteName.containsKey(path)) {\r\n        throw new IllegalArgumentException(\"Invalid [path] value [\" + path + \"] for field alias [\" + aliasName + \"]: an alias cannot refer to another alias.\");\r\n    }\r\n    if (!fullNameToFieldType.containsKey(path)) {\r\n        throw new IllegalArgumentException(\"Invalid [path] value [\" + path + \"] for field alias [\" + aliasName + \"]: an alias must refer to an existing field in the mappings.\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.getIndexBufferRAMBytesUsed",
	"Comment": "returns number of heap bytes used by the indexing buffer for this shard, or 0 if the shard is closed",
	"Method": "long getIndexBufferRAMBytesUsed(){\r\n    Engine engine = getEngineOrNull();\r\n    if (engine == null) {\r\n        return 0;\r\n    }\r\n    try {\r\n        return engine.getIndexBufferRAMBytesUsed();\r\n    } catch (AlreadyClosedException ex) {\r\n        return 0;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.rest.AbstractRestChannel.bytesOutputOrNull",
	"Comment": "an accessor to the raw value of the channel bytes output. this method will not instantiatea new stream if one does not exist and this method will not reset the stream.",
	"Method": "BytesStreamOutput bytesOutputOrNull(){\r\n    return bytesOut;\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.readLines",
	"Comment": "returns an iterable of the lines in the file.the file reader will be closed when the iterator is exhausted.",
	"Method": "Iterable<String> readLines(String path,Iterable<String> readLines,String path,String encoding,Iterable<String> readLines,File file,Iterable<String> readLines,File file,Class<? extends InputStream> fileInputStreamWrapper,Iterable<String> readLines,File file,Class<? extends InputStream> fileInputStreamWrapper,String encoding){\r\n    return new GetLinesIterable(file, fileInputStreamWrapper, encoding);\r\n}"
}, {
	"Path": "org.elasticsearch.indices.IndexingMemoryController.recordOperationBytes",
	"Comment": "called by indexshard to record estimated bytes written to translog for the operation",
	"Method": "void recordOperationBytes(Engine.Operation operation,Engine.Result result){\r\n    if (result.getResultType() == Engine.Result.Type.SUCCESS) {\r\n        statusChecker.bytesWritten(operation.estimatedSizeInBytes());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.mapping.get.GetFieldMappingsRequest.includeDefaults",
	"Comment": "indicates whether default mapping settings should be returned",
	"Method": "boolean includeDefaults(GetFieldMappingsRequest includeDefaults,boolean includeDefaults){\r\n    this.includeDefaults = includeDefaults;\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.ArrayMath.safeStdev",
	"Comment": "returns the standard deviation of a vector of doubles.any values whichare nan or infinite are ignored.if the vector contains fewer than twovalues, 1.0 is returned.",
	"Method": "double safeStdev(double[] v){\r\n    double[] u = filterNaNAndInfinite(v);\r\n    if (numRows(u) < 2)\r\n        return 1.0;\r\n    return stdev(u);\r\n}"
}, {
	"Path": "edu.stanford.nlp.maxent.iis.LambdaSolve.checkCorrectness",
	"Comment": "check whether the constraints are satisfied, the probabilities sum to one, etc. prints out a messageif there is something wrong.",
	"Method": "boolean checkCorrectness(){\r\n    boolean flag = true;\r\n    for (int f = 0; f < lambda.length; f++) {\r\n        if (Math.abs(lambda[f]) > 100) {\r\n            log.info(\"lambda \" + f + \" too big \" + lambda[f]);\r\n            log.info(\"empirical \" + ftildeArr[f] + \" expected \" + fExpected(p.functions.get(f)));\r\n        }\r\n    }\r\n    log.info(\" x size\" + p.data.xSize + \" \" + \" ysize \" + p.data.ySize);\r\n    double summAllExp = 0;\r\n    for (int i = 0; i < ftildeArr.length; i++) {\r\n        double exp = Math.abs(ftildeArr[i] - fExpected(p.functions.get(i)));\r\n        summAllExp += ftildeArr[i];\r\n        if (exp > 0.001) {\r\n            flag = false;\r\n            log.info(\"Constraint not satisfied  \" + i + \" \" + fExpected(p.functions.get(i)) + \" \" + ftildeArr[i] + \" lambda \" + lambda[i]);\r\n        }\r\n    }\r\n    log.info(\" The sum of all empirical expectations is \" + summAllExp);\r\n    for (int x = 0; x < p.data.xSize; x++) {\r\n        double s = 0.0;\r\n        for (int y = 0; y < probConds[x].length; y++) {\r\n            s = s + probConds[x][y];\r\n        }\r\n        if (Math.abs(s - 1) > 0.0001) {\r\n            for (// log.info(y+\" : \"+ probConds[x][y]);\r\n            int y = 0; // log.info(y+\" : \"+ probConds[x][y]);\r\n            y < probConds[x].length; y++) {\r\n                log.info(\"probabilities do not sum to one \" + x + \" \" + (float) s);\r\n            }\r\n        }\r\n    }\r\n    return flag;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequest.aliases",
	"Comment": "sets the aliases that will be associated with the index when it gets created",
	"Method": "Set<Alias> aliases(PutIndexTemplateRequest aliases,Map<String, ?> source,PutIndexTemplateRequest aliases,XContentBuilder source,PutIndexTemplateRequest aliases,String source,PutIndexTemplateRequest aliases,BytesReference source){\r\n    try (XContentParser parser = XContentHelper.createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, source)) {\r\n        parser.nextToken();\r\n        while ((parser.nextToken()) != XContentParser.Token.END_OBJECT) {\r\n            alias(Alias.fromXContent(parser));\r\n        }\r\n        return this;\r\n    } catch (IOException e) {\r\n        throw new ElasticsearchParseException(\"Failed to parse aliases\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequest.routing",
	"Comment": "controls the shard routing of the request. using this value to hash the shardand not the id.",
	"Method": "UpdateRequest routing(String routing,String routing){\r\n    return this.routing;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryShardContext.queryTypes",
	"Comment": "returns the narrowed down explicit types, or, if not set, all types.",
	"Method": "Collection<String> queryTypes(){\r\n    String[] types = getTypes();\r\n    if (types == null || types.length == 0 || (types.length == 1 && types[0].equals(\"_all\"))) {\r\n        DocumentMapper mapper = getMapperService().documentMapper();\r\n        return mapper == null ? Collections.emptyList() : Collections.singleton(mapper.type());\r\n    }\r\n    return Arrays.asList(types);\r\n}"
}, {
	"Path": "java.io.EmulatedFieldsForLoading.getObjectStreamClass",
	"Comment": "return the class descriptor for which the emulated fields are defined.",
	"Method": "ObjectStreamClass getObjectStreamClass(){\r\n    return streamClass;\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkPrimaryExecutionContext.requiresWaitingForMappingUpdate",
	"Comment": "returns true if the request needs to wait for a mapping update to arrive from the master",
	"Method": "boolean requiresWaitingForMappingUpdate(){\r\n    return currentItemState == ItemProcessingState.WAIT_FOR_MAPPING_UPDATE;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.BigDoubleArray.estimateRamBytes",
	"Comment": "estimates the number of bytes that would be consumed by an array of the given size.",
	"Method": "long estimateRamBytes(long size){\r\n    return ESTIMATOR.ramBytesEstimated(size);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.WordFactory.newLabel",
	"Comment": "create a new word label, where the label isformed fromthe label object passed in.depending on what fieldseach label has, other things will be null.",
	"Method": "Label newLabel(String word,Label newLabel,String word,int options,Label newLabel,Label oldLabel){\r\n    return new Word(oldLabel);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryStringQueryBuilder.fields",
	"Comment": "returns the fields including their respective boosts to run the query against.",
	"Method": "QueryStringQueryBuilder fields(Map<String, Float> fields,Map<String, Float> fields){\r\n    return this.fieldsAndWeights;\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequestBuilder.setUpsert",
	"Comment": "sets the doc source of the update request to be used when the document does not exists. the docincludes field and value pairs.",
	"Method": "UpdateRequestBuilder setUpsert(IndexRequest indexRequest,UpdateRequestBuilder setUpsert,XContentBuilder source,UpdateRequestBuilder setUpsert,Map<String, Object> source,UpdateRequestBuilder setUpsert,Map<String, Object> source,XContentType contentType,UpdateRequestBuilder setUpsert,String source,XContentType xContentType,UpdateRequestBuilder setUpsert,byte[] source,XContentType xContentType,UpdateRequestBuilder setUpsert,byte[] source,int offset,int length,XContentType xContentType,UpdateRequestBuilder setUpsert,Object source,UpdateRequestBuilder setUpsert,XContentType xContentType,Object source){\r\n    request.upsert(xContentType, source);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.tools.PunctEquivalenceClasser.getPunctClass",
	"Comment": "return the equivalence class of the argument. if the argument is not contained inand equivalence class, then an empty string is returned.",
	"Method": "String getPunctClass(String punc){\r\n    if (punc.equals(\"%\") || punc.equals(\"-PLUS-\"))\r\n        return \"perc\";\r\n    else if (punc.startsWith(\"*\"))\r\n        return \"bullet\";\r\n    else if (sfClass.contains(punc))\r\n        return \"sf\";\r\n    else if (colonClass.contains(punc) || pEllipsis.matcher(punc).matches())\r\n        return \"colon\";\r\n    else if (commaClass.contains(punc))\r\n        return \"comma\";\r\n    else if (currencyClass.contains(punc))\r\n        return \"curr\";\r\n    else if (slashClass.contains(punc))\r\n        return \"slash\";\r\n    else if (lBracketClass.contains(punc))\r\n        return \"lrb\";\r\n    else if (rBracketClass.contains(punc))\r\n        return \"rrb\";\r\n    else if (quoteClass.contains(punc))\r\n        return \"quote\";\r\n    return \"\";\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.extractExtendedAttributes",
	"Comment": "other attribute extract object.extracted object group by attributeclassname",
	"Method": "Map<String, Object> extractExtendedAttributes(TokenStream stream,Set<String> includeAttributes){\r\n    final Map<String, Object> extendedAttributes = new TreeMap();\r\n    stream.reflectWith((attClass, key, value) -> {\r\n        if (CharTermAttribute.class.isAssignableFrom(attClass)) {\r\n            return;\r\n        }\r\n        if (PositionIncrementAttribute.class.isAssignableFrom(attClass)) {\r\n            return;\r\n        }\r\n        if (OffsetAttribute.class.isAssignableFrom(attClass)) {\r\n            return;\r\n        }\r\n        if (TypeAttribute.class.isAssignableFrom(attClass)) {\r\n            return;\r\n        }\r\n        if (includeAttributes == null || includeAttributes.isEmpty() || includeAttributes.contains(key.toLowerCase(Locale.ROOT))) {\r\n            if (value instanceof BytesRef) {\r\n                final BytesRef p = (BytesRef) value;\r\n                value = p.toString();\r\n            }\r\n            extendedAttributes.put(key, value);\r\n        }\r\n    });\r\n    return extendedAttributes;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.DelayedAllocationService.currentNanoTime",
	"Comment": "override this to control time based decisions during delayed allocation",
	"Method": "long currentNanoTime(){\r\n    return System.nanoTime();\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.spanish.process.SpanishLexer.yypushback",
	"Comment": "pushes the specified amount of characters back into the input stream.they will be read again by then next call of the scanning method",
	"Method": "void yypushback(int number){\r\n    if (number > yylength())\r\n        zzScanError(ZZ_PUSHBACK_2BIG);\r\n    zzMarkedPos -= number;\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.GatewayAllocator.innerAllocatedUnassigned",
	"Comment": "allow for testing infra to change shard allocators implementation",
	"Method": "void innerAllocatedUnassigned(RoutingAllocation allocation,PrimaryShardAllocator primaryShardAllocator,ReplicaShardAllocator replicaShardAllocator){\r\n    RoutingNodes.UnassignedShards unassigned = allocation.routingNodes().unassigned();\r\n    unassigned.sort(PriorityComparator.getAllocationComparator(allocation));\r\n    primaryShardAllocator.allocateUnassigned(allocation);\r\n    replicaShardAllocator.processExistingRecoveries(allocation);\r\n    replicaShardAllocator.allocateUnassigned(allocation);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.MultiPatternMatcher.find",
	"Comment": "given a sequence, applies our patterns over the sequence and returnsall matches, depending on the findtype.when multiple patterns overlaps,matched patterns are selected by order specified by the comparator",
	"Method": "List<SequenceMatchResult<T>> find(List<? extends T> elements,SequenceMatcher.FindType findType){\r\n    Collection<SequencePattern<T>> triggered = getTriggeredPatterns(elements);\r\n    List<SequenceMatchResult<T>> all = new ArrayList();\r\n    int i = 0;\r\n    for (SequencePattern<T> p : triggered) {\r\n        if (Thread.interrupted()) {\r\n            throw new RuntimeInterruptedException();\r\n        }\r\n        SequenceMatcher<T> m = p.getMatcher(elements);\r\n        m.setMatchWithResult(matchWithResult);\r\n        m.setFindType(findType);\r\n        m.setOrder(i);\r\n        while (m.find()) {\r\n            all.add(m.toBasicSequenceMatchResult());\r\n        }\r\n        i++;\r\n    }\r\n    List<SequenceMatchResult<T>> res = IntervalTree.getNonOverlapping(all, SequenceMatchResult.TO_INTERVAL, SequenceMatchResult.DEFAULT_COMPARATOR);\r\n    res.sort(SequenceMatchResult.OFFSET_COMPARATOR);\r\n    return res;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.findEarliestLastModifiedAge",
	"Comment": "returns the age of the oldest entry in the translog files in seconds",
	"Method": "long findEarliestLastModifiedAge(long currentTime,Iterable<TranslogReader> readers,TranslogWriter writer){\r\n    long earliestTime = currentTime;\r\n    for (BaseTranslogReader r : readers) {\r\n        earliestTime = Math.min(r.getLastModifiedTime(), earliestTime);\r\n    }\r\n    return Math.max(0, currentTime - Math.min(earliestTime, writer.getLastModifiedTime()));\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequest.detectNoop",
	"Comment": "should this update attempt to detect if it is a noop? defaults to true.",
	"Method": "UpdateRequest detectNoop(boolean detectNoop,boolean detectNoop){\r\n    return detectNoop;\r\n}"
}, {
	"Path": "org.elasticsearch.index.search.QueryParserHelper.parseFieldsAndWeights",
	"Comment": "convert a list of field names encoded with optional boosts to a map that associatesthe field name and its boost.",
	"Method": "Map<String, Float> parseFieldsAndWeights(List<String> fields){\r\n    final Map<String, Float> fieldsAndWeights = new HashMap();\r\n    for (String field : fields) {\r\n        int boostIndex = field.indexOf('^');\r\n        String fieldName;\r\n        float boost = 1.0f;\r\n        if (boostIndex != -1) {\r\n            fieldName = field.substring(0, boostIndex);\r\n            boost = Float.parseFloat(field.substring(boostIndex + 1, field.length()));\r\n        } else {\r\n            fieldName = field;\r\n        }\r\n        fieldsAndWeights.put(fieldName, boost);\r\n    }\r\n    return fieldsAndWeights;\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.Store.associateIndexWithNewTranslog",
	"Comment": "force bakes the given translog generation as recovery information in the lucene index. this isused when recovering from a snapshot or peer file based recovery where a new empty translog iscreated and the existing lucene index needs should be changed to use it.",
	"Method": "void associateIndexWithNewTranslog(String translogUUID){\r\n    metadataLock.writeLock().lock();\r\n    try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.APPEND, directory, null)) {\r\n        if (translogUUID.equals(getUserData(writer).get(Translog.TRANSLOG_UUID_KEY))) {\r\n            throw new IllegalArgumentException(\"a new translog uuid can't be equal to existing one. got [\" + translogUUID + \"]\");\r\n        }\r\n        final Map<String, String> map = new HashMap();\r\n        map.put(Translog.TRANSLOG_GENERATION_KEY, \"1\");\r\n        map.put(Translog.TRANSLOG_UUID_KEY, translogUUID);\r\n        updateCommitData(writer, map);\r\n    } finally {\r\n        metadataLock.writeLock().unlock();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.ValidationException.addValidationErrors",
	"Comment": "add a sequence of validation errors to the accumulating validation errors",
	"Method": "void addValidationErrors(Iterable<String> errors){\r\n    for (String error : errors) {\r\n        validationErrors.add(error);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexOrAliasName",
	"Comment": "validate the name for an index or alias against some static rules.",
	"Method": "void validateIndexOrAliasName(String index,BiFunction<String, String, ? extends RuntimeException> exceptionCtor){\r\n    if (!Strings.validFileName(index)) {\r\n        throw exceptionCtor.apply(index, \"must not contain the following characters \" + Strings.INVALID_FILENAME_CHARS);\r\n    }\r\n    if (index.contains(\"#\")) {\r\n        throw exceptionCtor.apply(index, \"must not contain '#'\");\r\n    }\r\n    if (index.contains(\":\")) {\r\n        throw exceptionCtor.apply(index, \"must not contain ':'\");\r\n    }\r\n    if (index.charAt(0) == '_' || index.charAt(0) == '-' || index.charAt(0) == '+') {\r\n        throw exceptionCtor.apply(index, \"must not start with '_', '-', or '+'\");\r\n    }\r\n    int byteCount = 0;\r\n    try {\r\n        byteCount = index.getBytes(\"UTF-8\").length;\r\n    } catch (UnsupportedEncodingException e) {\r\n        throw new ElasticsearchException(\"Unable to determine length of index name\", e);\r\n    }\r\n    if (byteCount > MAX_INDEX_NAME_BYTES) {\r\n        throw exceptionCtor.apply(index, \"index name is too long, (\" + byteCount + \" > \" + MAX_INDEX_NAME_BYTES + \")\");\r\n    }\r\n    if (index.equals(\".\") || index.equals(\"..\")) {\r\n        throw exceptionCtor.apply(index, \"must not be '.' or '..'\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.indices.mapper.MapperRegistry.getMapperParsers",
	"Comment": "return a map of the mappers that have been registered. thereturned map uses the type of the field as a key.",
	"Method": "Map<String, Mapper.TypeParser> getMapperParsers(){\r\n    return mapperParsers;\r\n}"
}, {
	"Path": "org.elasticsearch.node.Node.writePortsFile",
	"Comment": "writes a file to the logs dir containing the ports for the given transport type",
	"Method": "void writePortsFile(String type,BoundTransportAddress boundAddress){\r\n    Path tmpPortsFile = environment.logsFile().resolve(type + \".ports.tmp\");\r\n    try (BufferedWriter writer = Files.newBufferedWriter(tmpPortsFile, Charset.forName(\"UTF-8\"))) {\r\n        for (TransportAddress address : boundAddress.boundAddresses()) {\r\n            InetAddress inetAddress = InetAddress.getByName(address.getAddress());\r\n            writer.write(NetworkAddress.format(new InetSocketAddress(inetAddress, address.getPort())) + \"\\n\");\r\n        }\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(\"Failed to write ports file\", e);\r\n    }\r\n    Path portsFile = environment.logsFile().resolve(type + \".ports\");\r\n    try {\r\n        Files.move(tmpPortsFile, portsFile, StandardCopyOption.ATOMIC_MOVE);\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(\"Failed to rename ports file\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.getIndexSortConfig",
	"Comment": "returns the index sort config that should be used for this index.",
	"Method": "IndexSortConfig getIndexSortConfig(){\r\n    return indexSortConfig;\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.Engine.failEngine",
	"Comment": "fail engine due to some error. the engine will also be closed.the underlying store is marked corrupted iff failure is caused by index corruption",
	"Method": "void failEngine(String reason,Exception failure){\r\n    if (failure != null) {\r\n        maybeDie(reason, failure);\r\n    }\r\n    if (failEngineLock.tryLock()) {\r\n        store.incRef();\r\n        try {\r\n            if (failedEngine.get() != null) {\r\n                logger.warn(() -> new ParameterizedMessage(\"tried to fail engine but engine is already failed. ignoring. [{}]\", reason), failure);\r\n                return;\r\n            }\r\n            failedEngine.set((failure != null) ? failure : new IllegalStateException(reason));\r\n            try {\r\n                closeNoLock(\"engine failed on: [\" + reason + \"]\", closedLatch);\r\n            } finally {\r\n                logger.warn(() -> new ParameterizedMessage(\"failed engine [{}]\", reason), failure);\r\n                if (Lucene.isCorruptionException(failure)) {\r\n                    try {\r\n                        store.markStoreCorrupted(new IOException(\"failed engine (reason: [\" + reason + \"])\", ExceptionsHelper.unwrapCorruption(failure)));\r\n                    } catch (IOException e) {\r\n                        logger.warn(\"Couldn't mark store corrupted\", e);\r\n                    }\r\n                }\r\n                eventListener.onFailedEngine(reason, failure);\r\n            }\r\n        } catch (Exception inner) {\r\n            if (failure != null)\r\n                inner.addSuppressed(failure);\r\n            logger.warn(\"failEngine threw exception\", inner);\r\n        } finally {\r\n            store.decRef();\r\n        }\r\n    } else {\r\n        logger.debug(() -> new ParameterizedMessage(\"tried to fail engine but could not acquire lock - engine should \" + \"be failed by now [{}]\", reason), failure);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractBulkByScrollRequest.searchToString",
	"Comment": "append a short description of the search request to a stringbuilder. usedto make tostring.",
	"Method": "void searchToString(StringBuilder b){\r\n    if (searchRequest.indices() != null && searchRequest.indices().length != 0) {\r\n        b.append(Arrays.toString(searchRequest.indices()));\r\n    } else {\r\n        b.append(\"[all indices]\");\r\n    }\r\n    if (searchRequest.types() != null && searchRequest.types().length != 0) {\r\n        b.append(Arrays.toString(searchRequest.types()));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.InjectorImpl.createUnitializedBinding",
	"Comment": "creates a binding for an injectable type with the given scope. looks for a scope on the type ifnone is specified.",
	"Method": "BindingImpl<T> createUnitializedBinding(Key<T> key,Scoping scoping,Object source,Errors errors){\r\n    Class<?> rawType = key.getTypeLiteral().getRawType();\r\n    if (rawType.isArray() || rawType.isEnum()) {\r\n        throw errors.missingImplementation(key).toException();\r\n    }\r\n    if (rawType == TypeLiteral.class) {\r\n        @SuppressWarnings(\"unchecked\")\r\n        BindingImpl<T> binding = (BindingImpl<T>) createTypeLiteralBinding((Key<TypeLiteral<Object>>) key, errors);\r\n        return binding;\r\n    }\r\n    ImplementedBy implementedBy = rawType.getAnnotation(ImplementedBy.class);\r\n    if (implementedBy != null) {\r\n        Annotations.checkForMisplacedScopeAnnotations(rawType, source, errors);\r\n        return createImplementedByBinding(key, scoping, implementedBy, errors);\r\n    }\r\n    ProvidedBy providedBy = rawType.getAnnotation(ProvidedBy.class);\r\n    if (providedBy != null) {\r\n        Annotations.checkForMisplacedScopeAnnotations(rawType, source, errors);\r\n        return createProvidedByBinding(key, scoping, providedBy, errors);\r\n    }\r\n    if (Modifier.isAbstract(rawType.getModifiers())) {\r\n        throw errors.missingImplementation(key).toException();\r\n    }\r\n    if (Classes.isInnerClass(rawType)) {\r\n        throw errors.cannotInjectInnerClass(rawType).toException();\r\n    }\r\n    if (!scoping.isExplicitlyScoped()) {\r\n        Class<? extends Annotation> scopeAnnotation = findScopeAnnotation(errors, rawType);\r\n        if (scopeAnnotation != null) {\r\n            scoping = Scopes.makeInjectable(Scoping.forAnnotation(scopeAnnotation), this, errors.withSource(rawType));\r\n        }\r\n    }\r\n    return ConstructorBindingImpl.create(this, key, source, scoping);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.common.ParserGrammar.apply",
	"Comment": "parses the list of hasword.if the parse fails for some reason,an x tree is returned instead of barfing.",
	"Method": "Tree apply(List<? extends HasWord> words){\r\n    return parse(words);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryStringQueryBuilder.field",
	"Comment": "adds a field to run the query string against with a specific boost.",
	"Method": "QueryStringQueryBuilder field(String field,QueryStringQueryBuilder field,String field,float boost){\r\n    this.fieldsAndWeights.put(field, boost);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.Store.tryOpenIndex",
	"Comment": "tries to open an index for the given location. this includes reading thesegment infos and possible corruption markers. if the index can notbe opened, an exception is thrown",
	"Method": "void tryOpenIndex(Path indexLocation,ShardId shardId,NodeEnvironment.ShardLocker shardLocker,Logger logger){\r\n    try (ShardLock lock = shardLocker.lock(shardId, TimeUnit.SECONDS.toMillis(5));\r\n        Directory dir = new SimpleFSDirectory(indexLocation)) {\r\n        failIfCorrupted(dir, shardId);\r\n        SegmentInfos segInfo = Lucene.readSegmentInfos(dir);\r\n        logger.trace(\"{} loaded segment info [{}]\", shardId, segInfo);\r\n    }\r\n}"
}, {
	"Path": "android.util.SparseIntArray.put",
	"Comment": "adds a mapping from the specified key to the specified value,replacing the previous mapping from the specified key if therewas one.",
	"Method": "void put(int key,int value){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i >= 0) {\r\n        mValues[i] = value;\r\n    } else {\r\n        i = ~i;\r\n        if (mSize >= mKeys.length) {\r\n            int n = ArrayUtils.idealIntArraySize(mSize + 1);\r\n            int[] nkeys = new int[n];\r\n            int[] nvalues = new int[n];\r\n            System.arraycopy(mKeys, 0, nkeys, 0, mKeys.length);\r\n            System.arraycopy(mValues, 0, nvalues, 0, mValues.length);\r\n            mKeys = nkeys;\r\n            mValues = nvalues;\r\n        }\r\n        if (mSize - i != 0) {\r\n            System.arraycopy(mKeys, i, mKeys, i + 1, mSize - i);\r\n            System.arraycopy(mValues, i, mValues, i + 1, mSize - i);\r\n        }\r\n        mKeys[i] = key;\r\n        mValues[i] = value;\r\n        mSize++;\r\n    }\r\n}"
}, {
	"Path": "com.android.internal.util.ArrayUtils.contains",
	"Comment": "checks that value is present as at least one of the elements of the array.",
	"Method": "boolean contains(T[] array,T value,boolean contains,int[] array,int value){\r\n    for (int element : array) {\r\n        if (element == value) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchResponseSections.getNumReducePhases",
	"Comment": "returns the number of reduce phases applied to obtain this search response",
	"Method": "int getNumReducePhases(){\r\n    return numReducePhases;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.IndexShardRoutingTable.allShardsStarted",
	"Comment": "returns true iff all shards in the routing table are started otherwise false",
	"Method": "boolean allShardsStarted(){\r\n    return allShardsStarted;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.BoolQueryBuilder.mustNot",
	"Comment": "gets the queries that must not appear in the matching documents.",
	"Method": "BoolQueryBuilder mustNot(QueryBuilder queryBuilder,List<QueryBuilder> mustNot){\r\n    return this.mustNotClauses;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.UnassignedInfo.getDetails",
	"Comment": "builds a string representation of the message and the failure if exists.",
	"Method": "String getDetails(){\r\n    if (message == null) {\r\n        return null;\r\n    }\r\n    return message + (failure == null ? \"\" : \", failure \" + ExceptionsHelper.detailedMessage(failure));\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.DocumentParser.expandCommonMappers",
	"Comment": "adds mappers from the end of the stack that exist as updates within those mappers.returns the next unprocessed index from nameparts.",
	"Method": "int expandCommonMappers(List<ObjectMapper> parentMappers,String[] nameParts,int i){\r\n    ObjectMapper last = parentMappers.get(parentMappers.size() - 1);\r\n    while (i < nameParts.length - 1 && last.getMapper(nameParts[i]) != null) {\r\n        Mapper newLast = last.getMapper(nameParts[i]);\r\n        assert newLast instanceof ObjectMapper;\r\n        last = (ObjectMapper) newLast;\r\n        parentMappers.add(last);\r\n        ++i;\r\n    }\r\n    return i;\r\n}"
}, {
	"Path": "org.apache.harmony.tests.java.util.HashtableTest.tearDown",
	"Comment": "tears down the fixture, for example, close a network connection. thismethod is called after a test is executed.",
	"Method": "void tearDown(){\r\n    ht10 = null;\r\n    ht100 = null;\r\n    htfull = null;\r\n    keyVector = null;\r\n    elmVector = null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.AbstractScopedSettings.updateSettings",
	"Comment": "updates a target settings builder with new, updated or deleted settings from a given settings builder.",
	"Method": "boolean updateSettings(Settings toApply,Settings.Builder target,Settings.Builder updates,String type,boolean updateSettings,Settings toApply,Settings.Builder target,Settings.Builder updates,String type,boolean onlyDynamic){\r\n    boolean changed = false;\r\n    final Set<String> toRemove = new HashSet();\r\n    Settings.Builder settingsBuilder = Settings.builder();\r\n    final Predicate<String> canUpdate = (key) -> (isFinalSetting(key) == false && ((onlyDynamic == false && get(key) != null) || isDynamicSetting(key)));\r\n    for (String key : toApply.keySet()) {\r\n        boolean isDelete = toApply.hasValue(key) == false;\r\n        if (isDelete && (isValidDelete(key, onlyDynamic) || key.endsWith(\"*\"))) {\r\n            toRemove.add(key);\r\n        } else if (get(key) == null) {\r\n            throw new IllegalArgumentException(type + \" setting [\" + key + \"], not recognized\");\r\n        } else if (isDelete == false && canUpdate.test(key)) {\r\n            validate(key, toApply, false);\r\n            settingsBuilder.copy(key, toApply);\r\n            updates.copy(key, toApply);\r\n            changed = true;\r\n        } else {\r\n            if (isFinalSetting(key)) {\r\n                throw new IllegalArgumentException(\"final \" + type + \" setting [\" + key + \"], not updateable\");\r\n            } else {\r\n                throw new IllegalArgumentException(type + \" setting [\" + key + \"], not dynamically updateable\");\r\n            }\r\n        }\r\n    }\r\n    changed |= applyDeletes(toRemove, target, k -> isValidDelete(k, onlyDynamic));\r\n    target.put(settingsBuilder.build());\r\n    return changed;\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.AsyncShardFetch.fetchData",
	"Comment": "fetches the data for the relevant shard. if there any ongoing async fetches going on, or new ones havebeen initiated by this call, the result will have no data.the ignorenodes are nodes that are supposed to be ignored for this round, since fetching is async, we needto keep them around and make sure we add them back when all the responses are fetched and returned.",
	"Method": "FetchResult<T> fetchData(DiscoveryNodes nodes,Set<String> ignoreNodes){\r\n    if (closed) {\r\n        throw new IllegalStateException(shardId + \": can't fetch data on closed async fetch\");\r\n    }\r\n    nodesToIgnore.addAll(ignoreNodes);\r\n    fillShardCacheWithDataNodes(cache, nodes);\r\n    List<NodeEntry<T>> nodesToFetch = findNodesToFetch(cache);\r\n    if (nodesToFetch.isEmpty() == false) {\r\n        final long fetchingRound = round.incrementAndGet();\r\n        for (NodeEntry<T> nodeEntry : nodesToFetch) {\r\n            nodeEntry.markAsFetching(fetchingRound);\r\n        }\r\n        DiscoveryNode[] discoNodesToFetch = nodesToFetch.stream().map(NodeEntry::getNodeId).map(nodes::get).toArray(DiscoveryNode[]::new);\r\n        asyncFetch(discoNodesToFetch, fetchingRound);\r\n    }\r\n    if (hasAnyNodeFetching(cache)) {\r\n        return new FetchResult(shardId, null, emptySet());\r\n    } else {\r\n        Map<DiscoveryNode, T> fetchData = new HashMap();\r\n        Set<String> failedNodes = new HashSet();\r\n        for (Iterator<Map.Entry<String, NodeEntry<T>>> it = cache.entrySet().iterator(); it.hasNext(); ) {\r\n            Map.Entry<String, NodeEntry<T>> entry = it.next();\r\n            String nodeId = entry.getKey();\r\n            NodeEntry<T> nodeEntry = entry.getValue();\r\n            DiscoveryNode node = nodes.get(nodeId);\r\n            if (node != null) {\r\n                if (nodeEntry.isFailed()) {\r\n                    it.remove();\r\n                    failedNodes.add(nodeEntry.getNodeId());\r\n                } else {\r\n                    if (nodeEntry.getValue() != null) {\r\n                        fetchData.put(node, nodeEntry.getValue());\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        Set<String> allIgnoreNodes = unmodifiableSet(new HashSet(nodesToIgnore));\r\n        nodesToIgnore.clear();\r\n        if (failedNodes.isEmpty() == false || allIgnoreNodes.isEmpty() == false) {\r\n            reroute(shardId, \"nodes failed [\" + failedNodes.size() + \"], ignored [\" + allIgnoreNodes.size() + \"]\");\r\n        }\r\n        return new FetchResult(shardId, fetchData, allIgnoreNodes);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MoreLikeThisQueryBuilder.maxDocFreq",
	"Comment": "set the maximum frequency in which words may still appear. words that appearin more than this many docs will be ignored. defaults to unbounded.",
	"Method": "MoreLikeThisQueryBuilder maxDocFreq(int maxDocFreq,int maxDocFreq){\r\n    return maxDocFreq;\r\n}"
}, {
	"Path": "android.util.SparseArray.delete",
	"Comment": "removes the mapping from the specified key, if there was any.",
	"Method": "void delete(int key){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i >= 0) {\r\n        if (mValues[i] != DELETED) {\r\n            mValues[i] = DELETED;\r\n            mGarbage = true;\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.copyTokens",
	"Comment": "create a copy of srctokens, detecting on the fly if character offsets need adjusting",
	"Method": "List<CoreLabel> copyTokens(List<CoreLabel> srcList,boolean adjustCharacterOffsets,boolean forceCopy,List<CoreLabel> copyTokens,List<CoreLabel> srcTokens,CoreMap srcSentence){\r\n    boolean adjustCharacterOffsets = false;\r\n    if (srcSentence == null || srcSentence.get(CoreAnnotations.TextAnnotation.class) == null || srcTokens.isEmpty() || srcTokens.get(0).get(CoreAnnotations.OriginalTextAnnotation.class) == null) {\r\n        adjustCharacterOffsets = true;\r\n    }\r\n    return copyTokens(srcTokens, adjustCharacterOffsets, true);\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.BigArrays.adjustBreaker",
	"Comment": "adjust the circuit breaker with the given delta, if the delta isnegative, or checkbreaker is false, the breaker will be adjustedwithout tripping.if the data was already created before callingthis method, and the breaker trips, we add the delta without breakingto account for the created data.if the data has not been created yet,we do not add the delta to the breaker if it trips.",
	"Method": "void adjustBreaker(long delta,boolean isDataAlreadyCreated){\r\n    if (this.breakerService != null) {\r\n        CircuitBreaker breaker = this.breakerService.getBreaker(CircuitBreaker.REQUEST);\r\n        if (this.checkBreaker) {\r\n            if (delta > 0) {\r\n                try {\r\n                    breaker.addEstimateBytesAndMaybeBreak(delta, \"<reused_arrays>\");\r\n                } catch (CircuitBreakingException e) {\r\n                    if (isDataAlreadyCreated) {\r\n                        breaker.addWithoutBreaking(delta);\r\n                    }\r\n                    throw e;\r\n                }\r\n            } else {\r\n                breaker.addWithoutBreaking(delta);\r\n            }\r\n        } else {\r\n            breaker.addWithoutBreaking(delta);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder.getTotalNumOrds",
	"Comment": "returns the number of document id to ordinal pairs in this builder.",
	"Method": "int getTotalNumOrds(){\r\n    return totalNumOrds;\r\n}"
}, {
	"Path": "java.io.EmulatedFieldsForLoading.emulatedFields",
	"Comment": "return the actual emulatedfields instance used by the receiver. we havethe actual work in a separate class so that the code can be shared. thereceiver has to be of a subclass of getfield.",
	"Method": "EmulatedFields emulatedFields(){\r\n    return emulatedFields;\r\n}"
}, {
	"Path": "org.elasticsearch.index.analysis.IndexAnalyzers.getWhitespaceNormalizer",
	"Comment": "returns a normalizer that splits on whitespace mapped to the given name or null if not present",
	"Method": "NamedAnalyzer getWhitespaceNormalizer(String name){\r\n    return whitespaceNormalizers.get(name);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.lastRefreshedCheckpoint",
	"Comment": "returned the last local checkpoint value has been refreshed internally.",
	"Method": "long lastRefreshedCheckpoint(){\r\n    return lastRefreshedCheckpointListener.refreshedCheckpoint.get();\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.KeyStoreWrapper.encrypt",
	"Comment": "encrypt the keystore entries and return the encrypted data.",
	"Method": "byte[] encrypt(char[] password,byte[] salt,byte[] iv){\r\n    assert isLoaded();\r\n    ByteArrayOutputStream bytes = new ByteArrayOutputStream();\r\n    Cipher cipher = createCipher(Cipher.ENCRYPT_MODE, password, salt, iv);\r\n    try (CipherOutputStream cipherStream = new CipherOutputStream(bytes, cipher);\r\n        DataOutputStream output = new DataOutputStream(cipherStream)) {\r\n        output.writeInt(entries.get().size());\r\n        for (Map.Entry<String, Entry> mapEntry : entries.get().entrySet()) {\r\n            output.writeUTF(mapEntry.getKey());\r\n            Entry entry = mapEntry.getValue();\r\n            output.writeUTF(entry.type.name());\r\n            output.writeInt(entry.bytes.length);\r\n            output.write(entry.bytes);\r\n        }\r\n    }\r\n    return bytes.toByteArray();\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.ParseContext.switchDoc",
	"Comment": "return a new context that has the provided document as the current document.",
	"Method": "ParseContext switchDoc(Document document){\r\n    return new FilterParseContext(this) {\r\n        @Override\r\n        public Document doc() {\r\n            return document;\r\n        }\r\n    };\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.ParseContext.switchDoc",
	"Comment": "return a new context that has the provided document as the current document.",
	"Method": "ParseContext switchDoc(Document document){\r\n    return document;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDatum.setLabels",
	"Comment": "removes all currently assigned labels for this datum then adds allof the given labels.",
	"Method": "void setLabels(Collection<LabelType> labels){\r\n    this.labels.clear();\r\n    if (labels != null) {\r\n        this.labels.addAll(labels);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.NeuralUtils.paramsToVector",
	"Comment": "given a sequence of iterators over the matrices, builds a vectorout of those matrices in the order given.the vector is scaledaccording to the scale parameter.asks for anexpected total size as a time savings.assertionerror thrown ifthe vector sizes do not exactly match.",
	"Method": "double[] paramsToVector(int totalSize,Iterator<SimpleMatrix> matrices,double[] paramsToVector,double scale,int totalSize,Iterator<SimpleMatrix> matrices){\r\n    double[] theta = new double[totalSize];\r\n    int index = 0;\r\n    for (Iterator<SimpleMatrix> matrixIterator : matrices) {\r\n        while (matrixIterator.hasNext()) {\r\n            SimpleMatrix matrix = matrixIterator.next();\r\n            int numElements = matrix.getNumElements();\r\n            for (int i = 0; i < numElements; ++i) {\r\n                theta[index] = matrix.get(i) * scale;\r\n                ++index;\r\n            }\r\n        }\r\n    }\r\n    if (index != totalSize) {\r\n        throw new AssertionError(\"Did not entirely fill the theta vector: expected \" + totalSize + \" used \" + index);\r\n    }\r\n    return theta;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.settings.put.UpdateSettingsClusterStateUpdateRequest.setPreserveExisting",
	"Comment": "iff set to true this settings update will only add settings not already set on an index. existing settings remainunchanged.",
	"Method": "UpdateSettingsClusterStateUpdateRequest setPreserveExisting(boolean preserveExisting){\r\n    this.preserveExisting = preserveExisting;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.single.shard.TransportSingleShardAction.isSubAction",
	"Comment": "tells whether the action is a main one or a subaction. used to decide whether we need to registerthe main transport handler. in fact if the action is a subaction, its execute methodwill be called locally to its parent action.",
	"Method": "boolean isSubAction(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.ElasticsearchException.guessRootCauses",
	"Comment": "returns the root cause of this exception or multiple if different shards caused different exceptions",
	"Method": "ElasticsearchException[] guessRootCauses(ElasticsearchException[] guessRootCauses,Throwable t){\r\n    Throwable ex = ExceptionsHelper.unwrapCause(t);\r\n    if (ex instanceof ElasticsearchException) {\r\n        return ((ElasticsearchException) ex).guessRootCauses();\r\n    }\r\n    if (ex instanceof XContentParseException) {\r\n        Throwable cause = ex.getCause();\r\n        if (cause != null) {\r\n            if (cause instanceof XContentParseException || cause instanceof ElasticsearchException) {\r\n                return guessRootCauses(ex.getCause());\r\n            }\r\n        }\r\n    }\r\n    return new ElasticsearchException[] { new ElasticsearchException(t.getMessage(), t) {\r\n        @Override\r\n        protected String getExceptionName() {\r\n            return getExceptionName(getCause());\r\n        }\r\n    } };\r\n}"
}, {
	"Path": "org.elasticsearch.ElasticsearchException.guessRootCauses",
	"Comment": "returns the root cause of this exception or multiple if different shards caused different exceptions",
	"Method": "ElasticsearchException[] guessRootCauses(ElasticsearchException[] guessRootCauses,Throwable t){\r\n    return getExceptionName(getCause());\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.StringLabelFactory.newLabel",
	"Comment": "create a new stringlabel, where the label isformed fromthe label object passed in.depending on what fieldseach label has, other things will be null.",
	"Method": "Label newLabel(String labelStr,Label newLabel,String labelStr,int options,Label newLabel,Label oldLabel){\r\n    return new StringLabel(oldLabel);\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVectorNamespace.ensureFeature",
	"Comment": "an optimization, this lets clients inform the concatvectornamespace of how many features to expect, sothat we can avoid resizing concatvectors.",
	"Method": "int ensureFeature(String featureName){\r\n    synchronized (featureToIndex) {\r\n        if (!featureToIndex.containsKey(featureName)) {\r\n            featureToIndex.put(featureName, featureToIndex.size());\r\n        }\r\n        return featureToIndex.get(featureName);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor.createExtractorFromFile",
	"Comment": "creates an extractor using the specified environment, and reading the rules from the given filename.",
	"Method": "CoreMapExpressionExtractor createExtractorFromFile(Env env,String filename){\r\n    return createExtractorFromFiles(env, Collections.singletonList(filename));\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.RerankingParserQuery.parseSkipped",
	"Comment": "the sentence was skipped, probably because it was too long or of length 0",
	"Method": "boolean parseSkipped(){\r\n    return parserQuery.parseSkipped();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.DiffableUtils.getIntKeySerializer",
	"Comment": "returns a map key serializer for integer keys. encodes as int.",
	"Method": "KeySerializer<Integer> getIntKeySerializer(){\r\n    return IntKeySerializer.INSTANCE;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.checkDeprecation",
	"Comment": "logs a deprecation warning if the setting is deprecated and used.",
	"Method": "void checkDeprecation(Settings settings){\r\n    if (this.isDeprecated() && this.exists(settings)) {\r\n        final String key = getKey();\r\n        Settings.DeprecationLoggerHolder.deprecationLogger.deprecatedAndMaybeLog(key, \"[{}] setting was deprecated in Elasticsearch and will be removed in a future release! \" + \"See the breaking changes documentation for the next major version.\", key);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotRequest.indices",
	"Comment": "returns a list of indices that should be included into the snapshot",
	"Method": "CreateSnapshotRequest indices(String indices,CreateSnapshotRequest indices,List<String> indices,String[] indices){\r\n    return indices;\r\n}"
}, {
	"Path": "org.elasticsearch.common.collect.CopyOnWriteHashMap.copyAndPut",
	"Comment": "associate key with value and return a new copyof the hash table. the current hash table is not modified.",
	"Method": "CopyOnWriteHashMap<K, V> copyAndPut(K key,V value){\r\n    if (key == null) {\r\n        throw new IllegalArgumentException(\"null keys are not supported\");\r\n    }\r\n    if (value == null) {\r\n        throw new IllegalArgumentException(\"null values are not supported\");\r\n    }\r\n    final int hash = key.hashCode();\r\n    final MutableValueInt newValue = new MutableValueInt();\r\n    final InnerNode<K, V> newRoot = root.put(key, hash, TOTAL_HASH_BITS, value, newValue);\r\n    final int newSize = size + newValue.value;\r\n    return new CopyOnWriteHashMap(newRoot, newSize);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.geoWithinQuery",
	"Comment": "a filter to filter indexed shapes that are contained by a shape",
	"Method": "GeoShapeQueryBuilder geoWithinQuery(String name,ShapeBuilder shape,GeoShapeQueryBuilder geoWithinQuery,String name,String indexedShapeId,String indexedShapeType){\r\n    GeoShapeQueryBuilder builder = geoShapeQuery(name, indexedShapeId, indexedShapeType);\r\n    builder.relation(ShapeRelation.WITHIN);\r\n    return builder;\r\n}"
}, {
	"Path": "android.util.ArraySet.add",
	"Comment": "adds the specified object to this set. the set is not modified if italready contains the object.",
	"Method": "boolean add(E value){\r\n    final int hash;\r\n    int index;\r\n    if (value == null) {\r\n        hash = 0;\r\n        index = indexOfNull();\r\n    } else {\r\n        hash = value.hashCode();\r\n        index = indexOf(value, hash);\r\n    }\r\n    if (index >= 0) {\r\n        return false;\r\n    }\r\n    index = ~index;\r\n    if (mSize >= mHashes.length) {\r\n        final int n = mSize >= (BASE_SIZE * 2) ? (mSize + (mSize >> 1)) : (mSize >= BASE_SIZE ? (BASE_SIZE * 2) : BASE_SIZE);\r\n        if (DEBUG)\r\n            Log.d(TAG, \"add: grow from \" + mHashes.length + \" to \" + n);\r\n        final int[] ohashes = mHashes;\r\n        final Object[] oarray = mArray;\r\n        allocArrays(n);\r\n        if (mHashes.length > 0) {\r\n            if (DEBUG)\r\n                Log.d(TAG, \"add: copy 0-\" + mSize + \" to 0\");\r\n            System.arraycopy(ohashes, 0, mHashes, 0, ohashes.length);\r\n            System.arraycopy(oarray, 0, mArray, 0, oarray.length);\r\n        }\r\n        freeArrays(ohashes, oarray, mSize);\r\n    }\r\n    if (index < mSize) {\r\n        if (DEBUG)\r\n            Log.d(TAG, \"add: move \" + index + \"-\" + (mSize - index) + \" to \" + (index + 1));\r\n        System.arraycopy(mHashes, index, mHashes, index + 1, mSize - index);\r\n        System.arraycopy(mArray, index, mArray, index + 1, mSize - index);\r\n    }\r\n    mHashes[index] = hash;\r\n    mArray[index] = value;\r\n    mSize++;\r\n    return true;\r\n}"
}, {
	"Path": "org.apache.dubbo.registry.integration.RegistryProtocol.getRegisteredProviderUrl",
	"Comment": "return the url that is registered to the registry and filter the url parameter once",
	"Method": "URL getRegisteredProviderUrl(Invoker<?> originInvoker){\r\n    URL providerUrl = getProviderUrl(originInvoker);\r\n    return providerUrl.removeParameters(getFilteredKeys(providerUrl)).removeParameter(Constants.MONITOR_KEY).removeParameter(Constants.BIND_IP_KEY).removeParameter(Constants.BIND_PORT_KEY).removeParameter(QOS_ENABLE).removeParameter(QOS_PORT).removeParameter(ACCEPT_FOREIGN_IP).removeParameter(VALIDATION_KEY).removeParameter(INTERFACES);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoBoundingBoxQueryBuilder.setValidationMethod",
	"Comment": "specify whether or not to ignore validation errors of bounding boxes.can only be set if coerce set to false, otherwise calling thismethod has no effect.",
	"Method": "GeoBoundingBoxQueryBuilder setValidationMethod(GeoValidationMethod method){\r\n    this.validationMethod = method;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.matcher.Matchers.inPackage",
	"Comment": "returns a matcher which matches classes in the given package. packages are specific to theirclassloader, so classes with the same package name may not have the same package at runtime.",
	"Method": "Matcher<Class> inPackage(Package targetPackage){\r\n    return new InPackage(targetPackage);\r\n}"
}, {
	"Path": "org.elasticsearch.ElasticsearchException.classes",
	"Comment": "returns an array of all registered pairs of handle ids and exception classes. these pairs areprovided for every registered exception.",
	"Method": "Tuple<Integer, Class<? extends ElasticsearchException>>[] classes(){\r\n    @SuppressWarnings(\"unchecked\")\r\n    final Tuple<Integer, Class<? extends ElasticsearchException>>[] ts = Arrays.stream(ElasticsearchExceptionHandle.values()).map(h -> Tuple.tuple(h.id, h.exceptionClass)).toArray(Tuple[]::new);\r\n    return ts;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchShardIterator.skip",
	"Comment": "returns true if the search execution should skip this shard since it can not match any documents given the query.",
	"Method": "boolean skip(){\r\n    return skip;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.learning.AbstractBatchOptimizer.addDenseConstraint",
	"Comment": "this adds a constraint on the weight vector, that a certain component must be set to a dense array",
	"Method": "void addDenseConstraint(int component,double[] arr){\r\n    constraints.add(new Constraint(component, arr));\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.pascal.ISODateInstance.isDayCompatible",
	"Comment": "looks if the days for the two dates are compatible.this method does not consider ranges and uses only thestart date.",
	"Method": "boolean isDayCompatible(String date1,String date2,boolean isDayCompatible,ISODateInstance other){\r\n    if (this.isUnparseable() || other.isUnparseable()) {\r\n        return this.isoDate.equals(other.isoDate);\r\n    }\r\n    return isDayCompatible(isoDate, other.getDateString());\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.ReindexRequest.setSourceDocTypes",
	"Comment": "set the document types which need to be copied from the source indices",
	"Method": "ReindexRequest setSourceDocTypes(String docTypes){\r\n    if (docTypes != null) {\r\n        this.getSearchRequest().types(docTypes);\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.ReaderIteratorFactory.iterator",
	"Comment": "returns an iterator over the input sources in the underlying collection.",
	"Method": "Iterator<Reader> iterator(){\r\n    return new ReaderIterator();\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.internal.Annotations.isRetainedAtRuntime",
	"Comment": "returns true if the given annotation is retained at runtime.",
	"Method": "boolean isRetainedAtRuntime(Class<? extends Annotation> annotationType){\r\n    Retention retention = annotationType.getAnnotation(Retention.class);\r\n    return retention != null && retention.value() == RetentionPolicy.RUNTIME;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.SentenceUtils.listToString",
	"Comment": "pretty print coremap classes using the same semantics as the toshorterstring method.",
	"Method": "String listToString(List<T> list,String listToString,List<T> list,boolean justValue,String listToString,List<T> list,boolean justValue,String separator,String listToString,List<T> list,String keys){\r\n    StringBuilder s = new StringBuilder();\r\n    for (Iterator<T> wordIterator = list.iterator(); wordIterator.hasNext(); ) {\r\n        T o = wordIterator.next();\r\n        s.append(o.toShorterString(keys));\r\n        if (wordIterator.hasNext()) {\r\n            s.append(' ');\r\n        }\r\n    }\r\n    return s.toString();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.IndexMetaData.selectSplitShard",
	"Comment": "returns the source shard id to split the given target shard off",
	"Method": "ShardId selectSplitShard(int shardId,IndexMetaData sourceIndexMetadata,int numTargetShards){\r\n    int numSourceShards = sourceIndexMetadata.getNumberOfShards();\r\n    if (shardId >= numTargetShards) {\r\n        throw new IllegalArgumentException(\"the number of target shards (\" + numTargetShards + \") must be greater than the shard id: \" + shardId);\r\n    }\r\n    final int routingFactor = getRoutingFactor(numSourceShards, numTargetShards);\r\n    assertSplitMetadata(numSourceShards, numTargetShards, sourceIndexMetadata);\r\n    return new ShardId(sourceIndexMetadata.getIndex(), shardId / routingFactor);\r\n}"
}, {
	"Path": "org.apache.dubbo.remoting.exchange.support.DefaultFuture.closeChannel",
	"Comment": "close a channel when a channel is inactivedirectly return the unfinished requests.",
	"Method": "void closeChannel(Channel channel){\r\n    for (long id : CHANNELS.keySet()) {\r\n        if (channel.equals(CHANNELS.get(id))) {\r\n            DefaultFuture future = getFuture(id);\r\n            if (future != null && !future.isDone()) {\r\n                Response disconnectResponse = new Response(future.getId());\r\n                disconnectResponse.setStatus(Response.CHANNEL_INACTIVE);\r\n                disconnectResponse.setErrorMessage(\"Channel \" + channel + \" is inactive. Directly return the unFinished request : \" + future.getRequest());\r\n                DefaultFuture.received(channel, disconnectResponse);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.NumberMatchingRegex.isDouble",
	"Comment": "returns true if the number can be successfully parsed by double.locale specific to english and ascii numerals.",
	"Method": "boolean isDouble(String string){\r\n    return (fpPattern.matcher(string).matches());\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.metrics.EvaluationMetric.numRelevantExamples",
	"Comment": "a convenience method that returns the number of true positive examples fromamong the test instances. mathematically, this value is the denominator of the recall.",
	"Method": "double numRelevantExamples(){\r\n    return rnums2;\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.NumberRangesFileFilter.accept",
	"Comment": "checks whether a string satisfies the number range selection filter.the test is evaluated based on the rightmost natural number found inthe string. note that this is just evaluated on the string as given.it is not trying to interpret it as a filename and to decide whetherthe file exists, is a directory or anything like that.",
	"Method": "boolean accept(File file,boolean accept,String str){\r\n    int k = str.length() - 1;\r\n    char c = str.charAt(k);\r\n    while (k >= 0 && !Character.isDigit(c)) {\r\n        k--;\r\n        if (k >= 0) {\r\n            c = str.charAt(k);\r\n        }\r\n    }\r\n    if (k < 0) {\r\n        return false;\r\n    }\r\n    int j = k;\r\n    c = str.charAt(j);\r\n    while (j >= 0 && Character.isDigit(c)) {\r\n        j--;\r\n        if (j >= 0) {\r\n            c = str.charAt(j);\r\n        }\r\n    }\r\n    j++;\r\n    k++;\r\n    String theNumber = str.substring(j, k);\r\n    int number = Integer.parseInt(theNumber);\r\n    for (Pair<Integer, Integer> p : ranges) {\r\n        int low = p.first().intValue();\r\n        int high = p.second().intValue();\r\n        if (number >= low && number <= high) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotRequest.partial",
	"Comment": "set to true to allow indices with unavailable shards to be partially snapshotted.",
	"Method": "boolean partial(CreateSnapshotRequest partial,boolean partial){\r\n    this.partial = partial;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MatchPhraseQueryBuilder.analyzer",
	"Comment": "explicitly set the analyzer to use. defaults to use explicit mappingconfig for the field, or, if not set, the default search analyzer.",
	"Method": "MatchPhraseQueryBuilder analyzer(String analyzer,String analyzer){\r\n    return this.analyzer;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequestBuilder.setAliases",
	"Comment": "sets the aliases that will be associated with the index when it gets created",
	"Method": "PutIndexTemplateRequestBuilder setAliases(Map<String, Object> source,PutIndexTemplateRequestBuilder setAliases,String source,PutIndexTemplateRequestBuilder setAliases,XContentBuilder source,PutIndexTemplateRequestBuilder setAliases,BytesReference source){\r\n    request.aliases(source);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.Store.cleanupAndVerify",
	"Comment": "this method deletes every file in this store that is not contained in the given source meta data or is alegacy checksum file. after the delete it pulls the latest metadata snapshot from the store and compares itto the given snapshot. if the snapshots are inconsistent an illegal state exception is thrown.",
	"Method": "void cleanupAndVerify(String reason,MetadataSnapshot sourceMetaData){\r\n    metadataLock.writeLock().lock();\r\n    try (Lock writeLock = directory.obtainLock(IndexWriter.WRITE_LOCK_NAME)) {\r\n        for (String existingFile : directory.listAll()) {\r\n            if (Store.isAutogenerated(existingFile) || sourceMetaData.contains(existingFile)) {\r\n                continue;\r\n            }\r\n            try {\r\n                directory.deleteFile(reason, existingFile);\r\n            } catch (IOException ex) {\r\n                if (existingFile.startsWith(IndexFileNames.SEGMENTS) || existingFile.equals(IndexFileNames.OLD_SEGMENTS_GEN) || existingFile.startsWith(CORRUPTED)) {\r\n                    throw new IllegalStateException(\"Can't delete \" + existingFile + \" - cleanup failed\", ex);\r\n                }\r\n                logger.debug(() -> new ParameterizedMessage(\"failed to delete file [{}]\", existingFile), ex);\r\n            }\r\n        }\r\n        directory.syncMetaData();\r\n        final Store.MetadataSnapshot metadataOrEmpty = getMetadata(null);\r\n        verifyAfterCleanup(sourceMetaData, metadataOrEmpty);\r\n    } finally {\r\n        metadataLock.writeLock().unlock();\r\n    }\r\n}"
}, {
	"Path": "java.io.DataOutputStream.size",
	"Comment": "returns the total number of bytes written to the target stream so far.",
	"Method": "int size(){\r\n    if (written < 0) {\r\n        written = Integer.MAX_VALUE;\r\n    }\r\n    return written;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.getExpectedShardSize",
	"Comment": "returns the expected shard size for the given shard or the default value provided if not enough information are availableto estimate the shards size.",
	"Method": "long getExpectedShardSize(ShardRouting shard,RoutingAllocation allocation,long defaultValue){\r\n    final IndexMetaData metaData = allocation.metaData().getIndexSafe(shard.index());\r\n    final ClusterInfo info = allocation.clusterInfo();\r\n    if (metaData.getResizeSourceIndex() != null && shard.active() == false && shard.recoverySource().getType() == RecoverySource.Type.LOCAL_SHARDS) {\r\n        long targetShardSize = 0;\r\n        final Index mergeSourceIndex = metaData.getResizeSourceIndex();\r\n        final IndexMetaData sourceIndexMeta = allocation.metaData().index(mergeSourceIndex);\r\n        if (sourceIndexMeta != null) {\r\n            final Set<ShardId> shardIds = IndexMetaData.selectRecoverFromShards(shard.id(), sourceIndexMeta, metaData.getNumberOfShards());\r\n            for (IndexShardRoutingTable shardRoutingTable : allocation.routingTable().index(mergeSourceIndex.getName())) {\r\n                if (shardIds.contains(shardRoutingTable.shardId())) {\r\n                    targetShardSize += info.getShardSize(shardRoutingTable.primaryShard(), 0);\r\n                }\r\n            }\r\n        }\r\n        return targetShardSize == 0 ? defaultValue : targetShardSize;\r\n    } else {\r\n        return info.getShardSize(shard, defaultValue);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.recycler.DequeRecycler.beforeRelease",
	"Comment": "called before releasing an object, returns true if the object should be recycled and false otherwise.",
	"Method": "boolean beforeRelease(){\r\n    return deque.size() < maxSize;\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.arabic.process.ArabicLexer.yytext",
	"Comment": "returns the text matched by the current regular expression.",
	"Method": "String yytext(){\r\n    return new String(zzBuffer, zzStartRead, zzMarkedPos - zzStartRead);\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.ClauseSplitterSearchProblem.topClauses",
	"Comment": "get the top few clauses from this searcher, cutting off at the given minimumprobability.",
	"Method": "List<SentenceFragment> topClauses(double thresholdProbability,int maxClauses){\r\n    List<SentenceFragment> results = new ArrayList();\r\n    search(triple -> {\r\n        assert triple.first <= 0.0;\r\n        double prob = Math.exp(triple.first);\r\n        assert prob <= 1.0;\r\n        assert prob >= 0.0;\r\n        assert !Double.isNaN(prob);\r\n        if (prob >= thresholdProbability) {\r\n            SentenceFragment fragment = triple.third.get();\r\n            fragment.score = prob;\r\n            results.add(fragment);\r\n            return true;\r\n        } else {\r\n            return false;\r\n        }\r\n    });\r\n    return results;\r\n}"
}, {
	"Path": "org.openqa.selenium.ReferrerTest.crossDomainHistoryNavigationWithADirectProxy",
	"Comment": "tests navigation across multiple domains when the browser is configured to use a proxy thatpermits direct access to those domains.",
	"Method": "void crossDomainHistoryNavigationWithADirectProxy(){\r\n    testServer1.start();\r\n    testServer2.start();\r\n    pacFileServer.setPacFileContents(\"function FindProxyForURL(url, host) { return 'DIRECT'; }\");\r\n    pacFileServer.start();\r\n    WebDriver driver = customDriverFactory.createDriver(pacFileServer.getBaseUrl());\r\n    String page1Url = buildPage1Url(testServer1, buildPage2Url(testServer2));\r\n    String page2Url = buildPage2Url(testServer2, buildPage3Url(testServer1));\r\n    String page3Url = buildPage3Url(testServer1);\r\n    performNavigation(driver, page1Url);\r\n    assertThat(testServer1.getRequests()).isEqualTo(ImmutableList.of(new HttpRequest(page1Url, null), new HttpRequest(page3Url, page2Url)));\r\n    assertThat(testServer2.getRequests()).isEqualTo(ImmutableList.of(new HttpRequest(page2Url, page1Url)));\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.ActionPlugin.getTaskHeaders",
	"Comment": "returns headers which should be copied from internal requests into tasks.",
	"Method": "Collection<String> getTaskHeaders(){\r\n    return Collections.emptyList();\r\n}"
}, {
	"Path": "android.util.SparseArray.get",
	"Comment": "gets the object mapped from the specified key, or the specified objectif no such mapping has been made.",
	"Method": "E get(int key,E get,int key,E valueIfKeyNotFound){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i < 0 || mValues[i] == DELETED) {\r\n        return valueIfKeyNotFound;\r\n    } else {\r\n        return (E) mValues[i];\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.XMoreLikeThis.setMaxQueryTerms",
	"Comment": "sets the maximum number of query terms that will be included in any generated query.",
	"Method": "void setMaxQueryTerms(int maxQueryTerms){\r\n    this.maxQueryTerms = maxQueryTerms;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.BoolQueryBuilder.should",
	"Comment": "gets the list of clauses that should be matched by the returned documents.",
	"Method": "BoolQueryBuilder should(QueryBuilder queryBuilder,List<QueryBuilder> should){\r\n    return this.shouldClauses;\r\n}"
}, {
	"Path": "org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder.maxDoc",
	"Comment": "returns the maximum document id this builder can associate with an ordinal",
	"Method": "int maxDoc(){\r\n    return maxDoc;\r\n}"
}, {
	"Path": "android.os.SystemClock.setCurrentTimeMillis",
	"Comment": "sets the current wall time, in milliseconds.requires the callingprocess to have appropriate permissions.",
	"Method": "boolean setCurrentTimeMillis(long millis){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexService.getShardOrNull",
	"Comment": "return the shard with the provided id, or null if there is no such shard.",
	"Method": "IndexShard getShardOrNull(int shardId){\r\n    return shards.get(shardId);\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksClusterService.persistentTasksChanged",
	"Comment": "returns true if the persistent tasks are not equal between the previous and the current cluster state",
	"Method": "boolean persistentTasksChanged(ClusterChangedEvent event){\r\n    String type = PersistentTasksCustomMetaData.TYPE;\r\n    return Objects.equals(event.state().metaData().custom(type), event.previousState().metaData().custom(type)) == false;\r\n}"
}, {
	"Path": "org.elasticsearch.env.NodeEnvironment.loadOrCreateNodeMetaData",
	"Comment": "scans the node paths and loads existing metadata file. if not found a new meta data will be generatedand persisted into the nodepaths",
	"Method": "NodeMetaData loadOrCreateNodeMetaData(Settings settings,Logger logger,NodePath nodePaths){\r\n    final Path[] paths = Arrays.stream(nodePaths).map(np -> np.path).toArray(Path[]::new);\r\n    NodeMetaData metaData = NodeMetaData.FORMAT.loadLatestState(logger, NamedXContentRegistry.EMPTY, paths);\r\n    if (metaData == null) {\r\n        metaData = new NodeMetaData(generateNodeId(settings));\r\n    }\r\n    NodeMetaData.FORMAT.write(metaData, paths);\r\n    return metaData;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.RoutingAllocation.getCurrentNanoTime",
	"Comment": "returns the nano time captured at the beginning of the allocation. used to make sure all time based decisions are aligned",
	"Method": "long getCurrentNanoTime(){\r\n    return currentNanoTime;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.GroupShardsIterator.totalSizeWith1ForEmpty",
	"Comment": "returns the total number of shards plus the number of empty groups",
	"Method": "int totalSizeWith1ForEmpty(){\r\n    int size = 0;\r\n    for (ShardIt shard : iterators) {\r\n        size += Math.max(1, shard.size());\r\n    }\r\n    return size;\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.NetworkUtils.sortAddresses",
	"Comment": "sorts addresses by order of preference. this is used to pick the first one for publishing",
	"Method": "void sortAddresses(List<InetAddress> list){\r\n    Collections.sort(list, new Comparator<InetAddress>() {\r\n        @Override\r\n        public int compare(InetAddress left, InetAddress right) {\r\n            int cmp = Integer.compare(sortKey(left, PREFER_V6), sortKey(right, PREFER_V6));\r\n            if (cmp == 0) {\r\n                cmp = new BytesRef(left.getAddress()).compareTo(new BytesRef(right.getAddress()));\r\n            }\r\n            return cmp;\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.NetworkUtils.sortAddresses",
	"Comment": "sorts addresses by order of preference. this is used to pick the first one for publishing",
	"Method": "void sortAddresses(List<InetAddress> list){\r\n    int cmp = Integer.compare(sortKey(left, PREFER_V6), sortKey(right, PREFER_V6));\r\n    if (cmp == 0) {\r\n        cmp = new BytesRef(left.getAddress()).compareTo(new BytesRef(right.getAddress()));\r\n    }\r\n    return cmp;\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.Util.dumpAccuracy",
	"Comment": "a helper function for dumping the accuracy of the trained classifier.",
	"Method": "void dumpAccuracy(Classifier<ClauseSplitter.ClauseClassifierLabel, String> classifier,GeneralDataset<ClauseSplitter.ClauseClassifierLabel, String> dataset){\r\n    DecimalFormat df = new DecimalFormat(\"0.00%\");\r\n    log(\"size:         \" + dataset.size());\r\n    log(\"split count:  \" + StreamSupport.stream(dataset.spliterator(), false).filter(x -> x.label() == ClauseSplitter.ClauseClassifierLabel.CLAUSE_SPLIT).collect(Collectors.toList()).size());\r\n    log(\"interm count: \" + StreamSupport.stream(dataset.spliterator(), false).filter(x -> x.label() == ClauseSplitter.ClauseClassifierLabel.CLAUSE_INTERM).collect(Collectors.toList()).size());\r\n    Pair<Double, Double> pr = classifier.evaluatePrecisionAndRecall(dataset, ClauseSplitter.ClauseClassifierLabel.CLAUSE_SPLIT);\r\n    log(\"p  (split):   \" + df.format(pr.first));\r\n    log(\"r  (split):   \" + df.format(pr.second));\r\n    log(\"f1 (split):   \" + df.format(2 * pr.first * pr.second / (pr.first + pr.second)));\r\n    pr = classifier.evaluatePrecisionAndRecall(dataset, ClauseSplitter.ClauseClassifierLabel.CLAUSE_INTERM);\r\n    log(\"p  (interm):  \" + df.format(pr.first));\r\n    log(\"r  (interm):  \" + df.format(pr.second));\r\n    log(\"f1 (interm):  \" + df.format(2 * pr.first * pr.second / (pr.first + pr.second)));\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequestBuilder.setFieldStatistics",
	"Comment": "sets whether to return the field statistics for each term in the shard or skip.",
	"Method": "TermVectorsRequestBuilder setFieldStatistics(boolean fieldStatistics){\r\n    request.fieldStatistics(fieldStatistics);\r\n    return this;\r\n}"
}, {
	"Path": "org.apache.harmony.tests.javax.xml.parsers.SAXParserTestSupport.equalsMaps",
	"Comment": "compares the content of two hashmaps. one map should be the referencecontaining the correct string for each xml document element and the othershould contain the elements filled with output from the parser.",
	"Method": "boolean equalsMaps(HashMap<String, String> original,HashMap<String, String> result){\r\n    if (original == null && result == null) {\r\n        return true;\r\n    } else {\r\n        if (original.size() != result.size())\r\n            return false;\r\n        for (int i = 0; i < KEYS.length; i++) {\r\n            if (!original.get(KEYS[i]).equals(result.get(KEYS[i]))) {\r\n                System.out.println(\"for \" + KEYS[i] + \": original:\" + original.get(KEYS[i]));\r\n                System.out.println();\r\n                System.out.println(\"  result:\" + result.get(KEYS[i]));\r\n                System.out.println();\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.NeuralUtils.loadTextMatrices",
	"Comment": "convert a file into a list of matrices. the expected format is one rowper line, one entry per column for each matrix, with each matrix separatedby an empty line.",
	"Method": "List<SimpleMatrix> loadTextMatrices(String path){\r\n    List<SimpleMatrix> matrices = new ArrayList();\r\n    for (String mString : IOUtils.stringFromFile(path).trim().split(\"\\n\\n\")) {\r\n        matrices.add(NeuralUtils.convertTextMatrix(mString).transpose());\r\n    }\r\n    return matrices;\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.DanglingIndicesState.cleanupAllocatedDangledIndices",
	"Comment": "cleans dangling indices if they are already allocated on the provided meta data.",
	"Method": "void cleanupAllocatedDangledIndices(MetaData metaData){\r\n    for (Index index : danglingIndices.keySet()) {\r\n        final IndexMetaData indexMetaData = metaData.index(index);\r\n        if (indexMetaData != null && indexMetaData.getIndex().getName().equals(index.getName())) {\r\n            if (indexMetaData.getIndex().getUUID().equals(index.getUUID()) == false) {\r\n                logger.warn(\"[{}] can not be imported as a dangling index, as there is already another index \" + \"with the same name but a different uuid. local index will be ignored (but not deleted)\", index);\r\n            } else {\r\n                logger.debug(\"[{}] no longer dangling (created), removing from dangling list\", index);\r\n            }\r\n            danglingIndices.remove(index);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.BaseTranslogReader.getPrimaryTerm",
	"Comment": "returns the primary term associated with this translog reader.",
	"Method": "long getPrimaryTerm(){\r\n    return header.getPrimaryTerm();\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVector.mapInPlace",
	"Comment": "apply a function to every element of every component of this vector, and replace with the result.",
	"Method": "void mapInPlace(DoubleUnaryOperator fn){\r\n    for (int i = 0; i < pointers.length; i++) {\r\n        if (pointers[i] == null)\r\n            continue;\r\n        if (copyOnWrite[i]) {\r\n            copyOnWrite[i] = false;\r\n            pointers[i] = pointers[i].clone();\r\n        }\r\n        if (sparse[i]) {\r\n            pointers[i][1] = fn.applyAsDouble(pointers[i][1]);\r\n        } else {\r\n            for (int j = 0; j < pointers[i].length; j++) {\r\n                pointers[i][j] = fn.applyAsDouble(pointers[i][j]);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.BaseUnknownWordModel.addTagging",
	"Comment": "adds the tagging with count to the data structures in this lexicon.",
	"Method": "void addTagging(boolean seen,IntTaggedWord itw,double count){\r\n    if (seen) {\r\n        log.info(\"UWM.addTagging: Shouldn't call with seen word!\");\r\n    } else {\r\n        unSeenCounter.incrementCount(itw, count);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MultiMatchQueryBuilder.analyzer",
	"Comment": "explicitly set the analyzer to use. defaults to use explicit mapping config for the field, or, if notset, the default search analyzer.",
	"Method": "MultiMatchQueryBuilder analyzer(String analyzer,String analyzer){\r\n    return analyzer;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotRequestBuilder.setPartial",
	"Comment": "if set to true the request should snapshot indices with unavailable shards",
	"Method": "CreateSnapshotRequestBuilder setPartial(boolean partial){\r\n    request.partial(partial);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.Engine.isRecovering",
	"Comment": "returns true iff this engine is currently recovering from translog.",
	"Method": "boolean isRecovering(){\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.SimpleQueryStringBuilder.lenient",
	"Comment": "specifies whether query parsing should be lenient. defaults to false.",
	"Method": "SimpleQueryStringBuilder lenient(boolean lenient,boolean lenient){\r\n    return this.settings.lenient();\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksClusterService.completePersistentTask",
	"Comment": "restarts a record about a running persistent task from cluster state",
	"Method": "void completePersistentTask(String id,long allocationId,Exception failure,ActionListener<PersistentTask<?>> listener){\r\n    final String source;\r\n    if (failure != null) {\r\n        logger.warn(\"persistent task \" + id + \" failed\", failure);\r\n        source = \"finish persistent task (failed)\";\r\n    } else {\r\n        source = \"finish persistent task (success)\";\r\n    }\r\n    clusterService.submitStateUpdateTask(source, new ClusterStateUpdateTask() {\r\n        @Override\r\n        public ClusterState execute(ClusterState currentState) {\r\n            PersistentTasksCustomMetaData.Builder tasksInProgress = builder(currentState);\r\n            if (tasksInProgress.hasTask(id, allocationId)) {\r\n                tasksInProgress.removeTask(id);\r\n                return update(currentState, tasksInProgress);\r\n            } else {\r\n                if (tasksInProgress.hasTask(id)) {\r\n                    logger.warn(\"The task [{}] with id [{}] was found but it has a different allocation id [{}], status is not updated\", PersistentTasksCustomMetaData.getTaskWithId(currentState, id).getTaskName(), id, allocationId);\r\n                } else {\r\n                    logger.warn(\"The task [{}] wasn't found, status is not updated\", id);\r\n                }\r\n                throw new ResourceNotFoundException(\"the task with id [\" + id + \"] and allocation id [\" + allocationId + \"] not found\");\r\n            }\r\n        }\r\n        @Override\r\n        public void onFailure(String source, Exception e) {\r\n            listener.onFailure(e);\r\n        }\r\n        @Override\r\n        public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {\r\n            listener.onResponse(PersistentTasksCustomMetaData.getTaskWithId(oldState, id));\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksClusterService.completePersistentTask",
	"Comment": "restarts a record about a running persistent task from cluster state",
	"Method": "void completePersistentTask(String id,long allocationId,Exception failure,ActionListener<PersistentTask<?>> listener){\r\n    PersistentTasksCustomMetaData.Builder tasksInProgress = builder(currentState);\r\n    if (tasksInProgress.hasTask(id, allocationId)) {\r\n        tasksInProgress.removeTask(id);\r\n        return update(currentState, tasksInProgress);\r\n    } else {\r\n        if (tasksInProgress.hasTask(id)) {\r\n            logger.warn(\"The task [{}] with id [{}] was found but it has a different allocation id [{}], status is not updated\", PersistentTasksCustomMetaData.getTaskWithId(currentState, id).getTaskName(), id, allocationId);\r\n        } else {\r\n            logger.warn(\"The task [{}] wasn't found, status is not updated\", id);\r\n        }\r\n        throw new ResourceNotFoundException(\"the task with id [\" + id + \"] and allocation id [\" + allocationId + \"] not found\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksClusterService.completePersistentTask",
	"Comment": "restarts a record about a running persistent task from cluster state",
	"Method": "void completePersistentTask(String id,long allocationId,Exception failure,ActionListener<PersistentTask<?>> listener){\r\n    listener.onFailure(e);\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksClusterService.completePersistentTask",
	"Comment": "restarts a record about a running persistent task from cluster state",
	"Method": "void completePersistentTask(String id,long allocationId,Exception failure,ActionListener<PersistentTask<?>> listener){\r\n    listener.onResponse(PersistentTasksCustomMetaData.getTaskWithId(oldState, id));\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocator.buildNodeDecisions",
	"Comment": "builds a map of nodes to the corresponding allocation decisions for those nodes.",
	"Method": "List<NodeAllocationResult> buildNodeDecisions(NodesToAllocate nodesToAllocate,FetchResult<NodeGatewayStartedShards> fetchedShardData,Set<String> inSyncAllocationIds){\r\n    List<NodeAllocationResult> nodeResults = new ArrayList();\r\n    Collection<NodeGatewayStartedShards> ineligibleShards;\r\n    if (nodesToAllocate != null) {\r\n        final Set<DiscoveryNode> discoNodes = new HashSet();\r\n        nodeResults.addAll(Stream.of(nodesToAllocate.yesNodeShards, nodesToAllocate.throttleNodeShards, nodesToAllocate.noNodeShards).flatMap(Collection::stream).map(dnode -> {\r\n            discoNodes.add(dnode.nodeShardState.getNode());\r\n            return new NodeAllocationResult(dnode.nodeShardState.getNode(), shardStoreInfo(dnode.nodeShardState, inSyncAllocationIds), dnode.decision);\r\n        }).collect(Collectors.toList()));\r\n        ineligibleShards = fetchedShardData.getData().values().stream().filter(shardData -> discoNodes.contains(shardData.getNode()) == false).collect(Collectors.toList());\r\n    } else {\r\n        ineligibleShards = fetchedShardData.getData().values();\r\n    }\r\n    nodeResults.addAll(ineligibleShards.stream().map(shardData -> new NodeAllocationResult(shardData.getNode(), shardStoreInfo(shardData, inSyncAllocationIds), null)).collect(Collectors.toList()));\r\n    return nodeResults;\r\n}"
}, {
	"Path": "org.elasticsearch.node.AdaptiveSelectionStats.getOutgoingConnections",
	"Comment": "returns a map of node id to the outgoing search requests to that node",
	"Method": "Map<String, Long> getOutgoingConnections(){\r\n    return clientOutgoingConnections;\r\n}"
}, {
	"Path": "org.openqa.selenium.TakesScreenshotTest.saveImageToTmpFile",
	"Comment": "simple helper to save screenshot to tmp file. for debug purposes.",
	"Method": "void saveImageToTmpFile(BufferedImage im){\r\n    File outputfile = new File(testName.getMethodName() + \"_image.png\");\r\n    System.out.println(\"Image file is at \" + outputfile.getAbsolutePath());\r\n    System.out.println(\"Sizes  -> \" + im.getWidth() + \"x\" + im.getHeight());\r\n    try {\r\n        ImageIO.write(im, \"png\", outputfile);\r\n    } catch (IOException e) {\r\n        fail(\"Unable to write image to file: \" + e.getMessage());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractBulkByScrollRequestBuilder.filter",
	"Comment": "set the query that will filter the source. just a convenience method foreasy chaining.",
	"Method": "Self filter(QueryBuilder filter){\r\n    source.setQuery(filter);\r\n    return self();\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.spanish.SpanishVerbStripper.isStrippable",
	"Comment": "determine if the given word is a verb which needs to be stripped.",
	"Method": "boolean isStrippable(String word){\r\n    return pStrippable.matcher(word).find() || pIrregulars.matcher(word).find();\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.ClauseSplitterSearchProblem.mockNode",
	"Comment": "create a mock node, to be added to the dependency tree but which is not part of the original sentence.",
	"Method": "CoreLabel mockNode(CoreLabel toCopy,String word,String POS){\r\n    CoreLabel mock = new CoreLabel(toCopy);\r\n    mock.setWord(word);\r\n    mock.setLemma(word);\r\n    mock.setValue(word);\r\n    mock.setNER(\"O\");\r\n    mock.setTag(POS);\r\n    mock.setIndex(sentenceLength + 5);\r\n    return mock;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.ArabicTreebankParserParams.lex",
	"Comment": "returns a lexicon for arabic.at the moment this is just a baselexicon.",
	"Method": "Lexicon lex(Options op,Index<String> wordIndex,Index<String> tagIndex){\r\n    if (op.lexOptions.uwModelTrainer == null) {\r\n        op.lexOptions.uwModelTrainer = \"edu.stanford.nlp.parser.lexparser.ArabicUnknownWordModelTrainer\";\r\n    }\r\n    if (morphoSpec != null) {\r\n        return new FactoredLexicon(op, morphoSpec, wordIndex, tagIndex);\r\n    }\r\n    return new BaseLexicon(op, wordIndex, tagIndex);\r\n}"
}, {
	"Path": "org.elasticsearch.ExceptionsHelper.maybeThrowRuntimeAndSuppress",
	"Comment": "throws a runtime exception with all given exceptions added as suppressed.if the given list is empty no exception is thrown",
	"Method": "void maybeThrowRuntimeAndSuppress(List<T> exceptions){\r\n    T main = null;\r\n    for (T ex : exceptions) {\r\n        main = useOrSuppress(main, ex);\r\n    }\r\n    if (main != null) {\r\n        throw new ElasticsearchException(main);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.RefreshListeners.fireListeners",
	"Comment": "fire some listeners. does nothing if the list of listeners is null.",
	"Method": "void fireListeners(List<Tuple<Translog.Location, Consumer<Boolean>>> listenersToFire){\r\n    if (listenersToFire != null) {\r\n        listenerExecutor.execute(() -> {\r\n            for (Tuple<Translog.Location, Consumer<Boolean>> listener : listenersToFire) {\r\n                try {\r\n                    listener.v2().accept(false);\r\n                } catch (Exception e) {\r\n                    logger.warn(\"Error firing refresh listener\", e);\r\n                }\r\n            }\r\n        });\r\n    }\r\n}"
}, {
	"Path": "android.text.SpannableStringBuilder.getChars",
	"Comment": "copy the specified range of chars from this buffer into thespecified array, beginning at the specified offset.",
	"Method": "void getChars(int start,int end,char[] dest,int destoff){\r\n    checkRange(\"getChars\", start, end);\r\n    if (end <= mGapStart) {\r\n        System.arraycopy(mText, start, dest, destoff, end - start);\r\n    } else if (start >= mGapStart) {\r\n        System.arraycopy(mText, start + mGapLength, dest, destoff, end - start);\r\n    } else {\r\n        System.arraycopy(mText, start, dest, destoff, mGapStart - start);\r\n        System.arraycopy(mText, mGapStart + mGapLength, dest, destoff + (mGapStart - start), end - mGapStart);\r\n    }\r\n}"
}, {
	"Path": "com.android.internal.util.ArrayUtils.appendElement",
	"Comment": "appends an element to a copy of the array and returns the copy.",
	"Method": "T[] appendElement(Class<T> kind,T[] array,T element){\r\n    final T[] result;\r\n    final int end;\r\n    if (array != null) {\r\n        end = array.length;\r\n        result = (T[]) Array.newInstance(kind, end + 1);\r\n        System.arraycopy(array, 0, result, 0, end);\r\n    } else {\r\n        end = 0;\r\n        result = (T[]) Array.newInstance(kind, 1);\r\n    }\r\n    result[end] = element;\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.files",
	"Comment": "returns an iterable that allows to iterate over all files in this segments info",
	"Method": "Iterable<String> files(SegmentInfos infos){\r\n    final List<Collection<String>> list = new ArrayList();\r\n    list.add(Collections.singleton(infos.getSegmentsFileName()));\r\n    for (SegmentCommitInfo info : infos) {\r\n        list.add(info.files());\r\n    }\r\n    return Iterables.flatten(list);\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequest.doc",
	"Comment": "sets the doc to use for updates when a script is not specified, the doc providedis a field and value pairs.",
	"Method": "UpdateRequest doc(IndexRequest doc,UpdateRequest doc,XContentBuilder source,UpdateRequest doc,Map<String, Object> source,UpdateRequest doc,Map<String, Object> source,XContentType contentType,UpdateRequest doc,String source,XContentType xContentType,UpdateRequest doc,byte[] source,XContentType xContentType,UpdateRequest doc,byte[] source,int offset,int length,XContentType xContentType,UpdateRequest doc,Object source,UpdateRequest doc,XContentType xContentType,Object source,IndexRequest doc){\r\n    return this.doc;\r\n}"
}, {
	"Path": "org.elasticsearch.common.cache.Cache.get",
	"Comment": "returns the value to which the specified key is mapped, or null if this map contains no mapping for the key.",
	"Method": "Entry<K, V> get(K key,long now,Predicate<Entry<K, V>> isExpired,Consumer<Entry<K, V>> onExpiration,V get,K key,V get,K key,long now,Consumer<Entry<K, V>> onExpiration){\r\n    CacheSegment<K, V> segment = getCacheSegment(key);\r\n    Entry<K, V> entry = segment.get(key, now, e -> isExpired(e, now), onExpiration);\r\n    if (entry == null) {\r\n        return null;\r\n    } else {\r\n        promote(entry, now);\r\n        return entry.value;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.RerankingParserQuery.parseUnparsable",
	"Comment": "the model could not parse the most recent sentence for some reason",
	"Method": "boolean parseUnparsable(){\r\n    return parserQuery.parseUnparsable();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.awaitShardSearchActive",
	"Comment": "registers the given listener and invokes it once the shard is active again and allpending refresh translog location has been refreshed. if there is no pending refresh location registered the listener will beinvoked immediately.",
	"Method": "void awaitShardSearchActive(Consumer<Boolean> listener){\r\n    if (isSearchIdle()) {\r\n        markSearcherAccessed();\r\n    }\r\n    final Translog.Location location = pendingRefreshLocation.get();\r\n    if (location != null) {\r\n        addRefreshListener(location, (b) -> {\r\n            pendingRefreshLocation.compareAndSet(location, null);\r\n            listener.accept(true);\r\n        });\r\n    } else {\r\n        listener.accept(false);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.MultiPatternMatcher.findNonOverlappingMaxScore",
	"Comment": "given a sequence, applies our patterns over the sequence and returnsall non overlapping matches.when multiple patterns overlaps,matched patterns are selected to give the overall maximum score.",
	"Method": "List<SequenceMatchResult<T>> findNonOverlappingMaxScore(List<? extends T> elements,List<SequenceMatchResult<T>> findNonOverlappingMaxScore,List<? extends T> elements,ToDoubleFunction<? super SequenceMatchResult> scorer){\r\n    Collection<SequencePattern<T>> triggered = getTriggeredPatterns(elements);\r\n    List<SequenceMatchResult<T>> all = new ArrayList();\r\n    int i = 0;\r\n    for (SequencePattern<T> p : triggered) {\r\n        SequenceMatcher<T> m = p.getMatcher(elements);\r\n        m.setMatchWithResult(matchWithResult);\r\n        m.setOrder(i);\r\n        while (m.find()) {\r\n            all.add(m.toBasicSequenceMatchResult());\r\n        }\r\n        i++;\r\n    }\r\n    List<SequenceMatchResult<T>> res = IntervalTree.getNonOverlappingMaxScore(all, SequenceMatchResult.TO_INTERVAL, scorer);\r\n    res.sort(SequenceMatchResult.OFFSET_COMPARATOR);\r\n    return res;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.pascal.TeXHyphenator.loadDefault",
	"Comment": "loads the default hyphenation rules in defaulttexhyphenator.",
	"Method": "void loadDefault(){\r\n    try {\r\n        load(new BufferedReader(new StringReader(DefaultTeXHyphenData.hyphenData)));\r\n    } catch (IOException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.time.DateMathParser.parse",
	"Comment": "parse a date math expression without timzeone info and rounding down.",
	"Method": "long parse(String text,LongSupplier now,long parse,String text,LongSupplier now,boolean roundUp,DateTimeZone tz,long parse,String text,LongSupplier now,boolean roundUp,ZoneId tz){\r\n    return parse(text, now, roundUp, tz == null ? null : ZoneId.of(tz.getID()));\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.ReaderIteratorFactory.removeAll",
	"Comment": "removes all objects in collection c from the underlying collection ofinput sources.",
	"Method": "boolean removeAll(Collection<?> c){\r\n    return this.c.removeAll(c);\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.ReindexRequest.setSourceQuery",
	"Comment": "set the query for selecting documents from the source indices",
	"Method": "ReindexRequest setSourceQuery(QueryBuilder queryBuilder){\r\n    if (queryBuilder != null) {\r\n        this.getSearchRequest().source().query(queryBuilder);\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "android.text.TextUtils.isGraphic",
	"Comment": "returns whether the given charsequence contains any printable characters.",
	"Method": "boolean isGraphic(CharSequence str,boolean isGraphic,char c){\r\n    int gc = Character.getType(c);\r\n    return gc != Character.CONTROL && gc != Character.FORMAT && gc != Character.SURROGATE && gc != Character.UNASSIGNED && gc != Character.LINE_SEPARATOR && gc != Character.PARAGRAPH_SEPARATOR && gc != Character.SPACE_SEPARATOR;\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.UnknownNamedObjectException.getCategoryClass",
	"Comment": "category class that was missing a parser. this is a string instead of a class because the class might not be on the classpathof all nodes or it might be exclusive to a plugin or something.",
	"Method": "String getCategoryClass(){\r\n    return categoryClass;\r\n}"
}, {
	"Path": "org.elasticsearch.index.search.NestedHelper.mightMatchNonNestedDocs",
	"Comment": "returns true if a query on the given field might match parent documents or documents that are nested under a different path.",
	"Method": "boolean mightMatchNonNestedDocs(Query query,String nestedPath,boolean mightMatchNonNestedDocs,String field,String nestedPath){\r\n    if (field.startsWith(\"_\")) {\r\n        return true;\r\n    }\r\n    if (mapperService.fullName(field) == null) {\r\n        return false;\r\n    }\r\n    for (String parent = parentObject(field); parent != null; parent = parentObject(parent)) {\r\n        ObjectMapper mapper = mapperService.getObjectMapper(parent);\r\n        if (mapper != null && mapper.nested().isNested()) {\r\n            if (mapper.fullPath().equals(nestedPath)) {\r\n                return mapper.nested().isIncludeInParent() || mapper.nested().isIncludeInRoot();\r\n            } else {\r\n                return true;\r\n            }\r\n        }\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequest.offsets",
	"Comment": "return the start and stop offsets for each term if they were stored orskip offsets.",
	"Method": "TermVectorsRequest offsets(boolean offsets,boolean offsets){\r\n    return flagsEnum.contains(Flag.Offsets);\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.support.XContentMapValues.filter",
	"Comment": "returns a function that filters a document map based on the given include and exclude rules.",
	"Method": "Map<String, Object> filter(Map<String, ?> map,String[] includes,String[] excludes,Function<Map<String, ?>, Map<String, Object>> filter,String[] includes,String[] excludes,Map<String, Object> filter,Map<String, ?> map,CharacterRunAutomaton includeAutomaton,int initialIncludeState,CharacterRunAutomaton excludeAutomaton,int initialExcludeState,CharacterRunAutomaton matchAllAutomaton,List<Object> filter,Iterable<?> iterable,CharacterRunAutomaton includeAutomaton,int initialIncludeState,CharacterRunAutomaton excludeAutomaton,int initialExcludeState,CharacterRunAutomaton matchAllAutomaton){\r\n    List<Object> filtered = new ArrayList();\r\n    boolean isInclude = includeAutomaton.isAccept(initialIncludeState);\r\n    for (Object value : iterable) {\r\n        if (value instanceof Map) {\r\n            int includeState = includeAutomaton.step(initialIncludeState, '.');\r\n            int excludeState = initialExcludeState;\r\n            if (excludeState != -1) {\r\n                excludeState = excludeAutomaton.step(excludeState, '.');\r\n            }\r\n            Map<String, Object> filteredValue = filter((Map<String, ?>) value, includeAutomaton, includeState, excludeAutomaton, excludeState, matchAllAutomaton);\r\n            if (filteredValue.isEmpty() == false) {\r\n                filtered.add(filteredValue);\r\n            }\r\n        } else if (value instanceof Iterable) {\r\n            List<Object> filteredValue = filter((Iterable<?>) value, includeAutomaton, initialIncludeState, excludeAutomaton, initialExcludeState, matchAllAutomaton);\r\n            if (filteredValue.isEmpty() == false) {\r\n                filtered.add(filteredValue);\r\n            }\r\n        } else if (isInclude) {\r\n            filtered.add(value);\r\n        }\r\n    }\r\n    return filtered;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.Rewriteable.rewrite",
	"Comment": "rewrites this instance based on the provided context. the returnedobjects will be the same instance as this if no changes during therewrite were applied.",
	"Method": "T rewrite(QueryRewriteContext ctx,T rewrite,T original,QueryRewriteContext context,T rewrite,T original,QueryRewriteContext context,boolean assertNoAsyncTasks,List<T> rewrite,List<T> rewritables,QueryRewriteContext context){\r\n    List<T> list = rewritables;\r\n    boolean changed = false;\r\n    if (rewritables != null && rewritables.isEmpty() == false) {\r\n        list = new ArrayList(rewritables.size());\r\n        for (T instance : rewritables) {\r\n            T rewrite = rewrite(instance, context);\r\n            if (instance != rewrite) {\r\n                changed = true;\r\n            }\r\n            list.add(rewrite);\r\n        }\r\n    }\r\n    return changed ? list : rewritables;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteWriteIndex",
	"Comment": "utility method that allows to resolve an index expression to its corresponding single write index.",
	"Method": "Index concreteWriteIndex(ClusterState state,IndicesRequest request){\r\n    if (request.indices() == null || (request.indices() != null && request.indices().length != 1)) {\r\n        throw new IllegalArgumentException(\"indices request must specify a single index expression\");\r\n    }\r\n    Context context = new Context(state, request.indicesOptions(), false, true);\r\n    Index[] indices = concreteIndices(context, request.indices()[0]);\r\n    if (indices.length != 1) {\r\n        throw new IllegalArgumentException(\"The index expression [\" + request.indices()[0] + \"] and options provided did not point to a single write-index\");\r\n    }\r\n    return indices[0];\r\n}"
}, {
	"Path": "org.elasticsearch.index.merge.MergeStats.getTotalStoppedTimeInMillis",
	"Comment": "the total time large merges were stopped so smaller merges could finish.",
	"Method": "long getTotalStoppedTimeInMillis(){\r\n    return this.totalStoppedTimeInMillis;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.DocumentParser.addToLastMapper",
	"Comment": "adds a mapper as an update into the last mapper. if merge is true, the new mapperwill be merged in with other child mappers of the last parent, otherwise it will be a new update.",
	"Method": "void addToLastMapper(List<ObjectMapper> parentMappers,Mapper mapper,boolean merge){\r\n    assert parentMappers.size() >= 1;\r\n    int lastIndex = parentMappers.size() - 1;\r\n    ObjectMapper withNewMapper = parentMappers.get(lastIndex).mappingUpdate(mapper);\r\n    if (merge) {\r\n        withNewMapper = parentMappers.get(lastIndex).merge(withNewMapper);\r\n    }\r\n    parentMappers.set(lastIndex, withNewMapper);\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.tail",
	"Comment": "a java implementation of the unix tail functionality.that is, read the last n lines of the input file f.",
	"Method": "String[] tail(File f,int n,String encoding,String[] tail,File f,int n){\r\n    return tail(f, n, \"utf-8\");\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ack.OpenIndexClusterStateUpdateResponse.isShardsAcknowledged",
	"Comment": "returns whether the requisite number of shard copies started before the completion of the operation.",
	"Method": "boolean isShardsAcknowledged(){\r\n    return shardsAcknowledged;\r\n}"
}, {
	"Path": "com.android.internal.util.ArrayUtils.removeElement",
	"Comment": "removes an element from a copy of the array and returns the copy.if the element is not present, then the original array is returned unmodified.",
	"Method": "T[] removeElement(Class<T> kind,T[] array,T element){\r\n    if (array != null) {\r\n        final int length = array.length;\r\n        for (int i = 0; i < length; i++) {\r\n            if (array[i] == element) {\r\n                if (length == 1) {\r\n                    return null;\r\n                }\r\n                T[] result = (T[]) Array.newInstance(kind, length - 1);\r\n                System.arraycopy(array, 0, result, 0, i);\r\n                System.arraycopy(array, i + 1, result, i, length - i - 1);\r\n                return result;\r\n            }\r\n        }\r\n    }\r\n    return array;\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.updateGlobalCheckpointOnPrimary",
	"Comment": "scans through the currently known local checkpoint and updates the global checkpoint accordingly.",
	"Method": "void updateGlobalCheckpointOnPrimary(){\r\n    assert primaryMode;\r\n    final CheckpointState cps = checkpoints.get(shardAllocationId);\r\n    final long globalCheckpoint = cps.globalCheckpoint;\r\n    final long computedGlobalCheckpoint = computeGlobalCheckpoint(pendingInSync, checkpoints.values(), getGlobalCheckpoint());\r\n    assert computedGlobalCheckpoint >= globalCheckpoint : \"new global checkpoint [\" + computedGlobalCheckpoint + \"] is lower than previous one [\" + globalCheckpoint + \"]\";\r\n    if (globalCheckpoint != computedGlobalCheckpoint) {\r\n        cps.globalCheckpoint = computedGlobalCheckpoint;\r\n        logger.trace(\"updated global checkpoint to [{}]\", computedGlobalCheckpoint);\r\n        onGlobalCheckpointUpdated.accept(computedGlobalCheckpoint);\r\n    }\r\n}"
}, {
	"Path": "org.openqa.selenium.ReferrerTest.basicHistoryNavigationWithoutAProxy",
	"Comment": "tests navigation when all of the files are hosted on the same domain and the browserdoes not have a proxy configured.",
	"Method": "void basicHistoryNavigationWithoutAProxy(){\r\n    testServer1.start();\r\n    String page1Url = buildPage1Url(testServer1, buildPage2Url(testServer1));\r\n    String page2Url = buildPage2Url(testServer1, buildPage3Url(testServer1));\r\n    String page3Url = buildPage3Url(testServer1);\r\n    performNavigation(driver, page1Url);\r\n    assertThat(testServer1.getRequests()).isEqualTo(ImmutableList.of(new HttpRequest(page1Url, null), new HttpRequest(page2Url, page1Url), new HttpRequest(page3Url, page2Url)));\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.RerankingParserQuery.parseFallback",
	"Comment": "the model had to fall back to a simpler model on the previous parse",
	"Method": "boolean parseFallback(){\r\n    return parserQuery.parseFallback();\r\n}"
}, {
	"Path": "org.elasticsearch.common.logging.PrefixLogger.markersSize",
	"Comment": "return the size of the cached markers. this size can vary as markers are cached but collected during gc activity when a given prefixis no longer in use.",
	"Method": "int markersSize(){\r\n    return markers.size();\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.AtomicArray.length",
	"Comment": "the size of the expected results, including potential null values.",
	"Method": "int length(){\r\n    return array.length();\r\n}"
}, {
	"Path": "org.openqa.grid.selenium.proxy.DefaultRemoteProxy.getNewSession",
	"Comment": "overwrites the session allocation to discard the proxy that are down.",
	"Method": "TestSession getNewSession(Map<String, Object> requestedCapability){\r\n    if (down) {\r\n        return null;\r\n    }\r\n    return super.getNewSession(requestedCapability);\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoryData.resolveNewIndices",
	"Comment": "resolve the given index names to index ids, creating new index ids fornew indices in the repository.",
	"Method": "List<IndexId> resolveNewIndices(List<String> indicesToResolve){\r\n    List<IndexId> snapshotIndices = new ArrayList();\r\n    for (String index : indicesToResolve) {\r\n        final IndexId indexId;\r\n        if (indices.containsKey(index)) {\r\n            indexId = indices.get(index);\r\n        } else {\r\n            indexId = new IndexId(index, UUIDs.randomBase64UUID());\r\n        }\r\n        snapshotIndices.add(indexId);\r\n    }\r\n    return snapshotIndices;\r\n}"
}, {
	"Path": "org.elasticsearch.common.io.stream.StreamInput.readBytesReference",
	"Comment": "reads a bytes reference from this stream, might hold an actual reference to the underlyingbytes of the stream.",
	"Method": "BytesReference readBytesReference(BytesReference readBytesReference,int length){\r\n    if (length == 0) {\r\n        return BytesArray.EMPTY;\r\n    }\r\n    byte[] bytes = new byte[length];\r\n    readBytes(bytes, 0, length);\r\n    return new BytesArray(bytes, 0, length);\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.BigFloatArray.estimateRamBytes",
	"Comment": "estimates the number of bytes that would be consumed by an array of the given size.",
	"Method": "long estimateRamBytes(long size){\r\n    return ESTIMATOR.ramBytesEstimated(size);\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.spanish.process.AnCoraPronounDisambiguator.disambiguatePersonalPronoun",
	"Comment": "determine whether the given clitic pronoun is an indirect objectpronoun or a reflexive pronoun.this method is only defined when the pronoun is one ofme, te, se, nos, osi.e., those in which the meaning is actually ambiguous.",
	"Method": "PersonalPronounType disambiguatePersonalPronoun(SpanishVerbStripper.StrippedVerb strippedVerb,int pronounIdx,String clauseYield){\r\n    List<String> pronouns = strippedVerb.getPronouns();\r\n    String pronoun = pronouns.get(pronounIdx).toLowerCase();\r\n    if (!ambiguousPersonalPronouns.contains(pronoun))\r\n        throw new IllegalArgumentException(\"We don't support disambiguating pronoun '\" + pronoun + \"'\");\r\n    if (pronouns.size() == 1 && pronoun.equalsIgnoreCase(\"se\"))\r\n        return PersonalPronounType.REFLEXIVE;\r\n    String verb = strippedVerb.getStem();\r\n    if (alwaysReflexiveVerbs.contains(verb))\r\n        return PersonalPronounType.REFLEXIVE;\r\n    else if (neverReflexiveVerbs.contains(verb))\r\n        return PersonalPronounType.OBJECT;\r\n    Pair<String, String> bruteForceKey = new Pair(verb, clauseYield);\r\n    if (bruteForceDecisions.containsKey(bruteForceKey))\r\n        return bruteForceDecisions.get(bruteForceKey);\r\n    log.info(\"Failed to disambiguate: \" + verb + \"\\nContaining clause:\\t\" + clauseYield + \"\\n\");\r\n    return PersonalPronounType.UNKNOWN;\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.matcher.Matchers.only",
	"Comment": "returns a matcher which matches objects equal to the given object.",
	"Method": "Matcher<Object> only(Object value){\r\n    return new Only(value);\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequestBuilder.setId",
	"Comment": "sets the id to index the document under. optional, and if not set, one will be automaticallygenerated.",
	"Method": "IndexRequestBuilder setId(String id){\r\n    request.id(id);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.MappedFieldType.valueForDisplay",
	"Comment": "given a value that comes from the stored fields api, convert it to the expected type. for instance a date field would store dates as longs and format it back to a string in this method.",
	"Method": "Object valueForDisplay(Object value){\r\n    return value;\r\n}"
}, {
	"Path": "org.elasticsearch.action.get.GetRequestBuilder.setRouting",
	"Comment": "controls the shard routing of the request. using this value to hash the shardand not the id.",
	"Method": "GetRequestBuilder setRouting(String routing){\r\n    request.routing(routing);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractBulkByScrollRequest.setTimeout",
	"Comment": "timeout to wait for the shards on to be available for each bulk request?",
	"Method": "Self setTimeout(TimeValue timeout,Self setTimeout,String timeout){\r\n    this.timeout = TimeValue.parseTimeValue(timeout, this.timeout, getClass().getSimpleName() + \".timeout\");\r\n    return self();\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkProcessor.add",
	"Comment": "adds the data from the bytes to be processed by the bulk processor",
	"Method": "BulkProcessor add(IndexRequest request,BulkProcessor add,DeleteRequest request,BulkProcessor add,DocWriteRequest<?> request,BulkProcessor add,DocWriteRequest<?> request,Object payload,BulkProcessor add,BytesReference data,String defaultIndex,String defaultType,XContentType xContentType,BulkProcessor add,BytesReference data,String defaultIndex,String defaultType,String defaultPipeline,Object payload,XContentType xContentType){\r\n    bulkRequest.add(data, defaultIndex, defaultType, null, null, defaultPipeline, payload, true, xContentType);\r\n    executeIfNeeded();\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.FetchSearchPhase.releaseIrrelevantSearchContext",
	"Comment": "releases shard targets that are not used in the docsidstoload.",
	"Method": "void releaseIrrelevantSearchContext(QuerySearchResult queryResult){\r\n    if (context.getRequest().scroll() == null && queryResult.hasSearchContext()) {\r\n        try {\r\n            SearchShardTarget searchShardTarget = queryResult.getSearchShardTarget();\r\n            Transport.Connection connection = context.getConnection(searchShardTarget.getClusterAlias(), searchShardTarget.getNodeId());\r\n            context.sendReleaseSearchContext(queryResult.getRequestId(), connection, searchShardTarget.getOriginalIndices());\r\n        } catch (Exception e) {\r\n            context.getLogger().trace(\"failed to release context\", e);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.CountedCollector.countDown",
	"Comment": "forcefully counts down an operation and executes the provided runnableif all expected operations where executed",
	"Method": "void countDown(){\r\n    assert counter.isCountedDown() == false : \"more operations executed than specified\";\r\n    if (counter.countDown()) {\r\n        onFinish.run();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchPhaseController.reducedQueryPhase",
	"Comment": "reduces the given query results and consumes all aggregations and profile results.",
	"Method": "ReducedQueryPhase reducedQueryPhase(Collection<? extends SearchPhaseResult> queryResults,boolean isScrollRequest,ReducedQueryPhase reducedQueryPhase,Collection<? extends SearchPhaseResult> queryResults,boolean isScrollRequest,boolean trackTotalHits,ReducedQueryPhase reducedQueryPhase,Collection<? extends SearchPhaseResult> queryResults,List<InternalAggregations> bufferedAggs,List<TopDocs> bufferedTopDocs,TopDocsStats topDocsStats,int numReducePhases,boolean isScrollRequest){\r\n    assert numReducePhases >= 0 : \"num reduce phases must be >= 0 but was: \" + numReducePhases;\r\n    numReducePhases++;\r\n    boolean timedOut = false;\r\n    Boolean terminatedEarly = null;\r\n    if (queryResults.isEmpty()) {\r\n        return new ReducedQueryPhase(topDocsStats.totalHits, topDocsStats.fetchHits, topDocsStats.maxScore, timedOut, terminatedEarly, null, null, null, EMPTY_DOCS, null, null, numReducePhases, false, 0, 0, true);\r\n    }\r\n    final QuerySearchResult firstResult = queryResults.stream().findFirst().get().queryResult();\r\n    final boolean hasSuggest = firstResult.suggest() != null;\r\n    final boolean hasProfileResults = firstResult.hasProfileResults();\r\n    final boolean consumeAggs;\r\n    final List<InternalAggregations> aggregationsList;\r\n    if (bufferedAggs != null) {\r\n        consumeAggs = false;\r\n        assert firstResult.hasAggs() : \"firstResult has no aggs but we got non null buffered aggs?\";\r\n        aggregationsList = bufferedAggs;\r\n    } else if (firstResult.hasAggs()) {\r\n        aggregationsList = new ArrayList(queryResults.size());\r\n        consumeAggs = true;\r\n    } else {\r\n        aggregationsList = Collections.emptyList();\r\n        consumeAggs = false;\r\n    }\r\n    final Map<String, List<Suggestion>> groupedSuggestions = hasSuggest ? new HashMap() : Collections.emptyMap();\r\n    final Map<String, ProfileShardResult> profileResults = hasProfileResults ? new HashMap(queryResults.size()) : Collections.emptyMap();\r\n    int from = 0;\r\n    int size = 0;\r\n    for (SearchPhaseResult entry : queryResults) {\r\n        QuerySearchResult result = entry.queryResult();\r\n        from = result.from();\r\n        size = result.size();\r\n        if (result.searchTimedOut()) {\r\n            timedOut = true;\r\n        }\r\n        if (result.terminatedEarly() != null) {\r\n            if (terminatedEarly == null) {\r\n                terminatedEarly = result.terminatedEarly();\r\n            } else if (result.terminatedEarly()) {\r\n                terminatedEarly = true;\r\n            }\r\n        }\r\n        if (hasSuggest) {\r\n            assert result.suggest() != null;\r\n            for (Suggestion<? extends Suggestion.Entry<? extends Suggestion.Entry.Option>> suggestion : result.suggest()) {\r\n                List<Suggestion> suggestionList = groupedSuggestions.computeIfAbsent(suggestion.getName(), s -> new ArrayList());\r\n                suggestionList.add(suggestion);\r\n            }\r\n        }\r\n        if (consumeAggs) {\r\n            aggregationsList.add((InternalAggregations) result.consumeAggs());\r\n        }\r\n        if (hasProfileResults) {\r\n            String key = result.getSearchShardTarget().toString();\r\n            profileResults.put(key, result.consumeProfileResult());\r\n        }\r\n    }\r\n    final Suggest suggest = groupedSuggestions.isEmpty() ? null : new Suggest(Suggest.reduce(groupedSuggestions));\r\n    ReduceContext reduceContext = reduceContextFunction.apply(true);\r\n    final InternalAggregations aggregations = aggregationsList.isEmpty() ? null : reduceAggs(aggregationsList, firstResult.pipelineAggregators(), reduceContext);\r\n    final SearchProfileShardResults shardResults = profileResults.isEmpty() ? null : new SearchProfileShardResults(profileResults);\r\n    final SortedTopDocs scoreDocs = this.sortDocs(isScrollRequest, queryResults, bufferedTopDocs, topDocsStats, from, size);\r\n    return new ReducedQueryPhase(topDocsStats.totalHits, topDocsStats.fetchHits, topDocsStats.maxScore, timedOut, terminatedEarly, suggest, aggregations, shardResults, scoreDocs.scoreDocs, scoreDocs.sortFields, firstResult != null ? firstResult.sortValueFormats() : null, numReducePhases, scoreDocs.isSortedByField, size, from, firstResult == null);\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.DocumentParser.getMapper",
	"Comment": "looks up a child mapper, but takes into account field names that expand to objects",
	"Method": "Mapper getMapper(ObjectMapper objectMapper,String fieldName,String[] subfields){\r\n    for (int i = 0; i < subfields.length - 1; ++i) {\r\n        Mapper mapper = objectMapper.getMapper(subfields[i]);\r\n        if (mapper == null || (mapper instanceof ObjectMapper) == false) {\r\n            return null;\r\n        }\r\n        objectMapper = (ObjectMapper) mapper;\r\n        if (objectMapper.nested().isNested()) {\r\n            throw new MapperParsingException(\"Cannot add a value for field [\" + fieldName + \"] since one of the intermediate objects is mapped as a nested object: [\" + mapper.name() + \"]\");\r\n        }\r\n    }\r\n    return objectMapper.getMapper(subfields[subfields.length - 1]);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.alignSentence",
	"Comment": "copies one sentence replicating only information necessary for sutime",
	"Method": "CoreMap alignSentence(CoreMap sentence){\r\n    String text = sentence.get(CoreAnnotations.TextAnnotation.class);\r\n    if (text != null) {\r\n        return sentence;\r\n    }\r\n    CoreMap newSentence = buildSentenceFromTokens(sentence.get(CoreAnnotations.TokensAnnotation.class), sentence.get(CoreAnnotations.CharacterOffsetBeginAnnotation.class), sentence.get(CoreAnnotations.CharacterOffsetEndAnnotation.class));\r\n    newSentence.set(CoreAnnotations.TokenBeginAnnotation.class, sentence.get(CoreAnnotations.TokenBeginAnnotation.class));\r\n    newSentence.set(CoreAnnotations.TokenEndAnnotation.class, sentence.get(CoreAnnotations.TokenEndAnnotation.class));\r\n    return newSentence;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.NegraPennTreebankParserParams.memoryTreebank",
	"Comment": "returns a memorytreebank with a negrapenntokenizer and anegrapenntreenormalizer",
	"Method": "MemoryTreebank memoryTreebank(){\r\n    return new MemoryTreebank(treeReaderFactory(), inputEncoding);\r\n}"
}, {
	"Path": "org.elasticsearch.indices.IndexingMemoryController.getShardWritingBytes",
	"Comment": "returns how many bytes this shard is currently writing to disk",
	"Method": "long getShardWritingBytes(IndexShard shard){\r\n    return shard.getWritingBytes();\r\n}"
}, {
	"Path": "edu.stanford.nlp.misc.DependencyAnalyzer.addStartingClasses",
	"Comment": "adds the starting classes to depqueue and closure.as a wildcard for class names.",
	"Method": "void addStartingClasses(LinkedList<Identifier> depQueue,Set<Identifier> closure,List<String> startingClasses){\r\n    Pattern[] startingPatterns = new Pattern[startingClasses.size()];\r\n    boolean[] matched = new boolean[startingClasses.size()];\r\n    for (int i = 0; i < startingClasses.size(); ++i) {\r\n        String startingClass = startingClasses.get(i);\r\n        startingClass = startingClass.replaceAll(\"\\\\.\", \"\\\\\\\\\\\\.\");\r\n        startingClass = startingClass.replaceAll(\"\\\\$\", \"\\\\\\\\\\\\$\");\r\n        startingClass = startingClass.replaceAll(\"\\\\*\", \".*\");\r\n        startingPatterns[i] = Pattern.compile(startingClass);\r\n        matched[i] = false;\r\n    }\r\n    for (Identifier id : identifiers.values()) {\r\n        if (!id.isClass)\r\n            continue;\r\n        for (int i = 0; i < startingClasses.size(); ++i) {\r\n            if (startingPatterns[i].matcher(id.name).matches()) {\r\n                depQueue.addLast(id);\r\n                closure.add(id);\r\n                matched[i] = true;\r\n                if (VERBOSE) {\r\n                    log.info(\"Starting class: \" + id.name);\r\n                }\r\n                break;\r\n            }\r\n        }\r\n    }\r\n    for (int i = 0; i < startingClasses.size(); ++i) {\r\n        if (!matched[i]) {\r\n            log.info(\"Warning: pattern \" + startingClasses.get(i) + \" matched nothing\");\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.LocalCheckpointTracker.waitForOpsToComplete",
	"Comment": "waits for all operations up to the provided sequence number to complete.",
	"Method": "void waitForOpsToComplete(long seqNo){\r\n    while (checkpoint < seqNo) {\r\n        this.wait();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.constantScoreQuery",
	"Comment": "a query that wraps another query and simply returns a constant score equal to thequery boost for every document in the query.",
	"Method": "ConstantScoreQueryBuilder constantScoreQuery(QueryBuilder queryBuilder){\r\n    return new ConstantScoreQueryBuilder(queryBuilder);\r\n}"
}, {
	"Path": "org.elasticsearch.http.AbstractHttpServerTransport.incomingRequestError",
	"Comment": "this method handles an incoming http request that has encountered an error.",
	"Method": "void incomingRequestError(HttpRequest httpRequest,HttpChannel httpChannel,Exception exception){\r\n    handleIncomingRequest(httpRequest, httpChannel, exception);\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.GeoUtils.isValidLongitude",
	"Comment": "returns true if longitude is actually a valid longitude value.",
	"Method": "boolean isValidLongitude(double longitude){\r\n    if (Double.isNaN(longitude) || Double.isInfinite(longitude) || longitude < GeoUtils.MIN_LON || longitude > GeoUtils.MAX_LON) {\r\n        return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.node.Node.getCustomNameResolvers",
	"Comment": "get custom name resolvers list based on a discovery plugins list",
	"Method": "List<NetworkService.CustomNameResolver> getCustomNameResolvers(List<DiscoveryPlugin> discoveryPlugins){\r\n    List<NetworkService.CustomNameResolver> customNameResolvers = new ArrayList();\r\n    for (DiscoveryPlugin discoveryPlugin : discoveryPlugins) {\r\n        NetworkService.CustomNameResolver customNameResolver = discoveryPlugin.getCustomNameResolver(settings);\r\n        if (customNameResolver != null) {\r\n            customNameResolvers.add(customNameResolver);\r\n        }\r\n    }\r\n    return customNameResolvers;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.mapper.MapperRegistry.isMetaDataField",
	"Comment": "returns true if the provide field is a registered metadata field, false otherwise",
	"Method": "boolean isMetaDataField(String field){\r\n    return getMetadataMapperParsers().containsKey(field);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchResponse.isTerminatedEarly",
	"Comment": "has the search operation terminated early due to reachingterminateafter",
	"Method": "Boolean isTerminatedEarly(){\r\n    return internalResponse.terminatedEarly();\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.PluginsService.reloadLuceneSPI",
	"Comment": "reloads all lucene spi implementations using the new classloader.this method must be called after the new classloader has been created toregister the services for use.",
	"Method": "void reloadLuceneSPI(ClassLoader loader){\r\n    PostingsFormat.reloadPostingsFormats(loader);\r\n    DocValuesFormat.reloadDocValuesFormats(loader);\r\n    Codec.reloadCodecs(loader);\r\n    CharFilterFactory.reloadCharFilters(loader);\r\n    TokenFilterFactory.reloadTokenFilters(loader);\r\n    TokenizerFactory.reloadTokenizers(loader);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaData.hasAliases",
	"Comment": "checks if at least one of the specified aliases exists in the specified concrete indices. wildcards are supported in thealias names for partial matches.",
	"Method": "boolean hasAliases(String[] aliases,String[] concreteIndices){\r\n    assert aliases != null;\r\n    assert concreteIndices != null;\r\n    if (concreteIndices.length == 0) {\r\n        return false;\r\n    }\r\n    Iterable<String> intersection = HppcMaps.intersection(ObjectHashSet.from(concreteIndices), indices.keys());\r\n    for (String index : intersection) {\r\n        IndexMetaData indexMetaData = indices.get(index);\r\n        List<AliasMetaData> filteredValues = new ArrayList();\r\n        for (ObjectCursor<AliasMetaData> cursor : indexMetaData.getAliases().values()) {\r\n            AliasMetaData value = cursor.value;\r\n            if (Regex.simpleMatch(aliases, value.alias())) {\r\n                filteredValues.add(value);\r\n            }\r\n        }\r\n        if (!filteredValues.isEmpty()) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.internal.Errors.format",
	"Comment": "returns the formatted message for an exception with the specified messages.",
	"Method": "String format(String messageFormat,Object arguments,String format,String heading,Collection<Message> errorMessages){\r\n    try (Formatter fmt = new Formatter(Locale.ROOT)) {\r\n        fmt.format(heading).format(\":%n%n\");\r\n        int index = 1;\r\n        boolean displayCauses = getOnlyCause(errorMessages) == null;\r\n        for (Message errorMessage : errorMessages) {\r\n            fmt.format(\"%s) %s%n\", index++, errorMessage.getMessage());\r\n            List<Object> dependencies = errorMessage.getSources();\r\n            for (int i = dependencies.size() - 1; i >= 0; i--) {\r\n                Object source = dependencies.get(i);\r\n                formatSource(fmt, source);\r\n            }\r\n            Throwable cause = errorMessage.getCause();\r\n            if (displayCauses && cause != null) {\r\n                StringWriter writer = new StringWriter();\r\n                cause.printStackTrace(new PrintWriter(writer));\r\n                fmt.format(\"Caused by: %s\", writer.getBuffer());\r\n            }\r\n            fmt.format(\"%n\");\r\n        }\r\n        if (errorMessages.size() == 1) {\r\n            fmt.format(\"1 error\");\r\n        } else {\r\n            fmt.format(\"%s errors\", errorMessages.size());\r\n        }\r\n        return fmt.toString();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.search.NestedHelper.mightMatchNestedDocs",
	"Comment": "returns true if a query on the given field might match nested documents.",
	"Method": "boolean mightMatchNestedDocs(Query query,boolean mightMatchNestedDocs,String field){\r\n    if (field.startsWith(\"_\")) {\r\n        return true;\r\n    }\r\n    if (mapperService.fullName(field) == null) {\r\n        return false;\r\n    }\r\n    for (String parent = parentObject(field); parent != null; parent = parentObject(parent)) {\r\n        ObjectMapper mapper = mapperService.getObjectMapper(parent);\r\n        if (mapper != null && mapper.nested().isNested()) {\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.DeleteByQueryRequest.setBatchSize",
	"Comment": "the scroll size to control number of documents processed per batch",
	"Method": "DeleteByQueryRequest setBatchSize(int size){\r\n    getSearchRequest().source().size(size);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.ArrayMath.safeMax",
	"Comment": "returns the largest value in a vector of doubles.any values whichare nan or infinite are ignored.if the vector is empty, 0.0 isreturned.",
	"Method": "double safeMax(double[] v){\r\n    double[] u = filterNaNAndInfinite(v);\r\n    if (numRows(u) == 0)\r\n        return 0.0;\r\n    return max(u);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequest.types",
	"Comment": "the types of documents the query will run against. defaults to all types.",
	"Method": "String[] types(ValidateQueryRequest types,String types){\r\n    this.types = types;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkPrimaryExecutionContext.getRetryCounter",
	"Comment": "returns the number of times the current operation has been retried",
	"Method": "int getRetryCounter(){\r\n    return retryCounter;\r\n}"
}, {
	"Path": "org.elasticsearch.common.bytes.BytesReference.iterator",
	"Comment": "returns a bytesrefiterator for this bytesreference. this method allowsaccess to the internal pages of this reference without copying them. use with care!",
	"Method": "BytesRefIterator iterator(){\r\n    return new BytesRefIterator() {\r\n        BytesRef ref = length() == 0 ? null : toBytesRef();\r\n        @Override\r\n        public BytesRef next() throws IOException {\r\n            BytesRef r = ref;\r\n            ref = null;\r\n            return r;\r\n        }\r\n    };\r\n}"
}, {
	"Path": "org.elasticsearch.common.bytes.BytesReference.iterator",
	"Comment": "returns a bytesrefiterator for this bytesreference. this method allowsaccess to the internal pages of this reference without copying them. use with care!",
	"Method": "BytesRefIterator iterator(){\r\n    BytesRef r = ref;\r\n    ref = null;\r\n    return r;\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.NetworkUtils.getGlobalAddresses",
	"Comment": "returns all global scope addresses for interfaces that are up.",
	"Method": "InetAddress[] getGlobalAddresses(){\r\n    List<InetAddress> list = new ArrayList();\r\n    for (NetworkInterface intf : getInterfaces()) {\r\n        if (intf.isUp()) {\r\n            for (InetAddress address : Collections.list(intf.getInetAddresses())) {\r\n                if (address.isLoopbackAddress() == false && address.isSiteLocalAddress() == false && address.isLinkLocalAddress() == false) {\r\n                    list.add(address);\r\n                }\r\n            }\r\n        }\r\n    }\r\n    if (list.isEmpty()) {\r\n        throw new IllegalArgumentException(\"No up-and-running global-scope (public) addresses found, got \" + getInterfaces());\r\n    }\r\n    return list.toArray(new InetAddress[list.size()]);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEvent.indexRoutingTableChanged",
	"Comment": "returns true iff the routing table has changed for the given index.note that this is an object reference equality test, not an equals test.",
	"Method": "boolean indexRoutingTableChanged(String index){\r\n    Objects.requireNonNull(index, \"index must not be null\");\r\n    if (!state.routingTable().hasIndex(index) && !previousState.routingTable().hasIndex(index)) {\r\n        return false;\r\n    }\r\n    if (state.routingTable().hasIndex(index) && previousState.routingTable().hasIndex(index)) {\r\n        return state.routingTable().index(index) != previousState.routingTable().index(index);\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.NaturalLogicAnnotator.getProperNounSubtreeSpan",
	"Comment": "returns the yield span for the word rooted at the given node, but only traversing relations indicativeof staying in the same noun phrase.",
	"Method": "Pair<Integer, Integer> getProperNounSubtreeSpan(SemanticGraph tree,IndexedWord root){\r\n    return getGeneralizedSubtreeSpan(tree, root, NOUN_COMPONENT_ARCS);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.LocalClusterUpdateTask.unchanged",
	"Comment": "no changes were made to the cluster state. useful to execute a runnable on the cluster state applier thread",
	"Method": "ClusterTasksResult<LocalClusterUpdateTask> unchanged(){\r\n    return new ClusterTasksResult(null, null);\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVectorNamespace.setSparseFeature",
	"Comment": "this adds a sparse feature to a vector, setting the appropriate component of the given vector to the passed invalue.",
	"Method": "void setSparseFeature(ConcatVector vector,String featureName,String index,double value){\r\n    vector.setSparseComponent(ensureFeature(featureName), ensureSparseFeature(featureName, index), value);\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.ConfigurationException.getPartialValue",
	"Comment": "returns a value that was only partially computed due to this exception. the caller can usethis while collecting additional configuration problems.",
	"Method": "E getPartialValue(){\r\n    return (E) partialValue;\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.isDefaultAllowUnmappedFields",
	"Comment": "returns true if queries should be lenient about unmapped fields. the default is true",
	"Method": "boolean isDefaultAllowUnmappedFields(){\r\n    return defaultAllowUnmappedFields;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.support.NestedScope.previousLevel",
	"Comment": "sets the previous nested level as current nested level and removes and returns the current nested level.",
	"Method": "ObjectMapper previousLevel(){\r\n    return levelStack.pop();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.settings.put.UpdateSettingsClusterStateUpdateRequest.isPreserveExisting",
	"Comment": "returns true iff the settings update should only add but not update settings. if the setting already existsit should not be overwritten by this update. the default is false",
	"Method": "boolean isPreserveExisting(){\r\n    return preserveExisting;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEvent.localNodeMaster",
	"Comment": "returns true iff the local node is the mater node of the cluster.",
	"Method": "boolean localNodeMaster(){\r\n    return state.nodes().isLocalNodeElectedMaster();\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.LiveVersionMap.pruneTombstones",
	"Comment": "try to prune tombstones whose timestamp is less than maxtimestamptoprune and seqno at most the maxseqnotoprune.",
	"Method": "void pruneTombstones(long maxTimestampToPrune,long maxSeqNoToPrune){\r\n    for (Map.Entry<BytesRef, DeleteVersionValue> entry : tombstones.entrySet()) {\r\n        if (canRemoveTombstone(maxTimestampToPrune, maxSeqNoToPrune, entry.getValue())) {\r\n            final BytesRef uid = entry.getKey();\r\n            try (Releasable lock = keyedLock.tryAcquire(uid)) {\r\n                if (lock != null) {\r\n                    final DeleteVersionValue versionValue = tombstones.get(uid);\r\n                    if (versionValue != null) {\r\n                        if (canRemoveTombstone(maxTimestampToPrune, maxSeqNoToPrune, versionValue)) {\r\n                            removeTombstoneUnderLock(uid);\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusRequest.ignoreUnavailable",
	"Comment": "returns whether the request permits unavailable snapshots to be ignored.",
	"Method": "SnapshotsStatusRequest ignoreUnavailable(boolean ignoreUnavailable,boolean ignoreUnavailable){\r\n    return ignoreUnavailable;\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.writeObjectToFile",
	"Comment": "write an object to a specified file. the file is silently gzipped if the filename ends with .gz.",
	"Method": "File writeObjectToFile(Object o,String filename,File writeObjectToFile,Object o,File file,File writeObjectToFile,Object o,File file,boolean append){\r\n    OutputStream os = new FileOutputStream(file, append);\r\n    if (file.getName().endsWith(\".gz\")) {\r\n        os = new GZIPOutputStream(os);\r\n    }\r\n    os = new BufferedOutputStream(os);\r\n    ObjectOutputStream oos = new ObjectOutputStream(os);\r\n    oos.writeObject(o);\r\n    oos.close();\r\n    return file;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequest.waitForCompletion",
	"Comment": "if this parameter is set to true the operation will wait for completion of restore process before returning.",
	"Method": "RestoreSnapshotRequest waitForCompletion(boolean waitForCompletion,boolean waitForCompletion){\r\n    return waitForCompletion;\r\n}"
}, {
	"Path": "org.elasticsearch.common.ValidationException.addValidationError",
	"Comment": "add a new validation error to the accumulating validation errors",
	"Method": "void addValidationError(String error){\r\n    validationErrors.add(error);\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.NDArray.getTableAccessOffset",
	"Comment": "compute the distance into the one dimensional factortable array that corresponds to a setting of all theneighbors of the factor.",
	"Method": "int getTableAccessOffset(int[] assignment){\r\n    assert (assignment.length == dimensions.length);\r\n    int offset = 0;\r\n    for (int i = 0; i < assignment.length; i++) {\r\n        assert (assignment[i] < dimensions[i]);\r\n        offset = (offset * dimensions[i]) + assignment[i];\r\n    }\r\n    return offset;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchPhaseContext.sendReleaseSearchContext",
	"Comment": "releases a search context with the given context id on the node the given connection is connected to.",
	"Method": "void sendReleaseSearchContext(long contextId,Transport.Connection connection,OriginalIndices originalIndices){\r\n    if (connection != null) {\r\n        getSearchTransport().sendFreeContext(connection, contextId, originalIndices);\r\n    }\r\n}"
}, {
	"Path": "android.util.SparseIntArray.delete",
	"Comment": "removes the mapping from the specified key, if there was any.",
	"Method": "void delete(int key){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i >= 0) {\r\n        removeAt(i);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.Plugin.getSettingsFilter",
	"Comment": "returns a list of additional settings filter for this plugin",
	"Method": "List<String> getSettingsFilter(){\r\n    return Collections.emptyList();\r\n}"
}, {
	"Path": "edu.stanford.nlp.maxent.iis.LambdaSolve.getDerivatives",
	"Comment": "assuming we have the lambdas in the array and we need only thederivatives now.",
	"Method": "double[] getDerivatives(){\r\n    double[] drvs = new double[lambda.length];\r\n    Experiments exp = p.data;\r\n    for (int fNo = 0; fNo < drvs.length; fNo++) {\r\n        Feature f = p.functions.get(fNo);\r\n        double sum = ftildeArr[fNo] * exp.getNumber();\r\n        drvs[fNo] = -sum;\r\n        for (int index = 0, length = f.len(); index < length; index++) {\r\n            int x = f.getX(index);\r\n            int y = f.getY(index);\r\n            if (ASSUME_BINARY) {\r\n                drvs[fNo] += probConds[x][y] * exp.ptildeX(x) * exp.getNumber();\r\n            } else {\r\n                double val = f.getVal(index);\r\n                drvs[fNo] += probConds[x][y] * val * exp.ptildeX(x) * exp.getNumber();\r\n            }\r\n        }\r\n    }\r\n    return drvs;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.AllocationService.removeDelayMarkers",
	"Comment": "removes delay markers from unassigned shards based on current time stamp.",
	"Method": "void removeDelayMarkers(RoutingAllocation allocation){\r\n    final RoutingNodes.UnassignedShards.UnassignedIterator unassignedIterator = allocation.routingNodes().unassigned().iterator();\r\n    final MetaData metaData = allocation.metaData();\r\n    while (unassignedIterator.hasNext()) {\r\n        ShardRouting shardRouting = unassignedIterator.next();\r\n        UnassignedInfo unassignedInfo = shardRouting.unassignedInfo();\r\n        if (unassignedInfo.isDelayed()) {\r\n            final long newComputedLeftDelayNanos = unassignedInfo.getRemainingDelay(allocation.getCurrentNanoTime(), metaData.getIndexSafe(shardRouting.index()).getSettings());\r\n            if (newComputedLeftDelayNanos == 0) {\r\n                unassignedIterator.updateUnassigned(new UnassignedInfo(unassignedInfo.getReason(), unassignedInfo.getMessage(), unassignedInfo.getFailure(), unassignedInfo.getNumFailedAllocations(), unassignedInfo.getUnassignedTimeInNanos(), unassignedInfo.getUnassignedTimeInMillis(), false, unassignedInfo.getLastAllocationStatus()), shardRouting.recoverySource(), allocation.changes());\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.apache.dubbo.registry.dubbo.RegistryDirectoryTest.testNofityOverrideUrls_CleanOnly",
	"Comment": "the test clears the override rule and only sends the override cleanup ruleswhether the test can be restored to the providerurl when it is pushed",
	"Method": "void testNofityOverrideUrls_CleanOnly(){\r\n    RegistryDirectory registryDirectory = getRegistryDirectory();\r\n    invocation = new RpcInvocation();\r\n    List<URL> durls = new ArrayList<URL>();\r\n    durls.add(SERVICEURL.setHost(\"10.20.30.140\").addParameter(\"timeout\", \"1\"));\r\n    registryDirectory.notify(durls);\r\n    Assert.assertEquals(null, registryDirectory.getUrl().getParameter(\"mock\"));\r\n    durls = new ArrayList<URL>();\r\n    durls.add(URL.valueOf(\"override://0.0.0.0?timeout=1000&mock=fail\"));\r\n    registryDirectory.notify(durls);\r\n    List<Invoker<?>> invokers = registryDirectory.list(invocation);\r\n    Invoker<?> aInvoker = invokers.get(0);\r\n    Assert.assertEquals(\"1000\", aInvoker.getUrl().getParameter(\"timeout\"));\r\n    Assert.assertEquals(\"fail\", registryDirectory.getUrl().getParameter(\"mock\"));\r\n    durls = new ArrayList<URL>();\r\n    durls.add(URL.valueOf(\"override://0.0.0.0/dubbo.test.api.HelloService\"));\r\n    registryDirectory.notify(durls);\r\n    invokers = registryDirectory.list(invocation);\r\n    aInvoker = invokers.get(0);\r\n    Assert.assertEquals(\"1\", aInvoker.getUrl().getParameter(\"timeout\"));\r\n    Assert.assertEquals(null, registryDirectory.getUrl().getParameter(\"mock\"));\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.afterWriteOperation",
	"Comment": "schedules a flush or translog generation roll if needed but will not schedule more than one concurrently. the operation will beexecuted asynchronously on the flush thread pool.",
	"Method": "void afterWriteOperation(){\r\n    if (shouldPeriodicallyFlush() || shouldRollTranslogGeneration()) {\r\n        if (flushOrRollRunning.compareAndSet(false, true)) {\r\n            if (shouldPeriodicallyFlush()) {\r\n                logger.debug(\"submitting async flush request\");\r\n                final AbstractRunnable flush = new AbstractRunnable() {\r\n                    @Override\r\n                    public void onFailure(final Exception e) {\r\n                        if (state != IndexShardState.CLOSED) {\r\n                            logger.warn(\"failed to flush index\", e);\r\n                        }\r\n                    }\r\n                    @Override\r\n                    protected void doRun() throws IOException {\r\n                        flush(new FlushRequest());\r\n                        periodicFlushMetric.inc();\r\n                    }\r\n                    @Override\r\n                    public void onAfter() {\r\n                        flushOrRollRunning.compareAndSet(true, false);\r\n                        afterWriteOperation();\r\n                    }\r\n                };\r\n                threadPool.executor(ThreadPool.Names.FLUSH).execute(flush);\r\n            } else if (shouldRollTranslogGeneration()) {\r\n                logger.debug(\"submitting async roll translog generation request\");\r\n                final AbstractRunnable roll = new AbstractRunnable() {\r\n                    @Override\r\n                    public void onFailure(final Exception e) {\r\n                        if (state != IndexShardState.CLOSED) {\r\n                            logger.warn(\"failed to roll translog generation\", e);\r\n                        }\r\n                    }\r\n                    @Override\r\n                    protected void doRun() throws Exception {\r\n                        rollTranslogGeneration();\r\n                    }\r\n                    @Override\r\n                    public void onAfter() {\r\n                        flushOrRollRunning.compareAndSet(true, false);\r\n                        afterWriteOperation();\r\n                    }\r\n                };\r\n                threadPool.executor(ThreadPool.Names.FLUSH).execute(roll);\r\n            } else {\r\n                flushOrRollRunning.compareAndSet(true, false);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.afterWriteOperation",
	"Comment": "schedules a flush or translog generation roll if needed but will not schedule more than one concurrently. the operation will beexecuted asynchronously on the flush thread pool.",
	"Method": "void afterWriteOperation(){\r\n    if (state != IndexShardState.CLOSED) {\r\n        logger.warn(\"failed to flush index\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.afterWriteOperation",
	"Comment": "schedules a flush or translog generation roll if needed but will not schedule more than one concurrently. the operation will beexecuted asynchronously on the flush thread pool.",
	"Method": "void afterWriteOperation(){\r\n    flush(new FlushRequest());\r\n    periodicFlushMetric.inc();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.afterWriteOperation",
	"Comment": "schedules a flush or translog generation roll if needed but will not schedule more than one concurrently. the operation will beexecuted asynchronously on the flush thread pool.",
	"Method": "void afterWriteOperation(){\r\n    flushOrRollRunning.compareAndSet(true, false);\r\n    afterWriteOperation();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.afterWriteOperation",
	"Comment": "schedules a flush or translog generation roll if needed but will not schedule more than one concurrently. the operation will beexecuted asynchronously on the flush thread pool.",
	"Method": "void afterWriteOperation(){\r\n    if (state != IndexShardState.CLOSED) {\r\n        logger.warn(\"failed to roll translog generation\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.afterWriteOperation",
	"Comment": "schedules a flush or translog generation roll if needed but will not schedule more than one concurrently. the operation will beexecuted asynchronously on the flush thread pool.",
	"Method": "void afterWriteOperation(){\r\n    rollTranslogGeneration();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.afterWriteOperation",
	"Comment": "schedules a flush or translog generation roll if needed but will not schedule more than one concurrently. the operation will beexecuted asynchronously on the flush thread pool.",
	"Method": "void afterWriteOperation(){\r\n    flushOrRollRunning.compareAndSet(true, false);\r\n    afterWriteOperation();\r\n}"
}, {
	"Path": "android.util.SparseBooleanArray.delete",
	"Comment": "removes the mapping from the specified key, if there was any.",
	"Method": "void delete(int key){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i >= 0) {\r\n        System.arraycopy(mKeys, i + 1, mKeys, i, mSize - (i + 1));\r\n        System.arraycopy(mValues, i + 1, mValues, i, mSize - (i + 1));\r\n        mSize--;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.readOperation",
	"Comment": "reads and returns the operation from the given location if the generation it references is still available. otherwisethis method will return null.",
	"Method": "Operation readOperation(Location location,Operation readOperation,StreamInput input,Translog.Operation readOperation,BufferedChecksumStreamInput in){\r\n    final Translog.Operation operation;\r\n    try {\r\n        final int opSize = in.readInt();\r\n        if (opSize < 4) {\r\n            throw new TranslogCorruptedException(in.getSource(), \"operation size must be at least 4 but was: \" + opSize);\r\n        }\r\n        in.resetDigest();\r\n        if (in.markSupported()) {\r\n            in.mark(opSize);\r\n            in.skip(opSize - 4);\r\n            verifyChecksum(in);\r\n            in.reset();\r\n        }\r\n        operation = Translog.Operation.readOperation(in);\r\n        verifyChecksum(in);\r\n    } catch (EOFException e) {\r\n        throw new TruncatedTranslogException(in.getSource(), \"reached premature end of file, translog is truncated\", e);\r\n    }\r\n    return operation;\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.blobstore.BlobStoreRepository.chunkSize",
	"Comment": "returns data file chunk size.this method should return null if no chunking is needed.",
	"Method": "ByteSizeValue chunkSize(){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.spi.InjectionPoint.newDependency",
	"Comment": "this method is necessary to create a dependency with proper generic type information",
	"Method": "Dependency<T> newDependency(Key<T> key,boolean allowsNull,int parameterIndex){\r\n    return new Dependency(this, key, allowsNull, parameterIndex);\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequestBuilder.setPipeline",
	"Comment": "sets the ingest pipeline to be executed before indexing the document",
	"Method": "IndexRequestBuilder setPipeline(String pipeline){\r\n    request.setPipeline(pipeline);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.nndep.DependencyTree.set",
	"Comment": "establish a labeled dependency relation between the two givennodes.",
	"Method": "void set(int k,int h,String l){\r\n    head.set(k, h);\r\n    label.set(k, l);\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.arabic.process.IOBUtils.IOBToString",
	"Comment": "convert a list of labeled characters to a string. include segmentation markersfor prefixes and suffixes in the string, and add a space at segmentations.",
	"Method": "String IOBToString(List<CoreLabel> labeledSequence,String prefixMarker,String suffixMarker,String IOBToString,List<CoreLabel> labeledSequence,String prefixMarker,String suffixMarker,int startIndex,int endIndex,String IOBToString,List<CoreLabel> labeledSequence,String segmentationMarker,String IOBToString,List<CoreLabel> labeledSequence,String IOBToString,List<CoreLabel> labeledSequence,String prefixMarker,String suffixMarker,boolean addSpace,boolean applyRewrites,int startIndex,int endIndex){\r\n    StringBuilder sb = new StringBuilder();\r\n    String lastLabel = \"\";\r\n    final boolean addPrefixMarker = prefixMarker != null && prefixMarker.length() > 0;\r\n    final boolean addSuffixMarker = suffixMarker != null && suffixMarker.length() > 0;\r\n    if (addPrefixMarker || addSuffixMarker)\r\n        annotateMarkers(labeledSequence);\r\n    for (int i = startIndex; i < endIndex; ++i) {\r\n        CoreLabel labeledChar = labeledSequence.get(i);\r\n        String token = labeledChar.get(CoreAnnotations.CharAnnotation.class);\r\n        if (addPrefixMarker && token.equals(prefixMarker))\r\n            token = \"#pm#\";\r\n        if (addSuffixMarker && token.equals(suffixMarker))\r\n            token = \"#sm#\";\r\n        String label = labeledChar.get(CoreAnnotations.AnswerAnnotation.class);\r\n        if (token.equals(BoundaryChar)) {\r\n            sb.append(\" \");\r\n        } else if (label.equals(BeginSymbol)) {\r\n            if (lastLabel.equals(ContinuationSymbol) || lastLabel.equals(BeginSymbol) || lastLabel.equals(RewriteSymbol)) {\r\n                if (addPrefixMarker && (!addSpace || addPrefixMarker(i, labeledSequence))) {\r\n                    sb.append(prefixMarker);\r\n                }\r\n                if (addSpace) {\r\n                    sb.append(\" \");\r\n                }\r\n                if (addSuffixMarker && (!addSpace || addSuffixMarker(i, labeledSequence))) {\r\n                    sb.append(suffixMarker);\r\n                }\r\n            }\r\n            sb.append(token);\r\n        } else if (label.equals(ContinuationSymbol) || label.equals(BoundarySymbol)) {\r\n            sb.append(token);\r\n        } else if (label.equals(NosegSymbol)) {\r\n            if (!lastLabel.equals(BoundarySymbol) && addSpace) {\r\n                sb.append(\" \");\r\n            }\r\n            sb.append(token);\r\n        } else if (label.equals(RewriteSymbol) || label.equals(\"REWAL\") || label.equals(\"REWTA\")) {\r\n            switch(token) {\r\n                case \"ت\":\r\n                case \"ه\":\r\n                    sb.append(applyRewrites ? \"ة\" : token);\r\n                    break;\r\n                case \"ل\":\r\n                    sb.append((addPrefixMarker ? prefixMarker : \"\") + (addSpace ? \" \" : \"\") + (applyRewrites ? \"ال\" : \"ل\"));\r\n                    break;\r\n                case \"ي\":\r\n                case \"ا\":\r\n                    sb.append(applyRewrites ? \"ى\" : token);\r\n                    break;\r\n                case \"ى\":\r\n                    sb.append(applyRewrites ? \"ي\" : token);\r\n                    break;\r\n                default:\r\n                    sb.append(token);\r\n                    break;\r\n            }\r\n        } else {\r\n            throw new RuntimeException(\"Unknown label: \" + label);\r\n        }\r\n        lastLabel = label;\r\n    }\r\n    return sb.toString().trim();\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequestBuilder.setPositions",
	"Comment": "sets whether to return the positions for each term if stored or skip.",
	"Method": "TermVectorsRequestBuilder setPositions(boolean positions){\r\n    request.positions(positions);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.command.BasePrimaryAllocationCommand.acceptDataLoss",
	"Comment": "the operation only executes if the user explicitly agrees to possible data loss",
	"Method": "boolean acceptDataLoss(){\r\n    return acceptDataLoss;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVectorNamespace.ensureSparseFeature",
	"Comment": "an optimization, this lets clients inform the concatvectornamespace of how many sparse feature components toexpect, again so that we can avoid resizing concatvectors.",
	"Method": "int ensureSparseFeature(String featureName,String index){\r\n    ensureFeature(featureName);\r\n    synchronized (sparseFeatureIndex) {\r\n        if (!sparseFeatureIndex.containsKey(featureName)) {\r\n            sparseFeatureIndex.put(featureName, new HashMap());\r\n            reverseSparseFeatureIndex.put(featureName, new HashMap());\r\n        }\r\n    }\r\n    final Map<String, Integer> sparseIndex = sparseFeatureIndex.get(featureName);\r\n    final Map<Integer, String> reverseSparseIndex = reverseSparseFeatureIndex.get(featureName);\r\n    synchronized (sparseIndex) {\r\n        if (!sparseIndex.containsKey(index)) {\r\n            reverseSparseIndex.put(sparseIndex.size(), index);\r\n            sparseIndex.put(index, sparseIndex.size());\r\n        }\r\n        return sparseIndex.get(index);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.blobstore.BlobStoreRepository.getRateLimiter",
	"Comment": "configures ratelimiter based on repository and global settings",
	"Method": "RateLimiter getRateLimiter(Settings repositorySettings,String setting,ByteSizeValue defaultRate){\r\n    ByteSizeValue maxSnapshotBytesPerSec = repositorySettings.getAsBytesSize(setting, settings.getAsBytesSize(setting, defaultRate));\r\n    if (maxSnapshotBytesPerSec.getBytes() <= 0) {\r\n        return null;\r\n    } else {\r\n        return new RateLimiter.SimpleRateLimiter(maxSnapshotBytesPerSec.getMbFrac());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder.scoreMode",
	"Comment": "returns the score mode, meaning how results of individual score functions will be aggregated.",
	"Method": "FunctionScoreQueryBuilder scoreMode(FunctionScoreQuery.ScoreMode scoreMode,FunctionScoreQuery.ScoreMode scoreMode){\r\n    return this.scoreMode;\r\n}"
}, {
	"Path": "org.apache.dubbo.rpc.protocol.dubbo.support.EnumBak.testGenricCustomArg",
	"Comment": "verify compatibility when 2.0.5 invokes 2.0.3, enum in custom parameter",
	"Method": "void testGenricCustomArg(){\r\n    int port = 20880;\r\n    URL consumerurl = URL.valueOf(\"dubbo://127.0.0.1:\" + port + \"/test?timeout=2000000\");\r\n    Invoker<GenericService> reference = protocol.refer(GenericService.class, consumerurl);\r\n    GenericService demoProxy = (GenericService) proxy.getProxy(reference);\r\n    Map<String, Object> arg = new HashMap<String, Object>();\r\n    arg.put(\"type\", \"High\");\r\n    arg.put(\"name\", \"hi\");\r\n    Object obj = demoProxy.$invoke(\"get\", new String[] { \"org.apache.dubbo.rpc.CustomArgument\" }, new Object[] { arg });\r\n    System.out.println(\"obj---------->\" + obj);\r\n    reference.destroy();\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.LexicalizedParser.parse",
	"Comment": "parses the list of hasword.if the parse fails for some reason,an x tree is returned instead of barfing.",
	"Method": "Tree parse(List<? extends HasWord> lst){\r\n    try {\r\n        ParserQuery pq = parserQuery();\r\n        if (pq.parse(lst)) {\r\n            Tree bestparse = pq.getBestParse();\r\n            bestparse.setScore(pq.getPCFGScore() % -10000.0);\r\n            return bestparse;\r\n        }\r\n    } catch (Exception e) {\r\n        log.info(\"Following exception caught during parsing:\");\r\n        e.printStackTrace();\r\n        log.info(\"Recovering using fall through strategy: will construct an (X ...) tree.\");\r\n    }\r\n    return ParserUtils.xTree(lst);\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.ReindexRequest.setDestRouting",
	"Comment": "set the routing to decide which shard the documents need to be routed to",
	"Method": "ReindexRequest setDestRouting(String routing){\r\n    this.getDestination().routing(routing);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDocument.label",
	"Comment": "returns the first label for this document, or null if none have beenset.",
	"Method": "L label(){\r\n    return (labels.size() > 0) ? labels.get(0) : null;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.sizeOfRelocatingShards",
	"Comment": "returns the size of all shards that are currently being relocated tothe node, but may not be finished transferring yet.if subtractshardsmovingaway is true then the size of shards moving away is subtracted from the total size of all shards",
	"Method": "long sizeOfRelocatingShards(RoutingNode node,RoutingAllocation allocation,boolean subtractShardsMovingAway,String dataPath){\r\n    ClusterInfo clusterInfo = allocation.clusterInfo();\r\n    long totalSize = 0;\r\n    for (ShardRouting routing : node.shardsWithState(ShardRoutingState.RELOCATING, ShardRoutingState.INITIALIZING)) {\r\n        String actualPath = clusterInfo.getDataPath(routing);\r\n        if (dataPath.equals(actualPath)) {\r\n            if (routing.initializing() && routing.relocatingNodeId() != null) {\r\n                totalSize += getExpectedShardSize(routing, allocation, 0);\r\n            } else if (subtractShardsMovingAway && routing.relocating()) {\r\n                totalSize -= getExpectedShardSize(routing, allocation, 0);\r\n            }\r\n        }\r\n    }\r\n    return totalSize;\r\n}"
}, {
	"Path": "org.elasticsearch.action.delete.DeleteRequestBuilder.setRouting",
	"Comment": "controls the shard routing of the delete request. using this value to hash the shardand not the id.",
	"Method": "DeleteRequestBuilder setRouting(String routing){\r\n    request.routing(routing);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.XMLBeginEndIterator.getFactory",
	"Comment": "returns a factory that vends beginenditerators that reads the contents ofthe given reader, extracts text between the specified strings, thenreturns the result.",
	"Method": "IteratorFromReaderFactory<String> getFactory(String tag,IteratorFromReaderFactory<String> getFactory,String tag,boolean keepInternalTags,boolean keepDelimitingTags,IteratorFromReaderFactory<E> getFactory,String tag,Function<String, E> op,IteratorFromReaderFactory<E> getFactory,String tag,Function<String, E> op,boolean keepInternalTags,boolean keepDelimitingTags){\r\n    return new XMLBeginEndIterator.XMLBeginEndIteratorFactory(tag, op, keepInternalTags, keepDelimitingTags);\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.AsyncShardFetch.findNodesToFetch",
	"Comment": "finds all the nodes that need to be fetched. those are nodes that have nodata, and are not in fetch mode.",
	"Method": "List<NodeEntry<T>> findNodesToFetch(Map<String, NodeEntry<T>> shardCache){\r\n    List<NodeEntry<T>> nodesToFetch = new ArrayList();\r\n    for (NodeEntry<T> nodeEntry : shardCache.values()) {\r\n        if (nodeEntry.hasData() == false && nodeEntry.isFetching() == false) {\r\n            nodesToFetch.add(nodeEntry);\r\n        }\r\n    }\r\n    return nodesToFetch;\r\n}"
}, {
	"Path": "android.util.SparseArray.put",
	"Comment": "adds a mapping from the specified key to the specified value,replacing the previous mapping from the specified key if therewas one.",
	"Method": "void put(int key,E value){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i >= 0) {\r\n        mValues[i] = value;\r\n    } else {\r\n        i = ~i;\r\n        if (i < mSize && mValues[i] == DELETED) {\r\n            mKeys[i] = key;\r\n            mValues[i] = value;\r\n            return;\r\n        }\r\n        if (mGarbage && mSize >= mKeys.length) {\r\n            gc();\r\n            i = ~ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n        }\r\n        if (mSize >= mKeys.length) {\r\n            int n = ArrayUtils.idealIntArraySize(mSize + 1);\r\n            int[] nkeys = new int[n];\r\n            Object[] nvalues = new Object[n];\r\n            System.arraycopy(mKeys, 0, nkeys, 0, mKeys.length);\r\n            System.arraycopy(mValues, 0, nvalues, 0, mValues.length);\r\n            mKeys = nkeys;\r\n            mValues = nvalues;\r\n        }\r\n        if (mSize - i != 0) {\r\n            System.arraycopy(mKeys, i, mKeys, i + 1, mSize - i);\r\n            System.arraycopy(mValues, i, mValues, i + 1, mSize - i);\r\n        }\r\n        mKeys[i] = key;\r\n        mValues[i] = value;\r\n        mSize++;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.DiffableUtils.getVIntKeySerializer",
	"Comment": "returns a map key serializer for integer keys. encodes as vint.",
	"Method": "KeySerializer<Integer> getVIntKeySerializer(){\r\n    return VIntKeySerializer.INSTANCE;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoBoundingBoxQueryBuilder.parseBoundingBox",
	"Comment": "parses the bounding box and returns bottom, top, left, right coordinates",
	"Method": "double[] parseBoundingBox(XContentParser parser){\r\n    XContentParser.Token token = parser.currentToken();\r\n    if (token != XContentParser.Token.START_OBJECT) {\r\n        throw new ElasticsearchParseException(\"failed to parse bounding box. Expected start object but found [{}]\", token);\r\n    }\r\n    double top = Double.NaN;\r\n    double bottom = Double.NaN;\r\n    double left = Double.NaN;\r\n    double right = Double.NaN;\r\n    String currentFieldName;\r\n    GeoPoint sparse = new GeoPoint();\r\n    EnvelopeBuilder envelope = null;\r\n    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\r\n        if (token == XContentParser.Token.FIELD_NAME) {\r\n            currentFieldName = parser.currentName();\r\n            token = parser.nextToken();\r\n            if (WKT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                envelope = (EnvelopeBuilder) (GeoWKTParser.parseExpectedType(parser, GeoShapeType.ENVELOPE));\r\n            } else if (TOP_FIELD.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                top = parser.doubleValue();\r\n            } else if (BOTTOM_FIELD.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                bottom = parser.doubleValue();\r\n            } else if (LEFT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                left = parser.doubleValue();\r\n            } else if (RIGHT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                right = parser.doubleValue();\r\n            } else {\r\n                if (TOP_LEFT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                    GeoUtils.parseGeoPoint(parser, sparse, false, GeoUtils.EffectivePoint.TOP_LEFT);\r\n                    top = sparse.getLat();\r\n                    left = sparse.getLon();\r\n                } else if (BOTTOM_RIGHT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                    GeoUtils.parseGeoPoint(parser, sparse, false, GeoUtils.EffectivePoint.BOTTOM_RIGHT);\r\n                    bottom = sparse.getLat();\r\n                    right = sparse.getLon();\r\n                } else if (TOP_RIGHT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                    GeoUtils.parseGeoPoint(parser, sparse, false, GeoUtils.EffectivePoint.TOP_RIGHT);\r\n                    top = sparse.getLat();\r\n                    right = sparse.getLon();\r\n                } else if (BOTTOM_LEFT_FIELD.match(currentFieldName, parser.getDeprecationHandler())) {\r\n                    GeoUtils.parseGeoPoint(parser, sparse, false, GeoUtils.EffectivePoint.BOTTOM_LEFT);\r\n                    bottom = sparse.getLat();\r\n                    left = sparse.getLon();\r\n                } else {\r\n                    throw new ElasticsearchParseException(\"failed to parse bounding box. unexpected field [{}]\", currentFieldName);\r\n                }\r\n            }\r\n        } else {\r\n            throw new ElasticsearchParseException(\"failed to parse bounding box. field name expected but [{}] found\", token);\r\n        }\r\n    }\r\n    if (envelope != null) {\r\n        if (Double.isNaN(top) == false || Double.isNaN(bottom) == false || Double.isNaN(left) == false || Double.isNaN(right) == false) {\r\n            throw new ElasticsearchParseException(\"failed to parse bounding box. Conflicting definition found \" + \"using well-known text and explicit corners.\");\r\n        }\r\n        org.locationtech.spatial4j.shape.Rectangle r = envelope.buildS4J();\r\n        return new double[] { r.getMinY(), r.getMaxY(), r.getMinX(), r.getMaxX() };\r\n    }\r\n    return new double[] { bottom, top, left, right };\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.create.CreateIndexClusterStateUpdateRequest.resizeType",
	"Comment": "returns the resize type or null if this is an ordinary create index request",
	"Method": "CreateIndexClusterStateUpdateRequest resizeType(ResizeType resizeType,ResizeType resizeType){\r\n    return resizeType;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.SisterAnnotationStats.main",
	"Comment": "calculate sister annotation statistics suitable for doingselective sister splitting in the pcfgparser inside thefactoredparser.",
	"Method": "void main(String[] args){\r\n    ClassicCounter<String> c = new ClassicCounter();\r\n    c.setCount(\"A\", 0);\r\n    c.setCount(\"B\", 1);\r\n    double d = Counters.klDivergence(c, c);\r\n    System.out.println(\"KL Divergence: \" + d);\r\n    String encoding = \"UTF-8\";\r\n    if (args.length > 1) {\r\n        encoding = args[1];\r\n    }\r\n    if (args.length < 1) {\r\n        System.out.println(\"Usage: ParentAnnotationStats treebankPath\");\r\n    } else {\r\n        SisterAnnotationStats pas = new SisterAnnotationStats();\r\n        Treebank treebank = new DiskTreebank(in -> new PennTreeReader(in, new LabeledScoredTreeFactory(new StringLabelFactory()), new BobChrisTreeNormalizer()), encoding);\r\n        treebank.loadPath(args[0]);\r\n        treebank.apply(pas);\r\n        pas.printStats();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksExecutor.getDescription",
	"Comment": "returns task description that will be available via task manager",
	"Method": "String getDescription(PersistentTask<Params> taskInProgress){\r\n    return \"id=\" + taskInProgress.getId();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotResponse.getSnapshotInfo",
	"Comment": "returns snapshot information if snapshot was completed by the time this method returned or null otherwise.",
	"Method": "SnapshotInfo getSnapshotInfo(){\r\n    return snapshotInfo;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.SplittingGrammarExtractor.recalculateBetas",
	"Comment": "recalculates the betas for all known transitions.the currentbetas are used to produce probabilities, which then are used tocompute new betas.if splitstates is true, then theprobabilities produced are as if the states were split again fromthe last time betas were calculated.the return value is whether or not the betas have mostlyconverged from the last time this method was called.obviouslyif splitstates was true, the betas will be entirely different, sothis is false.otherwise, the new betas are compared against theold values, and convergence means they differ by less thanepsilon.",
	"Method": "boolean recalculateBetas(boolean splitStates){\r\n    if (splitStates) {\r\n        if (DEBUG()) {\r\n            System.out.println(\"Pre-split betas\");\r\n            outputBetas();\r\n        }\r\n        splitBetas();\r\n        if (DEBUG()) {\r\n            System.out.println(\"Post-split betas\");\r\n            outputBetas();\r\n        }\r\n    }\r\n    TwoDimensionalMap<String, String, double[][]> tempUnaryBetas = new TwoDimensionalMap();\r\n    ThreeDimensionalMap<String, String, String, double[][][]> tempBinaryBetas = new ThreeDimensionalMap();\r\n    recalculateTemporaryBetas(splitStates, null, tempUnaryBetas, tempBinaryBetas);\r\n    boolean converged = useNewBetas(!splitStates, tempUnaryBetas, tempBinaryBetas);\r\n    if (DEBUG()) {\r\n        outputBetas();\r\n    }\r\n    return converged;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.IndexingMemoryController.checkIdle",
	"Comment": "ask this shard to check now whether it is inactive, and reduces its indexing buffer if so.",
	"Method": "void checkIdle(IndexShard shard,long inactiveTimeNS){\r\n    try {\r\n        shard.checkIdle(inactiveTimeNS);\r\n    } catch (AlreadyClosedException e) {\r\n        logger.trace(() -> new ParameterizedMessage(\"ignore exception while checking if shard {} is inactive\", shard.shardId()), e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.FileSequentialCollection.iterator",
	"Comment": "return an iterator over files in the collection.this version lazily works its way down directories.",
	"Method": "Iterator<File> iterator(){\r\n    return new FileSequentialCollectionIterator();\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.SettingsModule.registerSetting",
	"Comment": "registers a new setting. this method should be used by plugins in order to expose any custom settings the plugin defines.unless a setting is registered the setting is unusable. if a setting is never the less specified the node will rejectthe setting during startup.",
	"Method": "void registerSetting(Setting<?> setting){\r\n    if (setting.isFiltered()) {\r\n        if (settingsFilterPattern.contains(setting.getKey()) == false) {\r\n            registerSettingsFilter(setting.getKey());\r\n        }\r\n    }\r\n    if (setting.hasNodeScope() || setting.hasIndexScope()) {\r\n        if (setting.hasNodeScope()) {\r\n            Setting<?> existingSetting = nodeSettings.get(setting.getKey());\r\n            if (existingSetting != null) {\r\n                throw new IllegalArgumentException(\"Cannot register setting [\" + setting.getKey() + \"] twice\");\r\n            }\r\n            nodeSettings.put(setting.getKey(), setting);\r\n        }\r\n        if (setting.hasIndexScope()) {\r\n            Setting<?> existingSetting = indexSettings.get(setting.getKey());\r\n            if (existingSetting != null) {\r\n                throw new IllegalArgumentException(\"Cannot register setting [\" + setting.getKey() + \"] twice\");\r\n            }\r\n            indexSettings.put(setting.getKey(), setting);\r\n        }\r\n    } else {\r\n        throw new IllegalArgumentException(\"No scope found for setting [\" + setting.getKey() + \"]\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.UpdateByQueryRequest.getPipeline",
	"Comment": "ingest pipeline to set on index requests made by this action.",
	"Method": "String getPipeline(){\r\n    return pipeline;\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.RelationTripleSegmenter.segment",
	"Comment": "segment the given parse tree, forcing all nodes to be consumed.",
	"Method": "Optional<RelationTriple> segment(SemanticGraph parse,Optional<Double> confidence,boolean consumeAll,Optional<RelationTriple> segment,SemanticGraph parse,Optional<Double> confidence){\r\n    return segment(parse, confidence, true);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.CommonTermsQueryBuilder.lowFreqMinimumShouldMatch",
	"Comment": "sets the minimum number of low frequent query terms that need to match in order toproduce a hit.",
	"Method": "CommonTermsQueryBuilder lowFreqMinimumShouldMatch(String lowFreqMinimumShouldMatch,String lowFreqMinimumShouldMatch){\r\n    return this.lowFreqMinimumShouldMatch;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryShardContext.getFullyQualifiedIndex",
	"Comment": "returns the fully qualified index including a remote cluster alias if applicable, and the index uuid",
	"Method": "Index getFullyQualifiedIndex(){\r\n    return fullyQualifiedIndex;\r\n}"
}, {
	"Path": "org.elasticsearch.common.PidFile.isDeleteOnExit",
	"Comment": "returns true iff the process id file is deleted on system exit. otherwise false.",
	"Method": "boolean isDeleteOnExit(){\r\n    return deleteOnExit;\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.readStreamFromString",
	"Comment": "returns an objectinputstream reading from any of a url, a classpath resource, or a file.the classpath takes priority over the file system.this stream is buffered and, if necessary, gunzipped.",
	"Method": "ObjectInputStream readStreamFromString(String filenameOrUrl){\r\n    InputStream is = getInputStreamFromURLOrClasspathOrFileSystem(filenameOrUrl);\r\n    return new ObjectInputStream(is);\r\n}"
}, {
	"Path": "android.util.ArrayMap.ensureCapacity",
	"Comment": "ensure the array map can hold at least minimumcapacityitems.",
	"Method": "void ensureCapacity(int minimumCapacity){\r\n    if (mHashes.length < minimumCapacity) {\r\n        final int[] ohashes = mHashes;\r\n        final Object[] oarray = mArray;\r\n        allocArrays(minimumCapacity);\r\n        if (mSize > 0) {\r\n            System.arraycopy(ohashes, 0, mHashes, 0, mSize);\r\n            System.arraycopy(oarray, 0, mArray, 0, mSize << 1);\r\n        }\r\n        freeArrays(ohashes, oarray, mSize);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.ReaderIteratorFactory.retainAll",
	"Comment": "removes all objects from the underlying collection of input sourcesexcept those in collection c",
	"Method": "boolean retainAll(Collection<?> c){\r\n    return this.c.retainAll(c);\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.SystemCallFilter.init",
	"Comment": "attempt to drop the capability to execute for the process.this is best effort and os and architecture dependent. it may throw any throwable.",
	"Method": "int init(Path tmpFile){\r\n    if (Constants.LINUX) {\r\n        return linuxImpl();\r\n    } else if (Constants.MAC_OS_X) {\r\n        bsdImpl();\r\n        macImpl(tmpFile);\r\n        return 1;\r\n    } else if (Constants.SUN_OS) {\r\n        solarisImpl();\r\n        return 1;\r\n    } else if (Constants.FREE_BSD || OPENBSD) {\r\n        bsdImpl();\r\n        return 1;\r\n    } else if (Constants.WINDOWS) {\r\n        windowsImpl();\r\n        return 1;\r\n    } else {\r\n        throw new UnsupportedOperationException(\"syscall filtering not supported for OS: '\" + Constants.OS_NAME + \"'\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility",
	"Comment": "checks the mappings for compatibility with the current version",
	"Method": "void checkMappingsCompatibility(IndexMetaData indexMetaData){\r\n    try {\r\n        IndexSettings indexSettings = new IndexSettings(indexMetaData, this.settings);\r\n        final Map<String, TriFunction<Settings, Version, ScriptService, Similarity>> similarityMap = new AbstractMap<String, TriFunction<Settings, Version, ScriptService, Similarity>>() {\r\n            @Override\r\n            public boolean containsKey(Object key) {\r\n                return true;\r\n            }\r\n            @Override\r\n            public TriFunction<Settings, Version, ScriptService, Similarity> get(Object key) {\r\n                assert key instanceof String : \"key must be a string but was: \" + key.getClass();\r\n                return SimilarityService.BUILT_IN.get(SimilarityService.DEFAULT_SIMILARITY);\r\n            }\r\n            @Override\r\n            public Set<Entry<String, TriFunction<Settings, Version, ScriptService, Similarity>>> entrySet() {\r\n                return Collections.emptySet();\r\n            }\r\n        };\r\n        SimilarityService similarityService = new SimilarityService(indexSettings, null, similarityMap);\r\n        final NamedAnalyzer fakeDefault = new NamedAnalyzer(\"default\", AnalyzerScope.INDEX, new Analyzer() {\r\n            @Override\r\n            protected TokenStreamComponents createComponents(String fieldName) {\r\n                throw new UnsupportedOperationException(\"shouldn't be here\");\r\n            }\r\n        });\r\n        final Map<String, NamedAnalyzer> analyzerMap = new AbstractMap<String, NamedAnalyzer>() {\r\n            @Override\r\n            public NamedAnalyzer get(Object key) {\r\n                assert key instanceof String : \"key must be a string but was: \" + key.getClass();\r\n                return new NamedAnalyzer((String) key, AnalyzerScope.INDEX, fakeDefault.analyzer());\r\n            }\r\n            @Override\r\n            public Set<Entry<String, NamedAnalyzer>> entrySet() {\r\n                return Collections.emptySet();\r\n            }\r\n        };\r\n        try (IndexAnalyzers fakeIndexAnalzyers = new IndexAnalyzers(indexSettings, fakeDefault, fakeDefault, fakeDefault, analyzerMap, analyzerMap, analyzerMap)) {\r\n            MapperService mapperService = new MapperService(indexSettings, fakeIndexAnalzyers, xContentRegistry, similarityService, mapperRegistry, () -> null);\r\n            mapperService.merge(indexMetaData, MapperService.MergeReason.MAPPING_RECOVERY);\r\n        }\r\n    } catch (Exception ex) {\r\n        throw new IllegalStateException(\"unable to upgrade the mappings for the index [\" + indexMetaData.getIndex() + \"]\", ex);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility",
	"Comment": "checks the mappings for compatibility with the current version",
	"Method": "void checkMappingsCompatibility(IndexMetaData indexMetaData){\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility",
	"Comment": "checks the mappings for compatibility with the current version",
	"Method": "void checkMappingsCompatibility(IndexMetaData indexMetaData){\r\n    assert key instanceof String : \"key must be a string but was: \" + key.getClass();\r\n    return SimilarityService.BUILT_IN.get(SimilarityService.DEFAULT_SIMILARITY);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility",
	"Comment": "checks the mappings for compatibility with the current version",
	"Method": "void checkMappingsCompatibility(IndexMetaData indexMetaData){\r\n    return Collections.emptySet();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility",
	"Comment": "checks the mappings for compatibility with the current version",
	"Method": "void checkMappingsCompatibility(IndexMetaData indexMetaData){\r\n    throw new UnsupportedOperationException(\"shouldn't be here\");\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility",
	"Comment": "checks the mappings for compatibility with the current version",
	"Method": "void checkMappingsCompatibility(IndexMetaData indexMetaData){\r\n    assert key instanceof String : \"key must be a string but was: \" + key.getClass();\r\n    return new NamedAnalyzer((String) key, AnalyzerScope.INDEX, fakeDefault.analyzer());\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility",
	"Comment": "checks the mappings for compatibility with the current version",
	"Method": "void checkMappingsCompatibility(IndexMetaData indexMetaData){\r\n    return Collections.emptySet();\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.maybeSyncGlobalCheckpoint",
	"Comment": "syncs the global checkpoint to the replicas if the global checkpoint on at least one replica is behind the global checkpoint on theprimary.",
	"Method": "void maybeSyncGlobalCheckpoint(String reason){\r\n    verifyNotClosed();\r\n    assert shardRouting.primary() : \"only call maybeSyncGlobalCheckpoint on primary shard\";\r\n    if (replicationTracker.isPrimaryMode() == false) {\r\n        return;\r\n    }\r\n    assert assertPrimaryMode();\r\n    final SeqNoStats stats = getEngine().getSeqNoStats(replicationTracker.getGlobalCheckpoint());\r\n    if (stats.getMaxSeqNo() == stats.getGlobalCheckpoint()) {\r\n        final ObjectLongMap<String> globalCheckpoints = getInSyncGlobalCheckpoints();\r\n        final String allocationId = routingEntry().allocationId().getId();\r\n        assert globalCheckpoints.containsKey(allocationId);\r\n        final long globalCheckpoint = globalCheckpoints.get(allocationId);\r\n        final boolean syncNeeded = StreamSupport.stream(globalCheckpoints.values().spliterator(), false).anyMatch(v -> v.value < globalCheckpoint);\r\n        if (syncNeeded) {\r\n            logger.trace(\"syncing global checkpoint for [{}]\", reason);\r\n            globalCheckpointSyncer.run();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.IndexMetaData.selectShrinkShards",
	"Comment": "returns the source shard ids to shrink into the given shard id.",
	"Method": "Set<ShardId> selectShrinkShards(int shardId,IndexMetaData sourceIndexMetadata,int numTargetShards){\r\n    if (shardId >= numTargetShards) {\r\n        throw new IllegalArgumentException(\"the number of target shards (\" + numTargetShards + \") must be greater than the shard id: \" + shardId);\r\n    }\r\n    if (sourceIndexMetadata.getNumberOfShards() < numTargetShards) {\r\n        throw new IllegalArgumentException(\"the number of target shards [\" + numTargetShards + \"] must be less that the number of source shards [\" + sourceIndexMetadata.getNumberOfShards() + \"]\");\r\n    }\r\n    int routingFactor = getRoutingFactor(sourceIndexMetadata.getNumberOfShards(), numTargetShards);\r\n    Set<ShardId> shards = new HashSet(routingFactor);\r\n    for (int i = shardId * routingFactor; i < routingFactor * shardId + routingFactor; i++) {\r\n        shards.add(new ShardId(sourceIndexMetadata.getIndex(), i));\r\n    }\r\n    return shards;\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.isRelocated",
	"Comment": "returns whether the replication tracker has relocated away to another shard copy.",
	"Method": "boolean isRelocated(){\r\n    return relocated;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.IndexMetaDataUpdater.changes",
	"Comment": "helper method that creates update entry for the given shard id if such an entry does not exist yet.",
	"Method": "Updates changes(ShardId shardId){\r\n    return shardChanges.computeIfAbsent(shardId, k -> new Updates());\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.MetaDataStateFormat.deleteMetaState",
	"Comment": "deletes all meta state directories recursively for the given data locations",
	"Method": "void deleteMetaState(Path dataLocations){\r\n    Path[] stateDirectories = new Path[dataLocations.length];\r\n    for (int i = 0; i < dataLocations.length; i++) {\r\n        stateDirectories[i] = dataLocations[i].resolve(STATE_DIR_NAME);\r\n    }\r\n    IOUtils.rm(stateDirectories);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.pascal.ISODateInstance.extractWeekday",
	"Comment": "this is a backup method if everything else fails.it searches for nameddays of the week and if it finds one, it sets that as the date in lowercase form",
	"Method": "boolean extractWeekday(String inputDate){\r\n    for (Pattern p : weekdayArray) {\r\n        Matcher m = p.matcher(inputDate);\r\n        if (m.find()) {\r\n            String extract = m.group(0);\r\n            isoDate = extract.toLowerCase();\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.updateGlobalCheckpointOnReplica",
	"Comment": "updates the global checkpoint on a replica shard after it has been updated by the primary.",
	"Method": "void updateGlobalCheckpointOnReplica(long globalCheckpoint,String reason){\r\n    assert invariant();\r\n    assert primaryMode == false;\r\n    updateGlobalCheckpoint(shardAllocationId, globalCheckpoint, current -> {\r\n        logger.trace(\"updated global checkpoint from [{}] to [{}] due to [{}]\", current, globalCheckpoint, reason);\r\n        onGlobalCheckpointUpdated.accept(globalCheckpoint);\r\n    });\r\n    assert invariant();\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Settings.filter",
	"Comment": "returns a new settings object that contains all setting of the current one filtered by the given settings key predicate.",
	"Method": "Settings filter(Predicate<String> predicate){\r\n    return new Settings(new FilteredMap(this.settings, predicate, null), secureSettings == null ? null : new PrefixedSecureSettings(secureSettings, \"\", predicate));\r\n}"
}, {
	"Path": "android.text.TextUtils.packRangeInLong",
	"Comment": "pack 2 int values into a long, useful as a return value for a range",
	"Method": "long packRangeInLong(int start,int end){\r\n    return (((long) start) << 32) | end;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.IndexMetaData.selectRecoverFromShards",
	"Comment": "selects the source shards for a local shard recovery. this might either be a split or a shrink operation.",
	"Method": "Set<ShardId> selectRecoverFromShards(int shardId,IndexMetaData sourceIndexMetadata,int numTargetShards){\r\n    if (sourceIndexMetadata.getNumberOfShards() > numTargetShards) {\r\n        return selectShrinkShards(shardId, sourceIndexMetadata, numTargetShards);\r\n    } else if (sourceIndexMetadata.getNumberOfShards() < numTargetShards) {\r\n        return Collections.singleton(selectSplitShard(shardId, sourceIndexMetadata, numTargetShards));\r\n    }\r\n    throw new IllegalArgumentException(\"can't select recover from shards if both indices have the same number of shards\");\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.Initializer.validateOustandingInjections",
	"Comment": "prepares member injectors for all injected instances. this prompts guice to do static analysison the injected instances.",
	"Method": "void validateOustandingInjections(Errors errors){\r\n    for (InjectableReference<?> reference : pendingInjection.values()) {\r\n        try {\r\n            reference.validate(errors);\r\n        } catch (ErrorsException e) {\r\n            errors.merge(e.getErrors());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addWithoutBreaking",
	"Comment": "add an exact number of bytes, not checking for tripping thecircuit breaker. this bypasses the overheadconstant multiplication.also does not check with the parent breaker to see if the parent limithas been exceeded.",
	"Method": "long addWithoutBreaking(long bytes){\r\n    long u = used.addAndGet(bytes);\r\n    if (logger.isTraceEnabled()) {\r\n        logger.trace(\"[{}] Adjusted breaker by [{}] bytes, now [{}]\", this.name, bytes, u);\r\n    }\r\n    assert u >= 0 : \"Used bytes: [\" + u + \"] must be >= 0\";\r\n    return u;\r\n}"
}, {
	"Path": "android.util.Log.getStackTraceString",
	"Comment": "handy function to get a loggable stack trace from a throwable",
	"Method": "String getStackTraceString(Throwable tr){\r\n    if (tr == null) {\r\n        return \"\";\r\n    }\r\n    Throwable t = tr;\r\n    while (t != null) {\r\n        if (t instanceof UnknownHostException) {\r\n            return \"\";\r\n        }\r\n        t = t.getCause();\r\n    }\r\n    StringWriter sw = new StringWriter();\r\n    PrintWriter pw = new PrintWriter(sw, false);\r\n    tr.printStackTrace(pw);\r\n    pw.flush();\r\n    return sw.toString();\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.ArrayMath.safeMin",
	"Comment": "returns the smallest value in a vector of doubles.any values whichare nan or infinite are ignored.if the vector is empty, 0.0 isreturned.",
	"Method": "double safeMin(double[] v){\r\n    double[] u = filterNaNAndInfinite(v);\r\n    if (numRows(u) == 0)\r\n        return 0.0;\r\n    return min(u);\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.ElectMasterService.tieBreakActiveMasters",
	"Comment": "selects the best active master to join, where multiple are discovered",
	"Method": "DiscoveryNode tieBreakActiveMasters(Collection<DiscoveryNode> activeMasters){\r\n    return activeMasters.stream().min(ElectMasterService::compareNodes).get();\r\n}"
}, {
	"Path": "org.elasticsearch.ExceptionsHelper.maybeError",
	"Comment": "unwrap the specified throwable looking for any suppressed errors or errors as a root cause of the specified throwable.",
	"Method": "Optional<Error> maybeError(Throwable cause,Logger logger){\r\n    if (cause instanceof Error) {\r\n        return Optional.of((Error) cause);\r\n    }\r\n    final Queue<Throwable> queue = new LinkedList();\r\n    queue.add(cause);\r\n    int iterations = 0;\r\n    while (queue.isEmpty() == false) {\r\n        iterations++;\r\n        if (iterations > MAX_ITERATIONS) {\r\n            logger.warn(\"giving up looking for fatal errors\", cause);\r\n            break;\r\n        }\r\n        final Throwable current = queue.remove();\r\n        if (current instanceof Error) {\r\n            return Optional.of((Error) current);\r\n        }\r\n        Collections.addAll(queue, current.getSuppressed());\r\n        if (current.getCause() != null) {\r\n            queue.add(current.getCause());\r\n        }\r\n    }\r\n    return Optional.empty();\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BackoffPolicy.noBackoff",
	"Comment": "creates a backoff policy that will not allow any backoff, i.e. an operation will fail after the first attempt.",
	"Method": "BackoffPolicy noBackoff(){\r\n    return NO_BACKOFF;\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequestBuilder.setDoc",
	"Comment": "sets the doc to use for updates when a script is not specified, the doc providedis a field and value pairs.",
	"Method": "UpdateRequestBuilder setDoc(IndexRequest indexRequest,UpdateRequestBuilder setDoc,XContentBuilder source,UpdateRequestBuilder setDoc,Map<String, Object> source,UpdateRequestBuilder setDoc,Map<String, Object> source,XContentType contentType,UpdateRequestBuilder setDoc,String source,XContentType xContentType,UpdateRequestBuilder setDoc,byte[] source,XContentType xContentType,UpdateRequestBuilder setDoc,byte[] source,int offset,int length,XContentType xContentType,UpdateRequestBuilder setDoc,Object source,UpdateRequestBuilder setDoc,XContentType xContentType,Object source){\r\n    request.doc(xContentType, source);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MatchPhrasePrefixQueryBuilder.analyzer",
	"Comment": "explicitly set the analyzer to use. defaults to use explicit mappingconfig for the field, or, if not set, the default search analyzer.",
	"Method": "MatchPhrasePrefixQueryBuilder analyzer(String analyzer,String analyzer){\r\n    return this.analyzer;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.DocumentReader.readText",
	"Comment": "returns everything that can be read from the given reader as a string.returns null if the given reader is null.",
	"Method": "String readText(Reader in){\r\n    if (in == null) {\r\n        return (null);\r\n    }\r\n    BufferedReader br = getBufferedReader(in);\r\n    StringBuilder sb = new StringBuilder(16000);\r\n    int c;\r\n    while ((c = br.read()) >= 0) {\r\n        sb.append((char) c);\r\n    }\r\n    return sb.toString();\r\n}"
}, {
	"Path": "org.openqa.grid.common.RegistrationRequest.fromJson",
	"Comment": "create an object from a registration request formatted as a json string.",
	"Method": "RegistrationRequest fromJson(Map<String, Object> raw){\r\n    Json json = new Json();\r\n    RegistrationRequest request = new RegistrationRequest();\r\n    if (raw.get(\"name\") instanceof String) {\r\n        request.name = (String) raw.get(\"name\");\r\n    }\r\n    if (raw.get(\"description\") instanceof String) {\r\n        request.description = (String) raw.get(\"description\");\r\n    }\r\n    if (raw.get(\"configuration\") instanceof Map) {\r\n        String converted = json.toJson(raw.get(\"configuration\"));\r\n        request.configuration = GridConfiguredJson.toType(converted, GridNodeConfiguration.class);\r\n    }\r\n    return request;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.activateWithPrimaryContext",
	"Comment": "updates the known allocation ids and the local checkpoints for the corresponding allocations from a primary relocation source.",
	"Method": "void activateWithPrimaryContext(ReplicationTracker.PrimaryContext primaryContext){\r\n    assert shardRouting.primary() && shardRouting.isRelocationTarget() : \"only primary relocation target can update allocation IDs from primary context: \" + shardRouting;\r\n    assert primaryContext.getCheckpointStates().containsKey(routingEntry().allocationId().getId()) && getLocalCheckpoint() == primaryContext.getCheckpointStates().get(routingEntry().allocationId().getId()).getLocalCheckpoint();\r\n    synchronized (mutex) {\r\n        replicationTracker.activateWithPrimaryContext(primaryContext);\r\n        if (getMaxSeqNoOfUpdatesOrDeletes() == SequenceNumbers.UNASSIGNED_SEQ_NO) {\r\n            assert indexSettings.getIndexVersionCreated().before(Version.V_6_5_0);\r\n            getEngine().advanceMaxSeqNoOfUpdatesOrDeletes(seqNoStats().getMaxSeqNo());\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.apache.dubbo.registry.redis.RedisRegistry.clean",
	"Comment": "the monitoring center is responsible for deleting outdated dirty data",
	"Method": "void clean(Jedis jedis){\r\n    Set<String> keys = jedis.keys(root + Constants.ANY_VALUE);\r\n    if (keys != null && !keys.isEmpty()) {\r\n        for (String key : keys) {\r\n            Map<String, String> values = jedis.hgetAll(key);\r\n            if (values != null && values.size() > 0) {\r\n                boolean delete = false;\r\n                long now = System.currentTimeMillis();\r\n                for (Map.Entry<String, String> entry : values.entrySet()) {\r\n                    URL url = URL.valueOf(entry.getKey());\r\n                    if (url.getParameter(Constants.DYNAMIC_KEY, true)) {\r\n                        long expire = Long.parseLong(entry.getValue());\r\n                        if (expire < now) {\r\n                            jedis.hdel(key, entry.getKey());\r\n                            delete = true;\r\n                            if (logger.isWarnEnabled()) {\r\n                                logger.warn(\"Delete expired key: \" + key + \" -> value: \" + entry.getKey() + \", expire: \" + new Date(expire) + \", now: \" + new Date(now));\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n                if (delete) {\r\n                    jedis.publish(key, Constants.UNREGISTER);\r\n                }\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.resolveDocVersion",
	"Comment": "resolves the current version of the document, returning null if not found",
	"Method": "VersionValue resolveDocVersion(Operation op){\r\n    assert incrementVersionLookup();\r\n    VersionValue versionValue = getVersionFromMap(op.uid().bytes());\r\n    if (versionValue == null) {\r\n        assert incrementIndexVersionLookup();\r\n        final long currentVersion = loadCurrentVersionFromIndex(op.uid());\r\n        if (currentVersion != Versions.NOT_FOUND) {\r\n            versionValue = new IndexVersionValue(null, currentVersion, SequenceNumbers.UNASSIGNED_SEQ_NO, 0L);\r\n        }\r\n    } else if (engineConfig.isEnableGcDeletes() && versionValue.isDelete() && (engineConfig.getThreadPool().relativeTimeInMillis() - ((DeleteVersionValue) versionValue).time) > getGcDeletesInMillis()) {\r\n        versionValue = null;\r\n    }\r\n    return versionValue;\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.VersionValue.getLocation",
	"Comment": "returns the translog location for this version value or null. this is optional and might not be tracked all the time.",
	"Method": "Translog.Location getLocation(){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequest.getContentType",
	"Comment": "the content type. this will be used when generating a document from user provided objects like maps and when parsing thesource at index time",
	"Method": "XContentType getContentType(){\r\n    return contentType;\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.ActiveShardCount.enoughShardsActive",
	"Comment": "returns true iff the active shard count in the shard routing table is enoughto meet the required shard count represented by this instance.",
	"Method": "boolean enoughShardsActive(int activeShardCount,boolean enoughShardsActive,ClusterState clusterState,String indices,boolean enoughShardsActive,IndexShardRoutingTable shardRoutingTable){\r\n    final int activeShardCount = shardRoutingTable.activeShards().size();\r\n    if (this == ActiveShardCount.ALL) {\r\n        return activeShardCount == shardRoutingTable.replicaShards().size() + 1;\r\n    } else if (this == ActiveShardCount.DEFAULT) {\r\n        return activeShardCount >= 1;\r\n    } else {\r\n        return activeShardCount >= value;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.nndep.ParsingSystem.evaluate",
	"Comment": "evaluate performance on a list of sentences, predicted parses,and gold parses.",
	"Method": "Map<String, Double> evaluate(List<CoreMap> sentences,List<DependencyTree> trees,List<DependencyTree> goldTrees){\r\n    Map<String, Double> result = new HashMap();\r\n    Set<String> punctuationTags = getPunctuationTags();\r\n    if (trees.size() != goldTrees.size()) {\r\n        log.error(\"Incorrect number of trees.\");\r\n        return null;\r\n    }\r\n    int correctArcs = 0;\r\n    int correctArcsNoPunc = 0;\r\n    int correctHeads = 0;\r\n    int correctHeadsNoPunc = 0;\r\n    int correctTrees = 0;\r\n    int correctTreesNoPunc = 0;\r\n    int correctRoot = 0;\r\n    int sumArcs = 0;\r\n    int sumArcsNoPunc = 0;\r\n    for (int i = 0; i < trees.size(); ++i) {\r\n        List<CoreLabel> tokens = sentences.get(i).get(CoreAnnotations.TokensAnnotation.class);\r\n        if (trees.get(i).n != goldTrees.get(i).n) {\r\n            log.error(\"Tree \" + (i + 1) + \": incorrect number of nodes.\");\r\n            return null;\r\n        }\r\n        if (!trees.get(i).isTree()) {\r\n            log.error(\"Tree \" + (i + 1) + \": illegal.\");\r\n            return null;\r\n        }\r\n        int nCorrectHead = 0;\r\n        int nCorrectHeadNoPunc = 0;\r\n        int nNoPunc = 0;\r\n        for (int j = 1; j <= trees.get(i).n; ++j) {\r\n            if (trees.get(i).getHead(j) == goldTrees.get(i).getHead(j)) {\r\n                ++correctHeads;\r\n                ++nCorrectHead;\r\n                if (trees.get(i).getLabel(j).equals(goldTrees.get(i).getLabel(j)))\r\n                    ++correctArcs;\r\n            }\r\n            ++sumArcs;\r\n            String tag = tokens.get(j - 1).tag();\r\n            if (!punctuationTags.contains(tag)) {\r\n                ++sumArcsNoPunc;\r\n                ++nNoPunc;\r\n                if (trees.get(i).getHead(j) == goldTrees.get(i).getHead(j)) {\r\n                    ++correctHeadsNoPunc;\r\n                    ++nCorrectHeadNoPunc;\r\n                    if (trees.get(i).getLabel(j).equals(goldTrees.get(i).getLabel(j)))\r\n                        ++correctArcsNoPunc;\r\n                }\r\n            }\r\n        }\r\n        if (nCorrectHead == trees.get(i).n)\r\n            ++correctTrees;\r\n        if (nCorrectHeadNoPunc == nNoPunc)\r\n            ++correctTreesNoPunc;\r\n        if (trees.get(i).getRoot() == goldTrees.get(i).getRoot())\r\n            ++correctRoot;\r\n    }\r\n    result.put(\"UAS\", correctHeads * 100.0 / sumArcs);\r\n    result.put(\"UASnoPunc\", correctHeadsNoPunc * 100.0 / sumArcsNoPunc);\r\n    result.put(\"LAS\", correctArcs * 100.0 / sumArcs);\r\n    result.put(\"LASnoPunc\", correctArcsNoPunc * 100.0 / sumArcsNoPunc);\r\n    result.put(\"UEM\", correctTrees * 100.0 / trees.size());\r\n    result.put(\"UEMnoPunc\", correctTreesNoPunc * 100.0 / trees.size());\r\n    result.put(\"ROOT\", correctRoot * 100.0 / trees.size());\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder.toFunction",
	"Comment": "called on a data node, converts this scorefunctionbuilder into its corresponding lucene function object.",
	"Method": "ScoreFunction toFunction(QueryShardContext context){\r\n    ScoreFunction scoreFunction = doToFunction(context);\r\n    if (weight == null) {\r\n        return scoreFunction;\r\n    }\r\n    return new WeightFactorFunction(weight, scoreFunction);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequest.includeGlobalState",
	"Comment": "returns true if global state should be restored from this snapshot",
	"Method": "RestoreSnapshotRequest includeGlobalState(boolean includeGlobalState,boolean includeGlobalState){\r\n    return includeGlobalState;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.DocumentParser.dynamicOrDefault",
	"Comment": "find what the dynamic setting is given the current parse context and parent",
	"Method": "ObjectMapper.Dynamic dynamicOrDefault(ObjectMapper parentMapper,ParseContext context){\r\n    ObjectMapper.Dynamic dynamic = parentMapper.dynamic();\r\n    while (dynamic == null) {\r\n        int lastDotNdx = parentMapper.name().lastIndexOf('.');\r\n        if (lastDotNdx == -1) {\r\n            break;\r\n        }\r\n        String parentName = parentMapper.name().substring(0, lastDotNdx);\r\n        parentMapper = context.docMapper().objectMappers().get(parentName);\r\n        if (parentMapper == null) {\r\n            return ObjectMapper.Dynamic.TRUE;\r\n        }\r\n        dynamic = parentMapper.dynamic();\r\n    }\r\n    if (dynamic == null) {\r\n        return context.root().dynamic() == null ? ObjectMapper.Dynamic.TRUE : context.root().dynamic();\r\n    }\r\n    return dynamic;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.getOperationPrimaryTerm",
	"Comment": "returns the primary term that is currently being used to assign to operations",
	"Method": "long getOperationPrimaryTerm(){\r\n    return this.operationPrimaryTerm;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchResponse.getNumReducePhases",
	"Comment": "returns the number of reduce phases applied to obtain this search response",
	"Method": "int getNumReducePhases(){\r\n    return internalResponse.getNumReducePhases();\r\n}"
}, {
	"Path": "org.openqa.grid.internal.StatusServletTests.testHubGetSpecifiedConfig",
	"Comment": "if a certain set of parameters are requested to the hub, only those params are returned.",
	"Method": "void testHubGetSpecifiedConfig(){\r\n    String url = hubApi.toExternalForm();\r\n    HttpRequest r = new HttpRequest(POST, url);\r\n    Map<String, Object> j = ImmutableMap.of(\"configuration\", ImmutableList.of(\"timeout\", \"I'm not a valid key\", \"servlets\"));\r\n    r.setContent(new Json().toJson(j).getBytes(UTF_8));\r\n    HttpResponse response = client.execute(r);\r\n    assertEquals(200, response.getStatus());\r\n    Map<String, Object> o = extractObject(response);\r\n    assertTrue((Boolean) o.get(\"success\"));\r\n    assertEquals(12345L, o.get(\"timeout\"));\r\n    assertNull(o.get(\"I'm not a valid key\"));\r\n    assertTrue(((Collection) o.get(\"servlets\")).size() == 0);\r\n    assertNull(o.get(\"capabilityMatcher\"));\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.analyze.AnalyzeRequestBuilder.setField",
	"Comment": "sets the field that its analyzer will be used to analyze the text. note, requires an indexto be set.",
	"Method": "AnalyzeRequestBuilder setField(String field){\r\n    request.field(field);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaData.resolveWriteIndexRouting",
	"Comment": "returns indexing routing for the given aliasorindex. resolves routing from the alias metadata usedin the write index.",
	"Method": "String resolveWriteIndexRouting(String routing,String aliasOrIndex){\r\n    if (aliasOrIndex == null) {\r\n        return routing;\r\n    }\r\n    AliasOrIndex result = getAliasAndIndexLookup().get(aliasOrIndex);\r\n    if (result == null || result.isAlias() == false) {\r\n        return routing;\r\n    }\r\n    AliasOrIndex.Alias alias = (AliasOrIndex.Alias) result;\r\n    IndexMetaData writeIndex = alias.getWriteIndex();\r\n    if (writeIndex == null) {\r\n        throw new IllegalArgumentException(\"alias [\" + aliasOrIndex + \"] does not have a write index\");\r\n    }\r\n    AliasMetaData aliasMd = writeIndex.getAliases().get(alias.getAliasName());\r\n    if (aliasMd.indexRouting() != null) {\r\n        if (aliasMd.indexRouting().indexOf(',') != -1) {\r\n            throw new IllegalArgumentException(\"index/alias [\" + aliasOrIndex + \"] provided with routing value [\" + aliasMd.getIndexRouting() + \"] that resolved to several routing values, rejecting operation\");\r\n        }\r\n        if (routing != null) {\r\n            if (!routing.equals(aliasMd.indexRouting())) {\r\n                throw new IllegalArgumentException(\"Alias [\" + aliasOrIndex + \"] has index routing associated with it [\" + aliasMd.indexRouting() + \"], and was provided with routing value [\" + routing + \"], rejecting operation\");\r\n            }\r\n        }\r\n        return aliasMd.indexRouting();\r\n    }\r\n    return routing;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TragicExceptionHolder.setTragicException",
	"Comment": "sets the tragic exception or if the tragic exception is already set adds passed exception as suppressed exception",
	"Method": "void setTragicException(Exception ex){\r\n    assert ex != null;\r\n    if (tragedy.compareAndSet(null, ex) == false) {\r\n        if (tragedy.get() != ex) {\r\n            tragedy.get().addSuppressed(ex);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequestBuilder.setDoc",
	"Comment": "sets the artificial document from which to generate term vectors.",
	"Method": "TermVectorsRequestBuilder setDoc(XContentBuilder xContent){\r\n    request.doc(xContent);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.node.AdaptiveSelectionStats.getRanks",
	"Comment": "returns a map of node id to the ranking of the nodes based on the adaptive replica formula",
	"Method": "Map<String, Double> getRanks(){\r\n    return nodeComputedStats.entrySet().stream().collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().rank(clientOutgoingConnections.getOrDefault(e.getKey(), 0L))));\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.builders.PolygonBuilder.component",
	"Comment": "this method sets the component id of all edges in a ring to a given id and shifts thecoordinates of this component according to the dateline",
	"Method": "int component(Edge edge,int id,ArrayList<Edge> edges){\r\n    Edge any = edge;\r\n    while (any.coordinate.x == +DATELINE || any.coordinate.x == -DATELINE) {\r\n        if ((any = any.next) == edge) {\r\n            break;\r\n        }\r\n    }\r\n    double shiftOffset = any.coordinate.x > DATELINE ? DATELINE : (any.coordinate.x < -DATELINE ? -DATELINE : 0);\r\n    if (debugEnabled()) {\r\n        LOGGER.debug(\"shift: [{}]\", shiftOffset);\r\n    }\r\n    int length = 0, connectedComponents = 0;\r\n    int splitIndex = 1;\r\n    Edge current = edge;\r\n    Edge prev = edge;\r\n    HashMap<Coordinate, Tuple<Edge, Edge>> visitedEdge = new HashMap();\r\n    do {\r\n        current.coordinate = shift(current.coordinate, shiftOffset);\r\n        current.component = id;\r\n        if (edges != null) {\r\n            if (visitedEdge.containsKey(current.coordinate)) {\r\n                if (connectedComponents > 0 && current.next != edge) {\r\n                    throw new InvalidShapeException(\"Shape contains more than one shared point\");\r\n                }\r\n                final int visitID = -id;\r\n                Edge firstAppearance = visitedEdge.get(current.coordinate).v2();\r\n                Edge temp = firstAppearance.next;\r\n                firstAppearance.next = current.next;\r\n                current.next = temp;\r\n                current.component = visitID;\r\n                do {\r\n                    prev.component = visitID;\r\n                    prev = visitedEdge.get(prev.coordinate).v1();\r\n                    ++splitIndex;\r\n                } while (!current.coordinate.equals(prev.coordinate));\r\n                ++connectedComponents;\r\n            } else {\r\n                visitedEdge.put(current.coordinate, new Tuple<Edge, Edge>(prev, current));\r\n            }\r\n            edges.add(current);\r\n            prev = current;\r\n        }\r\n        length++;\r\n    } while (connectedComponents == 0 && (current = current.next) != edge);\r\n    return (splitIndex != 1) ? length - splitIndex : length;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.LeaderBulkByScrollTaskState.onSliceResponse",
	"Comment": "record a response from a slice and respond to the listener if the request is finished.",
	"Method": "void onSliceResponse(ActionListener<BulkByScrollResponse> listener,int sliceId,BulkByScrollResponse response){\r\n    results.setOnce(sliceId, new Result(sliceId, response));\r\n    recordSliceCompletionAndRespondIfAllDone(listener);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.shiftreduce.ShiftReduceParser.findRootStates",
	"Comment": "get all of the states which occur at the root, even if they occurelsewhere in the tree.useful for knowing when you can finalizea tree",
	"Method": "Set<String> findRootStates(List<Tree> trees){\r\n    Set<String> roots = Generics.newHashSet();\r\n    for (Tree tree : trees) {\r\n        roots.add(tree.value());\r\n    }\r\n    return Collections.unmodifiableSet(roots);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.MultiSearchResponse.getResponses",
	"Comment": "the list of responses, the order is the same as the one provided in the request.",
	"Method": "Item[] getResponses(){\r\n    return this.items;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterInfo.getNodeLeastAvailableDiskUsages",
	"Comment": "returns a node id to disk usage mapping for the path that has the least available space on the node.",
	"Method": "ImmutableOpenMap<String, DiskUsage> getNodeLeastAvailableDiskUsages(){\r\n    return this.leastAvailableSpaceUsage;\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.SentenceFragment.paddedWords",
	"Comment": "return the tokens in this fragment, but padded with null so that the index in thissentence matches the index of the parse tree.",
	"Method": "List<CoreLabel> paddedWords(){\r\n    int maxIndex = -1;\r\n    for (IndexedWord vertex : parseTree.vertexSet()) {\r\n        maxIndex = Math.max(maxIndex, vertex.index());\r\n    }\r\n    List<CoreLabel> tokens = new ArrayList(maxIndex);\r\n    for (int i = 0; i < maxIndex; ++i) {\r\n        tokens.add(null);\r\n    }\r\n    for (CoreLabel token : this.words) {\r\n        tokens.set(token.index() - 1, token);\r\n    }\r\n    return tokens;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.AbstractScopedSettings.applySettings",
	"Comment": "applies the given settings to all the settings consumers or to none of them. the settingswill be merged with the node settings before they are applied while given settings override existing nodesettings.",
	"Method": "Settings applySettings(Settings newSettings){\r\n    if (lastSettingsApplied != null && newSettings.equals(lastSettingsApplied)) {\r\n        return newSettings;\r\n    }\r\n    final Settings current = Settings.builder().put(this.settings).put(newSettings).build();\r\n    final Settings previous = Settings.builder().put(this.settings).put(this.lastSettingsApplied).build();\r\n    try {\r\n        List<Runnable> applyRunnables = new ArrayList();\r\n        for (SettingUpdater<?> settingUpdater : settingUpdaters) {\r\n            try {\r\n                applyRunnables.add(settingUpdater.updater(current, previous));\r\n            } catch (Exception ex) {\r\n                logger.warn(() -> new ParameterizedMessage(\"failed to prepareCommit settings for [{}]\", settingUpdater), ex);\r\n                throw ex;\r\n            }\r\n        }\r\n        for (Runnable settingUpdater : applyRunnables) {\r\n            settingUpdater.run();\r\n        }\r\n    } catch (Exception ex) {\r\n        logger.warn(\"failed to apply settings\", ex);\r\n        throw ex;\r\n    } finally {\r\n    }\r\n    return lastSettingsApplied = newSettings;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.sizeInBytesByMinGen",
	"Comment": "returns the size in bytes of the translog files at least the given generation",
	"Method": "long sizeInBytesByMinGen(long minGeneration){\r\n    try (ReleasableLock ignored = readLock.acquire()) {\r\n        ensureOpen();\r\n        return Stream.concat(readers.stream(), Stream.of(current)).filter(r -> r.getGeneration() >= minGeneration).mapToLong(BaseTranslogReader::sizeInBytes).sum();\r\n    }\r\n}"
}, {
	"Path": "org.json.JSONTokener.syntaxError",
	"Comment": "returns an exception containing the given message plus the currentposition and the entire input string.",
	"Method": "JSONException syntaxError(String message){\r\n    return new JSONException(message + this);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.EnglishTreebankParserParams.collinizer",
	"Comment": "the tree transformer used to produce trees for evaluation.it willbe applied both to the parser output and the gold tree.",
	"Method": "TreeTransformer collinizer(){\r\n    return new TreeCollinizer(tlp, true, englishTrain.splitBaseNP == 2, englishTrain.collapseWhCategories);\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster",
	"Comment": "the main function of a join thread. this function is guaranteed to join the clusteror spawn a new join thread upon failure to do so.",
	"Method": "void innerJoinCluster(){\r\n    DiscoveryNode masterNode = null;\r\n    final Thread currentThread = Thread.currentThread();\r\n    nodeJoinController.startElectionContext();\r\n    while (masterNode == null && joinThreadControl.joinThreadActive(currentThread)) {\r\n        masterNode = findMaster();\r\n    }\r\n    if (!joinThreadControl.joinThreadActive(currentThread)) {\r\n        logger.trace(\"thread is no longer in currentJoinThread. Stopping.\");\r\n        return;\r\n    }\r\n    if (transportService.getLocalNode().equals(masterNode)) {\r\n        final int requiredJoins = Math.max(0, electMaster.minimumMasterNodes() - 1);\r\n        logger.debug(\"elected as master, waiting for incoming joins ([{}] needed)\", requiredJoins);\r\n        nodeJoinController.waitToBeElectedAsMaster(requiredJoins, masterElectionWaitForJoinsTimeout, new NodeJoinController.ElectionCallback() {\r\n            @Override\r\n            public void onElectedAsMaster(ClusterState state) {\r\n                synchronized (stateMutex) {\r\n                    joinThreadControl.markThreadAsDone(currentThread);\r\n                }\r\n            }\r\n            @Override\r\n            public void onFailure(Throwable t) {\r\n                logger.trace(\"failed while waiting for nodes to join, rejoining\", t);\r\n                synchronized (stateMutex) {\r\n                    joinThreadControl.markThreadAsDoneAndStartNew(currentThread);\r\n                }\r\n            }\r\n        });\r\n    } else {\r\n        nodeJoinController.stopElectionContext(masterNode + \" elected\");\r\n        final boolean success = joinElectedMaster(masterNode);\r\n        synchronized (stateMutex) {\r\n            if (success) {\r\n                DiscoveryNode currentMasterNode = this.clusterState().getNodes().getMasterNode();\r\n                if (currentMasterNode == null) {\r\n                    logger.debug(\"no master node is set, despite of join request completing. retrying pings.\");\r\n                    joinThreadControl.markThreadAsDoneAndStartNew(currentThread);\r\n                } else if (currentMasterNode.equals(masterNode) == false) {\r\n                    joinThreadControl.stopRunningThreadAndRejoin(\"master_switched_while_finalizing_join\");\r\n                }\r\n                joinThreadControl.markThreadAsDone(currentThread);\r\n            } else {\r\n                joinThreadControl.markThreadAsDoneAndStartNew(currentThread);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster",
	"Comment": "the main function of a join thread. this function is guaranteed to join the clusteror spawn a new join thread upon failure to do so.",
	"Method": "void innerJoinCluster(){\r\n    synchronized (stateMutex) {\r\n        joinThreadControl.markThreadAsDone(currentThread);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster",
	"Comment": "the main function of a join thread. this function is guaranteed to join the clusteror spawn a new join thread upon failure to do so.",
	"Method": "void innerJoinCluster(){\r\n    logger.trace(\"failed while waiting for nodes to join, rejoining\", t);\r\n    synchronized (stateMutex) {\r\n        joinThreadControl.markThreadAsDoneAndStartNew(currentThread);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.ClusterPlugin.createAllocationDeciders",
	"Comment": "return deciders used to customize where shards are allocated.",
	"Method": "Collection<AllocationDecider> createAllocationDeciders(Settings settings,ClusterSettings clusterSettings){\r\n    return Collections.emptyList();\r\n}"
}, {
	"Path": "org.elasticsearch.plugins.Platforms.nativeControllerPath",
	"Comment": "the path to the native controller for a plugin with native components.",
	"Method": "Path nativeControllerPath(Path plugin){\r\n    return plugin.resolve(\"platform\").resolve(PLATFORM_NAME).resolve(\"bin\").resolve(PROGRAM_NAME);\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.Queries.not",
	"Comment": "return a query that matches all documents but those that match the given query.",
	"Method": "Query not(Query q){\r\n    return new BooleanQuery.Builder().add(new MatchAllDocsQuery(), Occur.MUST).add(q, Occur.MUST_NOT).build();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotRequestBuilder.setWaitForCompletion",
	"Comment": "if set to true the request should wait for the snapshot completion before returning.",
	"Method": "CreateSnapshotRequestBuilder setWaitForCompletion(boolean waitForCompletion){\r\n    request.waitForCompletion(waitForCompletion);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequest.fieldStatistics",
	"Comment": "return the field statistics for each term in the shard or skip.",
	"Method": "boolean fieldStatistics(TermVectorsRequest fieldStatistics,boolean fieldStatistics){\r\n    setFlag(Flag.FieldStatistics, fieldStatistics);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.DiffableUtils.readImmutableOpenIntMapDiff",
	"Comment": "loads an object that represents difference between two immutableopenintmaps of diffable objects using diffable proto object",
	"Method": "MapDiff<Integer, T, ImmutableOpenIntMap<T>> readImmutableOpenIntMapDiff(StreamInput in,KeySerializer<Integer> keySerializer,ValueSerializer<Integer, T> valueSerializer,MapDiff<Integer, T, ImmutableOpenIntMap<T>> readImmutableOpenIntMapDiff,StreamInput in,KeySerializer<Integer> keySerializer,Reader<T> reader,Reader<Diff<T>> diffReader){\r\n    return new ImmutableOpenIntMapDiff(in, keySerializer, new DiffableValueReader(reader, diffReader));\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.TypeConverterBindingProcessor.prepareBuiltInConverters",
	"Comment": "installs default converters for primitives, enums, and class literals.",
	"Method": "void prepareBuiltInConverters(InjectorImpl injector){\r\n    this.injector = injector;\r\n    try {\r\n        convertToPrimitiveType(int.class, Integer.class);\r\n        convertToPrimitiveType(long.class, Long.class);\r\n        convertToPrimitiveType(boolean.class, Boolean.class);\r\n        convertToPrimitiveType(byte.class, Byte.class);\r\n        convertToPrimitiveType(short.class, Short.class);\r\n        convertToPrimitiveType(float.class, Float.class);\r\n        convertToPrimitiveType(double.class, Double.class);\r\n        convertToClass(Character.class, new TypeConverter() {\r\n            @Override\r\n            public Object convert(String value, TypeLiteral<?> toType) {\r\n                value = value.trim();\r\n                if (value.length() != 1) {\r\n                    throw new RuntimeException(\"Length != 1.\");\r\n                }\r\n                return value.charAt(0);\r\n            }\r\n            @Override\r\n            public String toString() {\r\n                return \"TypeConverter<Character>\";\r\n            }\r\n        });\r\n        convertToClasses(Matchers.subclassesOf(Enum.class), new TypeConverter() {\r\n            @Override\r\n            @SuppressWarnings(\"unchecked\")\r\n            public Object convert(String value, TypeLiteral<?> toType) {\r\n                return Enum.valueOf((Class) toType.getRawType(), value);\r\n            }\r\n            @Override\r\n            public String toString() {\r\n                return \"TypeConverter<E extends Enum<E>>\";\r\n            }\r\n        });\r\n        internalConvertToTypes(new AbstractMatcher<TypeLiteral<?>>() {\r\n            @Override\r\n            public boolean matches(TypeLiteral<?> typeLiteral) {\r\n                return typeLiteral.getRawType() == Class.class;\r\n            }\r\n            @Override\r\n            public String toString() {\r\n                return \"Class<?>\";\r\n            }\r\n        }, new TypeConverter() {\r\n            @Override\r\n            public Object convert(String value, TypeLiteral<?> toType) {\r\n                try {\r\n                    return Class.forName(value);\r\n                } catch (ClassNotFoundException e) {\r\n                    throw new RuntimeException(e);\r\n                }\r\n            }\r\n            @Override\r\n            public String toString() {\r\n                return \"TypeConverter<Class<?>>\";\r\n            }\r\n        });\r\n    } finally {\r\n        this.injector = null;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.TypeConverterBindingProcessor.prepareBuiltInConverters",
	"Comment": "installs default converters for primitives, enums, and class literals.",
	"Method": "void prepareBuiltInConverters(InjectorImpl injector){\r\n    value = value.trim();\r\n    if (value.length() != 1) {\r\n        throw new RuntimeException(\"Length != 1.\");\r\n    }\r\n    return value.charAt(0);\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.TypeConverterBindingProcessor.prepareBuiltInConverters",
	"Comment": "installs default converters for primitives, enums, and class literals.",
	"Method": "void prepareBuiltInConverters(InjectorImpl injector){\r\n    return \"TypeConverter<Character>\";\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.TypeConverterBindingProcessor.prepareBuiltInConverters",
	"Comment": "installs default converters for primitives, enums, and class literals.",
	"Method": "void prepareBuiltInConverters(InjectorImpl injector){\r\n    return Enum.valueOf((Class) toType.getRawType(), value);\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.TypeConverterBindingProcessor.prepareBuiltInConverters",
	"Comment": "installs default converters for primitives, enums, and class literals.",
	"Method": "void prepareBuiltInConverters(InjectorImpl injector){\r\n    return \"TypeConverter<E extends Enum<E>>\";\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.TypeConverterBindingProcessor.prepareBuiltInConverters",
	"Comment": "installs default converters for primitives, enums, and class literals.",
	"Method": "void prepareBuiltInConverters(InjectorImpl injector){\r\n    return typeLiteral.getRawType() == Class.class;\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.TypeConverterBindingProcessor.prepareBuiltInConverters",
	"Comment": "installs default converters for primitives, enums, and class literals.",
	"Method": "void prepareBuiltInConverters(InjectorImpl injector){\r\n    return \"Class<?>\";\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.TypeConverterBindingProcessor.prepareBuiltInConverters",
	"Comment": "installs default converters for primitives, enums, and class literals.",
	"Method": "void prepareBuiltInConverters(InjectorImpl injector){\r\n    try {\r\n        return Class.forName(value);\r\n    } catch (ClassNotFoundException e) {\r\n        throw new RuntimeException(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.TypeConverterBindingProcessor.prepareBuiltInConverters",
	"Comment": "installs default converters for primitives, enums, and class literals.",
	"Method": "void prepareBuiltInConverters(InjectorImpl injector){\r\n    return \"TypeConverter<Class<?>>\";\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.SettingsModule.registerSettingsFilter",
	"Comment": "registers a settings filter pattern that allows to filter out certain settings that for instance contain sensitive informationor if a setting is for internal purposes only. the given pattern must either be a valid settings key or a simple regexp pattern.",
	"Method": "void registerSettingsFilter(String filter){\r\n    if (SettingsFilter.isValidPattern(filter) == false) {\r\n        throw new IllegalArgumentException(\"filter [\" + filter + \"] is invalid must be either a key or a regex pattern\");\r\n    }\r\n    if (settingsFilterPattern.contains(filter)) {\r\n        throw new IllegalArgumentException(\"filter [\" + filter + \"] has already been registered\");\r\n    }\r\n    settingsFilterPattern.add(filter);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.service.ClusterApplierService.state",
	"Comment": "the current cluster state.should be renamed to appliedclusterstate",
	"Method": "ClusterState state(){\r\n    assert assertNotCalledFromClusterStateApplier(\"the applied cluster state is not yet available\");\r\n    ClusterState clusterState = this.state.get();\r\n    assert clusterState != null : \"initial cluster state not set yet\";\r\n    return clusterState;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.QuantifiableEntityNormalizer.getOneSubstitutionMatch",
	"Comment": "this method returns the closest match in set such that the matchhas more than three letters and differs from word only by one substitution,deletion, or insertion.if not match exists, returns null.",
	"Method": "String getOneSubstitutionMatch(String word,Set<String> set){\r\n    EditDistance ed = new EditDistance();\r\n    for (String cur : set) {\r\n        if (isOneSubstitutionMatch(word, cur, ed)) {\r\n            return cur;\r\n        }\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.BigObjectArray.estimateRamBytes",
	"Comment": "estimates the number of bytes that would be consumed by an array of the given size.",
	"Method": "long estimateRamBytes(long size){\r\n    return ESTIMATOR.ramBytesEstimated(size);\r\n}"
}, {
	"Path": "android.util.LruCache.snapshot",
	"Comment": "returns a copy of the current contents of the cache, ordered from leastrecently accessed to most recently accessed.",
	"Method": "Map<K, V> snapshot(){\r\n    return new LinkedHashMap<K, V>(map);\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.morph.MorphoFeatures.getTag",
	"Comment": "build a pos tag consisting of a base category plus inflectional features.",
	"Method": "String getTag(String baseTag){\r\n    return baseTag + toString();\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.FileSystem.copyFile",
	"Comment": "copies a file. the ordering of the parameters corresponds to the unix cp command.",
	"Method": "void copyFile(File sourceFile,File destFile){\r\n    try {\r\n        if (!destFile.exists()) {\r\n            destFile.createNewFile();\r\n        }\r\n    } catch (IOException ioe) {\r\n        throw new RuntimeIOException(ioe);\r\n    }\r\n    try (FileChannel source = new FileInputStream(sourceFile).getChannel();\r\n        FileChannel destination = new FileOutputStream(destFile).getChannel()) {\r\n        destination.transferFrom(source, 0, source.size());\r\n    } catch (IOException e) {\r\n        throw new RuntimeIOException(String.format(\"FileSystem: Error copying %s to %s%n\", sourceFile.getPath(), destFile.getPath()), e);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.ui.ParserPanel.highlightText",
	"Comment": "highlights specified text region by changing the character attributes",
	"Method": "void highlightText(int start,int end,SimpleAttributeSet style){\r\n    if (start < end) {\r\n        textPane.getStyledDocument().setCharacterAttributes(start, end - start + 1, style, false);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.alias.get.TransportGetAliasesAction.postProcess",
	"Comment": "fills alias result with empty entries for requested indices when no specific aliases were requested.",
	"Method": "ImmutableOpenMap<String, List<AliasMetaData>> postProcess(GetAliasesRequest request,String[] concreteIndices,ImmutableOpenMap<String, List<AliasMetaData>> aliases){\r\n    boolean noAliasesSpecified = request.getOriginalAliases() == null || request.getOriginalAliases().length == 0;\r\n    ImmutableOpenMap.Builder<String, List<AliasMetaData>> mapBuilder = ImmutableOpenMap.builder(aliases);\r\n    for (String index : concreteIndices) {\r\n        if (aliases.get(index) == null && noAliasesSpecified) {\r\n            List<AliasMetaData> previous = mapBuilder.put(index, Collections.emptyList());\r\n            assert previous == null;\r\n        }\r\n    }\r\n    return mapBuilder.build();\r\n}"
}, {
	"Path": "org.json.JSONObject.names",
	"Comment": "returns an array containing the string names in this object. this methodreturns null if this object contains no mappings.",
	"Method": "JSONArray names(){\r\n    return nameValuePairs.isEmpty() ? null : new JSONArray(new ArrayList<String>(nameValuePairs.keySet()));\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.pascal.ISODateInstance.isRange",
	"Comment": "returns true iff this date represents a rangethe range must have at least a start or enddate, but is not guaranteed to have both",
	"Method": "boolean isRange(){\r\n    if (unparseable) {\r\n        return false;\r\n    }\r\n    return isoDate.matches(\"/\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.inference.CliqueTree.domainsOverlap",
	"Comment": "just a quick inline to check if two factors have overlapping domains. since factor neighbor sets are super small,this n^2 algorithm is fine.",
	"Method": "boolean domainsOverlap(TableFactor f1,TableFactor f2){\r\n    for (int n1 : f1.neighborIndices) {\r\n        for (int n2 : f2.neighborIndices) {\r\n            if (n1 == n2)\r\n                return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.french.process.FrenchLexer.yypushback",
	"Comment": "pushes the specified amount of characters back into the input stream.they will be read again by then next call of the scanning method",
	"Method": "void yypushback(int number){\r\n    if (number > yylength())\r\n        zzScanError(ZZ_PUSHBACK_2BIG);\r\n    zzMarkedPos -= number;\r\n}"
}, {
	"Path": "org.json.JSONStringer.close",
	"Comment": "closes the current scope by appending any necessary whitespace and thegiven bracket.",
	"Method": "JSONStringer close(Scope empty,Scope nonempty,String closeBracket){\r\n    Scope context = peek();\r\n    if (context != nonempty && context != empty) {\r\n        throw new JSONException(\"Nesting problem\");\r\n    }\r\n    stack.remove(stack.size() - 1);\r\n    if (context == nonempty) {\r\n        newline();\r\n    }\r\n    out.append(closeBracket);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.Pipeline.flattenAllProcessors",
	"Comment": "flattens the normal and on failure processors into a single list. the original order is lost.this can be useful for pipeline validation purposes.",
	"Method": "List<Processor> flattenAllProcessors(){\r\n    return compoundProcessor.flattenProcessors();\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.ObjectBank.remove",
	"Comment": "unsupported operation.if you wish to remove a data source,do so in the underlying readeriteratorfactory",
	"Method": "boolean remove(Object o){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "org.elasticsearch.common.blobstore.BlobContainer.deleteBlobIgnoringIfNotExists",
	"Comment": "deletes a blob with giving name, ignoring if the blob does not exist.",
	"Method": "void deleteBlobIgnoringIfNotExists(String blobName){\r\n    try {\r\n        deleteBlob(blobName);\r\n    } catch (final NoSuchFileException ignored) {\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.InjectorBuilder.primaryInjector",
	"Comment": "returns the injector being constructed. this is not necessarily the root injector.",
	"Method": "Injector primaryInjector(){\r\n    return shells.get(0).getInjector();\r\n}"
}, {
	"Path": "org.elasticsearch.action.get.GetRequest.routing",
	"Comment": "controls the shard routing of the request. using this value to hash the shardand not the id.",
	"Method": "GetRequest routing(String routing,String routing){\r\n    return this.routing;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.SecureString.ensureNotClosed",
	"Comment": "throw an exception if this string has been closed, indicating something is trying to access the data after being closed.",
	"Method": "void ensureNotClosed(){\r\n    if (chars == null) {\r\n        throw new IllegalStateException(\"SecureString has already been closed\");\r\n    }\r\n}"
}, {
	"Path": "org.apache.dubbo.rpc.RpcContext.getRequest",
	"Comment": "get the request object of the underlying rpc protocol, e.g. httpservletrequest",
	"Method": "Object getRequest(T getRequest,Class<T> clazz){\r\n    return (request != null && clazz.isAssignableFrom(request.getClass())) ? (T) request : null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.LazyInitializable.getOrCompute",
	"Comment": "returns a value that was created by supplier. the value mighthave been previously created, if not it will be created now, thread safe ofcourse.",
	"Method": "T getOrCompute(){\r\n    final T readOnce = value;\r\n    final T result = readOnce == null ? maybeCompute(supplier) : readOnce;\r\n    onGet.accept(result);\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchScrollRequestBuilder.setScroll",
	"Comment": "if set, will enable scrolling of the search request for the specified timeout.",
	"Method": "SearchScrollRequestBuilder setScroll(Scroll scroll,SearchScrollRequestBuilder setScroll,TimeValue keepAlive,SearchScrollRequestBuilder setScroll,String keepAlive){\r\n    request.scroll(keepAlive);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.updateGlobalCheckpointOnReplica",
	"Comment": "updates the global checkpoint on a replica shard after it has been updated by the primary.",
	"Method": "void updateGlobalCheckpointOnReplica(long globalCheckpoint,String reason){\r\n    assert assertReplicationTarget();\r\n    final long localCheckpoint = getLocalCheckpoint();\r\n    if (globalCheckpoint > localCheckpoint) {\r\n        assert state() != IndexShardState.POST_RECOVERY && state() != IndexShardState.STARTED : \"supposedly in-sync shard copy received a global checkpoint [\" + globalCheckpoint + \"] \" + \"that is higher than its local checkpoint [\" + localCheckpoint + \"]\";\r\n        return;\r\n    }\r\n    replicationTracker.updateGlobalCheckpointOnReplica(globalCheckpoint, reason);\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.internal.ProviderMethodsModule.forObject",
	"Comment": "returns a module which creates bindings for provider methods from the given object.this is useful notably for gin",
	"Method": "Module forObject(Object object){\r\n    if (object instanceof ProviderMethodsModule) {\r\n        return Modules.EMPTY_MODULE;\r\n    }\r\n    return new ProviderMethodsModule(object);\r\n}"
}, {
	"Path": "org.openqa.grid.internal.utils.configuration.json.HubJsonConfiguration.getCleanUpCycle",
	"Comment": "clean up cycle for remote proxies. default determined by configuration type.",
	"Method": "Integer getCleanUpCycle(){\r\n    return cleanUpCycle;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.Queries.maybeApplyMinimumShouldMatch",
	"Comment": "potentially apply minimum should match value if we have a query that it can be applied to,otherwise return the original query.",
	"Method": "Query maybeApplyMinimumShouldMatch(Query query,String minimumShouldMatch){\r\n    if (query instanceof BooleanQuery) {\r\n        return applyMinimumShouldMatch((BooleanQuery) query, minimumShouldMatch);\r\n    } else if (query instanceof ExtendedCommonTermsQuery) {\r\n        ((ExtendedCommonTermsQuery) query).setLowFreqMinimumNumberShouldMatch(minimumShouldMatch);\r\n    }\r\n    return query;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.existsQuery",
	"Comment": "a filter to filter only documents where a field exists in them.",
	"Method": "ExistsQueryBuilder existsQuery(String name){\r\n    return new ExistsQueryBuilder(name);\r\n}"
}, {
	"Path": "org.openqa.grid.internal.DefaultGridRegistry.assignRequestToProxy",
	"Comment": "iterates the list of incoming session request to find a potential match in the list of proxies.if something changes in the registry, the matcher iteration is stopped to account for thatchange.",
	"Method": "void assignRequestToProxy(){\r\n    while (!stop) {\r\n        try {\r\n            testSessionAvailable.await(5, TimeUnit.SECONDS);\r\n            newSessionQueue.processQueue(this::takeRequestHandler, getHub().getConfiguration().prioritizer);\r\n            LoggingManager.perSessionLogHandler().clearThreadTempLogs();\r\n        } catch (InterruptedException e) {\r\n            LOG.info(\"Shutting down registry.\");\r\n        } catch (Throwable t) {\r\n            LOG.log(Level.SEVERE, \"Unhandled exception in Matcher thread.\", t);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.KeyStoreWrapper.decrypt",
	"Comment": "decrypts the underlying keystore data.this may only be called once.",
	"Method": "void decrypt(char[] password){\r\n    if (entries.get() != null) {\r\n        throw new IllegalStateException(\"Keystore has already been decrypted\");\r\n    }\r\n    if (formatVersion <= 2) {\r\n        decryptLegacyEntries();\r\n        if (password.length != 0) {\r\n            throw new IllegalArgumentException(\"Keystore format does not accept non-empty passwords\");\r\n        }\r\n        return;\r\n    }\r\n    final byte[] salt;\r\n    final byte[] iv;\r\n    final byte[] encryptedBytes;\r\n    try (ByteArrayInputStream bytesStream = new ByteArrayInputStream(dataBytes);\r\n        DataInputStream input = new DataInputStream(bytesStream)) {\r\n        int saltLen = input.readInt();\r\n        salt = new byte[saltLen];\r\n        input.readFully(salt);\r\n        int ivLen = input.readInt();\r\n        iv = new byte[ivLen];\r\n        input.readFully(iv);\r\n        int encryptedLen = input.readInt();\r\n        encryptedBytes = new byte[encryptedLen];\r\n        input.readFully(encryptedBytes);\r\n        if (input.read() != -1) {\r\n            throw new SecurityException(\"Keystore has been corrupted or tampered with\");\r\n        }\r\n    } catch (EOFException e) {\r\n        throw new SecurityException(\"Keystore has been corrupted or tampered with\", e);\r\n    }\r\n    Cipher cipher = createCipher(Cipher.DECRYPT_MODE, password, salt, iv);\r\n    try (ByteArrayInputStream bytesStream = new ByteArrayInputStream(encryptedBytes);\r\n        CipherInputStream cipherStream = new CipherInputStream(bytesStream, cipher);\r\n        DataInputStream input = new DataInputStream(cipherStream)) {\r\n        entries.set(new HashMap());\r\n        int numEntries = input.readInt();\r\n        while (numEntries-- > 0) {\r\n            String setting = input.readUTF();\r\n            EntryType entryType = EntryType.valueOf(input.readUTF());\r\n            int entrySize = input.readInt();\r\n            byte[] entryBytes = new byte[entrySize];\r\n            input.readFully(entryBytes);\r\n            entries.get().put(setting, new Entry(entryType, entryBytes));\r\n        }\r\n        if (input.read() != -1) {\r\n            throw new SecurityException(\"Keystore has been corrupted or tampered with\");\r\n        }\r\n    } catch (IOException e) {\r\n        throw new SecurityException(\"Keystore has been corrupted or tampered with\", e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.closeNoLock",
	"Comment": "closes the engine without acquiring the write lock. this should only becalled while the write lock is hold or in a disaster condition ie. if the engineis failed.",
	"Method": "void closeNoLock(String reason,CountDownLatch closedLatch){\r\n    if (isClosed.compareAndSet(false, true)) {\r\n        assert rwl.isWriteLockedByCurrentThread() || failEngineLock.isHeldByCurrentThread() : \"Either the write lock must be held or the engine must be currently be failing itself\";\r\n        try {\r\n            this.versionMap.clear();\r\n            if (internalSearcherManager != null) {\r\n                internalSearcherManager.removeListener(versionMap);\r\n            }\r\n            try {\r\n                IOUtils.close(externalSearcherManager, internalSearcherManager);\r\n            } catch (Exception e) {\r\n                logger.warn(\"Failed to close SearcherManager\", e);\r\n            }\r\n            try {\r\n                IOUtils.close(translog);\r\n            } catch (Exception e) {\r\n                logger.warn(\"Failed to close translog\", e);\r\n            }\r\n            logger.trace(\"rollback indexWriter\");\r\n            try {\r\n                indexWriter.rollback();\r\n            } catch (AlreadyClosedException ex) {\r\n                failOnTragicEvent(ex);\r\n                throw ex;\r\n            }\r\n            logger.trace(\"rollback indexWriter done\");\r\n        } catch (Exception e) {\r\n            logger.warn(\"failed to rollback writer on close\", e);\r\n        } finally {\r\n            try {\r\n                store.decRef();\r\n                logger.debug(\"engine closed [{}]\", reason);\r\n            } finally {\r\n                closedLatch.countDown();\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.create.CreateSnapshotRequest.waitForCompletion",
	"Comment": "returns true if the request should wait for the snapshot completion before returning",
	"Method": "CreateSnapshotRequest waitForCompletion(boolean waitForCompletion,boolean waitForCompletion){\r\n    return waitForCompletion;\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.XMoreLikeThis.setMinWordLen",
	"Comment": "sets the minimum word length below which words will be ignored.",
	"Method": "void setMinWordLen(int minWordLen){\r\n    this.minWordLen = minWordLen;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.UnassignedInfo.getLastAllocationStatus",
	"Comment": "get the status for the last allocation attempt for this shard.",
	"Method": "AllocationStatus getLastAllocationStatus(){\r\n    return lastAllocationStatus;\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.Store.digestToString",
	"Comment": "produces a string representation of the given digest value.",
	"Method": "String digestToString(long digest){\r\n    return Long.toString(digest, Character.MAX_RADIX);\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkResponse.getTook",
	"Comment": "how long the bulk execution took. excluding ingest preprocessing.",
	"Method": "TimeValue getTook(){\r\n    return new TimeValue(tookInMillis);\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkPrimaryExecutionContext.markOperationAsNoOp",
	"Comment": "completes the operation without doing anything on the primary",
	"Method": "void markOperationAsNoOp(DocWriteResponse response){\r\n    assertInvariants(ItemProcessingState.INITIAL);\r\n    executionResult = new BulkItemResponse(getCurrentItem().id(), getCurrentItem().request().opType(), response);\r\n    currentItemState = ItemProcessingState.EXECUTED;\r\n    assertInvariants(ItemProcessingState.EXECUTED);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequestBuilder.setTypes",
	"Comment": "the document types to execute the search against. defaults to be executed againstall types.",
	"Method": "SearchRequestBuilder setTypes(String types){\r\n    request.types(types);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequestBuilder.setPartial",
	"Comment": "if set to true the restore procedure will restore partially snapshotted indices",
	"Method": "RestoreSnapshotRequestBuilder setPartial(boolean partial){\r\n    request.partial(partial);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.arabic.pipeline.LDCPosMapper.processShortTag",
	"Comment": "modifies the shortened tag based on information contained in the longer tag",
	"Method": "String processShortTag(String longTag,String shortTag){\r\n    if (shortTag == null)\r\n        return null;\r\n    if (shortTag.startsWith(\"DT+\"))\r\n        shortTag = LDCdeterminer.matcher(shortTag).replaceAll(\"\");\r\n    if (longTag.equals(\"NUMERIC_COMMA\"))\r\n        shortTag = \"PUNC\";\r\n    if (addDT && (longTag != null)) {\r\n        Matcher detInLongTag = determiner.matcher(longTag);\r\n        Matcher someKindOfNoun = nounBaseTag.matcher(shortTag);\r\n        Matcher someKindOfAdj = adjBaseTag.matcher(shortTag);\r\n        if (detInLongTag.find() && (someKindOfNoun.find() || someKindOfAdj.find()))\r\n            shortTag = \"DT\" + shortTag.trim();\r\n    }\r\n    if (tagMap.containsKey(longTag)) {\r\n        String existingShortTag = tagMap.get(longTag);\r\n        if (!existingShortTag.equals(shortTag))\r\n            System.err.printf(\"%s: Union of mapping files will cause overlap for %s (current: %s new: %s)%n\", this.getClass().getName(), longTag, existingShortTag, shortTag);\r\n        return existingShortTag;\r\n    }\r\n    return shortTag;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.GraphicalModel.addFactor",
	"Comment": "creates an instantiated factor in this graph, with neighborindices representing the neighbor variables by integerindex.",
	"Method": "Factor addFactor(int[] neighborIndices,int[] neighborDimensions,Function<int[], ConcatVector> assignmentFeaturizer,Factor addFactor,ConcatVectorTable featureTable,int[] neighborIndices){\r\n    assert (featureTable.getDimensions().length == neighborIndices.length);\r\n    Factor factor = new Factor(featureTable, neighborIndices);\r\n    factors.add(factor);\r\n    return factor;\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.NeuralUtils.elementwiseApplyTanhDerivative",
	"Comment": "applies the derivative of tanh to each of the elements in the vector.returns a new matrix.",
	"Method": "SimpleMatrix elementwiseApplyTanhDerivative(SimpleMatrix input){\r\n    SimpleMatrix output = new SimpleMatrix(input.numRows(), input.numCols());\r\n    output.set(1.0);\r\n    output = output.minus(input.elementMult(input));\r\n    return output;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.RoutingNodes.assignedShards",
	"Comment": "returns all shards that are not in the state unassigned with the same shardid as the given shard.",
	"Method": "List<ShardRouting> assignedShards(ShardId shardId){\r\n    final List<ShardRouting> replicaSet = assignedShards.get(shardId);\r\n    return replicaSet == null ? EMPTY : Collections.unmodifiableList(replicaSet);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.tasks.BaseTasksRequest.getTaskId",
	"Comment": "returns the id of the task that should be processed.by default tasks with any ids are returned.",
	"Method": "TaskId getTaskId(){\r\n    return taskId;\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.Store.checkIndex",
	"Comment": "checks and returns the status of the existing index in this store.",
	"Method": "CheckIndex.Status checkIndex(PrintStream out){\r\n    metadataLock.writeLock().lock();\r\n    try (CheckIndex checkIndex = new CheckIndex(directory)) {\r\n        checkIndex.setInfoStream(out);\r\n        return checkIndex.checkIndex();\r\n    } finally {\r\n        metadataLock.writeLock().unlock();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVectorNamespace.newVector",
	"Comment": "creates a new vector that is appropriately sized to accommodate all the features that have been named so far.",
	"Method": "ConcatVector newVector(){\r\n    return new ConcatVector(featureToIndex.size());\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PrimaryShardAllocator.buildNodeShardsResult",
	"Comment": "builds a list of nodes. if matchanyshard is set to false, only nodes that have an allocation id matchinginsyncallocationids are added to the list. otherwise, any node that has a shard is added to the list, butentries with matching allocation id are always at the front of the list.",
	"Method": "NodeShardsResult buildNodeShardsResult(ShardRouting shard,boolean matchAnyShard,Set<String> ignoreNodes,Set<String> inSyncAllocationIds,FetchResult<NodeGatewayStartedShards> shardState,Logger logger){\r\n    List<NodeGatewayStartedShards> nodeShardStates = new ArrayList();\r\n    int numberOfAllocationsFound = 0;\r\n    for (NodeGatewayStartedShards nodeShardState : shardState.getData().values()) {\r\n        DiscoveryNode node = nodeShardState.getNode();\r\n        String allocationId = nodeShardState.allocationId();\r\n        if (ignoreNodes.contains(node.getId())) {\r\n            continue;\r\n        }\r\n        if (nodeShardState.storeException() == null) {\r\n            if (allocationId == null) {\r\n                logger.trace(\"[{}] on node [{}] has no shard state information\", shard, nodeShardState.getNode());\r\n            } else {\r\n                logger.trace(\"[{}] on node [{}] has allocation id [{}]\", shard, nodeShardState.getNode(), allocationId);\r\n            }\r\n        } else {\r\n            final String finalAllocationId = allocationId;\r\n            if (nodeShardState.storeException() instanceof ShardLockObtainFailedException) {\r\n                logger.trace(() -> new ParameterizedMessage(\"[{}] on node [{}] has allocation id [{}] but the store can not be \" + \"opened as it's locked, treating as valid shard\", shard, nodeShardState.getNode(), finalAllocationId), nodeShardState.storeException());\r\n            } else {\r\n                logger.trace(() -> new ParameterizedMessage(\"[{}] on node [{}] has allocation id [{}] but the store can not be \" + \"opened, treating as no allocation id\", shard, nodeShardState.getNode(), finalAllocationId), nodeShardState.storeException());\r\n                allocationId = null;\r\n            }\r\n        }\r\n        if (allocationId != null) {\r\n            assert nodeShardState.storeException() == null || nodeShardState.storeException() instanceof ShardLockObtainFailedException : \"only allow store that can be opened or that throws a ShardLockObtainFailedException while being opened but got a \" + \"store throwing \" + nodeShardState.storeException();\r\n            numberOfAllocationsFound++;\r\n            if (matchAnyShard || inSyncAllocationIds.contains(nodeShardState.allocationId())) {\r\n                nodeShardStates.add(nodeShardState);\r\n            }\r\n        }\r\n    }\r\n    final Comparator<NodeGatewayStartedShards> comparator;\r\n    if (matchAnyShard) {\r\n        Comparator<NodeGatewayStartedShards> matchingAllocationsFirst = Comparator.comparing((NodeGatewayStartedShards state) -> inSyncAllocationIds.contains(state.allocationId())).reversed();\r\n        comparator = matchingAllocationsFirst.thenComparing(NO_STORE_EXCEPTION_FIRST_COMPARATOR).thenComparing(PRIMARY_FIRST_COMPARATOR);\r\n    } else {\r\n        comparator = NO_STORE_EXCEPTION_FIRST_COMPARATOR.thenComparing(PRIMARY_FIRST_COMPARATOR);\r\n    }\r\n    nodeShardStates.sort(comparator);\r\n    if (logger.isTraceEnabled()) {\r\n        logger.trace(\"{} candidates for allocation: {}\", shard, nodeShardStates.stream().map(s -> s.getNode().getName()).collect(Collectors.joining(\", \")));\r\n    }\r\n    return new NodeShardsResult(nodeShardStates, numberOfAllocationsFound);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.create.CreateIndexClusterStateUpdateRequest.getProvidedName",
	"Comment": "the name that was provided by the user. this might contain a date math expression.",
	"Method": "String getProvidedName(){\r\n    return providedName;\r\n}"
}, {
	"Path": "com.android.internal.util.ArrayUtils.emptyArray",
	"Comment": "returns an empty array of the specified type.the intent is thatit will return the same empty array every time to avoid reallocation,although this is not guaranteed.",
	"Method": "T[] emptyArray(Class<T> kind){\r\n    if (kind == Object.class) {\r\n        return (T[]) EMPTY;\r\n    }\r\n    int bucket = ((System.identityHashCode(kind) / 8) & 0x7FFFFFFF) % CACHE_SIZE;\r\n    Object cache = sCache[bucket];\r\n    if (cache == null || cache.getClass().getComponentType() != kind) {\r\n        cache = Array.newInstance(kind, 0);\r\n        sCache[bucket] = cache;\r\n    }\r\n    return (T[]) cache;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.analyze.AnalyzeRequestBuilder.addTokenFilter",
	"Comment": "add a name of token filter that will be used on top of a tokenizer provided.",
	"Method": "AnalyzeRequestBuilder addTokenFilter(Map<String, ?> tokenFilter,AnalyzeRequestBuilder addTokenFilter,String tokenFilter){\r\n    request.addTokenFilter(tokenFilter);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequestBuilder.setPerFieldAnalyzer",
	"Comment": "sets the analyzer used at each field when generating term vectors.",
	"Method": "TermVectorsRequestBuilder setPerFieldAnalyzer(Map<String, String> perFieldAnalyzer){\r\n    request.perFieldAnalyzer(perFieldAnalyzer);\r\n    return this;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.PriorityTest.validate",
	"Comment": "validate that the one with priority 5 has been assigned a proxy",
	"Method": "void validate(){\r\n    Thread.sleep(250);\r\n    assertNotNull(newSessionRequest5.getSession());\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVectorNamespace.debugVector",
	"Comment": "this prints out a concatvector by mapping to the namespace, to make debugging learning algorithms easier.",
	"Method": "void debugVector(ConcatVector vector,BufferedWriter bw){\r\n    for (String key : featureToIndex.keySet()) {\r\n        bw.write(key);\r\n        bw.write(\":\\n\");\r\n        int i = featureToIndex.get(key);\r\n        if (vector.isComponentSparse(i)) {\r\n            debugFeatureValue(key, vector.getSparseIndex(i), vector, bw);\r\n        } else {\r\n            double[] arr = vector.getDenseComponent(i);\r\n            for (int j = 0; j < arr.length; j++) {\r\n                debugFeatureValue(key, j, vector, bw);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.MultiMatchQueryBuilder.fields",
	"Comment": "add several fields to run the query against with a specific boost.",
	"Method": "MultiMatchQueryBuilder fields(Map<String, Float> fields,Map<String, Float> fields){\r\n    return fieldsBoosts;\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.updateFromMaster",
	"Comment": "notifies the tracker of the current allocation ids in the cluster state.",
	"Method": "void updateFromMaster(long applyingClusterStateVersion,Set<String> inSyncAllocationIds,IndexShardRoutingTable routingTable,Set<String> pre60AllocationIds){\r\n    assert invariant();\r\n    if (applyingClusterStateVersion > appliedClusterStateVersion) {\r\n        assert !primaryMode || inSyncAllocationIds.stream().allMatch(inSyncId -> checkpoints.containsKey(inSyncId) && checkpoints.get(inSyncId).inSync) : \"update from master in primary mode contains in-sync ids \" + inSyncAllocationIds + \" that have no matching entries in \" + checkpoints;\r\n        Set<String> initializingAllocationIds = routingTable.getAllInitializingShards().stream().map(ShardRouting::allocationId).map(AllocationId::getId).collect(Collectors.toSet());\r\n        boolean removedEntries = checkpoints.keySet().removeIf(aid -> !inSyncAllocationIds.contains(aid) && !initializingAllocationIds.contains(aid));\r\n        if (primaryMode) {\r\n            for (String initializingId : initializingAllocationIds) {\r\n                if (checkpoints.containsKey(initializingId) == false) {\r\n                    final boolean inSync = inSyncAllocationIds.contains(initializingId);\r\n                    assert inSync == false : \"update from master in primary mode has \" + initializingId + \" as in-sync but it does not exist locally\";\r\n                    final long localCheckpoint = pre60AllocationIds.contains(initializingId) ? SequenceNumbers.PRE_60_NODE_CHECKPOINT : SequenceNumbers.UNASSIGNED_SEQ_NO;\r\n                    final long globalCheckpoint = localCheckpoint;\r\n                    checkpoints.put(initializingId, new CheckpointState(localCheckpoint, globalCheckpoint, inSync, inSync));\r\n                }\r\n            }\r\n            if (removedEntries) {\r\n                pendingInSync.removeIf(aId -> checkpoints.containsKey(aId) == false);\r\n            }\r\n        } else {\r\n            for (String initializingId : initializingAllocationIds) {\r\n                if (shardAllocationId.equals(initializingId) == false) {\r\n                    final long localCheckpoint = pre60AllocationIds.contains(initializingId) ? SequenceNumbers.PRE_60_NODE_CHECKPOINT : SequenceNumbers.UNASSIGNED_SEQ_NO;\r\n                    final long globalCheckpoint = localCheckpoint;\r\n                    checkpoints.put(initializingId, new CheckpointState(localCheckpoint, globalCheckpoint, false, false));\r\n                }\r\n            }\r\n            for (String inSyncId : inSyncAllocationIds) {\r\n                if (shardAllocationId.equals(inSyncId)) {\r\n                    CheckpointState checkpointState = checkpoints.get(shardAllocationId);\r\n                    checkpointState.inSync = true;\r\n                    checkpointState.tracked = true;\r\n                } else {\r\n                    final long localCheckpoint = pre60AllocationIds.contains(inSyncId) ? SequenceNumbers.PRE_60_NODE_CHECKPOINT : SequenceNumbers.UNASSIGNED_SEQ_NO;\r\n                    final long globalCheckpoint = localCheckpoint;\r\n                    checkpoints.put(inSyncId, new CheckpointState(localCheckpoint, globalCheckpoint, true, true));\r\n                }\r\n            }\r\n        }\r\n        appliedClusterStateVersion = applyingClusterStateVersion;\r\n        this.routingTable = routingTable;\r\n        replicationGroup = calculateReplicationGroup();\r\n        if (primaryMode && removedEntries) {\r\n            updateGlobalCheckpointOnPrimary();\r\n            notifyAllWaiters();\r\n        }\r\n    }\r\n    assert invariant();\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.isQueryStringAnalyzeWildcard",
	"Comment": "returns true if the query string should analyze wildcards. the default is false",
	"Method": "boolean isQueryStringAnalyzeWildcard(){\r\n    return queryStringAnalyzeWildcard;\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.SloppyMath.main",
	"Comment": "tests the hypergeometric distribution code, or other functionsprovided in this module.",
	"Method": "void main(String[] args){\r\n    if (args.length == 0) {\r\n        log.info(\"Usage: java edu.stanford.nlp.math.SloppyMath \" + \"[-logAdd|-fishers k n r m|-binomial r n p\");\r\n    } else if (args[0].equals(\"-logAdd\")) {\r\n        System.out.println(\"Log adds of neg infinity numbers, etc.\");\r\n        System.out.println(\"(logs) -Inf + -Inf = \" + logAdd(Double.NEGATIVE_INFINITY, Double.NEGATIVE_INFINITY));\r\n        System.out.println(\"(logs) -Inf + -7 = \" + logAdd(Double.NEGATIVE_INFINITY, -7.0));\r\n        System.out.println(\"(logs) -7 + -Inf = \" + logAdd(-7.0, Double.NEGATIVE_INFINITY));\r\n        System.out.println(\"(logs) -50 + -7 = \" + logAdd(-50.0, -7.0));\r\n        System.out.println(\"(logs) -11 + -7 = \" + logAdd(-11.0, -7.0));\r\n        System.out.println(\"(logs) -7 + -11 = \" + logAdd(-7.0, -11.0));\r\n        System.out.println(\"real 1/2 + 1/2 = \" + logAdd(Math.log(0.5), Math.log(0.5)));\r\n    } else if (args[0].equals(\"-fishers\")) {\r\n        int k = Integer.parseInt(args[1]);\r\n        int n = Integer.parseInt(args[2]);\r\n        int r = Integer.parseInt(args[3]);\r\n        int m = Integer.parseInt(args[4]);\r\n        double ans = SloppyMath.hypergeometric(k, n, r, m);\r\n        System.out.println(\"hypg(\" + k + \"; \" + n + \", \" + r + \", \" + m + \") = \" + ans);\r\n        ans = SloppyMath.oneTailedFishersExact(k, n, r, m);\r\n        System.out.println(\"1-tailed Fisher's exact(\" + k + \"; \" + n + \", \" + r + \", \" + m + \") = \" + ans);\r\n        double ansChi = SloppyMath.chiSquare2by2(k, n, r, m);\r\n        System.out.println(\"chiSquare(\" + k + \"; \" + n + \", \" + r + \", \" + m + \") = \" + ansChi);\r\n        System.out.println(\"Swapping arguments should give same hypg:\");\r\n        ans = SloppyMath.hypergeometric(k, n, r, m);\r\n        System.out.println(\"hypg(\" + k + \"; \" + n + \", \" + m + \", \" + r + \") = \" + ans);\r\n        int othrow = n - m;\r\n        int othcol = n - r;\r\n        int cell12 = m - k;\r\n        int cell21 = r - k;\r\n        int cell22 = othrow - (r - k);\r\n        ans = SloppyMath.hypergeometric(cell12, n, othcol, m);\r\n        System.out.println(\"hypg(\" + cell12 + \"; \" + n + \", \" + othcol + \", \" + m + \") = \" + ans);\r\n        ans = SloppyMath.hypergeometric(cell21, n, r, othrow);\r\n        System.out.println(\"hypg(\" + cell21 + \"; \" + n + \", \" + r + \", \" + othrow + \") = \" + ans);\r\n        ans = SloppyMath.hypergeometric(cell22, n, othcol, othrow);\r\n        System.out.println(\"hypg(\" + cell22 + \"; \" + n + \", \" + othcol + \", \" + othrow + \") = \" + ans);\r\n    } else if (args[0].equals(\"-binomial\")) {\r\n        int k = Integer.parseInt(args[1]);\r\n        int n = Integer.parseInt(args[2]);\r\n        double p = Double.parseDouble(args[3]);\r\n        double ans = SloppyMath.exactBinomial(k, n, p);\r\n        System.out.println(\"Binomial p(X >= \" + k + \"; \" + n + \", \" + p + \") = \" + ans);\r\n    } else {\r\n        log.info(\"Unknown option: \" + args[0]);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.SecureSetting.secureFile",
	"Comment": "a setting which contains a file. reading the setting opens an input stream to the file.this may be any sensitive file, e.g. a set of credentials normally in plaintext.",
	"Method": "Setting<InputStream> secureFile(String name,Setting<InputStream> fallback,Property properties){\r\n    return new SecureFileSetting(name, fallback, properties);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.master.MasterNodeReadOperationRequestBuilder.setLocal",
	"Comment": "specifies if the request should be executed on local node rather than on master",
	"Method": "RequestBuilder setLocal(boolean local){\r\n    request.local(local);\r\n    return (RequestBuilder) this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoBoundingBoxQueryBuilder.fieldName",
	"Comment": "returns the name of the field to base the bounding box computation on.",
	"Method": "String fieldName(){\r\n    return this.fieldName;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.RefreshListeners.setCurrentRefreshLocationSupplier",
	"Comment": "setup the translog used to find the last refreshed location.",
	"Method": "void setCurrentRefreshLocationSupplier(Supplier<Translog.Location> currentRefreshLocationSupplier){\r\n    this.currentRefreshLocationSupplier = currentRefreshLocationSupplier;\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.ForwardEntailerSearchProblem.searchImplementation",
	"Comment": "the search algorithm, starting with a full sentence and iteratively shortening it to its entailed sentences.",
	"Method": "List<SearchResult> searchImplementation(){\r\n    SemanticGraph parseTree = new SemanticGraph(this.parseTree);\r\n    assert Util.isTree(parseTree);\r\n    List<String> determinerRemovals = new ArrayList();\r\n    parseTree.getLeafVertices().stream().filter(vertex -> \"the\".equalsIgnoreCase(vertex.word()) || \"a\".equalsIgnoreCase(vertex.word()) || \"an\".equalsIgnoreCase(vertex.word()) || \"this\".equalsIgnoreCase(vertex.word()) || \"that\".equalsIgnoreCase(vertex.word()) || \"those\".equalsIgnoreCase(vertex.word()) || \"these\".equalsIgnoreCase(vertex.word())).forEach(vertex -> {\r\n        parseTree.removeVertex(vertex);\r\n        assert Util.isTree(parseTree);\r\n        determinerRemovals.add(\"det\");\r\n    });\r\n    Set<SemanticGraphEdge> andsToAdd = new HashSet();\r\n    for (IndexedWord vertex : parseTree.vertexSet()) {\r\n        if (parseTree.inDegree(vertex) > 1) {\r\n            SemanticGraphEdge conjAnd = null;\r\n            for (SemanticGraphEdge edge : parseTree.incomingEdgeIterable(vertex)) {\r\n                if (\"conj:and\".equals(edge.getRelation().toString())) {\r\n                    conjAnd = edge;\r\n                }\r\n            }\r\n            if (conjAnd != null) {\r\n                parseTree.removeEdge(conjAnd);\r\n                assert Util.isTree(parseTree);\r\n                andsToAdd.add(conjAnd);\r\n            }\r\n        }\r\n    }\r\n    Util.cleanTree(parseTree);\r\n    assert Util.isTree(parseTree);\r\n    BitSet isSubject = new BitSet(256);\r\n    for (IndexedWord vertex : parseTree.vertexSet()) {\r\n        Iterator<SemanticGraphEdge> incomingEdges = parseTree.incomingEdgeIterator(vertex);\r\n        SemanticGraphEdge edge = null;\r\n        if (incomingEdges.hasNext()) {\r\n            edge = incomingEdges.next();\r\n        }\r\n        int numIters = 0;\r\n        while (edge != null) {\r\n            if (edge.getRelation().toString().endsWith(\"subj\")) {\r\n                assert vertex.index() > 0;\r\n                isSubject.set(vertex.index() - 1);\r\n                break;\r\n            }\r\n            incomingEdges = parseTree.incomingEdgeIterator(edge.getGovernor());\r\n            if (incomingEdges.hasNext()) {\r\n                edge = incomingEdges.next();\r\n            } else {\r\n                edge = null;\r\n            }\r\n            numIters += 1;\r\n            if (numIters > 100) {\r\n                return Collections.EMPTY_LIST;\r\n            }\r\n        }\r\n    }\r\n    List<SearchResult> results = new ArrayList();\r\n    if (!determinerRemovals.isEmpty()) {\r\n        if (andsToAdd.isEmpty()) {\r\n            double score = Math.pow(weights.deletionProbability(\"det\"), (double) determinerRemovals.size());\r\n            assert !Double.isNaN(score);\r\n            assert !Double.isInfinite(score);\r\n            results.add(new SearchResult(parseTree, determinerRemovals, score));\r\n        } else {\r\n            SemanticGraph treeWithAnds = new SemanticGraph(parseTree);\r\n            assert Util.isTree(treeWithAnds);\r\n            for (SemanticGraphEdge and : andsToAdd) {\r\n                treeWithAnds.addEdge(and.getGovernor(), and.getDependent(), and.getRelation(), Double.NEGATIVE_INFINITY, false);\r\n            }\r\n            assert Util.isTree(treeWithAnds);\r\n            results.add(new SearchResult(treeWithAnds, determinerRemovals, Math.pow(weights.deletionProbability(\"det\"), (double) determinerRemovals.size())));\r\n        }\r\n    }\r\n    assert Util.isTree(parseTree);\r\n    List<IndexedWord> topologicalVertices;\r\n    try {\r\n        topologicalVertices = parseTree.topologicalSort();\r\n    } catch (IllegalStateException e) {\r\n        topologicalVertices = parseTree.vertexListSorted();\r\n    }\r\n    if (topologicalVertices.isEmpty()) {\r\n        return results;\r\n    }\r\n    Stack<SearchState> fringe = new Stack();\r\n    fringe.push(new SearchState(new BitSet(256), 0, parseTree, null, null, 1.0));\r\n    int numTicks = 0;\r\n    while (!fringe.isEmpty()) {\r\n        if (numTicks >= maxTicks) {\r\n            return results;\r\n        }\r\n        numTicks += 1;\r\n        if (results.size() >= maxResults) {\r\n            return results;\r\n        }\r\n        SearchState state = fringe.pop();\r\n        assert state.score > 0.0;\r\n        IndexedWord currentWord = topologicalVertices.get(state.currentIndex);\r\n        int nextIndex = state.currentIndex + 1;\r\n        int numIters = 0;\r\n        while (nextIndex < topologicalVertices.size()) {\r\n            IndexedWord nextWord = topologicalVertices.get(nextIndex);\r\n            assert nextWord.index() > 0;\r\n            if (!state.deletionMask.get(nextWord.index() - 1)) {\r\n                fringe.push(new SearchState(state.deletionMask, nextIndex, state.tree, null, state, state.score));\r\n                break;\r\n            } else {\r\n                nextIndex += 1;\r\n            }\r\n            numIters += 1;\r\n            if (numIters > 10000) {\r\n                return results;\r\n            }\r\n        }\r\n        boolean canDelete = !state.tree.getFirstRoot().equals(currentWord);\r\n        for (SemanticGraphEdge edge : state.tree.incomingEdgeIterable(currentWord)) {\r\n            if (\"CD\".equals(edge.getGovernor().tag())) {\r\n                canDelete = false;\r\n            } else {\r\n                CoreLabel token = edge.getDependent().backingLabel();\r\n                OperatorSpec operator;\r\n                NaturalLogicRelation lexicalRelation;\r\n                Polarity tokenPolarity = token.get(NaturalLogicAnnotations.PolarityAnnotation.class);\r\n                if (tokenPolarity == null) {\r\n                    tokenPolarity = Polarity.DEFAULT;\r\n                }\r\n                if ((operator = token.get(NaturalLogicAnnotations.OperatorAnnotation.class)) != null) {\r\n                    lexicalRelation = operator.instance.deleteRelation;\r\n                } else {\r\n                    assert edge.getDependent().index() > 0;\r\n                    lexicalRelation = NaturalLogicRelation.forDependencyDeletion(edge.getRelation().toString(), isSubject.get(edge.getDependent().index() - 1));\r\n                }\r\n                NaturalLogicRelation projectedRelation = tokenPolarity.projectLexicalRelation(lexicalRelation);\r\n                if (!projectedRelation.applyToTruthValue(truthOfPremise).isTrue()) {\r\n                    canDelete = false;\r\n                }\r\n            }\r\n        }\r\n        if (canDelete) {\r\n            Lazy<Pair<SemanticGraph, BitSet>> treeWithDeletionsAndNewMask = Lazy.of(() -> {\r\n                SemanticGraph impl = new SemanticGraph(state.tree);\r\n                BitSet newMask = state.deletionMask;\r\n                for (IndexedWord vertex : state.tree.descendants(currentWord)) {\r\n                    impl.removeVertex(vertex);\r\n                    assert vertex.index() > 0;\r\n                    newMask.set(vertex.index() - 1);\r\n                    assert newMask.get(vertex.index() - 1);\r\n                }\r\n                return Pair.makePair(impl, newMask);\r\n            });\r\n            double newScore = state.score;\r\n            for (SemanticGraphEdge edge : state.tree.incomingEdgeIterable(currentWord)) {\r\n                double multiplier = weights.deletionProbability(edge, state.tree.outgoingEdgeIterable(edge.getGovernor()));\r\n                assert !Double.isNaN(multiplier);\r\n                assert !Double.isInfinite(multiplier);\r\n                newScore *= multiplier;\r\n            }\r\n            if (newScore > 0.0) {\r\n                SemanticGraph resultTree = new SemanticGraph(treeWithDeletionsAndNewMask.get().first);\r\n                andsToAdd.stream().filter(edge -> resultTree.containsVertex(edge.getGovernor()) && resultTree.containsVertex(edge.getDependent())).forEach(edge -> resultTree.addEdge(edge.getGovernor(), edge.getDependent(), edge.getRelation(), Double.NEGATIVE_INFINITY, false));\r\n                results.add(new SearchResult(resultTree, aggregateDeletedEdges(state, state.tree.incomingEdgeIterable(currentWord), determinerRemovals), newScore));\r\n                nextIndex = state.currentIndex + 1;\r\n                numIters = 0;\r\n                while (nextIndex < topologicalVertices.size()) {\r\n                    IndexedWord nextWord = topologicalVertices.get(nextIndex);\r\n                    BitSet newMask = treeWithDeletionsAndNewMask.get().second;\r\n                    SemanticGraph treeWithDeletions = treeWithDeletionsAndNewMask.get().first;\r\n                    if (!newMask.get(nextWord.index() - 1)) {\r\n                        assert treeWithDeletions.containsVertex(topologicalVertices.get(nextIndex));\r\n                        fringe.push(new SearchState(newMask, nextIndex, treeWithDeletions, null, state, newScore));\r\n                        break;\r\n                    } else {\r\n                        nextIndex += 1;\r\n                    }\r\n                    numIters += 1;\r\n                    if (numIters > 10000) {\r\n                        return results;\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return results;\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.DocumentParser.parseCopy",
	"Comment": "creates an copy of the current field with given field name and boost",
	"Method": "void parseCopy(String field,ParseContext context){\r\n    Mapper mapper = context.docMapper().mappers().getMapper(field);\r\n    if (mapper != null) {\r\n        if (mapper instanceof FieldMapper) {\r\n            ((FieldMapper) mapper).parse(context);\r\n        } else if (mapper instanceof FieldAliasMapper) {\r\n            throw new IllegalArgumentException(\"Cannot copy to a field alias [\" + mapper.name() + \"].\");\r\n        } else {\r\n            throw new IllegalStateException(\"The provided mapper [\" + mapper.name() + \"] has an unrecognized type [\" + mapper.getClass().getSimpleName() + \"].\");\r\n        }\r\n    } else {\r\n        context = context.overridePath(new ContentPath(0));\r\n        final String[] paths = splitAndValidatePath(field);\r\n        final String fieldName = paths[paths.length - 1];\r\n        Tuple<Integer, ObjectMapper> parentMapperTuple = getDynamicParentMapper(context, paths, null);\r\n        ObjectMapper objectMapper = parentMapperTuple.v2();\r\n        parseDynamicValue(context, objectMapper, fieldName, context.parser().currentToken());\r\n        for (int i = 0; i < parentMapperTuple.v1(); i++) {\r\n            context.path().remove();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.IngestMetric.postIngest",
	"Comment": "call this after the performing the ingest action, even if the action failed.",
	"Method": "void postIngest(long ingestTimeInMillis){\r\n    ingestCurrent.dec();\r\n    ingestTime.inc(ingestTimeInMillis);\r\n    ingestCount.inc();\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.readSegmentInfos",
	"Comment": "reads the segments infos from the given segments file name, failing if it fails to load",
	"Method": "SegmentInfos readSegmentInfos(Directory directory,SegmentInfos readSegmentInfos,IndexCommit commit,SegmentInfos readSegmentInfos,String segmentsFileName,Directory directory){\r\n    return SegmentInfos.readCommit(directory, segmentsFileName);\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.GeoHashUtils.stringEncode",
	"Comment": "encode to a level specific geohash string from full resolution longitude, latitude",
	"Method": "String stringEncode(long geoHashLong,String stringEncode,double lon,double lat,String stringEncode,double lon,double lat,int level){\r\n    final long ghLong = fromMorton(encodeLatLon(lat, lon), level);\r\n    return stringEncode(ghLong);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.CombinedDeletionPolicy.hasUnreferencedCommits",
	"Comment": "checks if the deletion policy can release some index commits with the latest global checkpoint.",
	"Method": "boolean hasUnreferencedCommits(){\r\n    final IndexCommit lastCommit = this.lastCommit;\r\n    if (safeCommit != lastCommit) {\r\n        if (lastCommit.getUserData().containsKey(SequenceNumbers.MAX_SEQ_NO)) {\r\n            final long maxSeqNoFromLastCommit = Long.parseLong(lastCommit.getUserData().get(SequenceNumbers.MAX_SEQ_NO));\r\n            return globalCheckpointSupplier.getAsLong() >= maxSeqNoFromLastCommit;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.UnaryGrammar.writeAllData",
	"Comment": "writes out a lot of redundant data from this object to the writer w.",
	"Method": "void writeAllData(Writer w){\r\n    int numStates = index.size();\r\n    PrintWriter out = new PrintWriter(w);\r\n    out.println(\"Unary ruleIterator\");\r\n    for (Iterator<UnaryRule> rI = ruleIterator(); rI.hasNext(); ) {\r\n        out.println(rI.next().toString(index));\r\n    }\r\n    out.println(\"Unary closedRuleIterator\");\r\n    for (Iterator<UnaryRule> rI = closedRuleIterator(); rI.hasNext(); ) {\r\n        out.println(rI.next().toString(index));\r\n    }\r\n    out.println(\"Unary rulesWithParentIterator\");\r\n    for (int i = 0; i < numStates; i++) {\r\n        out.println(index.get(i));\r\n        for (Iterator<UnaryRule> rI = ruleIteratorByParent(i); rI.hasNext(); ) {\r\n            out.print(\"  \");\r\n            out.println(rI.next().toString(index));\r\n        }\r\n    }\r\n    out.println(\"Unary closedRulesWithParentIterator\");\r\n    for (int i = 0; i < numStates; i++) {\r\n        out.println(index.get(i));\r\n        for (Iterator<UnaryRule> rI = closedRuleIteratorByParent(i); rI.hasNext(); ) {\r\n            out.print(\"  \");\r\n            out.println(rI.next().toString(index));\r\n        }\r\n    }\r\n    out.flush();\r\n}"
}, {
	"Path": "org.elasticsearch.action.delete.DeleteRequestBuilder.setVersion",
	"Comment": "sets the version, which will cause the delete operation to only be performed if a matchingversion exists and no changes happened on the doc since then.",
	"Method": "DeleteRequestBuilder setVersion(long version){\r\n    request.version(version);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.CoreMapExpressionExtractor.createExtractorFromString",
	"Comment": "creates an extractor using the specified environment, and reading the rules from the given string",
	"Method": "CoreMapExpressionExtractor createExtractorFromString(Env env,String str){\r\n    TokenSequenceParser parser = new TokenSequenceParser();\r\n    CoreMapExpressionExtractor extractor = parser.getExpressionExtractor(env, new StringReader(str));\r\n    return extractor;\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.getTranslogSyncInterval",
	"Comment": "returns the translog sync interval. this is the interval in which the transaction log is asynchronously fsynced unlessthe transaction log is fsyncing on every operations",
	"Method": "TimeValue getTranslogSyncInterval(){\r\n    return syncInterval;\r\n}"
}, {
	"Path": "org.elasticsearch.common.logging.LogConfigurator.setNodeName",
	"Comment": "sets the node name. this is called before logging is configured if thenode name is set in elasticsearch.yml. otherwise it is called as soonas the node id is available.",
	"Method": "void setNodeName(String nodeName){\r\n    NodeNamePatternConverter.setNodeName(nodeName);\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.BulkByScrollTask.setWorker",
	"Comment": "sets this task to be a worker task that performs search requests",
	"Method": "void setWorker(float requestsPerSecond,Integer sliceId){\r\n    if (isWorker()) {\r\n        throw new IllegalStateException(\"This task is already a worker\");\r\n    }\r\n    if (isLeader()) {\r\n        throw new IllegalStateException(\"This task is already a leader for other slice subtasks\");\r\n    }\r\n    workerState = new WorkerBulkByScrollTaskState(this, sliceId, requestsPerSecond);\r\n    if (isCancelled()) {\r\n        workerState.handleCancel();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequest.source",
	"Comment": "the source of the document to index, recopied to a new array if it is unsafe.",
	"Method": "BytesReference source(IndexRequest source,Map<String, ?> source,IndexRequest source,Map<String, ?> source,XContentType contentType,IndexRequest source,String source,XContentType xContentType,IndexRequest source,XContentBuilder sourceBuilder,IndexRequest source,Object source,IndexRequest source,XContentType xContentType,Object source,IndexRequest source,BytesReference source,XContentType xContentType,IndexRequest source,byte[] source,XContentType xContentType,IndexRequest source,byte[] source,int offset,int length,XContentType xContentType){\r\n    return source(new BytesArray(source, offset, length), xContentType);\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.parsers.GeoJsonParser.parseCoordinates",
	"Comment": "recursive method which parses the arrays of coordinates used to defineshapes",
	"Method": "CoordinateNode parseCoordinates(XContentParser parser,boolean ignoreZValue){\r\n    if (parser.currentToken() == XContentParser.Token.START_OBJECT) {\r\n        parser.skipChildren();\r\n        parser.nextToken();\r\n        throw new ElasticsearchParseException(\"coordinates cannot be specified as objects\");\r\n    }\r\n    XContentParser.Token token = parser.nextToken();\r\n    if (token != XContentParser.Token.START_ARRAY && token != XContentParser.Token.END_ARRAY && token != XContentParser.Token.VALUE_NULL) {\r\n        return new CoordinateNode(parseCoordinate(parser, ignoreZValue));\r\n    } else if (token == XContentParser.Token.VALUE_NULL) {\r\n        throw new IllegalArgumentException(\"coordinates cannot contain NULL values)\");\r\n    }\r\n    List<CoordinateNode> nodes = new ArrayList();\r\n    while (token != XContentParser.Token.END_ARRAY) {\r\n        CoordinateNode node = parseCoordinates(parser, ignoreZValue);\r\n        if (nodes.isEmpty() == false && nodes.get(0).numDimensions() != node.numDimensions()) {\r\n            throw new ElasticsearchParseException(\"Exception parsing coordinates: number of dimensions do not match\");\r\n        }\r\n        nodes.add(node);\r\n        token = parser.nextToken();\r\n    }\r\n    return new CoordinateNode(nodes);\r\n}"
}, {
	"Path": "edu.stanford.nlp.maxent.iis.LambdaSolve.logLikelihoodScratch",
	"Comment": "calculate the log likelihood from scratch, hashing the conditionalprobabilities in pcond which we will use for the derivative later.",
	"Method": "double logLikelihoodScratch(){\r\n    double s = 0;\r\n    for (int i = 0; i < probConds.length; i++) {\r\n        for (int j = 0; j < probConds[i].length; j++) {\r\n            probConds[i][j] = 0;\r\n        }\r\n        zlambda[i] = 0;\r\n    }\r\n    Experiments exp = p.data;\r\n    for (int fNo = 0, fSize = p.fSize; fNo < fSize; fNo++) {\r\n        Feature f = p.functions.get(fNo);\r\n        double fLambda = lambda[fNo];\r\n        double sum = ftildeArr[fNo];\r\n        sum *= exp.getNumber();\r\n        s -= sum * fLambda;\r\n        if (Math.abs(fLambda) > 200) {\r\n            log.info(\"lambda \" + fNo + \" too big: \" + fLambda);\r\n        }\r\n        for (int i = 0, length = f.len(); i < length; i++) {\r\n            int x = f.getX(i);\r\n            int y = f.getY(i);\r\n            if (ASSUME_BINARY) {\r\n                probConds[x][y] += fLambda;\r\n            } else {\r\n                double val = f.getVal(i);\r\n                probConds[x][y] += (val * fLambda);\r\n            }\r\n        }\r\n    }\r\n    for (int x = 0; x < probConds.length; x++) {\r\n        zlambda[x] = ArrayMath.logSum(probConds[x]);\r\n        s += zlambda[x] * exp.ptildeX(x) * exp.getNumber();\r\n        for (int y = 0; y < probConds[x].length; y++) {\r\n            probConds[x][y] = divide(probConds[x][y], zlambda[x]);\r\n        }\r\n    }\r\n    if (s < 0) {\r\n        throw new IllegalStateException(\"neg log lik smaller than 0: \" + s);\r\n    }\r\n    return s;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.service.ClusterApplierService.addLowPriorityApplier",
	"Comment": "adds an applier which will be called after all high priority and normal appliers have been called.",
	"Method": "void addLowPriorityApplier(ClusterStateApplier applier){\r\n    lowPriorityStateAppliers.add(applier);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.restore.RestoreSnapshotRequestBuilder.setWaitForCompletion",
	"Comment": "if this parameter is set to true the operation will wait for completion of restore process before returning.",
	"Method": "RestoreSnapshotRequestBuilder setWaitForCompletion(boolean waitForCompletion){\r\n    request.waitForCompletion(waitForCompletion);\r\n    return this;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.NewSessionRequestQueue.getNewSessionRequestCount",
	"Comment": "returns the number of unprocessed items in this request queue.",
	"Method": "int getNewSessionRequestCount(){\r\n    lock.readLock().lock();\r\n    try {\r\n        return newSessionRequests.size();\r\n    } finally {\r\n        lock.readLock().unlock();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.XContentHelper.update",
	"Comment": "updates the provided changes into the source. if the key exists in the changes, it overrides the one in sourceunless both are maps, in which case it recursively updated it.",
	"Method": "boolean update(Map<String, Object> source,Map<String, Object> changes,boolean checkUpdatesAreUnequal){\r\n    boolean modified = false;\r\n    for (Map.Entry<String, Object> changesEntry : changes.entrySet()) {\r\n        if (!source.containsKey(changesEntry.getKey())) {\r\n            source.put(changesEntry.getKey(), changesEntry.getValue());\r\n            modified = true;\r\n            continue;\r\n        }\r\n        Object old = source.get(changesEntry.getKey());\r\n        if (old instanceof Map && changesEntry.getValue() instanceof Map) {\r\n            modified |= update((Map<String, Object>) source.get(changesEntry.getKey()), (Map<String, Object>) changesEntry.getValue(), checkUpdatesAreUnequal && !modified);\r\n            continue;\r\n        }\r\n        source.put(changesEntry.getKey(), changesEntry.getValue());\r\n        if (modified) {\r\n            continue;\r\n        }\r\n        if (!checkUpdatesAreUnequal) {\r\n            modified = true;\r\n            continue;\r\n        }\r\n        modified = !Objects.equals(old, changesEntry.getValue());\r\n    }\r\n    return modified;\r\n}"
}, {
	"Path": "android.text.SpannableStringBuilder.toString",
	"Comment": "return a string containing a copy of the chars in this buffer.",
	"Method": "String toString(){\r\n    int len = length();\r\n    char[] buf = new char[len];\r\n    getChars(0, len, buf, 0);\r\n    return new String(buf);\r\n}"
}, {
	"Path": "org.elasticsearch.index.merge.MergeStats.getTotalStoppedTime",
	"Comment": "the total time large merges were stopped so smaller merges could finish.",
	"Method": "TimeValue getTotalStoppedTime(){\r\n    return new TimeValue(totalStoppedTimeInMillis);\r\n}"
}, {
	"Path": "android.text.util.Rfc822Token.quoteNameIfNecessary",
	"Comment": "returns the name, conservatively quoting it if there are anycharacters that are likely to cause trouble outside of aquoted string, or returning it literally if it seems safe.",
	"Method": "String quoteNameIfNecessary(String name){\r\n    int len = name.length();\r\n    for (int i = 0; i < len; i++) {\r\n        char c = name.charAt(i);\r\n        if (!((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z') || (c == ' ') || (c >= '0' && c <= '9'))) {\r\n            return '\"' + quoteName(name) + '\"';\r\n        }\r\n    }\r\n    return name;\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.Security.createPermissions",
	"Comment": "returns dynamic permissions to configured paths and bind ports",
	"Method": "Permissions createPermissions(Environment environment){\r\n    Permissions policy = new Permissions();\r\n    addClasspathPermissions(policy);\r\n    addFilePermissions(policy, environment);\r\n    addBindPermissions(policy, environment.settings());\r\n    return policy;\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.NaturalLogicAnnotator.getModifierSubtreeSpan",
	"Comment": "returns the yield span for the word rooted at the given node, but only traversing a fixed set of relations.",
	"Method": "Pair<Integer, Integer> getModifierSubtreeSpan(SemanticGraph tree,IndexedWord root){\r\n    if (tree.outgoingEdgeList(root).stream().anyMatch(x -> \"neg\".equals(x.getRelation().getShortName()))) {\r\n        return getGeneralizedSubtreeSpan(tree, root, Collections.singleton(\"nmod\"));\r\n    } else {\r\n        return getGeneralizedSubtreeSpan(tree, root, MODIFIER_ARCS);\r\n    }\r\n}"
}, {
	"Path": "org.apache.harmony.tests.java.util.HashtableTest.setUp",
	"Comment": "sets up the fixture, for example, open a network connection. this methodis called before a test is executed.",
	"Method": "void setUp(){\r\n    ht10 = new Hashtable(10);\r\n    ht100 = new Hashtable(100);\r\n    htfull = new Hashtable(10);\r\n    keyVector = new Vector(10);\r\n    elmVector = new Vector(10);\r\n    for (int i = 0; i < 10; i++) {\r\n        ht10.put(\"Key \" + i, \"Val \" + i);\r\n        keyVector.addElement(\"Key \" + i);\r\n        elmVector.addElement(\"Val \" + i);\r\n    }\r\n    for (int i = 0; i < 7; i++) htfull.put(\"FKey \" + i, \"FVal \" + i);\r\n}"
}, {
	"Path": "org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardSnapshot.totalFileCount",
	"Comment": "returns total number of files that are referenced by this snapshot",
	"Method": "int totalFileCount(){\r\n    return indexFiles.size();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.AbstractAllocationDecision.sortNodeDecisions",
	"Comment": "sorts a list of node level decisions by the decision type, then by weight ranking, and finally by node id.",
	"Method": "List<NodeAllocationResult> sortNodeDecisions(List<NodeAllocationResult> nodeDecisions){\r\n    return Collections.unmodifiableList(nodeDecisions.stream().sorted().collect(Collectors.toList()));\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequestBuilder.addSort",
	"Comment": "adds a sort against the given field name and the sort ordering.",
	"Method": "SearchRequestBuilder addSort(String field,SortOrder order,SearchRequestBuilder addSort,SortBuilder<?> sort){\r\n    sourceBuilder().sort(sort);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.LegacyTimeBasedUUIDGenerator.putLong",
	"Comment": "puts the lower numberoflongbytes from l into the array, starting index pos.",
	"Method": "void putLong(byte[] array,long l,int pos,int numberOfLongBytes){\r\n    for (int i = 0; i < numberOfLongBytes; ++i) {\r\n        array[pos + numberOfLongBytes - i - 1] = (byte) (l >>> (i * 8));\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractBulkByScrollRequest.getWaitForActiveShards",
	"Comment": "the number of shard copies that must be active before proceeding with the write.",
	"Method": "ActiveShardCount getWaitForActiveShards(){\r\n    return activeShardCount;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.IndicesService.clearIndexShardCache",
	"Comment": "clears the caches for the given shard id if the shard is still allocated on this node",
	"Method": "void clearIndexShardCache(ShardId shardId,boolean queryCache,boolean fieldDataCache,boolean requestCache,String fields){\r\n    final IndexService service = indexService(shardId.getIndex());\r\n    if (service != null) {\r\n        IndexShard shard = service.getShardOrNull(shardId.id());\r\n        final boolean clearedAtLeastOne = service.clearCaches(queryCache, fieldDataCache, fields);\r\n        if ((requestCache || (clearedAtLeastOne == false && fields.length == 0)) && shard != null) {\r\n            indicesRequestCache.clear(new IndexShardCacheEntity(shard));\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.openqa.selenium.remote.server.rest.Responses.success",
	"Comment": "creates a response object for a successful command execution.",
	"Method": "Response success(SessionId sessionId,Object value){\r\n    Response response = new Response();\r\n    response.setSessionId(sessionId != null ? sessionId.toString() : null);\r\n    response.setValue(value);\r\n    response.setStatus(ErrorCodes.SUCCESS);\r\n    response.setState(ErrorCodes.SUCCESS_STRING);\r\n    return response;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.service.ClusterApplierService.assertNotClusterStateUpdateThread",
	"Comment": "asserts that the current thread is not the cluster state update thread",
	"Method": "boolean assertNotClusterStateUpdateThread(String reason){\r\n    assert Thread.currentThread().getName().contains(CLUSTER_UPDATE_THREAD_NAME) == false : \"Expected current thread [\" + Thread.currentThread() + \"] to not be the cluster state update thread. Reason: [\" + reason + \"]\";\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.IngestDocument.hasField",
	"Comment": "checks whether the document contains a value for the provided path",
	"Method": "boolean hasField(TemplateScript.Factory fieldPathTemplate,boolean hasField,String path,boolean hasField,String path,boolean failOutOfRange){\r\n    FieldPath fieldPath = new FieldPath(path);\r\n    Object context = fieldPath.initialContext;\r\n    for (int i = 0; i < fieldPath.pathElements.length - 1; i++) {\r\n        String pathElement = fieldPath.pathElements[i];\r\n        if (context == null) {\r\n            return false;\r\n        }\r\n        if (context instanceof Map) {\r\n            @SuppressWarnings(\"unchecked\")\r\n            Map<String, Object> map = (Map<String, Object>) context;\r\n            context = map.get(pathElement);\r\n        } else if (context instanceof List) {\r\n            @SuppressWarnings(\"unchecked\")\r\n            List<Object> list = (List<Object>) context;\r\n            try {\r\n                int index = Integer.parseInt(pathElement);\r\n                if (index < 0 || index >= list.size()) {\r\n                    if (failOutOfRange) {\r\n                        throw new IllegalArgumentException(\"[\" + index + \"] is out of bounds for array with length [\" + list.size() + \"] as part of path [\" + path + \"]\");\r\n                    } else {\r\n                        return false;\r\n                    }\r\n                }\r\n                context = list.get(index);\r\n            } catch (NumberFormatException e) {\r\n                return false;\r\n            }\r\n        } else {\r\n            return false;\r\n        }\r\n    }\r\n    String leafKey = fieldPath.pathElements[fieldPath.pathElements.length - 1];\r\n    if (context instanceof Map) {\r\n        @SuppressWarnings(\"unchecked\")\r\n        Map<String, Object> map = (Map<String, Object>) context;\r\n        return map.containsKey(leafKey);\r\n    }\r\n    if (context instanceof List) {\r\n        @SuppressWarnings(\"unchecked\")\r\n        List<Object> list = (List<Object>) context;\r\n        try {\r\n            int index = Integer.parseInt(leafKey);\r\n            if (index >= 0 && index < list.size()) {\r\n                return true;\r\n            } else {\r\n                if (failOutOfRange) {\r\n                    throw new IllegalArgumentException(\"[\" + index + \"] is out of bounds for array with length [\" + list.size() + \"] as part of path [\" + path + \"]\");\r\n                } else {\r\n                    return false;\r\n                }\r\n            }\r\n        } catch (NumberFormatException e) {\r\n            return false;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVectorTable.writeToStream",
	"Comment": "convenience function to write this factor directly to a stream, encoded as proto. reversible with readfromstream.",
	"Method": "void writeToStream(OutputStream stream){\r\n    getProtoBuilder().build().writeTo(stream);\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.FieldTypeLookup.copyAndAddAll",
	"Comment": "return a new instance that contains the union of this instance and the field typesfrom the provided mappers. if a field already exists, its field type will be updatedto use the new type from the given field mapper. similarly if an alias alreadyexists, it will be updated to reference the field type from the new mapper.",
	"Method": "FieldTypeLookup copyAndAddAll(String type,Collection<FieldMapper> fieldMappers,Collection<FieldAliasMapper> fieldAliasMappers){\r\n    Objects.requireNonNull(type, \"type must not be null\");\r\n    if (MapperService.DEFAULT_MAPPING.equals(type)) {\r\n        throw new IllegalArgumentException(\"Default mappings should not be added to the lookup\");\r\n    }\r\n    CopyOnWriteHashMap<String, MappedFieldType> fullName = this.fullNameToFieldType;\r\n    CopyOnWriteHashMap<String, String> aliases = this.aliasToConcreteName;\r\n    for (FieldMapper fieldMapper : fieldMappers) {\r\n        MappedFieldType fieldType = fieldMapper.fieldType();\r\n        MappedFieldType fullNameFieldType = fullName.get(fieldType.name());\r\n        if (!Objects.equals(fieldType, fullNameFieldType)) {\r\n            validateField(fullNameFieldType, fieldType, aliases);\r\n            fullName = fullName.copyAndPut(fieldType.name(), fieldType);\r\n        }\r\n    }\r\n    for (FieldAliasMapper fieldAliasMapper : fieldAliasMappers) {\r\n        String aliasName = fieldAliasMapper.name();\r\n        String path = fieldAliasMapper.path();\r\n        validateAlias(aliasName, path, aliases, fullName);\r\n        aliases = aliases.copyAndPut(aliasName, path);\r\n    }\r\n    return new FieldTypeLookup(fullName, aliases);\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.PendingClusterStatesQueue.failAllStatesAndClear",
	"Comment": "clear the incoming queue. any committed state will be failed",
	"Method": "void failAllStatesAndClear(Exception reason){\r\n    for (ClusterStateContext pendingState : pendingStates) {\r\n        if (pendingState.committed()) {\r\n            pendingState.listener.onNewClusterStateFailed(reason);\r\n        }\r\n    }\r\n    pendingStates.clear();\r\n}"
}, {
	"Path": "edu.stanford.nlp.patterns.GetPatternsFromDataMultiClass.processSents",
	"Comment": "deleteexistingindex is def false for the second call to this function",
	"Method": "void processSents(Map<String, DataInstance> sents,Boolean deleteExistingIndex,Pair<Map<String, DataInstance>, Map<String, DataInstance>> processSents,Properties props,Set<String> labels){\r\n    String fileFormat = props.getProperty(\"fileFormat\");\r\n    Map<String, DataInstance> sents = null;\r\n    boolean batchProcessSents = Boolean.parseBoolean(props.getProperty(\"batchProcessSents\", \"false\"));\r\n    int numMaxSentencesPerBatchFile = Integer.parseInt(props.getProperty(\"numMaxSentencesPerBatchFile\", String.valueOf(Integer.MAX_VALUE)));\r\n    boolean preserveSentenceSequence = Boolean.parseBoolean(props.getProperty(\"preserveSentenceSequence\", \"false\"));\r\n    if (!batchProcessSents) {\r\n        if (preserveSentenceSequence)\r\n            sents = new LinkedHashMap();\r\n        else\r\n            sents = new HashMap();\r\n    } else {\r\n        Data.sentsFiles = new ArrayList();\r\n        Data.sentId2File = new ConcurrentHashMap();\r\n    }\r\n    String file = props.getProperty(\"file\");\r\n    String posModelPath = props.getProperty(\"posModelPath\");\r\n    boolean lowercase = Boolean.parseBoolean(props.getProperty(\"lowercaseText\"));\r\n    boolean useTargetNERRestriction = Boolean.parseBoolean(props.getProperty(\"useTargetNERRestriction\"));\r\n    boolean useTargetParserParentRestriction = Boolean.parseBoolean(props.getProperty(Flags.useTargetParserParentRestriction));\r\n    boolean useContextNERRestriction = Boolean.parseBoolean(props.getProperty(\"useContextNERRestriction\"));\r\n    boolean addEvalSentsToTrain = Boolean.parseBoolean(props.getProperty(\"addEvalSentsToTrain\", \"true\"));\r\n    String evalFileWithGoldLabels = props.getProperty(\"evalFileWithGoldLabels\");\r\n    if (file == null && (evalFileWithGoldLabels == null || addEvalSentsToTrain == false)) {\r\n        throw new RuntimeException(\"No training data! file is \" + file + \" and evalFileWithGoldLabels is \" + evalFileWithGoldLabels + \" and addEvalSentsToTrain is \" + addEvalSentsToTrain);\r\n    }\r\n    if (props.getProperty(Flags.patternType) == null)\r\n        throw new RuntimeException(\"PatternType not specified. Options are SURFACE and DEP\");\r\n    PatternFactory.PatternType patternType = PatternFactory.PatternType.valueOf(props.getProperty(Flags.patternType));\r\n    if (file != null) {\r\n        String saveSentencesSerDirstr = props.getProperty(\"saveSentencesSerDir\");\r\n        File saveSentencesSerDir = null;\r\n        if (saveSentencesSerDirstr != null) {\r\n            saveSentencesSerDir = new File(saveSentencesSerDirstr);\r\n            if (saveSentencesSerDir.exists() && !fileFormat.equalsIgnoreCase(\"ser\"))\r\n                IOUtils.deleteDirRecursively(saveSentencesSerDir);\r\n            IOUtils.ensureDir(saveSentencesSerDir);\r\n        }\r\n        String systemdir = System.getProperty(\"java.io.tmpdir\");\r\n        File tempSaveSentencesDir = File.createTempFile(\"sents\", \".tmp\", new File(systemdir));\r\n        tempSaveSentencesDir.deleteOnExit();\r\n        tempSaveSentencesDir.delete();\r\n        tempSaveSentencesDir.mkdir();\r\n        int numFilesTillNow = 0;\r\n        if (fileFormat == null || fileFormat.equalsIgnoreCase(\"text\") || fileFormat.equalsIgnoreCase(\"txt\")) {\r\n            Map<String, DataInstance> sentsthis;\r\n            if (preserveSentenceSequence)\r\n                sentsthis = new LinkedHashMap();\r\n            else\r\n                sentsthis = new HashMap();\r\n            for (File f : GetPatternsFromDataMultiClass.getAllFiles(file)) {\r\n                Redwood.log(Redwood.DBG, \"Annotating text in \" + f);\r\n                Iterator<String> reader = IOUtils.readLines(f).iterator();\r\n                while (reader.hasNext()) {\r\n                    numFilesTillNow = tokenize(reader, posModelPath, lowercase, useTargetNERRestriction || useContextNERRestriction, f.getName() + \"-\" + numFilesTillNow + \"-\", useTargetParserParentRestriction, props.getProperty(Flags.numThreads), batchProcessSents, numMaxSentencesPerBatchFile, saveSentencesSerDir == null ? tempSaveSentencesDir : saveSentencesSerDir, sentsthis, numFilesTillNow, patternType);\r\n                }\r\n                if (!batchProcessSents) {\r\n                    sents.putAll(sentsthis);\r\n                }\r\n            }\r\n            if (!batchProcessSents) {\r\n                String outfilename = (saveSentencesSerDir == null ? tempSaveSentencesDir : saveSentencesSerDir) + \"/sents_\" + numFilesTillNow;\r\n                if (saveSentencesSerDir != null)\r\n                    Data.inMemorySaveFileLocation = outfilename;\r\n                Redwood.log(Redwood.FORCE, \"Saving sentences in \" + outfilename);\r\n                IOUtils.writeObjectToFile(sents, outfilename);\r\n            }\r\n        } else if (fileFormat.equalsIgnoreCase(\"ser\")) {\r\n            for (File f : GetPatternsFromDataMultiClass.getAllFiles(file)) {\r\n                Redwood.log(Redwood.DBG, \"reading from ser file \" + f);\r\n                if (!batchProcessSents)\r\n                    sents.putAll((Map<String, DataInstance>) IOUtils.readObjectFromFile(f));\r\n                else {\r\n                    File newf = new File(tempSaveSentencesDir.getAbsolutePath() + \"/\" + f.getAbsolutePath().replaceAll(java.util.regex.Pattern.quote(\"/\"), \"_\"));\r\n                    IOUtils.cp(f, newf);\r\n                    Data.sentsFiles.add(newf);\r\n                }\r\n            }\r\n        } else {\r\n            throw new RuntimeException(\"Cannot identify the file format. Valid values are text (or txt) and ser, where the serialized file is of the type Map<String, DataInstance>.\");\r\n        }\r\n    }\r\n    Map<String, DataInstance> evalsents = new HashMap();\r\n    boolean evaluate = Boolean.parseBoolean(props.getProperty(\"evaluate\"));\r\n    if (evaluate) {\r\n        if (evalFileWithGoldLabels != null) {\r\n            String saveEvalSentencesSerFile = props.getProperty(\"saveEvalSentencesSerFile\");\r\n            File saveEvalSentencesSerFileFile = null;\r\n            if (saveEvalSentencesSerFile == null) {\r\n                String systemdir = System.getProperty(\"java.io.tmpdir\");\r\n                saveEvalSentencesSerFileFile = File.createTempFile(\"evalsents\", \".tmp\", new File(systemdir));\r\n            } else\r\n                saveEvalSentencesSerFileFile = new File(saveEvalSentencesSerFile);\r\n            Map setClassForTheseLabels = new HashMap<String, Class>();\r\n            List<File> allFiles = GetPatternsFromDataMultiClass.getAllFiles(evalFileWithGoldLabels);\r\n            int numFile = 0;\r\n            String evalFileFormat = props.getProperty(\"evalFileFormat\");\r\n            if (evalFileFormat == null || evalFileFormat.equalsIgnoreCase(\"text\") || evalFileFormat.equalsIgnoreCase(\"txt\") || evalFileFormat.startsWith(\"text\")) {\r\n                for (File f : allFiles) {\r\n                    numFile++;\r\n                    Redwood.log(Redwood.DBG, \"Annotating text in \" + f + \". Num file \" + numFile);\r\n                    if (evalFileFormat.equalsIgnoreCase(\"textCoNLLStyle\")) {\r\n                        Map<String, DataInstance> sentsEval = AnnotatedTextReader.parseColumnFile(new BufferedReader(new FileReader(f)), labels, setClassForTheseLabels, true, f.getName());\r\n                        evalsents.putAll(runPOSNERParseOnTokens(sentsEval, props));\r\n                    } else {\r\n                        List<CoreMap> sentsCMs = AnnotatedTextReader.parseFile(new BufferedReader(new FileReader(f)), labels, setClassForTheseLabels, true, f.getName());\r\n                        evalsents.putAll(runPOSNEROnTokens(sentsCMs, posModelPath, useTargetNERRestriction || useContextNERRestriction, \"\", useTargetParserParentRestriction, props.getProperty(Flags.numThreads), patternType));\r\n                    }\r\n                }\r\n            } else if (fileFormat.equalsIgnoreCase(\"ser\")) {\r\n                for (File f : allFiles) {\r\n                    evalsents.putAll((Map<? extends String, ? extends DataInstance>) IOUtils.readObjectFromFile(f));\r\n                }\r\n            }\r\n            if (addEvalSentsToTrain) {\r\n                Redwood.log(Redwood.DBG, \"Adding \" + evalsents.size() + \" eval sents to the training set\");\r\n            }\r\n            IOUtils.writeObjectToFile(evalsents, saveEvalSentencesSerFileFile);\r\n            if (batchProcessSents) {\r\n                Data.sentsFiles.add(saveEvalSentencesSerFileFile);\r\n                for (String k : evalsents.keySet()) Data.sentId2File.put(k, saveEvalSentencesSerFileFile);\r\n            } else\r\n                sents.putAll(evalsents);\r\n        }\r\n    }\r\n    return new Pair<Map<String, DataInstance>, Map<String, DataInstance>>(sents, evalsents);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.runSUTime",
	"Comment": "runs sutime and converts its output into namedentitytagannotations",
	"Method": "List<CoreMap> runSUTime(CoreMap sentence,CoreMap document){\r\n    List<CoreMap> timeExpressions = timexExtractor.extractTimeExpressionCoreMaps(sentence, document);\r\n    if (timeExpressions != null) {\r\n        if (DEBUG)\r\n            System.out.println(\"FOUND TEMPORALS: \" + timeExpressions);\r\n    }\r\n    return timeExpressions;\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.spanish.SpanishVerbStripper.getInstance",
	"Comment": "singleton pattern function for getting a verb stripper based onthe dictionary at dictpath.",
	"Method": "SpanishVerbStripper getInstance(SpanishVerbStripper getInstance,String dictPath){\r\n    SpanishVerbStripper svs = instances.get(dictPath);\r\n    if (svs == null) {\r\n        svs = new SpanishVerbStripper(dictPath);\r\n        instances.put(dictPath, svs);\r\n    }\r\n    return svs;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.ExternalSessionKey.fromResponseBody",
	"Comment": "extract the external key from the server response for a selenium1 new session request.",
	"Method": "ExternalSessionKey fromResponseBody(String responseBody){\r\n    if (responseBody != null && responseBody.startsWith(\"OK,\")) {\r\n        return new ExternalSessionKey(responseBody.replace(\"OK,\", \"\"));\r\n    }\r\n    throw new NewSessionException(\"The server returned an error : \" + responseBody);\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.ElectMasterService.compareNodes",
	"Comment": "master nodes go before other nodes, with a secondary sort by id",
	"Method": "int compareNodes(DiscoveryNode o1,DiscoveryNode o2){\r\n    if (o1.isMasterNode() && !o2.isMasterNode()) {\r\n        return -1;\r\n    }\r\n    if (!o1.isMasterNode() && o2.isMasterNode()) {\r\n        return 1;\r\n    }\r\n    return o1.getId().compareTo(o2.getId());\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PriorityComparator.getAllocationComparator",
	"Comment": "returns a prioritycomparator that uses the routingallocation index metadata to access the index setting per index.",
	"Method": "PriorityComparator getAllocationComparator(RoutingAllocation allocation){\r\n    return new PriorityComparator() {\r\n        @Override\r\n        protected Settings getIndexSettings(Index index) {\r\n            IndexMetaData indexMetaData = allocation.metaData().getIndexSafe(index);\r\n            return indexMetaData.getSettings();\r\n        }\r\n    };\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.PriorityComparator.getAllocationComparator",
	"Comment": "returns a prioritycomparator that uses the routingallocation index metadata to access the index setting per index.",
	"Method": "PriorityComparator getAllocationComparator(RoutingAllocation allocation){\r\n    IndexMetaData indexMetaData = allocation.metaData().getIndexSafe(index);\r\n    return indexMetaData.getSettings();\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.ChineseTreebankParserParams.diskTreebank",
	"Comment": "uses a disktreebank with a chtbtokenizer and abobchristreenormalizer.",
	"Method": "DiskTreebank diskTreebank(){\r\n    String encoding = inputEncoding;\r\n    if (!java.nio.charset.Charset.isSupported(encoding)) {\r\n        printlnErr(\"Warning: desired encoding \" + encoding + \" not accepted. \");\r\n        printlnErr(\"Using UTF-8 to construct DiskTreebank\");\r\n        encoding = \"UTF-8\";\r\n    }\r\n    return new DiskTreebank(treeReaderFactory(), encoding);\r\n}"
}, {
	"Path": "org.openqa.grid.internal.listener.RegistrationListenerTest.registerSomeSlow",
	"Comment": "register a regular proxy for app1 and a slow one. try to reserve 2app1 1 should be reserveddirectly. 1 should wait for the slow proxy to finish the registration properly beforereturning",
	"Method": "void registerSomeSlow(){\r\n    final GridRegistry registry = DefaultGridRegistry.newInstance(new Hub(new GridHubConfiguration()));\r\n    try {\r\n        registry.add(new BaseRemoteProxy(req, registry));\r\n        new Thread(() -> registry.add(new MySlowRemoteProxy(req, registry))).start();\r\n        assertEquals(registry.getAllProxies().size(), 1);\r\n        assertEquals(slowRemoteUp, false);\r\n        RequestHandler req = GridHelper.createNewSessionHandler(registry, app1);\r\n        req.process();\r\n        TestSession s1 = req.getSession();\r\n        assertNotNull(s1);\r\n        assertEquals(registry.getAllProxies().size(), 1);\r\n        assertEquals(false, slowRemoteUp);\r\n        RequestHandler req2 = GridHelper.createNewSessionHandler(registry, app1);\r\n        req2.process();\r\n        TestSession s2 = req2.getSession();\r\n        assertNotNull(s2);\r\n        assertEquals(2, registry.getAllProxies().size());\r\n        assertTrue(slowRemoteUp);\r\n    } finally {\r\n        registry.stop();\r\n    }\r\n}"
}, {
	"Path": "org.apache.dubbo.registry.integration.RegistryProtocol.getProviderUrl",
	"Comment": "get the address of the providerurl through the url of the invoker",
	"Method": "URL getProviderUrl(Invoker<?> origininvoker){\r\n    String export = origininvoker.getUrl().getParameterAndDecoded(Constants.EXPORT_KEY);\r\n    if (export == null || export.length() == 0) {\r\n        throw new IllegalArgumentException(\"The registry export url is null! registry: \" + origininvoker.getUrl());\r\n    }\r\n    URL providerUrl = URL.valueOf(export);\r\n    return providerUrl;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.CategoryWordTagFactory.newLabel",
	"Comment": "create a new categorywordtag label, where the label isformed fromthe label object passed in.depending on what fieldseach label has, other things will be null.",
	"Method": "Label newLabel(String labelStr,Label newLabel,String labelStr,int options,Label newLabel,String word,String tag,String category,Label newLabel,Label oldLabel){\r\n    return new CategoryWordTag(oldLabel);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.RoutingNodes.activePrimary",
	"Comment": "returns the active primary shard for the given shard id or null ifno primary is found or the primary is not active.",
	"Method": "ShardRouting activePrimary(ShardId shardId){\r\n    for (ShardRouting shardRouting : assignedShards(shardId)) {\r\n        if (shardRouting.primary() && shardRouting.active()) {\r\n            return shardRouting;\r\n        }\r\n    }\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequestBuilder.setTemplate",
	"Comment": "sets the match expression that will be used to match on indices created.",
	"Method": "PutIndexTemplateRequestBuilder setTemplate(String indexPattern){\r\n    return setPatterns(Collections.singletonList(indexPattern));\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.ArabicTreebankParserParams.setHeadFinder",
	"Comment": "reconfigures active features after a change in the default headfinder.",
	"Method": "void setHeadFinder(HeadFinder hf){\r\n    if (hf == null)\r\n        throw new IllegalArgumentException();\r\n    headFinder = hf;\r\n    initializeAnnotationPatterns();\r\n    activeAnnotations.clear();\r\n    for (String key : baselineFeatures) {\r\n        Pair<TregexPattern, Function<TregexMatcher, String>> p = annotationPatterns.get(key);\r\n        activeAnnotations.add(p);\r\n    }\r\n    for (String key : additionalFeatures) {\r\n        Pair<TregexPattern, Function<TregexMatcher, String>> p = annotationPatterns.get(key);\r\n        activeAnnotations.add(p);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequest.upsert",
	"Comment": "sets the doc source of the update request to be used when the document does not exists. the docincludes field and value pairs.",
	"Method": "UpdateRequest upsert(IndexRequest upsertRequest,UpdateRequest upsert,XContentBuilder source,UpdateRequest upsert,Map<String, Object> source,UpdateRequest upsert,Map<String, Object> source,XContentType contentType,UpdateRequest upsert,String source,XContentType xContentType,UpdateRequest upsert,byte[] source,XContentType xContentType,UpdateRequest upsert,byte[] source,int offset,int length,XContentType xContentType,UpdateRequest upsert,Object source,UpdateRequest upsert,XContentType xContentType,Object source){\r\n    safeUpsertRequest().source(xContentType, source);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDocument.labels",
	"Comment": "returns the complete list of labels for this document.this is an empty collection if none have been set.",
	"Method": "Collection<L> labels(){\r\n    return labels;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.RefreshListeners.addOrNotify",
	"Comment": "add a listener for refreshes, calling it immediately if the location is already visible. if this runs out of listener slots then itforces a refresh and calls the listener immediately as well.",
	"Method": "boolean addOrNotify(Translog.Location location,Consumer<Boolean> listener){\r\n    requireNonNull(listener, \"listener cannot be null\");\r\n    requireNonNull(location, \"location cannot be null\");\r\n    if (lastRefreshedLocation != null && lastRefreshedLocation.compareTo(location) >= 0) {\r\n        listener.accept(false);\r\n        return true;\r\n    }\r\n    synchronized (this) {\r\n        List<Tuple<Translog.Location, Consumer<Boolean>>> listeners = refreshListeners;\r\n        if (listeners == null) {\r\n            if (closed) {\r\n                throw new IllegalStateException(\"can't wait for refresh on a closed index\");\r\n            }\r\n            listeners = new ArrayList();\r\n            refreshListeners = listeners;\r\n        }\r\n        if (listeners.size() < getMaxRefreshListeners.getAsInt()) {\r\n            ThreadContext.StoredContext storedContext = threadContext.newStoredContext(true);\r\n            Consumer<Boolean> contextPreservingListener = forced -> {\r\n                try (ThreadContext.StoredContext ignore = threadContext.stashContext()) {\r\n                    storedContext.restore();\r\n                    listener.accept(forced);\r\n                }\r\n            };\r\n            listeners.add(new Tuple(location, contextPreservingListener));\r\n            return false;\r\n        }\r\n    }\r\n    forceRefresh.run();\r\n    listener.accept(true);\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequestBuilder.setTimeout",
	"Comment": "an optional timeout to control how long search is allowed to take.",
	"Method": "SearchRequestBuilder setTimeout(TimeValue timeout){\r\n    sourceBuilder().timeout(timeout);\r\n    return this;\r\n}"
}, {
	"Path": "android.util.SparseIntArray.get",
	"Comment": "gets the int mapped from the specified key, or the specified valueif no such mapping has been made.",
	"Method": "int get(int key,int get,int key,int valueIfKeyNotFound){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i < 0) {\r\n        return valueIfKeyNotFound;\r\n    } else {\r\n        return mValues[i];\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.master.TransportMasterNodeAction.getMasterActionName",
	"Comment": "allows to conditionally return a different master node action name in the case an action gets renamed.this mainly for backwards compatibility should be used rarely",
	"Method": "String getMasterActionName(DiscoveryNode node){\r\n    return actionName;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.termsLookupQuery",
	"Comment": "a terms query that can extract the terms from another doc in an index.",
	"Method": "TermsQueryBuilder termsLookupQuery(String name,TermsLookup termsLookup){\r\n    return new TermsQueryBuilder(name, termsLookup);\r\n}"
}, {
	"Path": "org.openqa.grid.internal.utils.configuration.StandaloneConfiguration.toJson",
	"Comment": "return a jsonelement representation of the configuration. does not serialize nulls.",
	"Method": "Map<String, Object> toJson(){\r\n    Map<String, Object> json = new HashMap();\r\n    json.put(\"browserTimeout\", browserTimeout);\r\n    json.put(\"debug\", debug);\r\n    json.put(\"jettyMaxThreads\", jettyMaxThreads);\r\n    json.put(\"log\", log);\r\n    json.put(\"host\", host);\r\n    json.put(\"port\", port);\r\n    json.put(\"role\", role);\r\n    json.put(\"timeout\", timeout);\r\n    serializeFields(json);\r\n    return json.entrySet().stream().filter(entry -> entry.getValue() != null).collect(toImmutableSortedMap(natural(), Map.Entry::getKey, Map.Entry::getValue));\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.MembersInjectorStore.get",
	"Comment": "returns a new complete members injector with injection listeners registered.",
	"Method": "MembersInjectorImpl<T> get(TypeLiteral<T> key,Errors errors){\r\n    return (MembersInjectorImpl<T>) cache.get(key, errors);\r\n}"
}, {
	"Path": "org.elasticsearch.node.Node.newSearchService",
	"Comment": "creates a new the searchservice. this method can be overwritten by tests to inject mock implementations.",
	"Method": "SearchService newSearchService(ClusterService clusterService,IndicesService indicesService,ThreadPool threadPool,ScriptService scriptService,BigArrays bigArrays,FetchPhase fetchPhase,ResponseCollectorService responseCollectorService){\r\n    return new SearchService(clusterService, indicesService, threadPool, scriptService, bigArrays, fetchPhase, responseCollectorService);\r\n}"
}, {
	"Path": "org.elasticsearch.common.network.NetworkModule.registerAllocationCommand",
	"Comment": "register an allocation command.this lives here instead of the more aptly named clustermodule because the transport client needs these to be registered.",
	"Method": "void registerAllocationCommand(Writeable.Reader<T> reader,CheckedFunction<XContentParser, T, IOException> parser,ParseField commandName){\r\n    namedXContents.add(new NamedXContentRegistry.Entry(AllocationCommand.class, commandName, parser));\r\n    namedWriteables.add(new NamedWriteableRegistry.Entry(AllocationCommand.class, commandName.getPreferredName(), reader));\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequest.getPipeline",
	"Comment": "returns the ingest pipeline to be executed before indexing the document",
	"Method": "String getPipeline(){\r\n    return this.pipeline;\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.Initializer.requestInjection",
	"Comment": "registers an instance for member injection when that step is performed.",
	"Method": "Initializable<T> requestInjection(InjectorImpl injector,T instance,Object source,Set<InjectionPoint> injectionPoints){\r\n    Objects.requireNonNull(source);\r\n    if (instance == null || (injectionPoints.isEmpty() && !injector.membersInjectorStore.hasTypeListeners())) {\r\n        return Initializables.of(instance);\r\n    }\r\n    InjectableReference<T> initializable = new InjectableReference(injector, instance, source);\r\n    pendingInjection.put(instance, initializable);\r\n    return initializable;\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.ClauseSplitter.train",
	"Comment": "train a clause searcher factory. that is, train a classifier for which arcs should benew clauses.",
	"Method": "ClauseSplitter train(Stream<Pair<CoreMap, Collection<Pair<Span, Span>>>> trainingData,Optional<File> modelPath,Optional<File> trainingDataDump,Featurizer featurizer,ClauseSplitter train,Stream<Pair<CoreMap, Collection<Pair<Span, Span>>>> trainingData,File modelPath,File trainingDataDump){\r\n    return train(trainingData, Optional.of(modelPath), Optional.of(trainingDataDump), ClauseSplitterSearchProblem.DEFAULT_FEATURIZER);\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.GlobalCheckpointListeners.getTimeoutFuture",
	"Comment": "the scheduled future for a listener that has a timeout associated with it, otherwise null.",
	"Method": "ScheduledFuture<?> getTimeoutFuture(GlobalCheckpointListener listener){\r\n    return listeners.get(listener).v2();\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.AbstractSearchAsyncAction.raisePhaseFailure",
	"Comment": "this method should be called if a search phase failed to ensure all relevant search contexts and resources are released.this method will also notify the listener and sends back a failure to the user.",
	"Method": "void raisePhaseFailure(SearchPhaseExecutionException exception){\r\n    results.getSuccessfulResults().forEach((entry) -> {\r\n        try {\r\n            SearchShardTarget searchShardTarget = entry.getSearchShardTarget();\r\n            Transport.Connection connection = getConnection(null, searchShardTarget.getNodeId());\r\n            sendReleaseSearchContext(entry.getRequestId(), connection, searchShardTarget.getOriginalIndices());\r\n        } catch (Exception inner) {\r\n            inner.addSuppressed(exception);\r\n            logger.trace(\"failed to release context\", inner);\r\n        }\r\n    });\r\n    listener.onFailure(exception);\r\n}"
}, {
	"Path": "org.elasticsearch.bootstrap.Spawner.spawnNativeController",
	"Comment": "attempt to spawn the controller daemon for a given module. the spawned process will remain connected to this jvm via its stdin,stdout, and stderr streams, but the references to these streams are not available to code outside this package.",
	"Method": "Process spawnNativeController(Path spawnPath,Path tmpPath){\r\n    final String command;\r\n    if (Constants.WINDOWS) {\r\n        command = Natives.getShortPathName(spawnPath.toString());\r\n    } else {\r\n        command = spawnPath.toString();\r\n    }\r\n    final ProcessBuilder pb = new ProcessBuilder(command);\r\n    pb.environment().clear();\r\n    pb.environment().put(\"TMPDIR\", tmpPath.toString());\r\n    return pb.start();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.analyze.AnalyzeRequestBuilder.setTokenizer",
	"Comment": "instead of setting the analyzer, sets the tokenizer using custom settings that will be used as part of a customanalyzer.",
	"Method": "AnalyzeRequestBuilder setTokenizer(String tokenizer,AnalyzeRequestBuilder setTokenizer,Map<String, ?> tokenizer){\r\n    request.tokenizer(tokenizer);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.ReindexRequest.setSourceIndices",
	"Comment": "set the indices which will act as the source for the reindexrequest",
	"Method": "ReindexRequest setSourceIndices(String sourceIndices){\r\n    if (sourceIndices != null) {\r\n        this.getSearchRequest().indices(sourceIndices);\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.openqa.grid.internal.listener.SessionListenerTest.buggyBefore",
	"Comment": "if before throws an exception, the resources are released for other tests to use.",
	"Method": "void buggyBefore(){\r\n    GridRegistry registry = DefaultGridRegistry.newInstance(new Hub(new GridHubConfiguration()));\r\n    registry.add(new MyBuggyBeforeRemoteProxy(req, registry));\r\n    RequestHandler req = GridHelper.createNewSessionHandler(registry, app1);\r\n    try {\r\n        req.process();\r\n    } catch (Exception ignore) {\r\n    }\r\n    while (registry.getActiveSessions().size() != 0) {\r\n        Thread.sleep(250);\r\n    }\r\n    assertEquals(registry.getActiveSessions().size(), 0);\r\n    RequestHandler req2 = GridHelper.createNewSessionHandler(registry, app1);\r\n    req2.process();\r\n    TestSession session = req2.getSession();\r\n    assertNotNull(session);\r\n    assertEquals(registry.getActiveSessions().size(), 1);\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.getWritingBytes",
	"Comment": "returns how many bytes we are currently moving from heap to disk",
	"Method": "long getWritingBytes(){\r\n    Engine engine = getEngineOrNull();\r\n    if (engine == null) {\r\n        return 0;\r\n    }\r\n    return engine.getWritingBytes();\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.getGeneration",
	"Comment": "returns the current generation of this translog. this corresponds to the latest uncommitted translog generation",
	"Method": "TranslogGeneration getGeneration(){\r\n    try (ReleasableLock lock = writeLock.acquire()) {\r\n        return new TranslogGeneration(translogUUID, currentFileGeneration());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequest.routing",
	"Comment": "controls the shard routing of the request. using this value to hash the shardand not the id.",
	"Method": "IndexRequest routing(String routing,String routing){\r\n    return this.routing;\r\n}"
}, {
	"Path": "android.util.LruCache.trimToSize",
	"Comment": "remove the eldest entries until the total of remaining entries is at orbelow the requested size.",
	"Method": "void trimToSize(int maxSize){\r\n    while (true) {\r\n        K key;\r\n        V value;\r\n        synchronized (this) {\r\n            if (size < 0 || (map.isEmpty() && size != 0)) {\r\n                throw new IllegalStateException(getClass().getName() + \".sizeOf() is reporting inconsistent results!\");\r\n            }\r\n            if (size <= maxSize) {\r\n                break;\r\n            }\r\n            Map.Entry<K, V> toEvict = map.eldest();\r\n            if (toEvict == null) {\r\n                break;\r\n            }\r\n            key = toEvict.getKey();\r\n            value = toEvict.getValue();\r\n            map.remove(key);\r\n            size -= safeSizeOf(key, value);\r\n            evictionCount++;\r\n        }\r\n        entryRemoved(true, key, value, null);\r\n    }\r\n}"
}, {
	"Path": "android.util.ArrayMap.containsAll",
	"Comment": "determine if the array map contains all of the keys in the given collection.",
	"Method": "boolean containsAll(Collection<?> collection){\r\n    return MapCollections.containsAllHelper(this, collection);\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.MetaDataStateFormat.read",
	"Comment": "reads the state from a given file and compares the expected version against the actual version ofthe state.",
	"Method": "T read(NamedXContentRegistry namedXContentRegistry,Path file){\r\n    try (Directory dir = newDirectory(file.getParent())) {\r\n        try (IndexInput indexInput = dir.openInput(file.getFileName().toString(), IOContext.DEFAULT)) {\r\n            CodecUtil.checksumEntireFile(indexInput);\r\n            CodecUtil.checkHeader(indexInput, STATE_FILE_CODEC, MIN_COMPATIBLE_STATE_FILE_VERSION, STATE_FILE_VERSION);\r\n            final XContentType xContentType = XContentType.values()[indexInput.readInt()];\r\n            if (xContentType != FORMAT) {\r\n                throw new IllegalStateException(\"expected state in \" + file + \" to be \" + FORMAT + \" format but was \" + xContentType);\r\n            }\r\n            long filePointer = indexInput.getFilePointer();\r\n            long contentSize = indexInput.length() - CodecUtil.footerLength() - filePointer;\r\n            try (IndexInput slice = indexInput.slice(\"state_xcontent\", filePointer, contentSize)) {\r\n                try (XContentParser parser = XContentFactory.xContent(FORMAT).createParser(namedXContentRegistry, LoggingDeprecationHandler.INSTANCE, new InputStreamIndexInput(slice, contentSize))) {\r\n                    return fromXContent(parser);\r\n                }\r\n            }\r\n        } catch (CorruptIndexException | IndexFormatTooOldException | IndexFormatTooNewException ex) {\r\n            throw new CorruptStateException(ex);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.TokenSequencePattern.toString",
	"Comment": "returns a string representation of the tokensequencepattern.",
	"Method": "String toString(){\r\n    return this.pattern();\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.hasBeenProcessedBefore",
	"Comment": "checks if the given operation has been processed in this engine or not.",
	"Method": "boolean hasBeenProcessedBefore(Operation op){\r\n    if (Assertions.ENABLED) {\r\n        assert op.seqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO : \"operation is not assigned seq_no\";\r\n        if (op.operationType() == Operation.TYPE.NO_OP) {\r\n            assert noOpKeyedLock.isHeldByCurrentThread(op.seqNo());\r\n        } else {\r\n            assert versionMap.assertKeyedLockHeldByCurrentThread(op.uid().bytes());\r\n        }\r\n    }\r\n    return localCheckpointTracker.contains(op.seqNo());\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.Lucene.parseVersionLenient",
	"Comment": "parses the version string lenient and returns the default value if the given string is null or empty",
	"Method": "Version parseVersionLenient(String toParse,Version defaultValue){\r\n    return LenientParser.parse(toParse, defaultValue);\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksExecutor.getAssignment",
	"Comment": "returns the node id where the params has to be executed,the default implementation returns the least loaded data node",
	"Method": "Assignment getAssignment(Params params,ClusterState clusterState){\r\n    DiscoveryNode discoveryNode = selectLeastLoadedNode(clusterState, DiscoveryNode::isDataNode);\r\n    if (discoveryNode == null) {\r\n        return NO_NODE_FOUND;\r\n    } else {\r\n        return new Assignment(discoveryNode.getId(), \"\");\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.SimpleTensor.random",
	"Comment": "returns a randomly initialized tensor with values draft from theuniform distribution between minvalue and maxvalue.",
	"Method": "SimpleTensor random(int numRows,int numCols,int numSlices,double minValue,double maxValue,java.util.Random rand){\r\n    SimpleTensor tensor = new SimpleTensor(numRows, numCols, numSlices);\r\n    for (int i = 0; i < numSlices; ++i) {\r\n        tensor.slices[i] = SimpleMatrix.random(numRows, numCols, minValue, maxValue, rand);\r\n    }\r\n    return tensor;\r\n}"
}, {
	"Path": "org.elasticsearch.common.io.Channels.readFromFileChannel",
	"Comment": "read from a file channel into a byte buffer, starting at a certain position.",
	"Method": "byte[] readFromFileChannel(FileChannel channel,long position,int length,int readFromFileChannel,FileChannel channel,long channelPosition,byte[] dest,int destOffset,int length,int readFromFileChannel,FileChannel channel,long channelPosition,ByteBuffer dest){\r\n    if (dest.isDirect() || (dest.remaining() < READ_CHUNK_SIZE)) {\r\n        return readSingleChunk(channel, channelPosition, dest);\r\n    } else {\r\n        int bytesRead = 0;\r\n        int bytesToRead = dest.remaining();\r\n        ByteBuffer tmpBuffer = dest.duplicate();\r\n        try {\r\n            while (dest.hasRemaining()) {\r\n                tmpBuffer.limit(Math.min(dest.limit(), tmpBuffer.position() + READ_CHUNK_SIZE));\r\n                int read = readSingleChunk(channel, channelPosition, tmpBuffer);\r\n                if (read < 0) {\r\n                    return read;\r\n                }\r\n                bytesRead += read;\r\n                channelPosition += read;\r\n                dest.position(tmpBuffer.position());\r\n            }\r\n        } finally {\r\n            dest.position(tmpBuffer.position());\r\n        }\r\n        assert bytesRead == bytesToRead : \"failed to read an entire buffer but also didn't get an EOF (read [\" + bytesRead + \"] needed [\" + bytesToRead + \"]\";\r\n        return bytesRead;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.ShardRouting.cancelRelocation",
	"Comment": "cancel relocation of a shard. the shards state must be setto relocating.",
	"Method": "ShardRouting cancelRelocation(){\r\n    assert state == ShardRoutingState.RELOCATING : this;\r\n    assert assignedToNode() : this;\r\n    assert relocatingNodeId != null : this;\r\n    return new ShardRouting(shardId, currentNodeId, null, primary, ShardRoutingState.STARTED, recoverySource, null, AllocationId.cancelRelocation(allocationId), UNAVAILABLE_EXPECTED_SHARD_SIZE);\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.OpenIE.tripleToString",
	"Comment": "prints an openie triple to a string, according to the output format requested inthe annotator.",
	"Method": "String tripleToString(RelationTriple extraction,String docid,CoreMap sentence){\r\n    switch(FORMAT) {\r\n        case REVERB:\r\n            return extraction.toReverbString(docid, sentence);\r\n        case OLLIE:\r\n            return extraction.confidenceGloss() + \": (\" + extraction.subjectGloss() + \"; \" + extraction.relationGloss() + \"; \" + extraction.objectGloss() + ')';\r\n        case DEFAULT:\r\n            return extraction.toString();\r\n        case QA_SRL:\r\n            return extraction.toQaSrlString(sentence);\r\n        default:\r\n            throw new IllegalStateException(\"Format is not implemented: \" + FORMAT);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.server.LexicalizedParserServer.handleTree",
	"Comment": "returns the result of applying the parser to arg as a serialized tree.",
	"Method": "void handleTree(String arg,OutputStream outStream){\r\n    Tree tree = parse(arg, false);\r\n    if (tree == null) {\r\n        return;\r\n    }\r\n    log.info(tree);\r\n    if (tree != null) {\r\n        ObjectOutputStream oos = new ObjectOutputStream(outStream);\r\n        oos.writeObject(tree);\r\n        oos.flush();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.update.UpdateRequest.retryOnConflict",
	"Comment": "sets the number of retries of a version conflict occurs because the document was updated betweengetting it and updating it. defaults to 0.",
	"Method": "UpdateRequest retryOnConflict(int retryOnConflict,int retryOnConflict){\r\n    return this.retryOnConflict;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterInfo.getNodeMostAvailableDiskUsages",
	"Comment": "returns a node id to disk usage mapping for the path that has the most available space on the node.",
	"Method": "ImmutableOpenMap<String, DiskUsage> getNodeMostAvailableDiskUsages(){\r\n    return this.mostAvailableSpaceUsage;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchPhaseController.fillDocIdsToLoad",
	"Comment": "builds an array, with potential null elements, with docs to load.",
	"Method": "IntArrayList[] fillDocIdsToLoad(int numShards,ScoreDoc[] shardDocs){\r\n    IntArrayList[] docIdsToLoad = new IntArrayList[numShards];\r\n    for (ScoreDoc shardDoc : shardDocs) {\r\n        IntArrayList shardDocIdsToLoad = docIdsToLoad[shardDoc.shardIndex];\r\n        if (shardDocIdsToLoad == null) {\r\n            shardDocIdsToLoad = docIdsToLoad[shardDoc.shardIndex] = new IntArrayList();\r\n        }\r\n        shardDocIdsToLoad.add(shardDoc.doc);\r\n    }\r\n    return docIdsToLoad;\r\n}"
}, {
	"Path": "android.text.SpannableStringBuilder.getSpans",
	"Comment": "return an array of the spans of the specified type that overlapthe specified range of the buffer.the kind may be object.class to geta list of all the spans regardless of type.",
	"Method": "T[] getSpans(int queryStart,int queryEnd,Class<T> kind){\r\n    if (kind == null)\r\n        return ArrayUtils.emptyArray(kind);\r\n    int spanCount = mSpanCount;\r\n    Object[] spans = mSpans;\r\n    int[] starts = mSpanStarts;\r\n    int[] ends = mSpanEnds;\r\n    int[] flags = mSpanFlags;\r\n    int gapstart = mGapStart;\r\n    int gaplen = mGapLength;\r\n    int count = 0;\r\n    T[] ret = null;\r\n    T ret1 = null;\r\n    for (int i = 0; i < spanCount; i++) {\r\n        int spanStart = starts[i];\r\n        if (spanStart > gapstart) {\r\n            spanStart -= gaplen;\r\n        }\r\n        if (spanStart > queryEnd) {\r\n            continue;\r\n        }\r\n        int spanEnd = ends[i];\r\n        if (spanEnd > gapstart) {\r\n            spanEnd -= gaplen;\r\n        }\r\n        if (spanEnd < queryStart) {\r\n            continue;\r\n        }\r\n        if (spanStart != spanEnd && queryStart != queryEnd) {\r\n            if (spanStart == queryEnd)\r\n                continue;\r\n            if (spanEnd == queryStart)\r\n                continue;\r\n        }\r\n        if (!kind.isInstance(spans[i]))\r\n            continue;\r\n        if (count == 0) {\r\n            ret1 = (T) spans[i];\r\n            count++;\r\n        } else {\r\n            if (count == 1) {\r\n                ret = (T[]) Array.newInstance(kind, spanCount - i + 1);\r\n                ret[0] = ret1;\r\n            }\r\n            int prio = flags[i] & SPAN_PRIORITY;\r\n            if (prio != 0) {\r\n                int j;\r\n                for (j = 0; j < count; j++) {\r\n                    int p = getSpanFlags(ret[j]) & SPAN_PRIORITY;\r\n                    if (prio > p) {\r\n                        break;\r\n                    }\r\n                }\r\n                System.arraycopy(ret, j, ret, j + 1, count - j);\r\n                ret[j] = (T) spans[i];\r\n                count++;\r\n            } else {\r\n                ret[count++] = (T) spans[i];\r\n            }\r\n        }\r\n    }\r\n    if (count == 0) {\r\n        return ArrayUtils.emptyArray(kind);\r\n    }\r\n    if (count == 1) {\r\n        ret = (T[]) Array.newInstance(kind, 1);\r\n        ret[0] = ret1;\r\n        return ret;\r\n    }\r\n    if (count == ret.length) {\r\n        return ret;\r\n    }\r\n    T[] nret = (T[]) Array.newInstance(kind, count);\r\n    System.arraycopy(ret, 0, nret, 0, count);\r\n    return nret;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.create.CreateIndexRequest.aliases",
	"Comment": "sets the aliases that will be associated with the index when it gets created",
	"Method": "CreateIndexRequest aliases(Map<String, ?> source,CreateIndexRequest aliases,XContentBuilder source,CreateIndexRequest aliases,String source,CreateIndexRequest aliases,BytesReference source,Set<Alias> aliases){\r\n    return this.aliases;\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Setting.existsOrFallbackExists",
	"Comment": "returns true if and only if this setting including fallback settings is present in the given settings instance.",
	"Method": "boolean existsOrFallbackExists(Settings settings){\r\n    return settings.keySet().contains(getKey()) || (fallbackSetting != null && fallbackSetting.existsOrFallbackExists(settings));\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MappingMetaData.sourceAsMap",
	"Comment": "converts the serialized compressed form of the mappings into a parsed map.",
	"Method": "Map<String, Object> sourceAsMap(){\r\n    Map<String, Object> mapping = XContentHelper.convertToMap(source.compressedReference(), true).v2();\r\n    if (mapping.size() == 1 && mapping.containsKey(type())) {\r\n        mapping = (Map<String, Object>) mapping.get(type());\r\n    }\r\n    return mapping;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoShapeQueryBuilder.indexedShapePath",
	"Comment": "sets the path of the field in the indexed shape document that has the shape itself",
	"Method": "GeoShapeQueryBuilder indexedShapePath(String indexedShapePath,String indexedShapePath){\r\n    return indexedShapePath;\r\n}"
}, {
	"Path": "org.elasticsearch.node.Node.settings",
	"Comment": "the settings that are used by this node. contains original settings as well as additional settings provided by plugins.",
	"Method": "Settings settings(){\r\n    return this.settings;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.IterativeCKYPCFGParser.doInsideScores",
	"Comment": "fills in the iscore array of each category over each span of length 2 or more.",
	"Method": "void doInsideScores(){\r\n    float threshold = STEP_SIZE;\r\n    while (!doInsideScoresHelper(threshold)) {\r\n        threshold += STEP_SIZE;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.matcher.Matchers.returns",
	"Comment": "returns a matcher which matches methods with matching return types.",
	"Method": "Matcher<Method> returns(Matcher<? super Class<?>> returnType){\r\n    return new Returns(returnType);\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequest.types",
	"Comment": "the document types to execute the search against. defaults to be executed againstall types.",
	"Method": "String[] types(SearchRequest types,String types){\r\n    Objects.requireNonNull(types, \"types must not be null\");\r\n    for (String type : types) {\r\n        Objects.requireNonNull(type, \"type must not be null\");\r\n    }\r\n    this.types = types;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.boolQuery",
	"Comment": "a query that matches documents matching boolean combinations of other queries.",
	"Method": "BoolQueryBuilder boolQuery(){\r\n    return new BoolQueryBuilder();\r\n}"
}, {
	"Path": "org.elasticsearch.node.InternalSettingsPreparer.checkSettingsForTerminalDeprecation",
	"Comment": "checks all settings values to make sure they do not have the old prompt settings. these were deprecated in 6.0.0.this check should be removed in 8.0.0.",
	"Method": "void checkSettingsForTerminalDeprecation(Settings.Builder output){\r\n    assert Version.CURRENT.major != 8 : \"Logic pertaining to config driven prompting should be removed\";\r\n    for (String setting : output.keys()) {\r\n        switch(output.get(setting)) {\r\n            case SECRET_PROMPT_VALUE:\r\n                throw new SettingsException(\"Config driven secret prompting was deprecated in 6.0.0. Use the keystore\" + \" for secure settings.\");\r\n            case TEXT_PROMPT_VALUE:\r\n                throw new SettingsException(\"Config driven text prompting was deprecated in 6.0.0. Use the keystore\" + \" for secure settings.\");\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.SizeBlockingQueue.forcePut",
	"Comment": "forces adding an element to the queue, without doing size checks.",
	"Method": "void forcePut(E e){\r\n    size.incrementAndGet();\r\n    try {\r\n        queue.put(e);\r\n    } catch (InterruptedException ie) {\r\n        size.decrementAndGet();\r\n        throw ie;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.CommonTermsQueryBuilder.analyzer",
	"Comment": "explicitly set the analyzer to use. defaults to use explicit mappingconfig for the field, or, if not set, the default search analyzer.",
	"Method": "CommonTermsQueryBuilder analyzer(String analyzer,String analyzer){\r\n    return this.analyzer;\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.TransportActions.isReadOverrideException",
	"Comment": "if a failure is already present, should this failure override it or not for read operations.",
	"Method": "boolean isReadOverrideException(Exception e){\r\n    return !isShardNotAvailableException(e);\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.ThreadContext.markAsSystemContext",
	"Comment": "marks this thread context as an internal system context. this signals that actions in this context are issuedby the system itself rather than by a user action.",
	"Method": "void markAsSystemContext(){\r\n    threadLocal.set(threadLocal.get().setSystemContext());\r\n}"
}, {
	"Path": "org.elasticsearch.gateway.DanglingIndicesState.processDanglingIndices",
	"Comment": "process dangling indices based on the provided meta data, handling cleanup, findingnew dangling indices, and allocating outstanding ones.",
	"Method": "void processDanglingIndices(MetaData metaData){\r\n    if (nodeEnv.hasNodeFile() == false) {\r\n        return;\r\n    }\r\n    cleanupAllocatedDangledIndices(metaData);\r\n    findNewAndAddDanglingIndices(metaData);\r\n    allocateDanglingIndices();\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationAction.resolveRequest",
	"Comment": "resolves derived values in the request. for example, the target shard id of the incoming request, if not set at request construction.additional processing or validation of the request should be done here.",
	"Method": "void resolveRequest(IndexMetaData indexMetaData,Request request){\r\n    if (request.waitForActiveShards() == ActiveShardCount.DEFAULT) {\r\n        request.waitForActiveShards(indexMetaData.getWaitForActiveShards());\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.snapshots.status.SnapshotsStatusRequestBuilder.addSnapshots",
	"Comment": "adds additional snapshots to the list of snapshots to return",
	"Method": "SnapshotsStatusRequestBuilder addSnapshots(String snapshots){\r\n    request.snapshots(ArrayUtils.concat(request.snapshots(), snapshots));\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.collect.MapBuilder.immutableMap",
	"Comment": "build an immutable copy of the map under construction. always copies the map under construction. prefer buildinga hashmap by hand and wrapping it in an unmodifiablemap",
	"Method": "Map<K, V> immutableMap(){\r\n    return unmodifiableMap(new HashMap(map));\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksService.waitForPersistentTaskCondition",
	"Comment": "waits for a given persistent task to comply with a given predicate, then call back the listener accordingly.",
	"Method": "void waitForPersistentTaskCondition(String taskId,Predicate<PersistentTask<?>> predicate,TimeValue timeout,WaitForPersistentTaskListener<?> listener){\r\n    final Predicate<ClusterState> clusterStatePredicate = clusterState -> predicate.test(PersistentTasksCustomMetaData.getTaskWithId(clusterState, taskId));\r\n    final ClusterStateObserver observer = new ClusterStateObserver(clusterService, timeout, logger, threadPool.getThreadContext());\r\n    final ClusterState clusterState = observer.setAndGetObservedState();\r\n    if (clusterStatePredicate.test(clusterState)) {\r\n        listener.onResponse(PersistentTasksCustomMetaData.getTaskWithId(clusterState, taskId));\r\n    } else {\r\n        observer.waitForNextChange(new ClusterStateObserver.Listener() {\r\n            @Override\r\n            public void onNewClusterState(ClusterState state) {\r\n                listener.onResponse(PersistentTasksCustomMetaData.getTaskWithId(state, taskId));\r\n            }\r\n            @Override\r\n            public void onClusterServiceClose() {\r\n                listener.onFailure(new NodeClosedException(clusterService.localNode()));\r\n            }\r\n            @Override\r\n            public void onTimeout(TimeValue timeout) {\r\n                listener.onTimeout(timeout);\r\n            }\r\n        }, clusterStatePredicate);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksService.waitForPersistentTaskCondition",
	"Comment": "waits for a given persistent task to comply with a given predicate, then call back the listener accordingly.",
	"Method": "void waitForPersistentTaskCondition(String taskId,Predicate<PersistentTask<?>> predicate,TimeValue timeout,WaitForPersistentTaskListener<?> listener){\r\n    listener.onResponse(PersistentTasksCustomMetaData.getTaskWithId(state, taskId));\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksService.waitForPersistentTaskCondition",
	"Comment": "waits for a given persistent task to comply with a given predicate, then call back the listener accordingly.",
	"Method": "void waitForPersistentTaskCondition(String taskId,Predicate<PersistentTask<?>> predicate,TimeValue timeout,WaitForPersistentTaskListener<?> listener){\r\n    listener.onFailure(new NodeClosedException(clusterService.localNode()));\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksService.waitForPersistentTaskCondition",
	"Comment": "waits for a given persistent task to comply with a given predicate, then call back the listener accordingly.",
	"Method": "void waitForPersistentTaskCondition(String taskId,Predicate<PersistentTask<?>> predicate,TimeValue timeout,WaitForPersistentTaskListener<?> listener){\r\n    listener.onTimeout(timeout);\r\n}"
}, {
	"Path": "android.text.SpannableStringBuilder.getSpanFlags",
	"Comment": "return the flags of the end of the specifiedmarkup object, or 0 if it is not attached to this buffer.",
	"Method": "int getSpanFlags(Object what){\r\n    int count = mSpanCount;\r\n    Object[] spans = mSpans;\r\n    for (int i = count - 1; i >= 0; i--) {\r\n        if (spans[i] == what) {\r\n            return mSpanFlags[i];\r\n        }\r\n    }\r\n    return 0;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.RoutingNodes.activeReplicaWithHighestVersion",
	"Comment": "returns one active replica shard for the given shard id or null ifno active replica is found.since replicas could possibly be on nodes with a older version of es thanthe primary is, this will return replicas on the highest version of es.",
	"Method": "ShardRouting activeReplicaWithHighestVersion(ShardId shardId){\r\n    return assignedShards(shardId).stream().filter(shr -> !shr.primary() && shr.active()).filter(shr -> node(shr.currentNodeId()) != null).max(Comparator.comparing(shr -> node(shr.currentNodeId()).node(), Comparator.nullsFirst(Comparator.comparing(DiscoveryNode::getVersion)))).orElse(null);\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.inference.TableFactor.observe",
	"Comment": "remove a variable by observing it at a certain value, return a new factor without that variable.",
	"Method": "TableFactor observe(int variable,int value){\r\n    return marginalize(variable, 0, (marginalizedVariableValue, assignment) -> {\r\n        if (marginalizedVariableValue == value) {\r\n            return (old, n) -> {\r\n                return n;\r\n            };\r\n        } else {\r\n            return (old, n) -> old;\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.internal.Scoping.isNoScope",
	"Comment": "returns true if this is the default scope. in this case a new instance will be provided foreach injection.",
	"Method": "boolean isNoScope(){\r\n    return getScopeInstance() == Scopes.NO_SCOPE;\r\n}"
}, {
	"Path": "org.openqa.selenium.remote.server.KnownElements.getNextId",
	"Comment": "webdriver is single threaded. expect only a single thread at a time to access this",
	"Method": "String getNextId(){\r\n    return String.valueOf(nextId++);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.IndexMetaData.addHumanReadableSettings",
	"Comment": "adds human readable version and creation date settings.this method is used to display the settings in a human readable format in rest api",
	"Method": "Settings addHumanReadableSettings(Settings settings){\r\n    Settings.Builder builder = Settings.builder().put(settings);\r\n    Version version = SETTING_INDEX_VERSION_CREATED.get(settings);\r\n    if (version != Version.V_EMPTY) {\r\n        builder.put(SETTING_VERSION_CREATED_STRING, version.toString());\r\n    }\r\n    Version versionUpgraded = settings.getAsVersion(SETTING_VERSION_UPGRADED, null);\r\n    if (versionUpgraded != null) {\r\n        builder.put(SETTING_VERSION_UPGRADED_STRING, versionUpgraded.toString());\r\n    }\r\n    Long creationDate = settings.getAsLong(SETTING_CREATION_DATE, null);\r\n    if (creationDate != null) {\r\n        ZonedDateTime creationDateTime = ZonedDateTime.ofInstant(Instant.ofEpochMilli(creationDate), ZoneOffset.UTC);\r\n        builder.put(SETTING_CREATION_DATE_STRING, creationDateTime.toString());\r\n    }\r\n    return builder.build();\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.getWritingBytes",
	"Comment": "returns how many bytes we are currently moving from indexing buffer to segments on disk",
	"Method": "long getWritingBytes(){\r\n    return indexWriter.getFlushingBytes() + versionMap.getRefreshingBytes();\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.DocumentReader.getKeepOriginalText",
	"Comment": "returns whether created documents will store their source text along with tokenized words.",
	"Method": "boolean getKeepOriginalText(){\r\n    return (keepOriginalText);\r\n}"
}, {
	"Path": "org.elasticsearch.common.PidFile.create",
	"Comment": "creates a new pidfile and writes the current process id into the provided path",
	"Method": "PidFile create(Path path,boolean deleteOnExit,PidFile create,Path path,boolean deleteOnExit,long pid){\r\n    Path parent = path.getParent();\r\n    if (parent != null) {\r\n        if (Files.exists(parent) && Files.isDirectory(parent) == false) {\r\n            throw new IllegalArgumentException(parent + \" exists but is not a directory\");\r\n        }\r\n        if (Files.exists(parent) == false) {\r\n            Files.createDirectories(parent);\r\n        }\r\n    }\r\n    if (Files.exists(path) && Files.isRegularFile(path) == false) {\r\n        throw new IllegalArgumentException(path + \" exists but is not a regular file\");\r\n    }\r\n    try (OutputStream stream = Files.newOutputStream(path, StandardOpenOption.CREATE, StandardOpenOption.TRUNCATE_EXISTING)) {\r\n        stream.write(Long.toString(pid).getBytes(StandardCharsets.UTF_8));\r\n    }\r\n    if (deleteOnExit) {\r\n        addShutdownHook(path);\r\n    }\r\n    return new PidFile(path, deleteOnExit, pid);\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.Settings.keySet",
	"Comment": "returns the fully qualified setting names contained in this settings object.",
	"Method": "Set<String> keySet(){\r\n    synchronized (keys) {\r\n        if (keys.get() == null) {\r\n            if (secureSettings == null) {\r\n                keys.set(settings.keySet());\r\n            } else {\r\n                Stream<String> stream = Stream.concat(settings.keySet().stream(), secureSettings.getSettingNames().stream());\r\n                keys.set(Collections.unmodifiableSet(stream.collect(Collectors.toSet())));\r\n            }\r\n        }\r\n    }\r\n    return keys.get();\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.Util.isTree",
	"Comment": "a little utility function to make sure a semanticgraph is a tree.",
	"Method": "boolean isTree(SemanticGraph tree){\r\n    for (IndexedWord vertex : tree.vertexSet()) {\r\n        if (tree.getRoots().contains(vertex)) {\r\n            if (tree.incomingEdgeIterator(vertex).hasNext()) {\r\n                return false;\r\n            }\r\n        } else {\r\n            Iterator<SemanticGraphEdge> iter = tree.incomingEdgeIterator(vertex);\r\n            if (!iter.hasNext()) {\r\n                return false;\r\n            }\r\n            iter.next();\r\n            if (iter.hasNext()) {\r\n                return false;\r\n            }\r\n        }\r\n        for (SemanticGraphEdge edge : tree.outgoingEdgeIterable(vertex)) {\r\n            boolean foundReverse = false;\r\n            for (SemanticGraphEdge reverse : tree.incomingEdgeIterable(edge.getDependent())) {\r\n                if (reverse == edge) {\r\n                    foundReverse = true;\r\n                }\r\n            }\r\n            if (!foundReverse) {\r\n                return false;\r\n            }\r\n        }\r\n        for (SemanticGraphEdge edge : tree.incomingEdgeIterable(vertex)) {\r\n            boolean foundReverse = false;\r\n            for (SemanticGraphEdge reverse : tree.outgoingEdgeIterable(edge.getGovernor())) {\r\n                if (reverse == edge) {\r\n                    foundReverse = true;\r\n                }\r\n            }\r\n            if (!foundReverse) {\r\n                return false;\r\n            }\r\n        }\r\n    }\r\n    if (isCyclic(tree)) {\r\n        return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.NodeJoinController.stopElectionContext",
	"Comment": "stopped accumulating joins. all pending joins will be processed. future joins will be processed immediately",
	"Method": "void stopElectionContext(String reason){\r\n    logger.trace(\"stopping election ([{}])\", reason);\r\n    synchronized (this) {\r\n        assert electionContext != null : \"stopElectionContext() called but not accumulating\";\r\n        electionContext.closeAndProcessPending(reason);\r\n        electionContext = null;\r\n    }\r\n}"
}, {
	"Path": "org.apache.dubbo.registry.dubbo.RegistryDirectoryTest.testNofityOverrideUrls_CleanNOverride",
	"Comment": "test the simultaneous push to clear the override and the override for a certain providersee if override can take effect",
	"Method": "void testNofityOverrideUrls_CleanNOverride(){\r\n    RegistryDirectory registryDirectory = getRegistryDirectory();\r\n    invocation = new RpcInvocation();\r\n    List<URL> durls = new ArrayList<URL>();\r\n    durls.add(SERVICEURL.setHost(\"10.20.30.140\").addParameter(\"timeout\", \"1\"));\r\n    registryDirectory.notify(durls);\r\n    durls = new ArrayList<URL>();\r\n    durls.add(URL.valueOf(\"override://0.0.0.0?timeout=3\"));\r\n    durls.add(URL.valueOf(\"override://0.0.0.0\"));\r\n    durls.add(URL.valueOf(\"override://10.20.30.140:9091?timeout=4\"));\r\n    registryDirectory.notify(durls);\r\n    List<Invoker<?>> invokers = registryDirectory.list(invocation);\r\n    Invoker<?> aInvoker = invokers.get(0);\r\n    Assert.assertEquals(\"4\", aInvoker.getUrl().getParameter(\"timeout\"));\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.MultiSearchRequest.maxConcurrentSearchRequests",
	"Comment": "sets how many search requests specified in this multi search requests are allowed to be ran concurrently.",
	"Method": "int maxConcurrentSearchRequests(MultiSearchRequest maxConcurrentSearchRequests,int maxConcurrentSearchRequests){\r\n    if (maxConcurrentSearchRequests < 1) {\r\n        throw new IllegalArgumentException(\"maxConcurrentSearchRequests must be positive\");\r\n    }\r\n    this.maxConcurrentSearchRequests = maxConcurrentSearchRequests;\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.rightScanFindsMoneyWord",
	"Comment": "look along cd words and see if next thing is a money wordlike cents or pounds.",
	"Method": "boolean rightScanFindsMoneyWord(List<CoreLabel> pl,int i){\r\n    int j = i;\r\n    if (DEBUG) {\r\n        log.info(\"rightScan from: \" + pl.get(j).word());\r\n    }\r\n    int sz = pl.size();\r\n    while (j < sz && pl.get(j).getString(CoreAnnotations.PartOfSpeechAnnotation.class).equals(\"CD\")) {\r\n        j++;\r\n    }\r\n    if (j >= sz) {\r\n        return false;\r\n    }\r\n    String tag = pl.get(j).getString(CoreAnnotations.PartOfSpeechAnnotation.class);\r\n    String word = pl.get(j).word();\r\n    if (DEBUG) {\r\n        log.info(\"rightScan testing: \" + word + '/' + tag + \"; answer is: \" + Boolean.toString((tag.equals(\"NN\") || tag.equals(\"NNS\")) && CURRENCY_WORD_PATTERN.matcher(word).matches()));\r\n    }\r\n    return (tag.equals(\"NN\") || tag.equals(\"NNS\")) && CURRENCY_WORD_PATTERN.matcher(word).matches();\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.BulkByScrollTask.isWorker",
	"Comment": "returns true if this task is a worker task that performs search requests. false otherwise",
	"Method": "boolean isWorker(){\r\n    return workerState != null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.SloppyMath.factorial",
	"Comment": "uses floating point so that it can represent the really big numbers that come up.",
	"Method": "double factorial(int x){\r\n    double result = 1.0;\r\n    for (int i = x; i > 1; i--) {\r\n        result *= i;\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.InternalEngine.getNumDocAppends",
	"Comment": "returns the number of documents have been appended since this engine was opened.this count does not include the appends from the existing segments before opening engine.",
	"Method": "long getNumDocAppends(){\r\n    return numDocAppends.count();\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.BoolQueryBuilder.must",
	"Comment": "gets the queries that must appear in the matching documents.",
	"Method": "BoolQueryBuilder must(QueryBuilder queryBuilder,List<QueryBuilder> must){\r\n    return this.mustClauses;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.rollover.RolloverResponse.getOldIndex",
	"Comment": "returns the name of the index that the request alias was pointing to",
	"Method": "String getOldIndex(){\r\n    return oldIndex;\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.GeoUtils.isValidLatitude",
	"Comment": "returns true if latitude is actually a valid latitude value.",
	"Method": "boolean isValidLatitude(double latitude){\r\n    if (Double.isNaN(latitude) || Double.isInfinite(latitude) || latitude < GeoUtils.MIN_LAT || latitude > GeoUtils.MAX_LAT) {\r\n        return false;\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.AbstractTreebankParserParams.ppAttachmentEval",
	"Comment": "returns a language specific object for evaluating pp attachment",
	"Method": "AbstractEval ppAttachmentEval(){\r\n    return null;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.SequenceMatcher.find",
	"Comment": "reset the matcher and then searches for pattern at the specified start index",
	"Method": "boolean find(int start,boolean find,int start,boolean matchStart,boolean find){\r\n    switch(findType) {\r\n        case FIND_NONOVERLAPPING:\r\n            return findNextNonOverlapping();\r\n        case FIND_ALL:\r\n            return findNextAll();\r\n        default:\r\n            throw new UnsupportedOperationException(\"Unsupported findType \" + findType);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.WordTagFactory.newLabel",
	"Comment": "create a new wordtag label, where the label isformed fromthe label object passed in.depending on what fieldseach label has, other things will be null.",
	"Method": "Label newLabel(String labelStr,Label newLabel,String labelStr,int options,Label newLabel,Label oldLabel){\r\n    if (oldLabel instanceof HasTag) {\r\n        return new WordTag(oldLabel.value(), ((HasTag) oldLabel).tag());\r\n    } else {\r\n        return new WordTag(oldLabel.value());\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.ClauseSplitterSearchProblem.splitToChildOfEdge",
	"Comment": "the basic method for splitting off a clause of a tree.this modifies the tree in place.",
	"Method": "void splitToChildOfEdge(SemanticGraph tree,SemanticGraphEdge toKeep){\r\n    Queue<IndexedWord> fringe = new LinkedList();\r\n    List<IndexedWord> nodesToRemove = new ArrayList();\r\n    for (IndexedWord root : tree.getRoots()) {\r\n        nodesToRemove.add(root);\r\n        for (SemanticGraphEdge out : tree.outgoingEdgeIterable(root)) {\r\n            if (!out.equals(toKeep)) {\r\n                fringe.add(out.getDependent());\r\n            }\r\n        }\r\n    }\r\n    while (!fringe.isEmpty()) {\r\n        IndexedWord node = fringe.poll();\r\n        nodesToRemove.add(node);\r\n        for (SemanticGraphEdge out : tree.outgoingEdgeIterable(node)) {\r\n            if (!out.equals(toKeep)) {\r\n                fringe.add(out.getDependent());\r\n            }\r\n        }\r\n    }\r\n    nodesToRemove.forEach(tree::removeVertex);\r\n    tree.setRoot(toKeep.getDependent());\r\n}"
}, {
	"Path": "android.util.SparseBooleanArray.put",
	"Comment": "adds a mapping from the specified key to the specified value,replacing the previous mapping from the specified key if therewas one.",
	"Method": "void put(int key,boolean value){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i >= 0) {\r\n        mValues[i] = value;\r\n    } else {\r\n        i = ~i;\r\n        if (mSize >= mKeys.length) {\r\n            int n = ArrayUtils.idealIntArraySize(mSize + 1);\r\n            int[] nkeys = new int[n];\r\n            boolean[] nvalues = new boolean[n];\r\n            System.arraycopy(mKeys, 0, nkeys, 0, mKeys.length);\r\n            System.arraycopy(mValues, 0, nvalues, 0, mValues.length);\r\n            mKeys = nkeys;\r\n            mValues = nvalues;\r\n        }\r\n        if (mSize - i != 0) {\r\n            System.arraycopy(mKeys, i, mKeys, i + 1, mSize - i);\r\n            System.arraycopy(mValues, i, mValues, i + 1, mSize - i);\r\n        }\r\n        mKeys[i] = key;\r\n        mValues[i] = value;\r\n        mSize++;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.EngineConfig.getIndexSort",
	"Comment": "return the sort order of this index, or null if the index has no sort.",
	"Method": "Sort getIndexSort(){\r\n    return indexSort;\r\n}"
}, {
	"Path": "edu.stanford.nlp.optimization.QNMinimizer.lineSearchBacktrack",
	"Comment": "linesearchbacktrack is the original line search used for the first versionof qnminimizer. it only satisfies sufficient descent not the wolfe conditions.",
	"Method": "double[] lineSearchBacktrack(Function func,double[] dir,double[] x,double[] newX,double[] grad,double lastValue,StringBuilder sb){\r\n    double normGradInDir = ArrayMath.innerProduct(dir, grad);\r\n    sb.append('(').append(nf.format(normGradInDir)).append(')');\r\n    if (normGradInDir > 0) {\r\n        sayln(\"{WARNING--- direction of positive gradient chosen!}\");\r\n    }\r\n    double step, c1;\r\n    if (its <= 2) {\r\n        step = 0.1;\r\n        c1 = 0.1;\r\n    } else {\r\n        step = 1.0;\r\n        c1 = 0.1;\r\n    }\r\n    double c = 0.01;\r\n    c = c * normGradInDir;\r\n    double[] newPoint = new double[3];\r\n    while ((newPoint[f] = func.valueAt((plusAndConstMult(x, dir, step, newX)))) > lastValue + c * step) {\r\n        fevals += 1;\r\n        if (newPoint[f] < lastValue) {\r\n            sb.append('!');\r\n        } else {\r\n            sb.append('.');\r\n        }\r\n        step = c1 * step;\r\n    }\r\n    newPoint[a] = step;\r\n    fevals += 1;\r\n    if (fevals > maxFevals) {\r\n        throw new MaxEvaluationsExceeded(\"Exceeded during lineSearch() Function.\");\r\n    }\r\n    return newPoint;\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.demo.OpenIEServlet.init",
	"Comment": "set the properties to the paths they appear at on the servlet.see build.xml for where these paths get copied.",
	"Method": "void init(){\r\n    Properties commonProps = new Properties() {\r\n        {\r\n            setProperty(\"depparse.extradependencies\", \"ref_only_uncollapsed\");\r\n            setProperty(\"parse.extradependencies\", \"ref_only_uncollapsed\");\r\n            setProperty(\"openie.splitter.threshold\", \"0.10\");\r\n            setProperty(\"openie.optimze_for\", \"GENERAL\");\r\n            setProperty(\"openie.ignoreaffinity\", \"false\");\r\n            setProperty(\"openie.max_entailments_per_clause\", \"1000\");\r\n            setProperty(\"openie.triple.strict\", \"true\");\r\n        }\r\n    };\r\n    try {\r\n        String dataDir = getServletContext().getRealPath(\"/WEB-INF/data\");\r\n        System.setProperty(\"de.jollyday.config\", getServletContext().getRealPath(\"/WEB-INF/classes/holidays/jollyday.properties\"));\r\n        commonProps.setProperty(\"pos.model\", dataDir + \"/english-left3words-distsim.tagger\");\r\n        commonProps.setProperty(\"ner.model\", dataDir + \"/english.all.3class.distsim.crf.ser.gz,\" + dataDir + \"/english.conll.4class.distsim.crf.ser.gz,\" + dataDir + \"/english.muc.7class.distsim.crf.ser.gz\");\r\n        commonProps.setProperty(\"depparse.model\", dataDir + \"/english_SD.gz\");\r\n        commonProps.setProperty(\"parse.model\", dataDir + \"/englishPCFG.ser.gz\");\r\n        commonProps.setProperty(\"sutime.rules\", dataDir + \"/defs.sutime.txt,\" + dataDir + \"/english.sutime.txt,\" + dataDir + \"/english.hollidays.sutime.txt\");\r\n        commonProps.setProperty(\"openie.splitter.model\", dataDir + \"/clauseSplitterModel.ser.gz\");\r\n        commonProps.setProperty(\"openie.affinity_models\", dataDir);\r\n    } catch (NullPointerException e) {\r\n        log.info(\"Could not load servlet context. Are you on the command line?\");\r\n    }\r\n    if (this.pipeline == null) {\r\n        Properties fullProps = new Properties(commonProps);\r\n        fullProps.setProperty(\"annotators\", \"tokenize,ssplit,pos,lemma,depparse,ner,natlog,openie\");\r\n        this.pipeline = new StanfordCoreNLP(fullProps);\r\n    }\r\n    if (this.backoff == null) {\r\n        Properties backoffProps = new Properties(commonProps);\r\n        backoffProps.setProperty(\"annotators\", \"parse,natlog,openie\");\r\n        backoffProps.setProperty(\"enforceRequirements\", \"false\");\r\n        this.backoff = new StanfordCoreNLP(backoffProps);\r\n    }\r\n}"
}, {
	"Path": "org.apache.harmony.tests.java.io.ObjectStreamFieldTest.setUp",
	"Comment": "sets up the fixture, for example, open a network connection. this methodis called before a test is executed.",
	"Method": "void setUp(){\r\n    osc = ObjectStreamClass.lookup(DummyClass.class);\r\n    bamField = osc.getField(\"bam\");\r\n    samField = osc.getField(\"sam\");\r\n    hamField = osc.getField(\"ham\");\r\n    holaField = osc.getField(\"hola\");\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryRewriteContext.registerAsyncAction",
	"Comment": "registers an async action that must be executed before the next rewrite round in order to make progress.this should be used if a rewriteabel needs to fetch some external resources in order to be executed ie. a documentfrom an index.",
	"Method": "void registerAsyncAction(BiConsumer<Client, ActionListener<?>> asyncAction){\r\n    asyncActions.add(asyncAction);\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.updateGlobalCheckpointForShard",
	"Comment": "update the local knowledge of the global checkpoint for the specified allocation id.",
	"Method": "void updateGlobalCheckpointForShard(String allocationId,long globalCheckpoint){\r\n    assert primaryMode;\r\n    assert handoffInProgress == false;\r\n    assert invariant();\r\n    updateGlobalCheckpoint(allocationId, globalCheckpoint, current -> logger.trace(\"updated local knowledge for [{}] on the primary of the global checkpoint from [{}] to [{}]\", allocationId, current, globalCheckpoint));\r\n    assert invariant();\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.ParseFieldRegistry.getNames",
	"Comment": "all the names under which values are registered. expect this to be used mostly for testing.",
	"Method": "Set<String> getNames(){\r\n    return registry.keySet();\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportWriteAction.syncOperationResultOrThrow",
	"Comment": "syncs operation result to the translog or throws a shard not available failure",
	"Method": "Location syncOperationResultOrThrow(Engine.Result operationResult,Location currentLocation){\r\n    final Location location;\r\n    if (operationResult.getFailure() != null) {\r\n        Exception failure = operationResult.getFailure();\r\n        assert failure instanceof MapperParsingException : \"expected mapper parsing failures. got \" + failure;\r\n        throw failure;\r\n    } else {\r\n        location = locationToSync(currentLocation, operationResult.getTranslogLocation());\r\n    }\r\n    return location;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.rollover.RolloverRequest.dryRun",
	"Comment": "sets if the rollover should not be executed when conditions are met",
	"Method": "void dryRun(boolean dryRun){\r\n    this.dryRun = dryRun;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.Translog.newSnapshot",
	"Comment": "snapshots the current transaction log allowing to safely iterate over the snapshot.snapshots are fixed in time and will not be updated with future operations.",
	"Method": "Snapshot newSnapshot(){\r\n    try (ReleasableLock ignored = readLock.acquire()) {\r\n        return newSnapshotFromGen(new TranslogGeneration(translogUUID, getMinFileGeneration()), Long.MAX_VALUE);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEvent.metaDataChanged",
	"Comment": "returns true iff the metadata for the cluster has changed betweenthe previous cluster state and the new cluster state. note that this is an objectreference equality test, not an equals test.",
	"Method": "boolean metaDataChanged(){\r\n    return state.metaData() != previousState.metaData();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.create.CreateIndexRequestBuilder.addAlias",
	"Comment": "adds an alias that will be associated with the index when it gets created",
	"Method": "CreateIndexRequestBuilder addAlias(Alias alias){\r\n    request.alias(alias);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.shard.IndexShard.initiateTracking",
	"Comment": "called when the recovery process for a shard has opened the engine on the target shard. ensures that the right data structureshave been set up locally to track local checkpoint information for the shard and that the shard is added to the replication group.",
	"Method": "void initiateTracking(String allocationId){\r\n    assert assertPrimaryMode();\r\n    replicationTracker.initiateTracking(allocationId);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.UnassignedInfo.getNumFailedAllocations",
	"Comment": "returns the number of previously failed allocations of this shard.",
	"Method": "int getNumFailedAllocations(){\r\n    return failedAllocations;\r\n}"
}, {
	"Path": "org.elasticsearch.indices.recovery.RecoveriesCollection.startRecovery",
	"Comment": "starts are new recovery for the given shard, source node and state",
	"Method": "long startRecovery(IndexShard indexShard,DiscoveryNode sourceNode,PeerRecoveryTargetService.RecoveryListener listener,TimeValue activityTimeout){\r\n    RecoveryTarget recoveryTarget = new RecoveryTarget(indexShard, sourceNode, listener, ensureClusterStateVersionCallback);\r\n    startRecoveryInternal(recoveryTarget, activityTimeout);\r\n    return recoveryTarget.recoveryId();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.RoutingNodes.startShard",
	"Comment": "applies the relevant logic to start an initializing shard.moves the initializing shard to started. if the shard is a relocation target, also removes the relocation source.if the started shard is a primary relocation target, this also reinitializes currently initializing replicas as theirrecovery source changes",
	"Method": "ShardRouting startShard(Logger logger,ShardRouting initializingShard,RoutingChangesObserver routingChangesObserver){\r\n    ensureMutable();\r\n    ShardRouting startedShard = started(initializingShard);\r\n    logger.trace(\"{} marked shard as started (routing: {})\", initializingShard.shardId(), initializingShard);\r\n    routingChangesObserver.shardStarted(initializingShard, startedShard);\r\n    if (initializingShard.relocatingNodeId() != null) {\r\n        RoutingNode relocationSourceNode = node(initializingShard.relocatingNodeId());\r\n        ShardRouting relocationSourceShard = relocationSourceNode.getByShardId(initializingShard.shardId());\r\n        assert relocationSourceShard.isRelocationSourceOf(initializingShard);\r\n        assert relocationSourceShard.getTargetRelocatingShard() == initializingShard : \"relocation target mismatch, expected: \" + initializingShard + \" but was: \" + relocationSourceShard.getTargetRelocatingShard();\r\n        remove(relocationSourceShard);\r\n        routingChangesObserver.relocationCompleted(relocationSourceShard);\r\n        if (startedShard.primary()) {\r\n            List<ShardRouting> assignedShards = assignedShards(startedShard.shardId());\r\n            for (ShardRouting routing : new ArrayList(assignedShards)) {\r\n                if (routing.initializing() && routing.primary() == false) {\r\n                    if (routing.isRelocationTarget()) {\r\n                        ShardRouting sourceShard = getByAllocationId(routing.shardId(), routing.allocationId().getRelocationId());\r\n                        ShardRouting startedReplica = cancelRelocation(sourceShard);\r\n                        remove(routing);\r\n                        routingChangesObserver.shardFailed(routing, new UnassignedInfo(UnassignedInfo.Reason.REINITIALIZED, \"primary changed\"));\r\n                        relocateShard(startedReplica, sourceShard.relocatingNodeId(), sourceShard.getExpectedShardSize(), routingChangesObserver);\r\n                    } else {\r\n                        ShardRouting reinitializedReplica = reinitReplica(routing);\r\n                        routingChangesObserver.initializedReplicaReinitialized(routing, reinitializedReplica);\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    return startedShard;\r\n}"
}, {
	"Path": "org.elasticsearch.index.fielddata.ScriptDocValues.getValues",
	"Comment": "return a copy of the list of the values for the current document.",
	"Method": "List<T> getValues(){\r\n    deprecated(\"ScriptDocValues#getValues\", \"Deprecated getValues used, the field is a list and should be accessed directly.\" + \" For example, use doc['foo'] instead of doc['foo'].values.\");\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.index.IndexSettings.customDataPath",
	"Comment": "returns the customdatapath for this index, if configured. null o.w.",
	"Method": "String customDataPath(){\r\n    return settings.get(IndexMetaData.SETTING_DATA_PATH);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.AllocationId.finishRelocation",
	"Comment": "creates a new allocation id finalizing a relocation.note that this is expected to be called on the allocation idshard and thus it only needs to clear the relocating id.",
	"Method": "AllocationId finishRelocation(AllocationId allocationId){\r\n    assert allocationId.getRelocationId() != null;\r\n    return new AllocationId(allocationId.getId(), null);\r\n}"
}, {
	"Path": "android.util.ArrayMap.containsValue",
	"Comment": "check whether a value exists in the array.this requires a linear searchthrough the entire array.",
	"Method": "boolean containsValue(Object value){\r\n    return indexOfValue(value) >= 0;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.ThreadContext.preserveContext",
	"Comment": "saves the current thread context and wraps command in a runnable that restores that context before running command. ifcommand has already been passed through this method then it is returned unaltered rather than wrapped twice.",
	"Method": "Runnable preserveContext(Runnable command){\r\n    if (command instanceof ContextPreservingAbstractRunnable) {\r\n        return command;\r\n    }\r\n    if (command instanceof ContextPreservingRunnable) {\r\n        return command;\r\n    }\r\n    if (command instanceof AbstractRunnable) {\r\n        return new ContextPreservingAbstractRunnable((AbstractRunnable) command);\r\n    }\r\n    return new ContextPreservingRunnable(command);\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkPrimaryExecutionContext.markAsCompleted",
	"Comment": "finishes the execution of the current request, with the response that should be returned to the user",
	"Method": "void markAsCompleted(BulkItemResponse translatedResponse){\r\n    assertInvariants(ItemProcessingState.EXECUTED);\r\n    assert executionResult != null && translatedResponse.getItemId() == executionResult.getItemId();\r\n    assert translatedResponse.getItemId() == getCurrentItem().id();\r\n    if (translatedResponse.isFailed() == false && requestToExecute != null && requestToExecute != getCurrent()) {\r\n        request.items()[currentIndex] = new BulkItemRequest(request.items()[currentIndex].id(), requestToExecute);\r\n    }\r\n    getCurrentItem().setPrimaryResponse(translatedResponse);\r\n    currentItemState = ItemProcessingState.COMPLETED;\r\n    advance();\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.BytesRefHash.rehash",
	"Comment": "feel free to remove rehashing if bytesref gets a better hash function",
	"Method": "int rehash(int hash){\r\n    return BitMixer.mix32(hash);\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.ResettableReaderIteratorFactory.iterator",
	"Comment": "returns an iterator over the input sources in the underlying collection.",
	"Method": "Iterator<Reader> iterator(){\r\n    Collection<Object> newCollection = new ArrayList();\r\n    for (Object o : c) {\r\n        if (o instanceof Reader) {\r\n            String name = o.toString() + \".tmp\";\r\n            File tmpFile;\r\n            try {\r\n                tmpFile = File.createTempFile(name, \"\");\r\n            } catch (Exception e) {\r\n                throw new RuntimeIOException(e);\r\n            }\r\n            tmpFile.deleteOnExit();\r\n            StringUtils.printToFile(tmpFile, IOUtils.slurpReader((Reader) o), false, false, enc);\r\n            newCollection.add(tmpFile);\r\n        } else {\r\n            newCollection.add(o);\r\n        }\r\n    }\r\n    c = newCollection;\r\n    return new ReaderIterator();\r\n}"
}, {
	"Path": "edu.stanford.nlp.math.ArrayMath.pairwiseDivideInPlace",
	"Comment": "divide the first array by the second elementwise,and store results in place. assume arrays havethe same length",
	"Method": "void pairwiseDivideInPlace(double[] a,double[] b){\r\n    if (a.length != b.length) {\r\n        throw new RuntimeException();\r\n    }\r\n    for (int i = 0; i < a.length; i++) {\r\n        a[i] = a[i] / b[i];\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.ChineseTreebankParserParams.memoryTreebank",
	"Comment": "uses a memorytreebank with a chtbtokenizer and abobchristreenormalizer",
	"Method": "MemoryTreebank memoryTreebank(){\r\n    String encoding = inputEncoding;\r\n    if (!java.nio.charset.Charset.isSupported(encoding)) {\r\n        System.out.println(\"Warning: desired encoding \" + encoding + \" not accepted. \");\r\n        System.out.println(\"Using UTF-8 to construct MemoryTreebank\");\r\n        encoding = \"UTF-8\";\r\n    }\r\n    return new MemoryTreebank(treeReaderFactory(), encoding);\r\n}"
}, {
	"Path": "org.openqa.selenium.remote.server.log.SessionLogsToFileRepository.createLogFileAndAddToMap",
	"Comment": "this creates log file object which represents logs in file form. this opens objectoutputstreamwhich is used to write logrecords to log file and opens a objectinputstream which is used toread logrecords from the file.",
	"Method": "void createLogFileAndAddToMap(SessionId sessionId){\r\n    File rcLogFile;\r\n    rcLogFile = File.createTempFile(sessionId.toString(), \".rclog\");\r\n    rcLogFile.deleteOnExit();\r\n    LogFile logFile = new LogFile(rcLogFile.getAbsolutePath());\r\n    sessionToLogFileMap.put(sessionId, logFile);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.MultiPatternMatcher.findNonOverlapping",
	"Comment": "given a sequence, applies our patterns over the sequence and returnsall non overlapping matches.when multiple patterns overlaps,matched patterns are selected by order specified by the comparator",
	"Method": "List<SequenceMatchResult<T>> findNonOverlapping(List<? extends T> elements,List<SequenceMatchResult<T>> findNonOverlapping,List<? extends T> elements,Comparator<? super SequenceMatchResult> cmp){\r\n    Collection<SequencePattern<T>> triggered = getTriggeredPatterns(elements);\r\n    List<SequenceMatchResult<T>> all = new ArrayList();\r\n    int i = 0;\r\n    for (SequencePattern<T> p : triggered) {\r\n        if (Thread.interrupted()) {\r\n            throw new RuntimeInterruptedException();\r\n        }\r\n        SequenceMatcher<T> m = p.getMatcher(elements);\r\n        m.setMatchWithResult(matchWithResult);\r\n        m.setOrder(i);\r\n        while (m.find()) {\r\n            all.add(m.toBasicSequenceMatchResult());\r\n        }\r\n        i++;\r\n    }\r\n    List<SequenceMatchResult<T>> res = IntervalTree.getNonOverlapping(all, SequenceMatchResult.TO_INTERVAL, cmp);\r\n    res.sort(SequenceMatchResult.OFFSET_COMPARATOR);\r\n    return res;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVectorTable.valueEquals",
	"Comment": "deep comparison for equality of value, plus tolerance, for every concatvector in the table, plus dimensionalarrangement. this is mostly useful for testing.",
	"Method": "boolean valueEquals(ConcatVectorTable other,double tolerance){\r\n    if (!Arrays.equals(other.getDimensions(), getDimensions()))\r\n        return false;\r\n    for (int[] assignment : this) {\r\n        if (!getAssignmentValue(assignment).get().valueEquals(other.getAssignmentValue(assignment).get(), tolerance)) {\r\n            return false;\r\n        }\r\n    }\r\n    return true;\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.inference.TableFactor.multiply",
	"Comment": "product two factors, taking the multiplication at the intersections.",
	"Method": "TableFactor multiply(TableFactor other){\r\n    List<Integer> domain = new ArrayList();\r\n    List<Integer> otherDomain = new ArrayList();\r\n    List<Integer> resultDomain = new ArrayList();\r\n    for (int n : neighborIndices) {\r\n        domain.add(n);\r\n        resultDomain.add(n);\r\n    }\r\n    for (int n : other.neighborIndices) {\r\n        otherDomain.add(n);\r\n        if (!resultDomain.contains(n))\r\n            resultDomain.add(n);\r\n    }\r\n    int[] resultNeighborIndices = new int[resultDomain.size()];\r\n    int[] resultDimensions = new int[resultNeighborIndices.length];\r\n    for (int i = 0; i < resultDomain.size(); i++) {\r\n        int var = resultDomain.get(i);\r\n        resultNeighborIndices[i] = var;\r\n        assert ((getVariableSize(var) == 0 && other.getVariableSize(var) > 0) || (getVariableSize(var) > 0 && other.getVariableSize(var) == 0) || (getVariableSize(var) == other.getVariableSize(var)));\r\n        resultDimensions[i] = Math.max(getVariableSize(resultDomain.get(i)), other.getVariableSize(resultDomain.get(i)));\r\n    }\r\n    TableFactor result = new TableFactor(resultNeighborIndices, resultDimensions);\r\n    if (otherDomain.size() == 1 && (resultDomain.size() == domain.size()) && domain.size() == 2) {\r\n        int msgVar = otherDomain.get(0);\r\n        int msgIndex = resultDomain.indexOf(msgVar);\r\n        if (msgIndex == 0) {\r\n            for (int i = 0; i < resultDimensions[0]; i++) {\r\n                double d = other.values[i];\r\n                int k = i * resultDimensions[1];\r\n                for (int j = 0; j < resultDimensions[1]; j++) {\r\n                    int index = k + j;\r\n                    result.values[index] = values[index] + d;\r\n                }\r\n            }\r\n        } else if (msgIndex == 1) {\r\n            for (int i = 0; i < resultDimensions[0]; i++) {\r\n                int k = i * resultDimensions[1];\r\n                for (int j = 0; j < resultDimensions[1]; j++) {\r\n                    int index = k + j;\r\n                    result.values[index] = values[index] + other.values[j];\r\n                }\r\n            }\r\n        }\r\n    } else if (domain.size() == 1 && (resultDomain.size() == otherDomain.size()) && resultDomain.size() == 2) {\r\n        return other.multiply(this);\r\n    } else {\r\n        int[] mapping = new int[result.neighborIndices.length];\r\n        int[] otherMapping = new int[result.neighborIndices.length];\r\n        for (int i = 0; i < result.neighborIndices.length; i++) {\r\n            mapping[i] = domain.indexOf(result.neighborIndices[i]);\r\n            otherMapping[i] = otherDomain.indexOf(result.neighborIndices[i]);\r\n        }\r\n        int[] assignment = new int[neighborIndices.length];\r\n        int[] otherAssignment = new int[other.neighborIndices.length];\r\n        Iterator<int[]> fastPassByReferenceIterator = result.fastPassByReferenceIterator();\r\n        int[] resultAssignment = fastPassByReferenceIterator.next();\r\n        while (true) {\r\n            for (int i = 0; i < resultAssignment.length; i++) {\r\n                if (mapping[i] != -1)\r\n                    assignment[mapping[i]] = resultAssignment[i];\r\n                if (otherMapping[i] != -1)\r\n                    otherAssignment[otherMapping[i]] = resultAssignment[i];\r\n            }\r\n            result.setAssignmentLogValue(resultAssignment, getAssignmentLogValue(assignment) + other.getAssignmentLogValue(otherAssignment));\r\n            if (fastPassByReferenceIterator.hasNext())\r\n                fastPassByReferenceIterator.next();\r\n            else\r\n                break;\r\n        }\r\n    }\r\n    return result;\r\n}"
}, {
	"Path": "edu.stanford.nlp.optimization.QNMinimizer.lineSearchBacktrackOWL",
	"Comment": "linesearchbacktrackowl is the linesearch used for l1 regularization.it only satisfies sufficient descent not the wolfe conditions.",
	"Method": "double[] lineSearchBacktrackOWL(Function func,double[] dir,double[] x,double[] newX,double[] grad,double lastValue,StringBuilder sb){\r\n    double[] orthant = new double[x.length];\r\n    for (int i = 0; i < orthant.length; i++) {\r\n        orthant[i] = (x[i] == 0.0) ? -grad[i] : x[i];\r\n    }\r\n    double step, c1;\r\n    if (its <= 2) {\r\n        step = 0.1;\r\n        c1 = 0.1;\r\n    } else {\r\n        step = 1.0;\r\n        c1 = 0.1;\r\n    }\r\n    double c = 0.01;\r\n    double[] newPoint = new double[3];\r\n    while (true) {\r\n        plusAndConstMult(x, dir, step, newX);\r\n        projectOWL(newX, orthant, func);\r\n        double value = func.valueAt(newX);\r\n        double norm = l1NormOWL(newX, func);\r\n        value += norm * lambdaOWL;\r\n        newPoint[f] = value;\r\n        double dgtest = 0.0;\r\n        for (int i = 0; i < x.length; i++) {\r\n            dgtest += (newX[i] - x[i]) * grad[i];\r\n        }\r\n        if (newPoint[f] <= lastValue + c * dgtest)\r\n            break;\r\n        else {\r\n            if (newPoint[f] < lastValue) {\r\n                sb.append('!');\r\n            } else {\r\n                sb.append('.');\r\n            }\r\n        }\r\n        step = c1 * step;\r\n    }\r\n    newPoint[a] = step;\r\n    fevals += 1;\r\n    if (fevals > maxFevals) {\r\n        throw new MaxEvaluationsExceeded(\"Exceeded during linesearch() Function.\");\r\n    }\r\n    return newPoint;\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogReader.closeIntoTrimmedReader",
	"Comment": "closes current reader and creates new one with new checkoint and same file channel",
	"Method": "TranslogReader closeIntoTrimmedReader(long aboveSeqNo,ChannelFactory channelFactory){\r\n    if (closed.compareAndSet(false, true)) {\r\n        Closeable toCloseOnFailure = channel;\r\n        final TranslogReader newReader;\r\n        try {\r\n            if (aboveSeqNo < checkpoint.trimmedAboveSeqNo || aboveSeqNo < checkpoint.maxSeqNo && checkpoint.trimmedAboveSeqNo == SequenceNumbers.UNASSIGNED_SEQ_NO) {\r\n                final Path checkpointFile = path.getParent().resolve(getCommitCheckpointFileName(checkpoint.generation));\r\n                final Checkpoint newCheckpoint = new Checkpoint(checkpoint.offset, checkpoint.numOps, checkpoint.generation, checkpoint.minSeqNo, checkpoint.maxSeqNo, checkpoint.globalCheckpoint, checkpoint.minTranslogGeneration, aboveSeqNo);\r\n                Checkpoint.write(channelFactory, checkpointFile, newCheckpoint, StandardOpenOption.WRITE);\r\n                IOUtils.fsync(checkpointFile, false);\r\n                IOUtils.fsync(checkpointFile.getParent(), true);\r\n                newReader = new TranslogReader(newCheckpoint, channel, path, header);\r\n            } else {\r\n                newReader = new TranslogReader(checkpoint, channel, path, header);\r\n            }\r\n            toCloseOnFailure = null;\r\n            return newReader;\r\n        } finally {\r\n            IOUtils.close(toCloseOnFailure);\r\n        }\r\n    } else {\r\n        throw new AlreadyClosedException(toString() + \" is already closed\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.env.NodeEnvironment.nodeId",
	"Comment": "returns the unique uuid describing this node. the uuid is persistent in the data folder of this nodeand remains across restarts.",
	"Method": "String nodeId(){\r\n    return nodeMetaData.nodeId();\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.GraphicalModel.getVariableMetaDataByReference",
	"Comment": "gets the metadata for a variable. creates blank metadata if does not exists, then returns that. pass by reference.",
	"Method": "Map<String, String> getVariableMetaDataByReference(int variableIndex){\r\n    while (variableIndex >= variableMetaData.size()) {\r\n        variableMetaData.add(new HashMap());\r\n    }\r\n    return variableMetaData.get(variableIndex);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.freeDiskPercentageAfterShardAssigned",
	"Comment": "given the diskusage for a node and the size of the shard, return thepercentage of free disk if the shard were to be allocated to the node.",
	"Method": "double freeDiskPercentageAfterShardAssigned(DiskUsage usage,Long shardSize){\r\n    shardSize = (shardSize == null) ? 0 : shardSize;\r\n    DiskUsage newUsage = new DiskUsage(usage.getNodeId(), usage.getNodeName(), usage.getPath(), usage.getTotalBytes(), usage.getFreeBytes() - shardSize);\r\n    return newUsage.getFreeDiskAsPercentage();\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.AbstractBulkByScrollRequest.setRetryBackoffInitialTime",
	"Comment": "set the initial delay after a rejection before retrying request.",
	"Method": "Self setRetryBackoffInitialTime(TimeValue retryBackoffInitialTime){\r\n    this.retryBackoffInitialTime = retryBackoffInitialTime;\r\n    return self();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.mapping.get.GetFieldMappingsRequestBuilder.includeDefaults",
	"Comment": "indicates whether default mapping settings should be returned",
	"Method": "GetFieldMappingsRequestBuilder includeDefaults(boolean includeDefaults){\r\n    request.includeDefaults(includeDefaults);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.DocumentReader.setTokenizerFactory",
	"Comment": "sets the tokenizer used to chop up text into words for the documents.",
	"Method": "void setTokenizerFactory(TokenizerFactory<? extends HasWord> tokenizerFactory){\r\n    this.tokenizerFactory = tokenizerFactory;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchScrollAsyncAction.collectNodesAndRun",
	"Comment": "this method collects nodes from the remote clusters asynchronously if any of the scroll ids references a remote cluster.otherwise the action listener will be invoked immediately with a function based on the given discovery nodes.",
	"Method": "void collectNodesAndRun(Iterable<ScrollIdForNode> scrollIds,DiscoveryNodes nodes,SearchTransportService searchTransportService,ActionListener<BiFunction<String, String, DiscoveryNode>> listener){\r\n    Set<String> clusters = new HashSet();\r\n    for (ScrollIdForNode target : scrollIds) {\r\n        if (target.getClusterAlias() != null) {\r\n            clusters.add(target.getClusterAlias());\r\n        }\r\n    }\r\n    if (clusters.isEmpty()) {\r\n        listener.onResponse((cluster, node) -> nodes.get(node));\r\n    } else {\r\n        RemoteClusterService remoteClusterService = searchTransportService.getRemoteClusterService();\r\n        remoteClusterService.collectNodes(clusters, ActionListener.wrap(nodeFunction -> {\r\n            final BiFunction<String, String, DiscoveryNode> clusterNodeLookup = (clusterAlias, node) -> {\r\n                if (clusterAlias == null) {\r\n                    return nodes.get(node);\r\n                } else {\r\n                    return nodeFunction.apply(clusterAlias, node);\r\n                }\r\n            };\r\n            listener.onResponse(clusterNodeLookup);\r\n        }, listener::onFailure));\r\n    }\r\n}"
}, {
	"Path": "org.openqa.selenium.remote.server.log.PerSessionLogHandler.getLog",
	"Comment": "this returns selenium remote control logs associated with the sessionid.",
	"Method": "String getLog(SessionId sessionId){\r\n    String logs = formattedRecords(sessionId);\r\n    logs = \"\\n<RC_Logs RC_Session_ID=\" + sessionId + \">\\n\" + logs + \"\\n<\/RC_Logs>\\n\";\r\n    return logs;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.DiffableUtils.diff",
	"Comment": "calculates diff between two immutableopenintmaps of diffable objects",
	"Method": "MapDiff<K, T, ImmutableOpenMap<K, T>> diff(ImmutableOpenMap<K, T> before,ImmutableOpenMap<K, T> after,KeySerializer<K> keySerializer,MapDiff<K, T, ImmutableOpenMap<K, T>> diff,ImmutableOpenMap<K, T> before,ImmutableOpenMap<K, T> after,KeySerializer<K> keySerializer,ValueSerializer<K, T> valueSerializer,MapDiff<Integer, T, ImmutableOpenIntMap<T>> diff,ImmutableOpenIntMap<T> before,ImmutableOpenIntMap<T> after,KeySerializer<Integer> keySerializer,MapDiff<Integer, T, ImmutableOpenIntMap<T>> diff,ImmutableOpenIntMap<T> before,ImmutableOpenIntMap<T> after,KeySerializer<Integer> keySerializer,ValueSerializer<Integer, T> valueSerializer,MapDiff<K, T, Map<K, T>> diff,Map<K, T> before,Map<K, T> after,KeySerializer<K> keySerializer,MapDiff<K, T, Map<K, T>> diff,Map<K, T> before,Map<K, T> after,KeySerializer<K> keySerializer,ValueSerializer<K, T> valueSerializer,Diff<V> diff,V value,V beforePart,Diff<V> diff,V value,V beforePart,Diff<V> diff,V value,V beforePart){\r\n    assert after != null && before != null;\r\n    return new JdkMapDiff(before, after, keySerializer, valueSerializer);\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.spanish.process.SpanishTokenizer.coreLabelFactory",
	"Comment": "a factory that vends corelabel tokens with default tokenization.",
	"Method": "TokenizerFactory<CoreLabel> coreLabelFactory(){\r\n    return SpanishTokenizerFactory.newCoreLabelTokenizerFactory();\r\n}"
}, {
	"Path": "org.elasticsearch.common.collect.HppcMaps.newMap",
	"Comment": "returns a new map with the given number of expected elements.",
	"Method": "ObjectObjectHashMap<K, V> newMap(int expectedElements,ObjectObjectHashMap<K, V> newMap){\r\n    return newMap(16);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequest.rewrite",
	"Comment": "indicates whether the query should be rewritten into primitive queries",
	"Method": "void rewrite(boolean rewrite,boolean rewrite){\r\n    return rewrite;\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.Pipeline.execute",
	"Comment": "modifies the data of a document to be indexed based on the processor this pipeline holds",
	"Method": "IngestDocument execute(IngestDocument ingestDocument){\r\n    long startTimeInNanos = relativeTimeProvider.getAsLong();\r\n    try {\r\n        metrics.preIngest();\r\n        return compoundProcessor.execute(ingestDocument);\r\n    } catch (Exception e) {\r\n        metrics.ingestFailed();\r\n        throw e;\r\n    } finally {\r\n        long ingestTimeInMillis = TimeUnit.NANOSECONDS.toMillis(relativeTimeProvider.getAsLong() - startTimeInNanos);\r\n        metrics.postIngest(ingestTimeInMillis);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.seqno.ReplicationTracker.isPrimaryMode",
	"Comment": "returns whether the replication tracker is in primary mode, i.e., whether the current shard is acting as primary from the point ofview of replication.",
	"Method": "boolean isPrimaryMode(){\r\n    return primaryMode;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.DeleteByQueryRequest.setRouting",
	"Comment": "set routing limiting the process to the shards that match that routing value",
	"Method": "DeleteByQueryRequest setRouting(String routing){\r\n    if (routing != null) {\r\n        getSearchRequest().routing(routing);\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterChangedEvent.indicesDeletedFromClusterState",
	"Comment": "if an index exists in the previous cluster state, but not in the new cluster state, it must have been deleted.",
	"Method": "List<Index> indicesDeletedFromClusterState(){\r\n    if (metaDataChanged() == false || isNewCluster()) {\r\n        return Collections.emptyList();\r\n    }\r\n    List<Index> deleted = null;\r\n    for (ObjectCursor<IndexMetaData> cursor : previousState.metaData().indices().values()) {\r\n        IndexMetaData index = cursor.value;\r\n        IndexMetaData current = state.metaData().index(index.getIndex());\r\n        if (current == null) {\r\n            if (deleted == null) {\r\n                deleted = new ArrayList();\r\n            }\r\n            deleted.add(index.getIndex());\r\n        }\r\n    }\r\n    return deleted == null ? Collections.<Index>emptyList() : deleted;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.AllocationId.newInitializing",
	"Comment": "creates a new allocation id for initializing allocation based on an existing id.",
	"Method": "AllocationId newInitializing(AllocationId newInitializing,String existingAllocationId){\r\n    return new AllocationId(existingAllocationId, null);\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.spanish.pipeline.AnCoraProcessor.processTreeFile",
	"Comment": "processes a single file containing ancora xml trees. returns mwe statistics for the trees inthe file and the actual parsed trees.",
	"Method": "Pair<TwoDimensionalCounter<String, String>, List<Tree>> processTreeFile(File file,SpanishXMLTreeReaderFactory trf,String encoding){\r\n    TwoDimensionalCounter<String, String> tagger = new TwoDimensionalCounter();\r\n    try {\r\n        Reader in = new BufferedReader(new InputStreamReader(new FileInputStream(file), encoding));\r\n        TreeReader tr = trf.newTreeReader(file.getPath(), in);\r\n        List<Tree> trees = new ArrayList();\r\n        Tree t, splitPoint;\r\n        while ((t = tr.readTree()) != null) {\r\n            do {\r\n                splitPoint = findSplitPoint(t);\r\n                Pair<Tree, Tree> split = split(t, splitPoint);\r\n                Tree toAdd = split.first();\r\n                t = split.second();\r\n                trees.add(toAdd);\r\n                updateTagger(tagger, toAdd);\r\n            } while (splitPoint != null);\r\n        }\r\n        tr.close();\r\n        return new Pair(tagger, trees);\r\n    } catch (IOException e) {\r\n        e.printStackTrace();\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.ingest.Pipeline.getProcessors",
	"Comment": "unmodifiable list containing each processor that operates on the data.",
	"Method": "List<Processor> getProcessors(){\r\n    return compoundProcessor.getProcessors();\r\n}"
}, {
	"Path": "edu.stanford.nlp.loglinear.model.ConcatVector.readFromStream",
	"Comment": "static function to deserialize a concat vector from an input stream.",
	"Method": "ConcatVector readFromStream(InputStream stream){\r\n    return readFromProto(ConcatVectorProto.ConcatVector.parseDelimitedFrom(stream));\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksClusterService.reassignTasks",
	"Comment": "evaluates the cluster state and tries to assign tasks to nodes.",
	"Method": "ClusterState reassignTasks(ClusterState currentState){\r\n    ClusterState clusterState = currentState;\r\n    final PersistentTasksCustomMetaData tasks = currentState.getMetaData().custom(PersistentTasksCustomMetaData.TYPE);\r\n    if (tasks != null) {\r\n        logger.trace(\"reassigning {} persistent tasks\", tasks.tasks().size());\r\n        final DiscoveryNodes nodes = currentState.nodes();\r\n        for (PersistentTask<?> task : tasks.tasks()) {\r\n            if (needsReassignment(task.getAssignment(), nodes)) {\r\n                Assignment assignment = createAssignment(task.getTaskName(), task.getParams(), clusterState);\r\n                if (Objects.equals(assignment, task.getAssignment()) == false) {\r\n                    logger.trace(\"reassigning task {} from node {} to node {}\", task.getId(), task.getAssignment().getExecutorNode(), assignment.getExecutorNode());\r\n                    clusterState = update(clusterState, builder(clusterState).reassignTask(task.getId(), assignment));\r\n                } else {\r\n                    logger.trace(\"ignoring task {} because assignment is the same {}\", task.getId(), assignment);\r\n                }\r\n            } else {\r\n                logger.trace(\"ignoring task {} because it is still running\", task.getId());\r\n            }\r\n        }\r\n    }\r\n    return clusterState;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.MultiSearchRequest.add",
	"Comment": "add a search request to execute. note, the order is important, the search response will be returned in thesame order as the search requests.",
	"Method": "MultiSearchRequest add(SearchRequestBuilder request,MultiSearchRequest add,SearchRequest request){\r\n    requests.add(request);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataMappingService.refreshMapping",
	"Comment": "refreshes mappings if they are not the same between original and parsed version",
	"Method": "void refreshMapping(String index,String indexUUID){\r\n    final RefreshTask refreshTask = new RefreshTask(index, indexUUID);\r\n    clusterService.submitStateUpdateTask(\"refresh-mapping\", refreshTask, ClusterStateTaskConfig.build(Priority.HIGH), refreshExecutor, (source, e) -> logger.warn(() -> new ParameterizedMessage(\"failure during [{}]\", source), e));\r\n}"
}, {
	"Path": "org.elasticsearch.common.geo.builders.LineStringBuilder.close",
	"Comment": "closes the current linestring by adding the starting point as the end point.this will have no effect if starting and end point are already the same.",
	"Method": "LineStringBuilder close(){\r\n    Coordinate start = coordinates.get(0);\r\n    Coordinate end = coordinates.get(coordinates.size() - 1);\r\n    if (start.x != end.x || start.y != end.y) {\r\n        coordinates.add(start);\r\n    }\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.DelimitRegExIterator.getFactory",
	"Comment": "returns a factory that vends delimitregexiterators that reads the contents of thegiven reader, splits on the specified delimiter, applies op, then returns the result.",
	"Method": "IteratorFromReaderFactory<String> getFactory(String delim,IteratorFromReaderFactory<T> getFactory,String delim,Function<String, T> op){\r\n    return new DelimitRegExIteratorFactory(delim, op);\r\n}"
}, {
	"Path": "org.elasticsearch.common.settings.AbstractScopedSettings.upgradeSettings",
	"Comment": "upgrade all settings eligible for upgrade in the specified settings instance.",
	"Method": "Settings upgradeSettings(Settings settings){\r\n    final Settings.Builder builder = Settings.builder();\r\n    boolean changed = false;\r\n    for (final String key : settings.keySet()) {\r\n        final Setting<?> setting = getRaw(key);\r\n        final SettingUpgrader<?> upgrader = settingUpgraders.get(setting);\r\n        if (upgrader == null) {\r\n            builder.copy(key, settings);\r\n        } else {\r\n            changed = true;\r\n            if (setting.getConcreteSetting(key).isListSetting()) {\r\n                final List<String> value = settings.getAsList(key);\r\n                final String upgradedKey = upgrader.getKey(key);\r\n                final List<String> upgradedValue = upgrader.getListValue(value);\r\n                builder.putList(upgradedKey, upgradedValue);\r\n            } else {\r\n                final String value = settings.get(key);\r\n                final String upgradedKey = upgrader.getKey(key);\r\n                final String upgradedValue = upgrader.getValue(value);\r\n                builder.put(upgradedKey, upgradedValue);\r\n            }\r\n        }\r\n    }\r\n    return changed ? builder.build() : settings;\r\n}"
}, {
	"Path": "org.elasticsearch.common.xcontent.XContentHelper.mergeDefaults",
	"Comment": "merges the defaults provided as the second parameter into the content of the first. only does recursive mergefor inner maps.",
	"Method": "void mergeDefaults(Map<String, Object> content,Map<String, Object> defaults){\r\n    for (Map.Entry<String, Object> defaultEntry : defaults.entrySet()) {\r\n        if (!content.containsKey(defaultEntry.getKey())) {\r\n            content.put(defaultEntry.getKey(), defaultEntry.getValue());\r\n        } else {\r\n            if (content.get(defaultEntry.getKey()) instanceof Map && defaultEntry.getValue() instanceof Map) {\r\n                mergeDefaults((Map<String, Object>) content.get(defaultEntry.getKey()), (Map<String, Object>) defaultEntry.getValue());\r\n            } else if (content.get(defaultEntry.getKey()) instanceof List && defaultEntry.getValue() instanceof List) {\r\n                List defaultList = (List) defaultEntry.getValue();\r\n                List contentList = (List) content.get(defaultEntry.getKey());\r\n                List mergedList = new ArrayList();\r\n                if (allListValuesAreMapsOfOne(defaultList) && allListValuesAreMapsOfOne(contentList)) {\r\n                    Map<String, Map<String, Object>> processed = new LinkedHashMap();\r\n                    for (Object o : contentList) {\r\n                        Map<String, Object> map = (Map<String, Object>) o;\r\n                        Map.Entry<String, Object> entry = map.entrySet().iterator().next();\r\n                        processed.put(entry.getKey(), map);\r\n                    }\r\n                    for (Object o : defaultList) {\r\n                        Map<String, Object> map = (Map<String, Object>) o;\r\n                        Map.Entry<String, Object> entry = map.entrySet().iterator().next();\r\n                        if (processed.containsKey(entry.getKey())) {\r\n                            mergeDefaults(processed.get(entry.getKey()), map);\r\n                        } else {\r\n                            processed.put(entry.getKey(), map);\r\n                        }\r\n                    }\r\n                    for (Map<String, Object> map : processed.values()) {\r\n                        mergedList.add(map);\r\n                    }\r\n                } else {\r\n                    mergedList.addAll(defaultList);\r\n                    for (Object o : contentList) {\r\n                        if (!mergedList.contains(o)) {\r\n                            mergedList.add(o);\r\n                        }\r\n                    }\r\n                }\r\n                content.put(defaultEntry.getKey(), mergedList);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.NodeJoinController.checkPendingJoinsAndElectIfNeeded",
	"Comment": "checks if there is an on going request to become master and if it has enough pending joins. if so, the node willbecome master via a clusterstate update task.",
	"Method": "void checkPendingJoinsAndElectIfNeeded(){\r\n    assert electionContext != null : \"election check requested but no active context\";\r\n    final int pendingMasterJoins = electionContext.getPendingMasterJoinsCount();\r\n    if (electionContext.isEnoughPendingJoins(pendingMasterJoins) == false) {\r\n        if (logger.isTraceEnabled()) {\r\n            logger.trace(\"not enough joins for election. Got [{}], required [{}]\", pendingMasterJoins, electionContext.requiredMasterJoins);\r\n        }\r\n    } else {\r\n        if (logger.isTraceEnabled()) {\r\n            logger.trace(\"have enough joins for election. Got [{}], required [{}]\", pendingMasterJoins, electionContext.requiredMasterJoins);\r\n        }\r\n        electionContext.closeAndBecomeMaster();\r\n        electionContext = null;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.geoDisjointQuery",
	"Comment": "a filter to filter indexed shapes that are not intersection with the query shape",
	"Method": "GeoShapeQueryBuilder geoDisjointQuery(String name,ShapeBuilder shape,GeoShapeQueryBuilder geoDisjointQuery,String name,String indexedShapeId,String indexedShapeType){\r\n    GeoShapeQueryBuilder builder = geoShapeQuery(name, indexedShapeId, indexedShapeType);\r\n    builder.relation(ShapeRelation.DISJOINT);\r\n    return builder;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.BigArrays.overSize",
	"Comment": "returns the next size to grow when working with parallel arrays that may have different page sizes or number of bytes per element.",
	"Method": "long overSize(long minTargetSize,long overSize,long minTargetSize,int pageSize,int bytesPerElement){\r\n    if (minTargetSize < 0) {\r\n        throw new IllegalArgumentException(\"minTargetSize must be >= 0\");\r\n    }\r\n    if (pageSize < 0) {\r\n        throw new IllegalArgumentException(\"pageSize must be > 0\");\r\n    }\r\n    if (bytesPerElement <= 0) {\r\n        throw new IllegalArgumentException(\"bytesPerElement must be > 0\");\r\n    }\r\n    long newSize;\r\n    if (minTargetSize < pageSize) {\r\n        newSize = ArrayUtil.oversize((int) minTargetSize, bytesPerElement);\r\n    } else {\r\n        newSize = minTargetSize + (minTargetSize >>> 3);\r\n    }\r\n    if (newSize > pageSize) {\r\n        newSize = newSize - (newSize % pageSize) + pageSize;\r\n        assert newSize % pageSize == 0;\r\n    }\r\n    return newSize;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.HookChart.insert",
	"Comment": "this hacks up a collectionvaluedmap.maybe convert to using that class?",
	"Method": "void insert(Map<K, List<V>> map,K index,V item){\r\n    List<V> list = map.get(index);\r\n    if (list == null) {\r\n        list = new ArrayList(3);\r\n        map.put(index, list);\r\n    }\r\n    list.add(item);\r\n}"
}, {
	"Path": "android.text.SpanSet.hasSpansIntersecting",
	"Comment": "returns true if there are spans intersecting the given interval.",
	"Method": "boolean hasSpansIntersecting(int start,int end){\r\n    for (int i = 0; i < numberOfSpans; i++) {\r\n        if (spanStarts[i] >= end || spanEnds[i] <= start)\r\n            continue;\r\n        return true;\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.isAllIndices",
	"Comment": "identifies whether the array containing index names given as argument refers to all indicesthe empty or null array identifies all indices",
	"Method": "boolean isAllIndices(List<String> aliasesOrIndices){\r\n    return aliasesOrIndices == null || aliasesOrIndices.isEmpty() || isExplicitAllPattern(aliasesOrIndices);\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.BulkByScrollResponse.getSearchRetries",
	"Comment": "the number of times that the request had retry search actions.",
	"Method": "long getSearchRetries(){\r\n    return status.getSearchRetries();\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationAction.resolveIndex",
	"Comment": "true if provided index should be resolved when resolving request",
	"Method": "boolean resolveIndex(){\r\n    return true;\r\n}"
}, {
	"Path": "org.apache.dubbo.registry.dubbo.RegistryDirectoryTest.testNofityOverrideUrls_Clean1",
	"Comment": "test cleanup override rules, and sent remove rules and other override ruleswhether the test can be restored to the providerurl when it is pushed",
	"Method": "void testNofityOverrideUrls_Clean1(){\r\n    RegistryDirectory registryDirectory = getRegistryDirectory();\r\n    invocation = new RpcInvocation();\r\n    List<URL> durls = new ArrayList<URL>();\r\n    durls.add(SERVICEURL.setHost(\"10.20.30.140\").addParameter(\"timeout\", \"1\"));\r\n    registryDirectory.notify(durls);\r\n    durls = new ArrayList<URL>();\r\n    durls.add(URL.valueOf(\"override://0.0.0.0?timeout=1000\"));\r\n    registryDirectory.notify(durls);\r\n    durls = new ArrayList<URL>();\r\n    durls.add(URL.valueOf(\"override://0.0.0.0?timeout=3\"));\r\n    durls.add(URL.valueOf(\"override://0.0.0.0\"));\r\n    registryDirectory.notify(durls);\r\n    List<Invoker<?>> invokers = registryDirectory.list(invocation);\r\n    Invoker<?> aInvoker = invokers.get(0);\r\n    Assert.assertEquals(\"1\", aInvoker.getUrl().getParameter(\"timeout\"));\r\n}"
}, {
	"Path": "org.openqa.grid.internal.utils.SelfRegisteringRemote.addBrowser",
	"Comment": "adding the browser described by the capability, automatically finding out what platform thenode is launched from",
	"Method": "void addBrowser(DesiredCapabilities cap,int instances){\r\n    String s = cap.getBrowserName();\r\n    if (s == null || \"\".equals(s)) {\r\n        throw new InvalidParameterException(cap + \" does seems to be a valid browser.\");\r\n    }\r\n    if (cap.getPlatform() == null) {\r\n        cap.setPlatform(Platform.getCurrent());\r\n    }\r\n    cap.setCapability(RegistrationRequest.MAX_INSTANCES, instances);\r\n    registrationRequest.getConfiguration().capabilities.add(cap);\r\n    registrationRequest.getConfiguration().fixUpCapabilities();\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequest.allShards",
	"Comment": "indicates whether the query should be validated on all shards instead of one random shard",
	"Method": "void allShards(boolean allShards,boolean allShards){\r\n    return allShards;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder.maxBoost",
	"Comment": "returns the maximum boost that will be applied by function score.",
	"Method": "FunctionScoreQueryBuilder maxBoost(float maxBoost,float maxBoost){\r\n    return this.maxBoost;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.command.CancelAllocationCommand.node",
	"Comment": "get the id of the node that manages the shard which allocation should be canceled",
	"Method": "String node(){\r\n    return this.node;\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.Initializables.of",
	"Comment": "returns an initializable for an instance that requires no initialization.",
	"Method": "Initializable<T> of(T instance){\r\n    return new Initializable<T>() {\r\n        @Override\r\n        public T get(Errors errors) throws ErrorsException {\r\n            return instance;\r\n        }\r\n        @Override\r\n        public String toString() {\r\n            return String.valueOf(instance);\r\n        }\r\n    };\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.Initializables.of",
	"Comment": "returns an initializable for an instance that requires no initialization.",
	"Method": "Initializable<T> of(T instance){\r\n    return instance;\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.Initializables.of",
	"Comment": "returns an initializable for an instance that requires no initialization.",
	"Method": "Initializable<T> of(T instance){\r\n    return String.valueOf(instance);\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.RecordIterator.main",
	"Comment": "just for testing.reads from the file named on the command line, or fromstdin, and echoes the records it reads to stdout.",
	"Method": "void main(String[] args){\r\n    RecordIterator it = null;\r\n    if (args.length > 0) {\r\n        it = new RecordIterator(args[0]);\r\n    } else {\r\n        it = new RecordIterator(System.in);\r\n        log.info(\"[Reading from stdin...]\");\r\n    }\r\n    while (it != null && it.hasNext()) {\r\n        List<String> record = it.next();\r\n        for (String field : record) {\r\n            System.out.printf(\"[%-10s]\", field);\r\n        }\r\n        System.out.println();\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.CategoryWordTag.setFromString",
	"Comment": "set everything by reversing a tostring operation.this should be added at some point.",
	"Method": "void setFromString(String labelStr){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.TransportShardBulkAction.executeBulkItemRequest",
	"Comment": "executes bulk item requests and handles request execution exceptions",
	"Method": "void executeBulkItemRequest(BulkPrimaryExecutionContext context,UpdateHelper updateHelper,LongSupplier nowInMillisSupplier,MappingUpdatePerformer mappingUpdater,CheckedRunnable<Exception> waitForMappingUpdate){\r\n    final DocWriteRequest.OpType opType = context.getCurrent().opType();\r\n    final UpdateHelper.Result updateResult;\r\n    if (opType == DocWriteRequest.OpType.UPDATE) {\r\n        final UpdateRequest updateRequest = (UpdateRequest) context.getCurrent();\r\n        try {\r\n            updateResult = updateHelper.prepare(updateRequest, context.getPrimary(), nowInMillisSupplier);\r\n        } catch (Exception failure) {\r\n            final Engine.Result result = new Engine.IndexResult(failure, updateRequest.version(), SequenceNumbers.UNASSIGNED_SEQ_NO);\r\n            context.setRequestToExecute(updateRequest);\r\n            context.markOperationAsExecuted(result);\r\n            context.markAsCompleted(context.getExecutionResult());\r\n            return;\r\n        }\r\n        switch(updateResult.getResponseResult()) {\r\n            case CREATED:\r\n            case UPDATED:\r\n                IndexRequest indexRequest = updateResult.action();\r\n                IndexMetaData metaData = context.getPrimary().indexSettings().getIndexMetaData();\r\n                MappingMetaData mappingMd = metaData.mappingOrDefault(indexRequest.type());\r\n                indexRequest.process(metaData.getCreationVersion(), mappingMd, updateRequest.concreteIndex());\r\n                context.setRequestToExecute(indexRequest);\r\n                break;\r\n            case DELETED:\r\n                context.setRequestToExecute(updateResult.action());\r\n                break;\r\n            case NOOP:\r\n                context.markOperationAsNoOp(updateResult.action());\r\n                context.markAsCompleted(context.getExecutionResult());\r\n                return;\r\n            default:\r\n                throw new IllegalStateException(\"Illegal update operation \" + updateResult.getResponseResult());\r\n        }\r\n    } else {\r\n        context.setRequestToExecute(context.getCurrent());\r\n        updateResult = null;\r\n    }\r\n    assert context.getRequestToExecute() != null;\r\n    if (context.getRequestToExecute().opType() == DocWriteRequest.OpType.DELETE) {\r\n        executeDeleteRequestOnPrimary(context, mappingUpdater);\r\n    } else {\r\n        executeIndexRequestOnPrimary(context, mappingUpdater);\r\n    }\r\n    if (context.requiresWaitingForMappingUpdate()) {\r\n        try {\r\n            waitForMappingUpdate.run();\r\n            context.resetForExecutionForRetry();\r\n        } catch (Exception e) {\r\n            context.failOnMappingUpdate(e);\r\n        }\r\n        return;\r\n    }\r\n    assert context.isOperationExecuted();\r\n    if (opType == DocWriteRequest.OpType.UPDATE && context.getExecutionResult().isFailed() && isConflictException(context.getExecutionResult().getFailure().getCause())) {\r\n        final UpdateRequest updateRequest = (UpdateRequest) context.getCurrent();\r\n        if (context.getRetryCounter() < updateRequest.retryOnConflict()) {\r\n            context.resetForExecutionForRetry();\r\n            return;\r\n        }\r\n    }\r\n    finalizePrimaryOperationOnCompletion(context, opType, updateResult);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.nndep.Classifier.takeAdaGradientStep",
	"Comment": "update classifier weights using the given training costinformation.",
	"Method": "void takeAdaGradientStep(Cost cost,double adaAlpha,double adaEps){\r\n    validateTraining();\r\n    double[][] gradW1 = cost.getGradW1(), gradW2 = cost.getGradW2(), gradE = cost.getGradE();\r\n    double[] gradb1 = cost.getGradb1();\r\n    for (int i = 0; i < W1.length; ++i) {\r\n        for (int j = 0; j < W1[i].length; ++j) {\r\n            eg2W1[i][j] += gradW1[i][j] * gradW1[i][j];\r\n            W1[i][j] -= adaAlpha * gradW1[i][j] / Math.sqrt(eg2W1[i][j] + adaEps);\r\n        }\r\n    }\r\n    for (int i = 0; i < b1.length; ++i) {\r\n        eg2b1[i] += gradb1[i] * gradb1[i];\r\n        b1[i] -= adaAlpha * gradb1[i] / Math.sqrt(eg2b1[i] + adaEps);\r\n    }\r\n    for (int i = 0; i < W2.length; ++i) {\r\n        for (int j = 0; j < W2[i].length; ++j) {\r\n            eg2W2[i][j] += gradW2[i][j] * gradW2[i][j];\r\n            W2[i][j] -= adaAlpha * gradW2[i][j] / Math.sqrt(eg2W2[i][j] + adaEps);\r\n        }\r\n    }\r\n    if (config.doWordEmbeddingGradUpdate) {\r\n        for (int i = 0; i < E.length; ++i) {\r\n            for (int j = 0; j < E[i].length; ++j) {\r\n                eg2E[i][j] += gradE[i][j] * gradE[i][j];\r\n                E[i][j] -= adaAlpha * gradE[i][j] / Math.sqrt(eg2E[i][j] + adaEps);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.ActiveShardsObserver.waitForActiveShards",
	"Comment": "waits on the specified number of active shards to be started before executing the",
	"Method": "void waitForActiveShards(String[] indexNames,ActiveShardCount activeShardCount,TimeValue timeout,Consumer<Boolean> onResult,Consumer<Exception> onFailure){\r\n    if (activeShardCount == ActiveShardCount.NONE) {\r\n        onResult.accept(true);\r\n        return;\r\n    }\r\n    final ClusterState state = clusterService.state();\r\n    final ClusterStateObserver observer = new ClusterStateObserver(state, clusterService, null, logger, threadPool.getThreadContext());\r\n    if (activeShardCount.enoughShardsActive(state, indexNames)) {\r\n        onResult.accept(true);\r\n    } else {\r\n        final Predicate<ClusterState> shardsAllocatedPredicate = newState -> activeShardCount.enoughShardsActive(newState, indexNames);\r\n        final ClusterStateObserver.Listener observerListener = new ClusterStateObserver.Listener() {\r\n            @Override\r\n            public void onNewClusterState(ClusterState state) {\r\n                onResult.accept(true);\r\n            }\r\n            @Override\r\n            public void onClusterServiceClose() {\r\n                logger.debug(\"[{}] cluster service closed while waiting for enough shards to be started.\", Arrays.toString(indexNames));\r\n                onFailure.accept(new NodeClosedException(clusterService.localNode()));\r\n            }\r\n            @Override\r\n            public void onTimeout(TimeValue timeout) {\r\n                onResult.accept(false);\r\n            }\r\n        };\r\n        observer.waitForNextChange(observerListener, shardsAllocatedPredicate, timeout);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.ActiveShardsObserver.waitForActiveShards",
	"Comment": "waits on the specified number of active shards to be started before executing the",
	"Method": "void waitForActiveShards(String[] indexNames,ActiveShardCount activeShardCount,TimeValue timeout,Consumer<Boolean> onResult,Consumer<Exception> onFailure){\r\n    onResult.accept(true);\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.ActiveShardsObserver.waitForActiveShards",
	"Comment": "waits on the specified number of active shards to be started before executing the",
	"Method": "void waitForActiveShards(String[] indexNames,ActiveShardCount activeShardCount,TimeValue timeout,Consumer<Boolean> onResult,Consumer<Exception> onFailure){\r\n    logger.debug(\"[{}] cluster service closed while waiting for enough shards to be started.\", Arrays.toString(indexNames));\r\n    onFailure.accept(new NodeClosedException(clusterService.localNode()));\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.ActiveShardsObserver.waitForActiveShards",
	"Comment": "waits on the specified number of active shards to be started before executing the",
	"Method": "void waitForActiveShards(String[] indexNames,ActiveShardCount activeShardCount,TimeValue timeout,Consumer<Boolean> onResult,Consumer<Exception> onFailure){\r\n    onResult.accept(false);\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.FrenchUnknownWordModel.getSignature",
	"Comment": "todo can add various signatures, setting the signature via options.",
	"Method": "String getSignature(String word,int loc){\r\n    final String BASE_LABEL = \"UNK\";\r\n    StringBuilder sb = new StringBuilder(BASE_LABEL);\r\n    switch(unknownLevel) {\r\n        case 1:\r\n            sb.append(FrenchUnknownWordSignatures.nounSuffix(word));\r\n            if (sb.toString().equals(BASE_LABEL)) {\r\n                sb.append(FrenchUnknownWordSignatures.adjSuffix(word));\r\n                if (sb.toString().equals(BASE_LABEL)) {\r\n                    sb.append(FrenchUnknownWordSignatures.verbSuffix(word));\r\n                    if (sb.toString().equals(BASE_LABEL)) {\r\n                        sb.append(FrenchUnknownWordSignatures.advSuffix(word));\r\n                    }\r\n                }\r\n            }\r\n            sb.append(FrenchUnknownWordSignatures.possiblePlural(word));\r\n            String hasDigit = FrenchUnknownWordSignatures.hasDigit(word);\r\n            String isDigit = FrenchUnknownWordSignatures.isDigit(word);\r\n            if (!hasDigit.equals(\"\")) {\r\n                if (isDigit.equals(\"\")) {\r\n                    sb.append(hasDigit);\r\n                } else {\r\n                    sb.append(isDigit);\r\n                }\r\n            }\r\n            sb.append(FrenchUnknownWordSignatures.hasPunc(word));\r\n            sb.append(FrenchUnknownWordSignatures.isAllCaps(word));\r\n            if (loc > 0) {\r\n                if (FrenchUnknownWordSignatures.isAllCaps(word).equals(\"\"))\r\n                    sb.append(FrenchUnknownWordSignatures.isCapitalized(word));\r\n            }\r\n            if (unknownSuffixSize > 0 && sb.toString().equals(BASE_LABEL)) {\r\n                int min = word.length() < unknownSuffixSize ? word.length() : unknownSuffixSize;\r\n                sb.append('-').append(word.substring(word.length() - min));\r\n            }\r\n            break;\r\n        default:\r\n            System.err.printf(\"%s: Invalid unknown word signature! (%d)%n\", this.getClass().getName(), unknownLevel);\r\n    }\r\n    return sb.toString();\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.spanish.SpanishVerbStripper.getCase",
	"Comment": "determines the case of the letter as if it had been part of theoriginal string",
	"Method": "char getCase(String original,char letter){\r\n    if (Character.isUpperCase(original.charAt(original.length() - 1))) {\r\n        return Character.toUpperCase(letter);\r\n    } else {\r\n        return Character.toLowerCase(letter);\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.shiftreduce.ShiftTransition.apply",
	"Comment": "add the new preterminal to the stack, increment the queue position.",
	"Method": "State apply(State state,State apply,State state,double scoreDelta){\r\n    Tree tagNode = state.sentence.get(state.tokenPosition);\r\n    if (!tagNode.isPreTerminal()) {\r\n        throw new AssertionError(\"Only expected preterminal nodes\");\r\n    }\r\n    Tree wordNode = tagNode.children()[0];\r\n    String word = wordNode.label().value();\r\n    return new State(state.stack.push(tagNode), state.transitions.push(this), state.separators, state.sentence, state.tokenPosition + 1, state.score + scoreDelta, false);\r\n}"
}, {
	"Path": "org.elasticsearch.index.translog.TranslogReader.readBytes",
	"Comment": "reads an operation at the given position into the given buffer.",
	"Method": "void readBytes(ByteBuffer buffer,long position){\r\n    if (position >= length) {\r\n        throw new EOFException(\"read requested past EOF. pos [\" + position + \"] end: [\" + length + \"]\");\r\n    }\r\n    if (position < getFirstOperationOffset()) {\r\n        throw new IOException(\"read requested before position of first ops. pos [\" + position + \"] first op on: [\" + getFirstOperationOffset() + \"]\");\r\n    }\r\n    Channels.readFromFileChannelWithEofException(channel, position, buffer);\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.tokensregex.parser.ParseException.add_escapes",
	"Comment": "used to convert raw characters to their escaped versionwhen these raw version cannot be used as part of an asciistring literal.",
	"Method": "String add_escapes(String str){\r\n    StringBuffer retval = new StringBuffer();\r\n    char ch;\r\n    for (int i = 0; i < str.length(); i++) {\r\n        switch(str.charAt(i)) {\r\n            case 0:\r\n                continue;\r\n            case '\\b':\r\n                retval.append(\"\\\\b\");\r\n                continue;\r\n            case '\\t':\r\n                retval.append(\"\\\\t\");\r\n                continue;\r\n            case '\\n':\r\n                retval.append(\"\\\\n\");\r\n                continue;\r\n            case '\\f':\r\n                retval.append(\"\\\\f\");\r\n                continue;\r\n            case '\\r':\r\n                retval.append(\"\\\\r\");\r\n                continue;\r\n            case '\\\"':\r\n                retval.append(\"\\\\\\\"\");\r\n                continue;\r\n            case '\\'':\r\n                retval.append(\"\\\\\\'\");\r\n                continue;\r\n            case '\\\\':\r\n                retval.append(\"\\\\\\\\\");\r\n                continue;\r\n            default:\r\n                if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {\r\n                    String s = \"0000\" + Integer.toString(ch, 16);\r\n                    retval.append(\"\\\%u\" + s.substring(s.length() - 4, s.length()));\r\n                } else {\r\n                    retval.append(ch);\r\n                }\r\n                continue;\r\n        }\r\n    }\r\n    return retval.toString();\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchShardIterator.getOriginalIndices",
	"Comment": "returns the original indices associated with this shard iterator, specifically with the cluster that this shard belongs to.",
	"Method": "OriginalIndices getOriginalIndices(){\r\n    return originalIndices;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDocument.originalText",
	"Comment": "returns the text originally used to construct this document, or null ifthere was no original text.",
	"Method": "String originalText(){\r\n    return (originalText);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ack.ClusterStateUpdateRequest.ackTimeout",
	"Comment": "returns the maximum time interval to wait for acknowledgements",
	"Method": "TimeValue ackTimeout(T ackTimeout,TimeValue ackTimeout){\r\n    this.ackTimeout = ackTimeout;\r\n    return (T) this;\r\n}"
}, {
	"Path": "org.elasticsearch.common.blobstore.fs.FsBlobContainer.isTempBlobName",
	"Comment": "returns true if the blob is a leftover temporary blob.the temporary blobs might be left after failed atomic write operation.",
	"Method": "boolean isTempBlobName(String blobName){\r\n    return blobName.startsWith(TEMP_FILE_PREFIX);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.cluster.state.ClusterStateResponse.isWaitForTimedOut",
	"Comment": "returns whether the request timed out waiting for a cluster state with a metadata version equal orhigher than the specified metadata.",
	"Method": "boolean isWaitForTimedOut(){\r\n    return waitForTimedOut;\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.PersistentTasksService.sendStartRequest",
	"Comment": "notifies the master node to create new persistent task and to assign it to a node.",
	"Method": "void sendStartRequest(String taskId,String taskName,Params taskParams,ActionListener<PersistentTask<Params>> listener){\r\n    @SuppressWarnings(\"unchecked\")\r\n    final ActionListener<PersistentTask<?>> wrappedListener = ActionListener.wrap(t -> listener.onResponse((PersistentTask<Params>) t), listener::onFailure);\r\n    StartPersistentTaskAction.Request request = new StartPersistentTaskAction.Request(taskId, taskName, taskParams);\r\n    execute(request, StartPersistentTaskAction.INSTANCE, wrappedListener);\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoryData.getGenId",
	"Comment": "gets the generational index file id from which this instance was read.",
	"Method": "long getGenId(){\r\n    return genId;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.WordLemmaTag.toString",
	"Comment": "return a string representation of the label.for a multipart label,this will return all parts.",
	"Method": "String toString(String toString,String divider){\r\n    return word() + divider + lemma + divider + tag;\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequest.setPipeline",
	"Comment": "sets the ingest pipeline to be executed before indexing the document",
	"Method": "IndexRequest setPipeline(String pipeline){\r\n    this.pipeline = pipeline;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.allocation.MoveDecision.withRemainDecision",
	"Comment": "creates a new move decision from this decision, plus adding a remain decision.",
	"Method": "MoveDecision withRemainDecision(Decision canRemainDecision){\r\n    return new MoveDecision(canRemainDecision, clusterRebalanceDecision, allocationDecision, targetNode, nodeDecisions, currentNodeRanking);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.AliasMetaData.newAliasMetaData",
	"Comment": "creates a new aliasmetadata instance with same content as the given one, but with a different alias name",
	"Method": "AliasMetaData newAliasMetaData(AliasMetaData aliasMetaData,String newAlias){\r\n    return new AliasMetaData(aliasMetaData, newAlias);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.node.DiscoveryNodes.getMinNodeVersion",
	"Comment": "returns the version of the node with the oldest version in the cluster.",
	"Method": "Version getMinNodeVersion(){\r\n    return minNodeVersion;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.EnglishTreebankParserParams.testMemoryTreebank",
	"Comment": "returns a memorytreebank appropriate to the testing treebank source",
	"Method": "MemoryTreebank testMemoryTreebank(){\r\n    return new MemoryTreebank(in -> new PennTreeReader(in, new LabeledScoredTreeFactory(), new BobChrisTreeNormalizer(tlp)));\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.SpanishUnknownWordModel.getSignatureIndex",
	"Comment": "returns the index of the signature of the word numbered wordindex, wherethe signature is the string representation of unknown word features.",
	"Method": "int getSignatureIndex(int index,int sentencePosition,String word){\r\n    String uwSig = getSignature(word, sentencePosition);\r\n    int sig = wordIndex.addToIndex(uwSig);\r\n    return sig;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoShapeQueryBuilder.fetch",
	"Comment": "fetches the shape with the given id in the given type and index.",
	"Method": "void fetch(Client client,GetRequest getRequest,String path,ActionListener<ShapeBuilder> listener){\r\n    if (ShapesAvailability.JTS_AVAILABLE == false) {\r\n        throw new IllegalStateException(\"JTS not available\");\r\n    }\r\n    getRequest.preference(\"_local\");\r\n    client.get(getRequest, new ActionListener<GetResponse>() {\r\n        @Override\r\n        public void onResponse(GetResponse response) {\r\n            try {\r\n                if (!response.isExists()) {\r\n                    throw new IllegalArgumentException(\"Shape with ID [\" + getRequest.id() + \"] in type [\" + getRequest.type() + \"] not found\");\r\n                }\r\n                if (response.isSourceEmpty()) {\r\n                    throw new IllegalArgumentException(\"Shape with ID [\" + getRequest.id() + \"] in type [\" + getRequest.type() + \"] source disabled\");\r\n                }\r\n                String[] pathElements = path.split(\"\\\\.\");\r\n                int currentPathSlot = 0;\r\n                try (XContentParser parser = XContentHelper.createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, response.getSourceAsBytesRef())) {\r\n                    XContentParser.Token currentToken;\r\n                    while ((currentToken = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\r\n                        if (currentToken == XContentParser.Token.FIELD_NAME) {\r\n                            if (pathElements[currentPathSlot].equals(parser.currentName())) {\r\n                                parser.nextToken();\r\n                                if (++currentPathSlot == pathElements.length) {\r\n                                    listener.onResponse(ShapeParser.parse(parser));\r\n                                    return;\r\n                                }\r\n                            } else {\r\n                                parser.nextToken();\r\n                                parser.skipChildren();\r\n                            }\r\n                        }\r\n                    }\r\n                    throw new IllegalStateException(\"Shape with name [\" + getRequest.id() + \"] found but missing \" + path + \" field\");\r\n                }\r\n            } catch (Exception e) {\r\n                onFailure(e);\r\n            }\r\n        }\r\n        @Override\r\n        public void onFailure(Exception e) {\r\n            listener.onFailure(e);\r\n        }\r\n    });\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoShapeQueryBuilder.fetch",
	"Comment": "fetches the shape with the given id in the given type and index.",
	"Method": "void fetch(Client client,GetRequest getRequest,String path,ActionListener<ShapeBuilder> listener){\r\n    try {\r\n        if (!response.isExists()) {\r\n            throw new IllegalArgumentException(\"Shape with ID [\" + getRequest.id() + \"] in type [\" + getRequest.type() + \"] not found\");\r\n        }\r\n        if (response.isSourceEmpty()) {\r\n            throw new IllegalArgumentException(\"Shape with ID [\" + getRequest.id() + \"] in type [\" + getRequest.type() + \"] source disabled\");\r\n        }\r\n        String[] pathElements = path.split(\"\\\\.\");\r\n        int currentPathSlot = 0;\r\n        try (XContentParser parser = XContentHelper.createParser(NamedXContentRegistry.EMPTY, LoggingDeprecationHandler.INSTANCE, response.getSourceAsBytesRef())) {\r\n            XContentParser.Token currentToken;\r\n            while ((currentToken = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\r\n                if (currentToken == XContentParser.Token.FIELD_NAME) {\r\n                    if (pathElements[currentPathSlot].equals(parser.currentName())) {\r\n                        parser.nextToken();\r\n                        if (++currentPathSlot == pathElements.length) {\r\n                            listener.onResponse(ShapeParser.parse(parser));\r\n                            return;\r\n                        }\r\n                    } else {\r\n                        parser.nextToken();\r\n                        parser.skipChildren();\r\n                    }\r\n                }\r\n            }\r\n            throw new IllegalStateException(\"Shape with name [\" + getRequest.id() + \"] found but missing \" + path + \" field\");\r\n        }\r\n    } catch (Exception e) {\r\n        onFailure(e);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.GeoShapeQueryBuilder.fetch",
	"Comment": "fetches the shape with the given id in the given type and index.",
	"Method": "void fetch(Client client,GetRequest getRequest,String path,ActionListener<ShapeBuilder> listener){\r\n    listener.onFailure(e);\r\n}"
}, {
	"Path": "edu.stanford.nlp.naturalli.QuestionToStatementTranslator.toStatement",
	"Comment": "convert a question to a statement, if possible.the question must have words, lemmas, and part of speech tags.the question must have valid punctuation.",
	"Method": "List<List<CoreLabel>> toStatement(List<CoreLabel> question){\r\n    TokenSequenceMatcher matcher;\r\n    if ((matcher = triggerWhatIsThere.matcher(question)).matches()) {\r\n        return postProcess(question, processWhatIsThere(matcher));\r\n    } else if ((matcher = triggerWhNNWill.matcher(question)).matches()) {\r\n        return postProcess(question, processWhNNWill(matcher));\r\n    } else if ((matcher = triggerWhNNIs.matcher(question)).matches()) {\r\n        return postProcess(question, processWhNNIs(matcher));\r\n    } else if ((matcher = triggerWhNNHave.matcher(question)).matches()) {\r\n        return postProcess(question, processWhNNHaveIs(matcher));\r\n    } else if ((matcher = triggerWhNNHaveNN.matcher(question)).matches()) {\r\n        return postProcess(question, processWhNNHaveNN(matcher));\r\n    } else if ((matcher = triggerHow.matcher(question)).matches()) {\r\n        return postProcess(question, processHow(matcher));\r\n    } else if ((matcher = triggerHowMuchDo.matcher(question)).matches()) {\r\n        return postProcess(question, processHowMuchDo(matcher));\r\n    } else if ((matcher = triggerWhatIs.matcher(question)).matches()) {\r\n        return postProcess(question, processWhatIs(matcher));\r\n    } else if ((matcher = triggerWhatHave.matcher(question)).matches()) {\r\n        return postProcess(question, processWhatHave(matcher));\r\n    } else if ((matcher = triggerWhereDo.matcher(question)).matches()) {\r\n        return postProcess(question, processWhereDo(matcher, question));\r\n    } else if ((matcher = triggerWhereIs.matcher(question)).matches()) {\r\n        return postProcess(question, processWhereIs(matcher));\r\n    } else if ((matcher = triggerWhoIs.matcher(question)).matches()) {\r\n        return postProcess(question, processWhoIs(matcher));\r\n    } else if ((matcher = triggerWhoDid.matcher(question)).matches()) {\r\n        return postProcess(question, processWhoDid(matcher));\r\n    } else if ((matcher = triggerWhatDo.matcher(question)).matches()) {\r\n        return postProcess(question, processWhatDo(matcher));\r\n    } else if ((matcher = triggerWhenDo.matcher(question)).matches()) {\r\n        return postProcess(question, processWhenDo(matcher));\r\n    } else {\r\n        return Collections.emptyList();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.monitor.process.ProcessProbe.getUnixMethod",
	"Comment": "returns a given method of the unixoperatingsystemmxbean,or null if the method is not found or unavailable.",
	"Method": "Method getUnixMethod(String methodName){\r\n    try {\r\n        return Class.forName(\"com.sun.management.UnixOperatingSystemMXBean\").getMethod(methodName);\r\n    } catch (Exception t) {\r\n        return null;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.prefixQuery",
	"Comment": "a query that matches documents containing terms with a specified prefix.",
	"Method": "PrefixQueryBuilder prefixQuery(String name,String prefix){\r\n    return new PrefixQueryBuilder(name, prefix);\r\n}"
}, {
	"Path": "org.elasticsearch.index.engine.Segment.getSegmentSort",
	"Comment": "return the sort order of this segment, or null if the segment has no sort.",
	"Method": "Sort getSegmentSort(){\r\n    return segmentSort;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryStringQueryBuilder.defaultField",
	"Comment": "the default field to run against when no prefix field is specified. only relevant whennot explicitly adding fields the query string will run against.",
	"Method": "QueryStringQueryBuilder defaultField(String defaultField,String defaultField){\r\n    return this.defaultField;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchScrollRequest.scroll",
	"Comment": "if set, will enable scrolling of the search request for the specified timeout.",
	"Method": "Scroll scroll(SearchScrollRequest scroll,Scroll scroll,SearchScrollRequest scroll,TimeValue keepAlive,SearchScrollRequest scroll,String keepAlive){\r\n    return scroll(new Scroll(TimeValue.parseTimeValue(keepAlive, null, getClass().getSimpleName() + \".keepAlive\")));\r\n}"
}, {
	"Path": "org.elasticsearch.common.logging.PrefixLogger.prefix",
	"Comment": "obtain the prefix for this prefix logger. this can be used to create a logger with the same prefix as this one.",
	"Method": "String prefix(){\r\n    return marker.getName();\r\n}"
}, {
	"Path": "edu.stanford.nlp.ling.BasicDatum.labels",
	"Comment": "returns the complete list of labels for this datum, which may be empty.",
	"Method": "Collection<LabelType> labels(){\r\n    return labels;\r\n}"
}, {
	"Path": "org.elasticsearch.action.delete.DeleteResponse.parseXContentFields",
	"Comment": "parse the current token and update the parsing context appropriately.",
	"Method": "void parseXContentFields(XContentParser parser,Builder context){\r\n    DocWriteResponse.parseInnerToXContent(parser, context);\r\n}"
}, {
	"Path": "edu.stanford.nlp.international.arabic.pipeline.UniversalPOSMapper.map",
	"Comment": "first map to the ldc short tags. then map to the universal pos. then addmorphological annotations.",
	"Method": "String map(String posTag,String terminal){\r\n    String rawTag = posTag.trim();\r\n    String shortTag = tagsToEscape.contains(rawTag) ? rawTag : tagMap.get(rawTag);\r\n    if (shortTag == null) {\r\n        System.err.printf(\"%s: No LDC shortened tag for %s%n\", this.getClass().getName(), rawTag);\r\n        return rawTag;\r\n    }\r\n    String universalTag = universalMap.get(shortTag);\r\n    if (!universalMap.containsKey(shortTag)) {\r\n        System.err.printf(\"%s: No universal tag for LDC tag %s%n\", this.getClass().getName(), shortTag);\r\n        universalTag = shortTag;\r\n    }\r\n    MorphoFeatures feats = new MorphoFeatures(morphoSpec.strToFeatures(rawTag));\r\n    String functionalTag = feats.getTag(universalTag);\r\n    return functionalTag;\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest.indices",
	"Comment": "sets the indices this put mapping operation will execute on.",
	"Method": "PutMappingRequest indices(String indices,String[] indices){\r\n    return indices;\r\n}"
}, {
	"Path": "android.util.ArrayMap.retainAll",
	"Comment": "remove all keys in the array map that do not exist in the given collection.",
	"Method": "boolean retainAll(Collection<?> collection){\r\n    return MapCollections.retainAllHelper(this, collection);\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.concurrent.ThreadContext.getTransient",
	"Comment": "returns a transient header object or null if there is no header for the given key",
	"Method": "T getTransient(String key){\r\n    return (T) threadLocal.get().transientHeaders.get(key);\r\n}"
}, {
	"Path": "org.openqa.selenium.support.ui.QuotesTest.shouldProvideConcatenatedStringsWhenStringEndsWithQuote",
	"Comment": "tests that quotes.escape returns concatenated strings when the givenstring contains a tick and and ends with a quote.",
	"Method": "void shouldProvideConcatenatedStringsWhenStringEndsWithQuote(){\r\n    assertThat(escape(\"Bar \\\"Rock'n'Roll\\\"\")).isEqualTo(\"concat(\\\"Bar \\\", '\\\"', \\\"Rock'n'Roll\\\", '\\\"')\");\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryStringQueryBuilder.lenient",
	"Comment": "sets the query string parser to be lenient when parsing field values, defaults to the indexsetting and if not set, defaults to false.",
	"Method": "QueryStringQueryBuilder lenient(Boolean lenient,Boolean lenient){\r\n    return this.lenient;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.regexp.NumberSequenceClassifier.leftScanFindsWeightWord",
	"Comment": "look for a distance of up to 3 for something that indicates weight notmoney.",
	"Method": "boolean leftScanFindsWeightWord(List<CoreLabel> pl,int i){\r\n    if (DEBUG) {\r\n        log.info(\"leftScan from: \" + pl.get(i).word());\r\n    }\r\n    for (int j = i - 1; j >= 0 && j >= i - 3; j--) {\r\n        CoreLabel fl = pl.get(j);\r\n        if (fl.word().startsWith(\"weigh\")) {\r\n            if (DEBUG) {\r\n                log.info(\"leftScan found weight: \" + fl.word());\r\n            }\r\n            return true;\r\n        }\r\n    }\r\n    return false;\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.NeuralUtils.elementwiseApplyLog",
	"Comment": "applies log to each of the entries in the matrix.returns a new matrix.",
	"Method": "SimpleMatrix elementwiseApplyLog(SimpleMatrix input){\r\n    SimpleMatrix output = new SimpleMatrix(input);\r\n    for (int i = 0; i < output.numRows(); ++i) {\r\n        for (int j = 0; j < output.numCols(); ++j) {\r\n            output.set(i, j, Math.log(output.get(i, j)));\r\n        }\r\n    }\r\n    return output;\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilder.rewrite",
	"Comment": "rewrites this query builder into its primitive form. by default this method return the builder itself. if the builderdid not change the identity reference must be returned otherwise the builder will be rewritten infinitely.",
	"Method": "QueryBuilder rewrite(QueryRewriteContext queryShardContext){\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.discovery.zen.MembershipAction.sendValidateJoinRequestBlocking",
	"Comment": "validates the join request, throwing a failure if it failed.",
	"Method": "void sendValidateJoinRequestBlocking(DiscoveryNode node,ClusterState state,TimeValue timeout){\r\n    transportService.submitRequest(node, DISCOVERY_JOIN_VALIDATE_ACTION_NAME, new ValidateJoinRequest(state), EmptyTransportResponseHandler.INSTANCE_SAME).txGet(timeout.millis(), TimeUnit.MILLISECONDS);\r\n}"
}, {
	"Path": "org.elasticsearch.client.Client.getRemoteClusterClient",
	"Comment": "returns a client to a remote cluster with the given cluster alias.",
	"Method": "Client getRemoteClusterClient(String clusterAlias){\r\n    throw new UnsupportedOperationException(\"this client doesn't support remote cluster connections\");\r\n}"
}, {
	"Path": "edu.stanford.nlp.io.IOUtils.deleteDirRecursively",
	"Comment": "given a filepath, delete all files in the directory recursively",
	"Method": "boolean deleteDirRecursively(File dir){\r\n    if (dir.isDirectory()) {\r\n        for (File f : dir.listFiles()) {\r\n            boolean success = deleteDirRecursively(f);\r\n            if (!success)\r\n                return false;\r\n        }\r\n    }\r\n    return dir.delete();\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.termsQuery",
	"Comment": "a filer for a field based on several terms matching on any of them.",
	"Method": "TermsQueryBuilder termsQuery(String name,String values,TermsQueryBuilder termsQuery,String name,int values,TermsQueryBuilder termsQuery,String name,long values,TermsQueryBuilder termsQuery,String name,float values,TermsQueryBuilder termsQuery,String name,double values,TermsQueryBuilder termsQuery,String name,Object values,TermsQueryBuilder termsQuery,String name,Collection<?> values){\r\n    return new TermsQueryBuilder(name, values);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.rollover.Condition.includedInVersion",
	"Comment": "checks if this condition is available in a specific version.this makes sure bwc when introducing a new condition which is not recognized by older versions.",
	"Method": "boolean includedInVersion(Version version){\r\n    return true;\r\n}"
}, {
	"Path": "org.json.JSONStringerTest.testNumericRepresentations",
	"Comment": "test what happens when extreme values are emitted. such values are likelyto be rounded during parsing.",
	"Method": "void testNumericRepresentations(){\r\n    if (System.getProperty(\"os.arch\").equals(\"armv7\")) {\r\n        return;\r\n    }\r\n    JSONStringer stringer = new JSONStringer();\r\n    stringer.array();\r\n    stringer.value(Long.MAX_VALUE);\r\n    stringer.value(Double.MIN_VALUE);\r\n    stringer.endArray();\r\n    assertEquals(\"[9223372036854775807,4.9E-324]\", stringer.toString());\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.shiftreduce.PerceptronModel.condenseFeatures",
	"Comment": "iterate over the feature weight map.for each feature, remove all transitions with score of 0.any feature with no transitions left is then removed",
	"Method": "void condenseFeatures(){\r\n    Iterator<String> featureIt = featureWeights.keySet().iterator();\r\n    while (featureIt.hasNext()) {\r\n        String feature = featureIt.next();\r\n        Weight weights = featureWeights.get(feature);\r\n        weights.condense();\r\n        if (weights.size() == 0) {\r\n            featureIt.remove();\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.LongObjectPagedHashMap.remove",
	"Comment": "remove the entry which has this key in the hash table and return theassociated value or null if there was no entry associated with this key.",
	"Method": "T remove(long key){\r\n    for (long i = slot(hash(key), mask); ; i = nextSlot(i, mask)) {\r\n        final T previous = values.set(i, null);\r\n        if (previous == null) {\r\n            return null;\r\n        } else if (keys.get(i) == key) {\r\n            --size;\r\n            for (long j = nextSlot(i, mask); used(j); j = nextSlot(j, mask)) {\r\n                removeAndAdd(j);\r\n            }\r\n            return previous;\r\n        } else {\r\n            values.set(i, previous);\r\n        }\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.neural.NeuralUtils.softmax",
	"Comment": "applies softmax to all of the elements of the matrix.the returnmatrix will have all of its elements sum to 1.if your matrix isnot already a vector, be sure this is what you actually want.",
	"Method": "SimpleMatrix softmax(SimpleMatrix input){\r\n    SimpleMatrix output = new SimpleMatrix(input);\r\n    for (int i = 0; i < output.numRows(); ++i) {\r\n        for (int j = 0; j < output.numCols(); ++j) {\r\n            output.set(i, j, Math.exp(output.get(i, j)));\r\n        }\r\n    }\r\n    double sum = output.elementSum();\r\n    return output.scale(1.0 / sum);\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.TransportShardBulkAction.replicaItemExecutionMode",
	"Comment": "determines whether a bulk item request should be executed on the replica.",
	"Method": "ReplicaItemExecutionMode replicaItemExecutionMode(BulkItemRequest request,int index){\r\n    final BulkItemResponse primaryResponse = request.getPrimaryResponse();\r\n    assert primaryResponse != null : \"expected primary response to be set for item [\" + index + \"] request [\" + request.request() + \"]\";\r\n    if (primaryResponse.isFailed()) {\r\n        return primaryResponse.getFailure().getSeqNo() != SequenceNumbers.UNASSIGNED_SEQ_NO ? ReplicaItemExecutionMode.FAILURE : ReplicaItemExecutionMode.NOOP;\r\n    } else {\r\n        return primaryResponse.getResponse().getResult() != DocWriteResponse.Result.NOOP ? ReplicaItemExecutionMode.NORMAL : ReplicaItemExecutionMode.NOOP;\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.routing.RoutingNode.numberOfOwningShards",
	"Comment": "the number of shards on this node that will not be eventually relocated.",
	"Method": "int numberOfOwningShards(){\r\n    int count = 0;\r\n    for (ShardRouting shardEntry : this) {\r\n        if (shardEntry.state() != ShardRoutingState.RELOCATING) {\r\n            count++;\r\n        }\r\n    }\r\n    return count;\r\n}"
}, {
	"Path": "org.elasticsearch.action.explain.ExplainRequestBuilder.setStoredFields",
	"Comment": "explicitly specify the stored fields that will be returned for the explained document. by default, nothing is returned.",
	"Method": "ExplainRequestBuilder setStoredFields(String fields){\r\n    request.storedFields(fields);\r\n    return this;\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.util.RelationTriple.toReverbString",
	"Comment": "print a description of this triple, formatted like the reverb outputs.",
	"Method": "String toReverbString(String docid,CoreMap sentence){\r\n    int sentIndex = -1;\r\n    int subjIndex = -1;\r\n    int relationIndex = -1;\r\n    int objIndex = -1;\r\n    int subjIndexEnd = -1;\r\n    int relationIndexEnd = -1;\r\n    int objIndexEnd = -1;\r\n    if (!relation.isEmpty()) {\r\n        sentIndex = relation.get(0).sentIndex();\r\n        relationIndex = relation.get(0).index() - 1;\r\n        relationIndexEnd = relation.get(relation.size() - 1).index();\r\n    }\r\n    if (!subject.isEmpty()) {\r\n        if (sentIndex < 0) {\r\n            sentIndex = subject.get(0).sentIndex();\r\n        }\r\n        subjIndex = subject.get(0).index() - 1;\r\n        subjIndexEnd = subject.get(subject.size() - 1).index();\r\n    }\r\n    if (!object.isEmpty()) {\r\n        if (sentIndex < 0) {\r\n            sentIndex = subject.get(0).sentIndex();\r\n        }\r\n        objIndex = object.get(0).index() - 1;\r\n        objIndexEnd = object.get(object.size() - 1).index();\r\n    }\r\n    return (docid == null ? \"no_doc_id\" : docid) + '\\t' + sentIndex + '\\t' + subjectGloss().replace('\\t', ' ') + '\\t' + relationGloss().replace('\\t', ' ') + '\\t' + objectGloss().replace('\\t', ' ') + '\\t' + subjIndex + '\\t' + subjIndexEnd + '\\t' + relationIndex + '\\t' + relationIndexEnd + '\\t' + objIndex + '\\t' + objIndexEnd + '\\t' + confidenceGloss() + '\\t' + StringUtils.join(sentence.get(CoreAnnotations.TokensAnnotation.class).stream().map(x -> x.word().replace('\\t', ' ').replace(\" \", \"\")), \" \") + '\\t' + StringUtils.join(sentence.get(CoreAnnotations.TokensAnnotation.class).stream().map(CoreLabel::tag), \" \") + '\\t' + subjectLemmaGloss().replace('\\t', ' ') + '\\t' + relationLemmaGloss().replace('\\t', ' ') + '\\t' + objectLemmaGloss().replace('\\t', ' ');\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.LineIterator.getFactory",
	"Comment": "returns a factory that vends lineiterators that read the contents of thegiven reader, splitting on newlines.",
	"Method": "IteratorFromReaderFactory<X> getFactory(IteratorFromReaderFactory<X> getFactory,Function<String, X> op){\r\n    return new LineIteratorFactory(op);\r\n}"
}, {
	"Path": "org.elasticsearch.common.inject.spi.TypeListenerBinding.getTypeMatcher",
	"Comment": "returns the type matcher which chooses which types the listener should be notified of.",
	"Method": "Matcher<? super TypeLiteral<?>> getTypeMatcher(){\r\n    return typeMatcher;\r\n}"
}, {
	"Path": "org.json.JSONStringer.beforeValue",
	"Comment": "inserts any necessary separators and whitespace before a literal value,inline array, or inline object. also adjusts the stack to expect either aclosing bracket or another element.",
	"Method": "void beforeValue(){\r\n    if (stack.isEmpty()) {\r\n        return;\r\n    }\r\n    Scope context = peek();\r\n    if (context == Scope.EMPTY_ARRAY) {\r\n        replaceTop(Scope.NONEMPTY_ARRAY);\r\n        newline();\r\n    } else if (context == Scope.NONEMPTY_ARRAY) {\r\n        out.append(',');\r\n        newline();\r\n    } else if (context == Scope.DANGLING_KEY) {\r\n        out.append(indent == null ? \":\" : \": \");\r\n        replaceTop(Scope.NONEMPTY_OBJECT);\r\n    } else if (context != Scope.NULL) {\r\n        throw new JSONException(\"Nesting problem\");\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaDataUpdateSettingsService.maybeUpdateClusterBlock",
	"Comment": "updates the cluster block only iff the setting exists in the given settings",
	"Method": "void maybeUpdateClusterBlock(String[] actualIndices,ClusterBlocks.Builder blocks,ClusterBlock block,Setting<Boolean> setting,Settings openSettings){\r\n    if (setting.exists(openSettings)) {\r\n        final boolean updateBlock = setting.get(openSettings);\r\n        for (String index : actualIndices) {\r\n            if (updateBlock) {\r\n                blocks.addIndexBlock(index, block);\r\n            } else {\r\n                blocks.removeIndexBlock(index, block);\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.apache.dubbo.registry.dubbo.AbstractRegistryService.addListener",
	"Comment": "the listener of the consumer and the provider can be stored together, all based on the service name",
	"Method": "void addListener(String service,NotifyListener listener){\r\n    if (listener == null) {\r\n        return;\r\n    }\r\n    List<NotifyListener> listeners = notifyListeners.get(service);\r\n    if (listeners == null) {\r\n        notifyListeners.putIfAbsent(service, new CopyOnWriteArrayList<NotifyListener>());\r\n        listeners = notifyListeners.get(service);\r\n    }\r\n    if (listeners != null && !listeners.contains(listener)) {\r\n        listeners.add(listener);\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.validate.query.ValidateQueryRequestBuilder.setExplain",
	"Comment": "indicates if detailed information about the query should be returned.",
	"Method": "ValidateQueryRequestBuilder setExplain(boolean explain){\r\n    request.explain(explain);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.client.Requests.putMappingRequest",
	"Comment": "create a create mapping request against one or more indices.",
	"Method": "PutMappingRequest putMappingRequest(String indices){\r\n    return new PutMappingRequest(indices);\r\n}"
}, {
	"Path": "org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequestBuilder.addMapping",
	"Comment": "adds mapping that will be added when the index template gets created.",
	"Method": "PutIndexTemplateRequestBuilder addMapping(String type,String source,XContentType xContentType,PutIndexTemplateRequestBuilder addMapping,String type,Object source,PutIndexTemplateRequestBuilder addMapping,String type,XContentBuilder source,PutIndexTemplateRequestBuilder addMapping,String type,Map<String, Object> source){\r\n    request.mapping(type, source);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchTransportService.getPendingSearchRequests",
	"Comment": "return a map of nodeid to pending number of search requests.this is a snapshot of the current pending search and not a live map.",
	"Method": "Map<String, Long> getPendingSearchRequests(){\r\n    return new HashMap(clientConnections);\r\n}"
}, {
	"Path": "android.util.SparseLongArray.put",
	"Comment": "adds a mapping from the specified key to the specified value,replacing the previous mapping from the specified key if therewas one.",
	"Method": "void put(int key,long value){\r\n    int i = ContainerHelpers.binarySearch(mKeys, mSize, key);\r\n    if (i >= 0) {\r\n        mValues[i] = value;\r\n    } else {\r\n        i = ~i;\r\n        if (mSize >= mKeys.length) {\r\n            growKeyAndValueArrays(mSize + 1);\r\n        }\r\n        if (mSize - i != 0) {\r\n            System.arraycopy(mKeys, i, mKeys, i + 1, mSize - i);\r\n            System.arraycopy(mValues, i, mValues, i + 1, mSize - i);\r\n        }\r\n        mKeys[i] = key;\r\n        mValues[i] = value;\r\n        mSize++;\r\n    }\r\n}"
}, {
	"Path": "edu.stanford.nlp.ie.util.RelationTriple.relationHead",
	"Comment": "the head of the relation of this relation triple. this is usually the main verb.",
	"Method": "CoreLabel relationHead(CoreLabel relationHead){\r\n    return relation.stream().filter(x -> x.tag().startsWith(\"V\")).reduce((x, y) -> y).orElse(relation.get(relation.size() - 1));\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaData.findAllAliases",
	"Comment": "finds the specific index aliases that point to the requested concrete indices directlyor that match with the indices via wildcards.",
	"Method": "ImmutableOpenMap<String, List<AliasMetaData>> findAllAliases(String[] concreteIndices){\r\n    return findAliases(Strings.EMPTY_ARRAY, concreteIndices);\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.SnapshotDeletionsInProgress.getEntries",
	"Comment": "returns an unmodifiable list of snapshot deletion entries.",
	"Method": "List<Entry> getEntries(){\r\n    return entries;\r\n}"
}, {
	"Path": "com.google.devtools.cyclefinder.NameUtil.getSignature",
	"Comment": "generates a unique signature for this type that can be used as a key.",
	"Method": "String getSignature(TypeMirror type){\r\n    StringBuilder sb = new StringBuilder();\r\n    buildTypeSignature(type, sb);\r\n    return sb.toString();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.AckedClusterStateUpdateTask.onAllNodesAcked",
	"Comment": "called once all the nodes have acknowledged the cluster state update request. must bevery lightweight execution, since it gets executed on the cluster service thread.",
	"Method": "void onAllNodesAcked(Exception e){\r\n    listener.onResponse(newResponse(e == null));\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequestBuilder.setTermStatistics",
	"Comment": "sets whether to return the term statistics for each term in the shard or skip.",
	"Method": "TermVectorsRequestBuilder setTermStatistics(boolean termStatistics){\r\n    request.termStatistics(termStatistics);\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchRequest.getBatchedReduceSize",
	"Comment": "returns the number of shard results that should be reduced at once on the coordinating node. this value should be used as aprotection mechanism to reduce the memory overhead per search request if the potential number of shards in the request can be large.",
	"Method": "int getBatchedReduceSize(){\r\n    return batchedReduceSize;\r\n}"
}, {
	"Path": "edu.stanford.nlp.parser.lexparser.AbstractDependencyGrammar.intern",
	"Comment": "this is a custom interner that simultaneously creates and internsan intdependency.",
	"Method": "IntDependency intern(IntTaggedWord headTW,IntTaggedWord argTW,boolean leftHeaded,short dist){\r\n    Map<IntDependency, IntDependency> map = expandDependencyMap;\r\n    IntDependency internTempDependency = new IntDependency(itwInterner.intern(headTW), itwInterner.intern(argTW), leftHeaded, dist);\r\n    IntDependency returnDependency = internTempDependency;\r\n    if (map != null) {\r\n        returnDependency = map.get(internTempDependency);\r\n        if (returnDependency == null) {\r\n            map.put(internTempDependency, internTempDependency);\r\n            returnDependency = internTempDependency;\r\n        }\r\n    }\r\n    return returnDependency;\r\n}"
}, {
	"Path": "org.elasticsearch.index.reindex.BulkByScrollTask.getLeaderState",
	"Comment": "returns the object that tracks the state of sliced subtasks. throws illegalstateexception if this task is not set to bea leader task.",
	"Method": "LeaderBulkByScrollTaskState getLeaderState(){\r\n    if (!isLeader()) {\r\n        throw new IllegalStateException(\"This task is not set to be a leader for other slice subtasks\");\r\n    }\r\n    return leaderState;\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MetaData.getNumberOfShards",
	"Comment": "gets the number of primary shards from all indices, not includingreplicas.",
	"Method": "int getNumberOfShards(){\r\n    return this.numberOfShards;\r\n}"
}, {
	"Path": "org.elasticsearch.common.cache.CacheBuilder.setExpireAfterAccess",
	"Comment": "sets the amount of time before an entry in the cache expires after it was last accessed.",
	"Method": "CacheBuilder<K, V> setExpireAfterAccess(TimeValue expireAfterAccess){\r\n    Objects.requireNonNull(expireAfterAccess);\r\n    final long expireAfterAccessNanos = expireAfterAccess.getNanos();\r\n    if (expireAfterAccessNanos <= 0) {\r\n        throw new IllegalArgumentException(\"expireAfterAccess <= 0\");\r\n    }\r\n    this.expireAfterAccessNanos = expireAfterAccessNanos;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.search.SearchScrollRequest.fromXContent",
	"Comment": "parse a search scroll request from a request body provided through the rest layer.values that are already be set and are also found while parsing will be overridden.",
	"Method": "void fromXContent(XContentParser parser){\r\n    if (parser.nextToken() != XContentParser.Token.START_OBJECT) {\r\n        throw new IllegalArgumentException(\"Malformed content, must start with an object\");\r\n    } else {\r\n        XContentParser.Token token;\r\n        String currentFieldName = null;\r\n        while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\r\n            if (token == XContentParser.Token.FIELD_NAME) {\r\n                currentFieldName = parser.currentName();\r\n            } else if (\"scroll_id\".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) {\r\n                scrollId(parser.text());\r\n            } else if (\"scroll\".equals(currentFieldName) && token == XContentParser.Token.VALUE_STRING) {\r\n                scroll(new Scroll(TimeValue.parseTimeValue(parser.text(), null, \"scroll\")));\r\n            } else {\r\n                throw new IllegalArgumentException(\"Unknown parameter [\" + currentFieldName + \"] in request body or parameter is of the wrong type[\" + token + \"] \");\r\n            }\r\n        }\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.index.mapper.MapperService.simpleMatchToFullName",
	"Comment": "returns all the fields that match the given pattern. if the pattern is prefixed with a typethen the fields will be returned with a type prefix.",
	"Method": "Collection<String> simpleMatchToFullName(String pattern){\r\n    if (Regex.isSimpleMatchPattern(pattern) == false) {\r\n        return Collections.singletonList(pattern);\r\n    }\r\n    return fieldTypes.simpleMatchToFullName(pattern);\r\n}"
}, {
	"Path": "org.elasticsearch.repositories.RepositoryData.getAllSnapshotIds",
	"Comment": "returns an immutable collection of all the snapshot ids in the repository, both active andincompatible snapshots.",
	"Method": "Collection<SnapshotId> getAllSnapshotIds(){\r\n    List<SnapshotId> allSnapshotIds = new ArrayList(snapshotIds.size() + incompatibleSnapshotIds.size());\r\n    allSnapshotIds.addAll(snapshotIds.values());\r\n    allSnapshotIds.addAll(incompatibleSnapshotIds);\r\n    return Collections.unmodifiableList(allSnapshotIds);\r\n}"
}, {
	"Path": "org.elasticsearch.persistent.AllocatedPersistentTask.waitForPersistentTask",
	"Comment": "waits for a given persistent task to comply with a given predicate, then call back the listener accordingly.",
	"Method": "void waitForPersistentTask(Predicate<PersistentTasksCustomMetaData.PersistentTask<?>> predicate,TimeValue timeout,PersistentTasksService.WaitForPersistentTaskListener<?> listener){\r\n    persistentTasksService.waitForPersistentTaskCondition(persistentTaskId, predicate, timeout, listener);\r\n}"
}, {
	"Path": "org.elasticsearch.index.query.QueryBuilders.scriptScoreQuery",
	"Comment": "a query that allows to define a custom scoring function through script.",
	"Method": "ScriptScoreQueryBuilder scriptScoreQuery(QueryBuilder queryBuilder,ScriptScoreFunctionBuilder function){\r\n    return new ScriptScoreQueryBuilder(queryBuilder, function);\r\n}"
}, {
	"Path": "org.elasticsearch.common.lucene.search.XMoreLikeThis.setMaxDocFreq",
	"Comment": "set the maximum frequency in which words may still appear. words that appearin more than this many docs will be ignored.",
	"Method": "void setMaxDocFreq(int maxFreq){\r\n    this.maxDocFreq = maxFreq;\r\n}"
}, {
	"Path": "edu.stanford.nlp.patterns.GetPatternsFromDataMultiClass.calculateSufficientStats",
	"Comment": "for each pattern, it calculates positive, negative, and unlabeled words",
	"Method": "void calculateSufficientStats(Map<String, DataInstance> sents,PatternsForEachToken patternsForEachToken,String label,TwoDimensionalCounter<E, CandidatePhrase> patternsandWords4Label,TwoDimensionalCounter<E, CandidatePhrase> negPatternsandWords4Label,TwoDimensionalCounter<E, CandidatePhrase> unLabeledPatternsandWords4Label,Set<String> allCandidatePhrases){\r\n    Redwood.log(Redwood.DBG, \"calculating sufficient stats\");\r\n    patternsForEachToken.setupSearch();\r\n    Class answerClass4Label = constVars.getAnswerClass().get(label);\r\n    int sampleSize = constVars.sampleSentencesForSufficientStats == 1.0 ? sents.size() : (int) Math.round(constVars.sampleSentencesForSufficientStats * sents.size());\r\n    List<List<String>> sampledSentIds = splitIntoNumThreadsWithSampling(CollectionUtils.toList(sents.keySet()), sampleSize, constVars.numThreads);\r\n    Redwood.log(Redwood.DBG, \"sampled \" + sampleSize + \" sentences (\" + constVars.sampleSentencesForSufficientStats * 100 + \"%)\");\r\n    ExecutorService executor = Executors.newFixedThreadPool(constVars.numThreads);\r\n    List<Future<Triple<List<Pair<E, CandidatePhrase>>, List<Pair<E, CandidatePhrase>>, List<Pair<E, CandidatePhrase>>>>> list = new ArrayList();\r\n    for (List<String> sampledSents : sampledSentIds) {\r\n        Callable<Triple<List<Pair<E, CandidatePhrase>>, List<Pair<E, CandidatePhrase>>, List<Pair<E, CandidatePhrase>>>> task = new CalculateSufficientStatsThreads(patternsForEachToken, sampledSents, sents, label, answerClass4Label);\r\n        Future<Triple<List<Pair<E, CandidatePhrase>>, List<Pair<E, CandidatePhrase>>, List<Pair<E, CandidatePhrase>>>> submit = executor.submit(task);\r\n        list.add(submit);\r\n    }\r\n    for (Future<Triple<List<Pair<E, CandidatePhrase>>, List<Pair<E, CandidatePhrase>>, List<Pair<E, CandidatePhrase>>>> future : list) {\r\n        try {\r\n            Triple<List<Pair<E, CandidatePhrase>>, List<Pair<E, CandidatePhrase>>, List<Pair<E, CandidatePhrase>>> stats = future.get();\r\n            addStats(patternsandWords4Label, stats.first());\r\n            addStats(negPatternsandWords4Label, stats.second());\r\n            addStats(unLabeledPatternsandWords4Label, stats.third());\r\n        } catch (Exception e) {\r\n            executor.shutdownNow();\r\n            throw new RuntimeException(e);\r\n        }\r\n    }\r\n    executor.shutdown();\r\n}"
}, {
	"Path": "org.elasticsearch.index.store.Store.createEmpty",
	"Comment": "creates an empty lucene index and a corresponding empty translog. any existing data will be deleted.",
	"Method": "void createEmpty(){\r\n    metadataLock.writeLock().lock();\r\n    try (IndexWriter writer = newIndexWriter(IndexWriterConfig.OpenMode.CREATE, directory, null)) {\r\n        final Map<String, String> map = new HashMap();\r\n        map.put(Engine.HISTORY_UUID_KEY, UUIDs.randomBase64UUID());\r\n        map.put(SequenceNumbers.LOCAL_CHECKPOINT_KEY, Long.toString(SequenceNumbers.NO_OPS_PERFORMED));\r\n        map.put(SequenceNumbers.MAX_SEQ_NO, Long.toString(SequenceNumbers.NO_OPS_PERFORMED));\r\n        map.put(Engine.MAX_UNSAFE_AUTO_ID_TIMESTAMP_COMMIT_ID, \"-1\");\r\n        updateCommitData(writer, map);\r\n    } finally {\r\n        metadataLock.writeLock().unlock();\r\n    }\r\n}"
}, {
	"Path": "org.elasticsearch.action.bulk.BulkPrimaryExecutionContext.getPreviousPrimaryResponse",
	"Comment": "returns any primary response that was set by a previous primary",
	"Method": "BulkItemResponse getPreviousPrimaryResponse(){\r\n    return getCurrentItem().getPrimaryResponse();\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.ClusterState.stateUUID",
	"Comment": "this stateuuid is automatically generated for for each version of cluster state. it is used to make sure thatwe are applying diffs to the right previous state.",
	"Method": "String stateUUID(Builder stateUUID,String uuid){\r\n    return this.stateUUID;\r\n}"
}, {
	"Path": "edu.stanford.nlp.objectbank.ObjectBank.retainAll",
	"Comment": "unsupported operation.if you wish to retain only certain datasources, do so in the underlying readeriteratorfactory.",
	"Method": "boolean retainAll(Collection<?> c){\r\n    throw new UnsupportedOperationException();\r\n}"
}, {
	"Path": "org.elasticsearch.action.termvectors.TermVectorsRequest.doc",
	"Comment": "sets an artificial document from which term vectors are requested for.",
	"Method": "BytesReference doc(TermVectorsRequest doc,XContentBuilder documentBuilder,TermVectorsRequest doc,BytesReference doc,boolean generateRandomId,TermVectorsRequest doc,BytesReference doc,boolean generateRandomId,XContentType xContentType){\r\n    if (generateRandomId) {\r\n        this.id(String.valueOf(randomInt.getAndAdd(1)));\r\n    }\r\n    this.doc = doc;\r\n    this.xContentType = xContentType;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.index.IndexRequest.id",
	"Comment": "sets the id of the indexed document. if not set, will be automatically generated.",
	"Method": "String id(IndexRequest id,String id){\r\n    this.id = id;\r\n    return this;\r\n}"
}, {
	"Path": "org.elasticsearch.action.support.replication.TransportReplicationAction.globalBlockLevel",
	"Comment": "cluster level block to check before request execution. returning null means that no blocks need to be checked.",
	"Method": "ClusterBlockLevel globalBlockLevel(){\r\n    return null;\r\n}"
}, {
	"Path": "org.elasticsearch.common.util.CollectionUtils.rotate",
	"Comment": "return a rotated view of the given list with the given distance.",
	"Method": "List<T> rotate(List<T> list,int distance){\r\n    if (list.isEmpty()) {\r\n        return list;\r\n    }\r\n    int d = distance % list.size();\r\n    if (d < 0) {\r\n        d += list.size();\r\n    }\r\n    if (d == 0) {\r\n        return list;\r\n    }\r\n    return new RotatedList(list, d);\r\n}"
}, {
	"Path": "org.elasticsearch.common.logging.DeprecationLogger.formatWarning",
	"Comment": "format a warning string in the proper warning format by prepending a warn code, warn agent, wrapping the warning string in quotes,and appending the rfc 7231 date.",
	"Method": "String formatWarning(String s){\r\n    return String.format(Locale.ROOT, WARNING_FORMAT, escapeAndEncode(s), RFC_7231_DATE_TIME.format(ZonedDateTime.now(GMT)));\r\n}"
}, {
	"Path": "org.elasticsearch.cluster.metadata.MappingMetaData.getSourceAsMap",
	"Comment": "converts the serialized compressed form of the mappings into a parsed map.",
	"Method": "Map<String, Object> getSourceAsMap(){\r\n    return sourceAsMap();\r\n}"
}]